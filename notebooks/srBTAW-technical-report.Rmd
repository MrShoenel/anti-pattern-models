---
title: "Technical Report: Self-Regularizing Boundary Time Warping and Boundary Amplitude Warping"
author: "Sebastian HÃ¶nel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ../inst/REFERENCES.bib
urlcolor: blue
header-includes:
  - \usepackage{bm}
  - \usepackage{mathtools}
  - \usepackage{xurl}
output:
  md_document:
    toc: true
    toc_depth: 6
    df_print: kable
    variant: gfm
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  html_document:
    number_sections: true
    toc: true
    toc_depth: 6
    toc_float: true
    df_print: kable
  word_document: default
---

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\norm}[1]{\left\lvert#1\right\rvert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}

```{r setoptions, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(tidy = TRUE, tidy.opts = list(indent=2))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")

library(ggplot2)
library(ggpubr)
```

# Introduction\label{tr:srBTAW-technical-report}

In this notebook, we will go back to our initial Fire Drill (FD) problem, and design models to accommodate it. The goal is to bring together everything we learned from all the notebooks in between, and to come up with solutions specifically designed to address the matter.

Self-regularizing boundary time and amplitude warping, __`srBTAW`__, is a model that allows for rectifying signals. Similar to Dynamic time warping (DTW), one signal is aligned to another. However, srBTAW is not limited to the time-domain only, and can optionally correct the amplitude of a signal, too. The suggested models have built-in self-regularization, meaning that they will somewhat automatically correct extreme parameters without requiring additional regularization.

Among others, srBTAW has the following features:

* srBTAW is an optimization algorithm, and not based on dynamic programming. While DTW finds some best alignment for each point of discrete signals, srBTAW finds some best alignment for whole intervals. Warping for srBTAW means to linearly affine transform intervals.
* It is designed to foremost work with continuous signals, although it can be used for discrete signals just as well, too.
* Closed-, half- or fully-open matching: It is up to the user whether a signal must be matched from the start, to the end, both, or neither of them.
* Custom objectives: Unlike DTW, srBTAW allows for arbitrary user-defined objectives to compute, e.g., distances or similarities between (intervals of) signals. DTW almost always makes the stipulation of computing distances using the Euclidean distance. In this notebook, we demonstrate using losses from various domains, such as information- or signal-theory.
* srBTAW is multivariate: both the pattern (reference) and candidate (query) may consist of one or arbitrary many separate signals. Each of them may have their own support.
* For DTW the underlying assumption is that the query-signal is a time-distorted version of the reference signal, and that there are no portions of the query that have a higher amplitude. In other words, DTW assumes only degrees of freedom on the time axis and can actually not support alignment of signals with different amplitudes well. Furthermore, DTW performs poor for very dissimilar signals. This is not a problem for srBTAW because of its interval-alignments.

srBTAW was developed to solve some concrete problems in the project management domain, and this notebook uses some of the use cases to evolve the models. However, srBTAW can work with many various types of time series-related problems, such as classification, motif discovery etc.

All complementary data and results can be found at Zenodo [@honel_picha_2021]. This notebook was written in a way that it can be run without any additional efforts to reproduce the outputs (using the pre-computed results). This notebook has a canonical URL^[[Link]](https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/srBTAW-technical-report.Rmd)^ and can be read online as a rendered markdown^[[Link]](https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/srBTAW-technical-report.md)^ version (attention: the math is probably not rendered correctly there!). All code can be found in this repository, too.


# Problem description

Initially I thought the answer would be using dynamic time Warping, but it is not suitable for an analytical solution (actually, closed-form expression). What makes our problem a bit special are the following:

* __Reference__ and __Query__ signal have the same support: The FD is an AP that spans the entire project, start to finish. Matching must be attempted at the very beginning of the data, and is concluded at the very end. The query signal is scaled such that the extent of its support is the same as the reference's.
  * As we will see later, this constraint is optionally relaxed by allowing to the query-signal to cover a proper sub-support of the reference using open begin- and/or -end time warping.
* All data has to be used: We can apply local time warping to the query signal, but nothing can be left out. The warping may even result in a length of zero of some intervals, but they must not be negative.


## Optimization goals

We have __3__ optimization goals for the joint paper/article (each of them has a dedicated section in this notebook):

* Fitting of project data to a defined AP (here: the Fire Drill)
* Validate selection of sub-models and scores; also: calibration of scores
* Averaging the reference pattern over all known ground truths


# Optimization goal I: Fitting of project data to a defined AP

This is the primary problem, and we should start with a visualization:

```{r echo=FALSE}
# Save everything so it can be reused elsewhere:
dens_acp_data <- readRDS(file = "../data/dens_acp_data.rds")
# Also save the pattern:
fd_data_concat <- readRDS(file = "../data/fd_data_concat.rds")

bounds <- c(0.085, 0.625, 0.875)

ggarrange(
  plot_project_data(fd_data_concat, boundaries = bounds),
  plot_project_data(dens_acp_data),
  nrow = 2
)
```

The first plot is the reference pattern, the second is a random project. All data has already been scaled and normalized, for example, both supports are the same. We are going to use an entirely new approach to warp the project data from the 2nd plot the reference data from the first plot.

## Approach

In order to understand this problem, we need to make some statements:

* The reference pattern is subdivided into intervals by some boundaries (blue in the above plot). The data in each such interval is considered __constant__. That means, if we were to sample from that interval, these samples would always be identical. If we were to look at each interval as a function, then it will always show the same behavior. Each interval has its own __support__.
* The first interval starts at $0$ and the last interval ends at $1$ (we always expect that all data has been scaled and translated to $[0,1]$ prior to this, to keep things simple).
* Any and all variables are strictly __positive__, with a co-domain of $[0,1]$ as well. We require this for reasons of simplification, although negative values would be no problem (but require extra effort at this point).
* An interval's __length__ is never negative. In practice, one may even want to define a minimum length for all or for each individual interval.
* An interval's __offset__ is the sum of the lengths of its preceding intervals. All intervals' lengths sum up to $1$.

To find the sub-support of the query that best matches a variable (or many) in an interval, the query signal is translated and scaled. Another way to think about this is that the query signal is translated and only valid in the support that starts with the translate, and ends there plus the length of the current interval. I mention this because this is how we will begin.

In the above plot, there are __$4$__ intervals, and let's say we index them with the parameter $q$. If we were to describe the reference- and query-signals through the two functions $r,f$, and we were given some boundary $b_q$ that delimits the reference-interval, for $q=1$ we would get:

$$
\begin{aligned}
  r=&\;\mathbb{R}\mapsto\mathbb{R}^m\;\land\;m>0\text{,}
  \\[1ex]
  r_q=&\;r(\cdot)\;\text{, reference-signal for interval}\;q,
  \\[1ex]
  \text{supp}(r_q)=&\;[0,b_q)\;\text{, the support of}\;r_q.
\end{aligned}
$$

During optimization, the goal is to find the best length for each interval. Assuming a given vector that contains these four lengths, $\bm{\vartheta}^{(l)}$, we get:

$$
\begin{aligned}
  f=&\;\mathbb{R}\mapsto\mathbb{R}^n\;\land\;n>0\text{,}
  \\[1ex]
  f_q=&\;f(\cdot)\;\text{, query-signal for interval}\;q,
  \\[1ex]
  \text{supp}(f_q)=&\;\big\{\;x\in\mathbb{R}\;\rvert\;x\geq0\;\land\;x<\bm{\vartheta}^{(l)}_q\;\big\}\;\text{, the support of}\;f_q.
\end{aligned}
$$

The supports will almost always differ (especially for $q>1$). For the discrete case this may not be important, because the model may choose to always take the same amount of equidistantly-spaced samples from the reference- and query-intervals, regardless of the support. This is essentially the same as scaling and translating the query-support to be the same as the reference support. This is what would be required for the continuous case.

The result of the previous definition is an __unchanged__ query-signal; however, we specify its support that _corresponds_ to the support of the current reference interval. In general, to translate (and scale) an interval from one offset and length to another offset and length, the expression is:

$$
\begin{aligned}
  f(x)\dots&\;\text{function to be translated,}
  \\[1ex]
  f'(x)=&\;f\Bigg(\frac{(x - t_b) \times (s_e - s_b)}{t_e - t_b} + s_b\Bigg)\;\text{, where}
  \\[1ex]
  s_b,s_e,t_b,t_e\dots&\;\text{begin (b) and end (e) of the source- (s) and target-intervals/supports (t).}
\end{aligned}
$$

We will do some testing with the pattern and data we have, focusing on the __adaptive__ (__A__) variable.

However, notice how we have defined $r,f$ to map to dimensionalities of $1$ or greater, effectively making them vector-valued:

* A loss between both functions is concerning a specific $q$-th interval, and also usually concerning a specific index in each function, e.g., $\mathcal{L}_q=(r_q^{(13)}(x)-f_q^{(4)}(x))^2$. Each such loss may have its __individual weight__ assigned.
* This concept of being multivariate can be extended to having __multiple signals__ per _Warping Pattern/Candidate_, too, by simple concatenation of the output of two or more such vector-valued functions, e.g., $f(x)=f_a^{(1,2,3)}\frown f_b^{(7,11,13)}\frown\dots$, where $f_a,f_b$ are individual vector-valued/multivariate Warping Candidates.
* The dimensionalities of $r,f$ do not necessarily have to be the same: Imagine the case where we compute some loss of one variable of the Warping Pattern over the same variable of one or more Warping Candidates. Also, as in the previous example, not all of a signal's variables have to be used.


```{r echo=FALSE}
a_ref <- fd_data_concat[fd_data_concat$t == "A", ]
a_query <- dens_acp_data[dens_acp_data$t == "A", ]

r <- (function() {
  temp <- stats::approxfun(x = a_ref$x, y = a_ref$y)
  function(x) temp(v = x)
})()
f <- (function() {
  temp <- stats::approxfun(x = a_query$x, y = a_query$y)
  function(x) temp(v = x)
})()

ggarrange(
  plot_project_data(a_ref, boundaries = bounds),
  plot_project_data(a_query) + ylim(0,0.3),
  nrow = 2
)
```

We will do a test with $f_q$ and $f'_q$, and set $q$ to $1$ and then to $3$. In the first case, no re-definition of $f$ is required. We only need to define where to query-intervals should be (thus their length).

```{r warning=FALSE}
# Use these lengths for q=3:
vt_q3 <- c(0.15, 0.45, 0.1, 0.3)

a_ref_q3 <- a_ref[a_ref >= bounds[2] & a_ref < bounds[3], ]
a_query_q3 <- a_query[a_query >= (vt_q3[1] + vt_q3[2]) & a_query < (1 - vt_q3[4]), ]

r_q3 <- stats::approxfun(x = a_ref_q3$x, y = a_ref_q3$y)
f_q3 <- stats::approxfun(x = a_query_q3$x, y = a_query_q3$y)

ggplot() +
  stat_function(fun = r_q3, aes(color = "r_q3")) +
  stat_function(fun = f_q3, aes(color = "f_q3")) +
  xlim(.6, .9)
```

And now we re-define $f$ such that it has the same support as `r_q3`:

```{r warning=FALSE}
f_prime_proto <- function(x, source_b, source_e, target_b, target_e) {
  f((((x - target_b) * (source_e - source_b)) / (target_e - target_b)) + source_b)
}

f_q3_prime <- function(x) {
  # These two are where the signal currently is (translation source range)
  phi_q <- vt_q3[1] + vt_q3[2]
  vartheta_q <- vt_q3[3]
  # These two is where we want the signal to be (translation target range)
  # theta_q <- bounds[3] - bounds[2]
  
  source_b <- phi_q
  source_e <- phi_q + vartheta_q
  target_b <- bounds[2]
  target_e <- bounds[3]
  
  f_prime_proto(x, target_b = target_b, target_e = target_e,
                source_b = source_b, source_e = source_e)
}

curve(f_q3_prime, 0, 1)

ggplot() +
  stat_function(fun = r_q3, aes(color = "r_q3")) +
  stat_function(fun = f_q3_prime, aes(color = "f_q3_prime")) +
  geom_vline(xintercept = bounds[2]) +
  geom_vline(xintercept = bounds[3]) +
  xlim(bounds[2:3])
```

Another test, squeezing $f$ into the interval $[0.5,1]$:

```{r}
f_q3_test <- function(x) {
  f_prime_proto(x, source_b = 0, source_e = 1, target_b = .5, target_e = 1)
}

f_q3_test2 <- function(x) {
  f_prime_proto(x, source_b = .55, source_e = .7, target_b = 0, target_e = 1)
}

curve(f, 0, 1)
curve(f_q3_test, 0, 1)
curve(f_q3_test2, 0, 1)
```


## Sub-Model formulation

In practice, we want to re-define $f$, such that we scale and translate it in a way that its support and the reference support perfectly overlap. This will be done using above expression to derive $f$ into $f'$. Also, we want our model to satisfy additional constraints, without having to use regularization or (in-)equality constraints. These constraints are:

* All intervals must have a length greater than $0$ (or any arbitrary positive number).
* The lengths of all intervals must sum up to $1$.

During unconstrained optimization, there is no way to enforce these constraints, but we can build these into our model! To satisfy the first constraint, we will replace using any $l_q$ directly by $\max{(\bm{\lambda}_q,l_q)}$ (where $\bm{\lambda}_q$ is the lower bound for interval $q$ and $>0$). This could however lead to the sum of all such lengths being greater than the extent of the support. The solution to the second constraint will satisfy this, by normalizing all lengths using the sum of all lengths. This way, lengths are converted to ratios. Later, using the _target-extent_, these ratios are scaled back to actual lengths.

We define the new model with the built-in constraints ($m_q^c$):

$$
\begin{aligned}
  \bm{\theta}^{(b)}\;\dots&\;\text{ordered vector of boundaries, where the first and last boundary together}
  \\[0ex]
  &\;\text{represent the support of the signal they subdivide,}
  \\[1ex]
  b\dots&\;\text{the begin (absolute offset) of the first source-interval (usually}\;0\text{),}
  \\[1ex]
  e\dots&\;\text{the end (absolute offset) of the last source-interval (usually}\;1\text{), also}\;e>b\text{,}
  \\[1ex]
  \gamma_b,\gamma_e,\gamma_d\dots&\;\text{global min/max for}\;b,e\;\text{and min-distance between them,}
  \\[1ex]
  &\;\text{where}\;\gamma_d\geq 0\;\land\;\gamma_b+\gamma_d\leq\gamma_e\;\text{,}
  \\[1ex]
  \beta_l=&\;\min{\Big(\gamma_e-\gamma_d,\max{\big(\gamma_b, \min{(b,e)}\big)}\Big)}\;\text{, lower boundary as of}\;b,e\text{,}
  \\[1ex]
  \beta_u=&\;\max{\Big(\gamma_b+\gamma_d,\min{\big(\gamma_e, \max{(b,e)}\big)}\Big)}\;\text{, upper boundary as of}\;b,e\text{,}
  \\[1ex]
  \bm{\lambda}\dots&\;\text{vector with minimum lengths for each interval,}\;\forall\,\bm{\lambda}_q\geq 0\text{,}
  \\[1em]
  l_q\dots&\;\text{the length of the }q\text{-th source-interval,}
  \\[1ex]
  l'_q=&\;\max{\big(\bm{\lambda}_q,\norm{l_q}\big)}\;\text{, where}\;\norm{l_q}\equiv\max{\big(-l_q,l_q\big)}\equiv\mathcal{R}(2\times l_q)-l_q\text{,}
  \\[0ex]
  &\;\text{(the corrected (positive) interval-length that is strictly}\;\geq\bm{\lambda}_q\text{),}
  \\[1ex]
  \psi=&\;\sum_{i=1}^{\max{(Q)}}\,l'_i\equiv \bm{l'}^\top\hat{\bm{u}}\;\text{, the sum used to normalize each and every}\;q\text{-th}\;l\text{,}
  \\[0ex]
  &\;\text{(}\hat{\bm{u}}\;\text{is the unit-vector),}
  \\[1ex]
  l_q^{(c)}=&\;\frac{l'_q}{\psi}\times(\beta_u-\beta_l)\;\text{, corrected, normalized and re-scaled version of}\;l_q\text{,}
  \\[1ex]
  \phi_q\equiv&\;\begin{cases}
    0,&\text{if}\;q=1,
    \\
    \sum_{i=1}^{q-1}\,l_i^{(c)},&\text{otherwise}
  \end{cases}\;,
  \\[1ex]
  &\;\text{(the sum of the lengths of all corrected preceding intervals).}
\end{aligned}
$$

With all these, we can now formulate the sub-model using the previously used parameters $s_b,s_e,t_b,t_e$ for translation and scaling of one interval (the source) to another interval (the target):

$$
\begin{aligned}
  s_b^{(q)}=&\;\beta_l+\phi_q\;\text{, and}
  \\[1ex]
  s_e^{(q)}=&\;s_b^{(q)}+l_q^{(c)}\;\text{, begin- and end-offset of the }q\text{-th source-interval,}
  \\[1ex]
  t_b^{(q)}=&\;\bm{\theta}^{(b)}_q\;\text{, and}
  \\[1ex]
  t_e^{(q)}=&\;\bm{\theta}^{(b)}_{q+1}\;\text{, begin- and end-offset of the }q\text{-th target-interval,}
  \\[1ex]
  \delta_q^{(t)}=&\;t_e^{(q)}-t_b^{(q)}\;\text{, the extent of the }q\text{-th target-interval,}
  \\[1ex]
  m_q^c\Big(f,x,t_b^{q},l_q^{(c)},\delta_q^{(t)},s_b^{(q)}\Big)=&\;f\Bigg(\frac{\Big(x-t_b^{(q)}\Big)\times l_q^{(c)}}{\delta_q^{(t)}}+s_b^{(q)}\Bigg)\;\text{, with all non-constants expanded:}
  \\[1ex]
  =&\;f\Bigg(\overbrace{\Bigg[\overbrace{\min{\Big(\gamma_e-\gamma_d,\max{\big(\gamma_b, \min{(b,e)}\big)}\Big)}}^{\beta_l}+\overbrace{\begin{cases}
    0,&\text{if}\;q=1,
    \\
    \sum_{i=1}^{q-1}\,l_i^{(c)},&\text{otherwise}
  \end{cases}}^{\phi_q}\Bigg]}^{s_b^{(q)}}
  \\[0ex]
  &\;\;\;\;\;\;+\Big(x-t_b^{(q)}\Big)\times{\delta_q^{(t)}}^{-1}\times \overbrace{\max{\Big(\bm{\lambda}_q,\mathcal{R}(2\times l_q)-l_q\Big)}\times\Bigg[\sum_{i=1}^{\max{(Q)}}\,l'_i\Bigg]^{-1}}^{\text{recall}\;l_q^{(c)}=\frac{l'_q}{\psi}\times(\beta_u-\beta_l)\text{, here we got}\;l'_q\times\psi^{-1}}
  \\[0ex]
  &\;\;\;\;\;\;\times\bigg(\overbrace{\max{\Big(\gamma_b+\gamma_d,\min{\big(\gamma_e, \max{(b,e)}\big)}\Big)}}^{\beta_u}-\overbrace{\min{\Big(\gamma_e-\gamma_d,\max{\big(\gamma_b, \min{(b,e)}\big)}\Big)}}^{\beta_l}\bigg)\Bigg)\;\text{.}
\end{aligned}
$$


The model $m_q^c$ now satisfies these properties:

* Each interval has a length greater than or equal to $\bm{\lambda}_q$ (which must be strictly positive; a length of zero however is allowed).
* The first interval begins at $\beta_l$, the last interval ends at $\beta_u$ (these parameters are the absolute begin/end of the source-intervals, and hence apply to where onto the query will be mapped to). These parameters can either be constant or learned during optimization, effectively allowing open/closed begin- and/or -end time warping.
* Each interval begins exactly after its predecessor, such that there are no overlaps. Intervals are seamlessly strung together.
* The sum of the lengths of all intervals is normalized and then re-scaled using $\phi$, considering the constraints of $\gamma_d$ and the minimum length of each $q$-th interval (using its corresponding $\bm{\lambda}_q$).

### Reference vs. Query

Thus far, we have made the difference between these two notions. The implicit assumption thus far was, that the reference is constant, i.e., once given as a pattern, it is never altered, and the query is warped to it. I came across the important difference while thinking about closed- vs. open -begin and/or -end time warping: In the closed case, the __whole__ query is warped onto the __whole__ reference, i.e., both signals have to be used to their full extent. If, however, the begin or end (or both) are open, only a portion of the query is used to match still the entire reference. So, not only is the reference constant, it also has to be matched wholly, i.e., all of its reference-intervals need to be used. This raises two important points.

First, regarding the differences between both signals during warping, we should propose more appropriate names. I suggest __"Warping Pattern"__ for the constant signal that has to be used in its entirety, and __"Warping Candidate"__ for the signal that is translated and scaled to be closest to the Warping Pattern.

Second, in some cases it may be useful __not__ to use the entire Warping Pattern. In this case, both signals can simply be swapped with each other, and a non-closed model is fit. After fitting, the warping path is inverted. Also, this is how DTW works, one signal has to be mapped in full. This flipping case is useful to find a sub-support of the Warping Pattern within the Warping Candidate.

### Min/Max as ramp function

Note that we define $\min,\max$ in terms of the ramp-function and its derivative, the Heaviside step function.

$$
\begin{aligned}
  \max{(x,y)}\equiv&\;\mathcal{R}(y-x)+x\;\text{, with gradient}
  \\[1ex]
  \nabla\,\mathcal{R}(y-x)+x=\Bigg[\frac{\partial\,\mathcal{R}(\cdot)}{\partial\,x},\frac{\partial\,\mathcal{R}(\cdot)}{\partial\,y}\Bigg]=&\;\Bigg[\mathcal{H}(x-y)\;,\;\mathcal{H}(y-x)\Bigg]\;\text{, also, we define}\;\min\;\text{as:}
  \\[1ex]
  \min{(x,y)}=&\;\,y-\mathcal{R}(y-x)\;\text{, with gradient}
  \\[1ex]
  \nabla\,y-\mathcal{R}(y-x)=\Bigg[\frac{\partial\,\mathcal{R}(\cdot)}{\partial\,x},\frac{\partial\,\mathcal{R}(\cdot)}{\partial\,y}\Bigg]=&\;\Bigg[\mathcal{H}(y-x)\;,\;\mathcal{H}(x-y)\Bigg]\;\text{.}
\end{aligned}
$$


## Overall model formulation

Now that we have sub-models that are able to transform a section of the query-signal for a given a section of the reference-signal, the logical next step is to define the entire (or overall) model in terms of all intervals. The following is in large parts identical to how we defined it in the notebook _"Boundary Time Warping (final, update)"_.

It is important to once more recall the mechanics of this suggested model:

* The reference signal is segmented into one or more intervals, each with a length $>0$. This segmentation is done using a given vector of _reference-boundaries_.
  * These intervals are constant, and never changed afterwards. For each variable in each reference-interval, we have a constant segment of the corresponding reference-signal (continuous case), or a constant vector of data.
* Each $q$-th reference-interval is associated with a $q$-th sub-model. Each such sub-model has access to the whole query-signal and its gradient, and the goal is to translate and scale the query-signal to __best-fit__ the current reference interval (this requires a loss).
  * The sub-models are designed in a way that the intervals they cover cannot overlap and connect seamlessly, among other criteria (see above).

The input to the overall model is, among the signals, a set of reference- and query-boundaries, which are transformed into vectors of interval lengths. Albeit defined above with the sub-models, the parameters $b,e,\gamma_b,\gamma_e,\gamma_d$ are parameters of the overall model. In practice, it may be more or less convenient to work with boundaries, and internally the overall model uses lengths/ratios, and an absolute offset for the first interval. Both of these concepts are convertible to each other. In the following, we start with a definition that uses boundaries, and then transform them to lengths and an offset.

$$
\begin{aligned}
  \mathsf{T}^{(l)}\Big(\bm{\tau}^{(b)}\Big)=&\;\Big\{\;l_1=\bm{\tau}^{(b)}_2-\bm{\tau}^{(b)}_1,\;\dots,\;l_{\norm{\bm{\tau}}-1}=\bm{\tau}^{(b)}_{\norm{\bm{\tau}}}-\bm{\tau}^{(b)}_{\norm{\bm{\tau}}-1}\;\Big\}\;\text{,}
  \\[0ex]
  &\;\text{(boundaries-to-lengths transform operator),}
  \\[1ex]
  \mathcal{X}^{(\text{WP})}=&\;\Big[\min{\bm{\theta}^{(b)}}\,,\,\max{\bm{\theta}^{(b)}}\Big]\;\text{, the support of the Warping Pattern's signal,}
  \\[1ex]
  \bm{\vartheta}^{(l)}=\mathsf{T}^{(l)}\Big(\bm{\vartheta}^{(b)}\Big)\;\dots&\;\text{the query-intervals as lengths and boundaries,}
  \\[1ex]
  \mathcal{X}^{(\text{WC})}=&\;\Big[\beta_l\,,\,\beta_u\Big]\;\text{, the support of the Warping Candidate's signal,}
  \\[1ex]
  q\in Q;\;Q=&\;\Big\{1,\;\dots\;,\norm{\bm{\theta}^{(b)}}-1\Big\}\;\text{, where}\;Q\;\text{is ordered low to high, i.e.,}\;q_i\prec q_{i+1}\text{,}
  \\[0ex]
  &\;\text{(boundaries converted to interval-indices),}
  \\[1ex]
  \mathcal{I}^{(\text{WP})},\mathcal{I}^{(\text{WC})}\;\dots&\;\text{the set of Warping Pattern- and Candidate-intervals with length}\;[\max{Q}]\text{,}
  \\[0ex]
  &\;\text{with each Warping Candidate-interval delimited by}
  \\[0ex]
  \mathbf{x}_q^{(\text{WC})}\subset\mathcal{X}^{(\text{WC})}=&\;\Big[\bm{\vartheta}^{(b)}_q\,,\,\bm{\vartheta}^{(b)}_{q+1}\Big)\;\text{, for the first}\;Q-1\;\text{intervals,}
  \\[0ex]
  =&\;\Big[\bm{\vartheta}^{(b)}_q\,,\,\bm{\vartheta}^{(b)}_{q+1}\Big]\;\text{, for the last interval; }
  \\[0ex]
  &\;\text{proper sub-supports for model}\;m_q\;\text{and its interval}\;\mathbf{x}_q^{(\text{WC})}\text{, such that}
  \\[0ex]
  &\;\mathbf{x}_q^{(\text{WC})}\prec\mathbf{x}_{q+1}^{(\text{WC})}\;\text{, i.e., sub-supports are ordered,}
  \\[1ex]
  \mathbf{y}_q^{(\text{WP})}=&\;r\Big(I_q^{(\text{WP})}\Big)\;\text{, the reference data for the}\;q\text{-th interval,}
  \\[1ex]
  \mathbf{y}=&\;\Big\{\;\mathbf{y}_1^{(\text{WP})}\,\frown\,\dots\,\frown\,\mathbf{y}_q^{(\text{WP})}\;\Big\},\;\forall\,q\in Q\;\text{.}
\end{aligned}
$$


Likewise, we can obtain $\hat{\mathbf{y}}_q^{(\text{WC})}$ and concatenate those by calling each sub-model. During optimization, The vector of query-interval-lengths, $\bm{\vartheta}^{(l)}$, is altered. If the model allows either or both, open begin or end, then these parameters are altered, too.


$$
\begin{aligned}
  \mathbf{\hat{y}}=\mathsf{M}\Big(r,f,\bm{\theta}^{(b)},\bm{\vartheta}^{(l)},&b,e,\gamma_b,\gamma_e,\gamma_d,\bm{\lambda}\Big)=
  \\[1ex]
  \Big\{\;&m^c_1(\cdot)\,\frown\,\dots\,\frown\,m^c_q\Big(f,\mathbf{x}^{(\text{WP})}_q,t_b^{q},l_q^{(c)},\delta_q^{(t)},s_b^{(q)}\Big)\;\Big\},\;\forall\,q\in Q
\end{aligned}
$$

Instead of returning a discrete vector for each sub-model, $\mathbf{\hat{y}}_q$, in practice we probably want to return a tuple with the sub-model function, the support of it, as well as the relative length of its interval, i.e., $\langle m_q^c,\text{supp}(q),l_q^{\text{rel}} \rangle$. The advantage is, that any loss function can decide how to sample from this function, or even do numerical integration.

## Gradient of the model

_Attention_: I had previously fixed an error in the model formulation, so that some parts of this subsection are unfortunately outdated. Still, we keep them for historical reasons.

We have previously shown the sub-model with built-in constraints, $m_q^c$. We have quite many $\min,\max$-expressions and dynamically-long sums in our model, and for a symbolic solution, we suggest to expand all terms to create a sum, where each term can be derived for separately. Before we get there, however, it makes sense to derive some of the recurring expressions, so that we can later just plug them in.

It is important to create a fully-expanded version of the model $m_q^c$, to better understand its gradient. Fully expanded, the model $m_q^c$ is given as:

$$
\begin{aligned}
  m_q^c(\dots)=f\Bigg(s_b^{(q)}\;+\;&\bigg(\delta_q
  \\[1ex]
  &\times\bigg(x-\overbrace{\min{\Big(\gamma_e-\gamma_d,\max{\big(\gamma_b, \min{(b,e)}\big)}\Big)}}^{\beta_l}-\overbrace{\Big(\max{(\bm{\lambda}_1,l_1)}+\;\dots\;+\max{(\bm{\lambda}_{q-1},l_{q-1})}\Big)}^{\phi_q\;\text{(sum of all preceding intervals' lengths;}\;0\;\text{if}\;q=1\text{)}}\bigg)
  \\[1ex]
  &\times\overbrace{\Bigg(\overbrace{\max{\Big(\gamma_b+\gamma_d,\min{\big(\gamma_e, \max{(b,e)}\big)}\Big)}}^{\beta_u}-\overbrace{\min{\Big(\gamma_e-\gamma_d,\max{\big(\gamma_b, \min{(b,e)}\big)}\Big)}}^{\beta_l}\Bigg)}^{\phi^{(e)}\;\text{(extent of}\;\phi\text{)}}
  \\[1ex]
  &\times\overbrace{\max{\Big(\gamma_d,\;\max{(\bm{\lambda}_1,l_1)}+\;\dots\;+\max{(\bm{\lambda}_{\max{(Q)}},l_{\max{(Q)}})}\Big)}}^{\phi^{(s)}\;\text{(scale of}\;\phi\text{; sum of all intervals' minimum lengths)}}
  \\[1ex]
  &\times\max{\big(\bm{\lambda}_q,l_q\big)}^{-1}\bigg)\Bigg)
\end{aligned}
$$

In the remainder of this section, we will step-wise produce a gradient of the open begin- and -end model $m_{q=4}^c$ (meaning that there is a gradient for $b,e$, too). The reason to use $q=4$ is simple: This guarantees that $\phi_q$ is not zero (and that we can effortlessly see what would happen if $q=1$), and the $\phi$ is not zero, either. Recall the overall model, and how it is composed of these sub-models. If we have four intervals, we get for sub-models. The difference in these sub-models lies in $\phi_q$, which is different for every $q$-th sub-model ($\phi$ is the same).

The gradient of the overall model has "categories", meaning that partial derivation for some of the parameters is very similar:

* Derivation for $b$ or $e$: This is category 1, and affects all terms that use $\beta_l,\beta_u$.
* Derivation for any $l_r,r<q$: Category 2, this mainly affects $\phi$ and $\phi_q$.
* Derivation for the current $l_q$: Category 3, affects the denominator and $\phi$.


### Expanding the denominator

The denominator for the product/fraction given to $f$ is $\max{(\bm{\lambda_q},l_q)^{-1}}$ (the last factor in the above expansion). It can only be differentiated for $l_q$, as all $\bm{\lambda}$ are constant:

$$
\begin{aligned}
  \max{(\bm{\lambda}_q,l_q)}^{-1}=&\;\Big(\mathcal{R}(l_q-\bm{\lambda}_q)+\bm{\lambda}_q\Big)^{-1}\;\text{, with derivative for}\;l_q
  \\[1ex]
  \frac{\partial\,\max{(\bm{\lambda}_q,l_q)}^{-1}}{\partial\,l_q}=&\;\frac{\mathcal{H}(l_q-\bm{\lambda}_q)}{\Big(\mathcal{R}(l_q-\bm{\lambda}_q)+\bm{\lambda}_q\Big)^2}
\end{aligned}
$$



### Expanding $\beta_l,\beta_u$

We also should expand $\beta_l,\beta_u$ and do the substitutions for our $\min,\max$ definitions, so that we can derive our model without having to check conditions later:

$$
\begin{aligned}
  \beta_l=&\;\overbrace{\min{\Big(\gamma_e-\gamma_d,\overbrace{\max{\big(\gamma_b, \overbrace{\min{(b,e)}}^{s_2=e-\mathcal{R}(e-b)}\big)}\Big)}^{s_1=\mathcal{R}(s_2-\gamma_b)+\gamma_b}}}^{\beta_l=s_1-\mathcal{R}(s_1-(\gamma_e-\gamma_d))}\;\text{, rearranged as}
  \\[1ex]
  =&\;\overbrace{\mathcal{R}(\overbrace{e-\mathcal{R}(e-b)}^{s_2}-\gamma_b)+\gamma_b}^{s_1}-\mathcal{R}(\overbrace{\mathcal{R}(\overbrace{e-\mathcal{R}(e-b)}^{s_2}-\gamma_b)+\gamma_b}^{s_1}-(\gamma_e-\gamma_d))\;\text{, with gradient}
  \\[1ex]
  \bigg[\frac{\partial\,\beta_l}{\partial\,b},\frac{\partial\,\beta_l}{\partial\,e}\bigg]=&\bigg[\;-\mathcal{H}(e-b)\times\mathcal{H}\big(-\mathcal{R}(e-b)-\gamma_b+e\big)\times\Big(\mathcal{H}\big(\mathcal{R}(-\mathcal{R}(e-b)-\gamma_b+e)+\gamma_b-\gamma_e-\gamma_d\big)-1\Big),
  \\[0ex]
  &\;\;\;\;\Big(\mathcal{H}\big(-\mathcal{R}(e-b)-\gamma_b+e\big)-1\Big)\times\Big(e\times\mathcal{H}\Big(\mathcal{R}\big(-\mathcal{R}(e-b)-\gamma_b+e\big)+\gamma_b-\gamma_e+\gamma_d\Big)-1\Big)\;\bigg]\;\text{.}
\end{aligned}
$$

Since we get a lot of Heaviside step functions in these products, evaluation can be stopped early if the result of any is $0$. Here we do the same for $\beta_u$:

$$
\begin{aligned}
  \beta_u=&\;\overbrace{\max{\Big(\gamma_b+\gamma_d,\overbrace{\min{\big(\gamma_e,\overbrace{\max{(b,e)}\big)}^{s_2=\mathcal{R}(e-b)+b}}\Big)}^{s_1=s_2-\mathcal{R}(s_2-\gamma_e)}}}^{\beta_u=\mathcal{R}(s_1-(\gamma_b+\gamma_d))+(\gamma_b+\gamma_d)}\;\text{, rearranged as}
  \\[1ex]
  =&\;\mathcal{R}(\overbrace{\overbrace{\mathcal{R}(e-b)+b}^{s_2}-\mathcal{R}(\overbrace{\mathcal{R}(e-b)+b}^{s_2}-\gamma_e)}^{s_1}-(\gamma_b+\gamma_d))+(\gamma_b+\gamma_d)\;\text{, with gradient}
  \\[1ex]
  \bigg[\frac{\partial\,\beta_u}{\partial\,b},\frac{\partial\,\beta_u}{\partial\,e}\bigg]=&\bigg[\;\big(\mathcal{H}(e-b)-1\big)\times\Big(\mathcal{H}\big(\mathcal{R}(e-b)+b-\gamma_e\big)-1\Big)
  \\[0ex]
  &\;\;\;\;\;\;\;\;\times\mathcal{H}\Big(-\mathcal{R}\big(\mathcal{R}(e-b)+b-\gamma_e\big)+\mathcal{R}(e-b)+b-\gamma_b-\gamma_d\Big),
  \\[0ex]
  &\;\;\;\;e-\mathcal{H}(-\mathcal{R}(\mathcal{R}(e-b)+b-\gamma_e)+\mathcal{R}(e-b)+b-\gamma_b-\gamma_d)\;\bigg]\;\text{.}
\end{aligned}
$$


### Expanding $\phi$

Recall the definition of $\phi=(\beta_u-\beta_l)\times\max{\Bigg(\gamma_d,\Bigg[\sum_{j=1}^{\max{(Q)}}\,\max{(\bm{\lambda}_j,l_j)}\Bigg]\Bigg)}$.

As for the final expansion, we want to expand the denominator and $\phi$ together, i.e., $\phi\times\max{(\bm{\lambda_q},l_q)^{-1}}$. All these expansions will lead to very large expressions, and we are starting here to add substitution letters. The expansion with parts $j,k$ is:

$$
\begin{aligned}
  f\Big(s_b^{(q)}+\Big[\;\dots\;\times&\overbrace{\phi\times\max{(\bm{\lambda_q},l_q)}^{-1}}^{\text{(will become}\;[j-k]\text{ after expansion)}}\;\Big]\Big)
  \\[1ex]
  f\Big(s_b^{(q)}+\Big[\;\dots\;\times&\Big(\beta_u\times\max{(\bm{\lambda_q},l_q)}^{-1}-\beta_l\times\max{(\bm{\lambda_q},l_q)}^{-1}\Big)\times\max{(\gamma_d,\max{(\bm{\lambda}_1,l_1)}+\dots+\max{(\bm{\lambda}_4,l_4)})}
  \\[1ex]
  f\Big(s_b^{(q)}+\Big[\;\dots\;\times&\Bigg(\overbrace{\bigg(\beta_u\times\max{(\bm{\lambda_q},l_q)}^{-1}\times\overbrace{\max{(\gamma_d,\max{(\bm{\lambda}_1,l_1)}+\dots+\max{(\bm{\lambda}_4,l_4)})}\bigg)}^{\phi^{(s)}}}^{j}
  \\[0ex]
  &\;\;\;\;-\overbrace{\bigg(\beta_l\times\max{(\bm{\lambda_q},l_q)}^{-1}\times\overbrace{\max{(\gamma_d,\max{(\bm{\lambda}_1,l_1)}+\dots+\max{(\bm{\lambda}_4,l_4)})}\bigg)}^{\phi^{(s)}}}^{k}\Bigg)
\end{aligned}
$$

Previously, we split $\phi$ into $\phi^{(e)}$ and $\phi^{(s)}$, which represent the portions _extent_ and _scale_ of it. The extent only consists of $(\beta_u-\beta_l)$, and we have already derived them. The scale expression is a nested $\max$ with a summation inside, that affects $\forall\,l$, regardless of what $q$ equals.

$$
\begin{aligned}
  \phi^{(s)}=&\;\max{\Bigg(\gamma_d,\Bigg[\sum_{i=1}^{\max{(Q)}}\,\max{(\bm{\lambda}_i,l_i)}\Bigg]\Bigg)}
  \\[1ex]
  =&\;\max{\Big(\gamma_d,\max{(\bm{\lambda}_1,l_1)}+\dots+\max{(\bm{\lambda}_4,l_4)}\Big)}\;\text{, (for}\;q=4\text{),}
  \\[1ex]
  =&\;\overbrace{\mathcal{R}\Big(\Big[\;\overbrace{\mathcal{R}(l_1-\bm{\lambda}_1)+\bm{\lambda}_1}^{\text{(first }\max\text{ of summation)}}+\;\dots\;+\overbrace{\mathcal{R}(l_4-\bm{\lambda}_4)+\bm{\lambda}_4}^{\text{(}n\text{-th }\max\text{ of summation)}}\;\Big]-\gamma_d\Big)+\gamma_d}^{\text{(outer }\max\text{; recall that }\max{(x,y)=\mathcal{R}(y-x)+x}\text{)}}\;\text{.}
\end{aligned}
$$

In the above expansion, $\gamma_d$ is a constant defined at initialization time, and the same is true for $\bm{\lambda}$. However, $\phi^{(s)}$ will appear in each part of the model as we see later, and its gradient is sensitive $\forall\,l$, i.e., not just for, e.g., $l_q$ or $\forall\,l_r,r<q$. However, since it is the summation that will be affected, and since it does the same $\forall\,l$, the gradient, conveniently, will be the same for each.

$$
\begin{aligned}
  \nabla\,\phi^{(s)}=&\;\frac{\partial\,\phi^{(s)}}{\partial\,l_q,\forall\,q\in Q}\;\text{,}
  \\[1ex]
  =&\;\mathcal{H}\Big(l_q-\bm{\lambda}_q\Big)\times \mathcal{H}\Big(\Big[\;\overbrace{\mathcal{R}(l_1-\bm{\lambda}_1)+\bm{\lambda}_1+\;\dots\;+R(l_4-\bm{\lambda}_4)+\bm{\lambda}_4}^{\sum_{i=1}^{\max{(Q)}}\,\max{(\bm{\lambda}_i,l_i)}}\;\Big]-\gamma_d\Big)\;\text{.}
\end{aligned}
$$


### Expanding $m_{q=4}^c$

The goal is to fully expand the current sub-model into a big sum, where we can effortlessly derive each term separately. We also have now all of the sub-expressions and their partial gradient. Here we continue to assign substitution letters so that we can derive the model more easily.

$$
\begin{aligned}
  m_{q=4}^c=&\;f\bigg(s_b^{(q)}+\frac{(x-\beta_l-\phi_q)\times\delta_q\times\phi}{\max{(\bm{\lambda_q},l_q)}}\bigg)
  \\[1ex]
  =&\;f\bigg(s_b^{(q)}+\frac{(\delta_qx-\delta_q\beta_l-\delta_q\phi_q)\times\phi}{\max{(\bm{\lambda_q},l_q)}}\bigg)
  \\[1ex]
  =&\;f\Big(s_b^{(q)}+\Big[\;\overbrace{\delta_qx-\delta_q\beta_l-\overbrace{\delta_q\max{(\bm{\lambda_1},l_1)}-\delta_q\max{(\bm{\lambda_2},l_2)}-\delta_q\max{(\bm{\lambda_3},l_3)}}^{\phi_{q=4}}}^{(a-b-c-d-e)}\;\times\overbrace{\phi\times\max{(\bm{\lambda_q},l_q)}^{-1}}^{(j-k)}\;\Big]\Big)
  \\
  \vdots
  \\
  =&\;f\Big(s_b^{(q)}+aj-ak-bj+bk-cj+ck-dj+dk-ej+ek\Big)\;\text{.}
\end{aligned}
$$

The final expression for our model is now much more convenient to work with. It is worth noting that summands with $c$ exist if $q>1$, those with $d$ exist if $q>2$ and so on. Those are essentially what distinguishes each $q$-th sub-model from each other sub-model. For example, sub-model $m_{q=1}^c$ has none of these summands, whereas sub-model $m_{q=2}^c$ has those with $c$, and sub-model $m_{q=3}^c$ additionally has those with $d$ and so on. That is why we chose an example with $q$ up to $4$, to better illustrate the differences.

$$
\begin{aligned}
  f\Bigg(&s_b^{(q)}
  \\[0ex]
  &+aj=\Big[\;\delta_q\times x\times \overbrace{\beta_u\times\max{(\bm{\lambda}_q,l_q)}^{-1}\times\max{\Big(\gamma_d,\max{(\bm{\lambda}_1,l_1)}+\dots+\max{(\bm{\lambda}_4,l_4)}\Big)}}^{j}\;\Big]
  \\[1ex]
  &-ak=\Big[\;\delta_q\times x\times\overbrace{\beta_l\times\max{(\bm{\lambda}_q,l_q)}^{-1}\times\max{\Big(\gamma_d,\;\dots\;\Big)}}^{k}\;\Big]
  \\[1ex]
  &-bj=\Big[\;\delta_q\times\beta_l\times \beta_u\times\max{(\dots)}^{-1}\times\max{(\dots)}\;\Big]
  \\[1ex]
  &+bk=\Big[\;\delta_q\times{\beta_l}^2\times{\dots}\times{\dots}\;\Big]
  \\[1ex]
  &-cj=\Big[\;\delta_q\times\max{(\bm{\lambda}_1,l_1)}\times\overbrace{\beta_u\times{\dots}\times{\dots}}^{j}\;\Big]
  \\[1ex]
  &+ck=\Big[\;\delta_q\times\max{(\bm{\lambda}_1,l_1)}\times\overbrace{\beta_l\times{\dots}\times{\dots}}^{k}\;\Big]
  \\[1em]
  &\overbrace{-dj=[\dots]+dk=[\dots]}^{\text{(if }q>1\text{)}}
  \\[1ex]
  &\overbrace{-ej=[\dots]+ek=[\dots]}^{\text{(if }q>2\text{)}}
  \\
  &\;\;\;\;\vdots
  \\
  &\;\text{(}\pm\;\text{additional terms for larger}\;q\text{)}\Bigg)\text{.}
  \\[1ex]
\end{aligned}
$$

The derivation of this model is now straightforward, as we can tackle each summand separately. Also, we have already created the partial derivatives of all sub-expressions earlier. Before we assemble the actual analytical gradient, we will test our model.


## Testing the model

The main features of the proposed model are, that it is __self-regularizing__ and optionally allows for half- or full-open window time warping. In code, we will call the model just `SRBTW`, and have the optional openness as setters and getters.

Before we go any further, we should test the model. When this notebook was started, it was called _Closed-window Optimization_. However, we made some additions to the proposed model that allow partially- or fully-open window optimization. In this section, we will do some testing with the pattern and data we have, focusing on the __adaptive__ (__A__) variable. We will be using the four intervals as defined earlier


```{r echo=FALSE, message=FALSE}
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

vartheta_l <- c(.25,.275,.25,.225)
# vartheta_l <- 4:1
# These are from a JSD-only based fit and quite good!
# vartheta_l <- c(0.4119631, 0.1651927, 0.0737804, 0.3831419)
# .. slightly worse:
# vartheta_l <- c(0.35320284, 0.21300932, 0.07073679, 0.37062611)


srbtw <- SRBTW$new(
  wp = r,
  wc = f,
  theta_b = c(0, bounds, 1),
  gamma_bed = c(0, 1, 0),
  lambda = rep(0, length(vartheta_l)),
  begin = 0,
  end = 1,
  openBegin = FALSE,
  openEnd = FALSE
)

srbtw$setParams(vartheta_l = vartheta_l)

smData <- matrix(nrow = 0, ncol = 14)
for (q in seq_len(length.out = length(vartheta_l))) {
  sm <- srbtw$getSubModel(q)
  t <- sm$asTuple()
  smData <- rbind(smData, c(
    t$q, t$beta_l, t$beta_u, t$lambda_q, t$l_q, t$l_prime_q,
    t$l_q_c, t$psi, t$phi_q, t$delta_t_q, t$sb_q, t$se_q, t$tb_q, t$te_q
  ))
}
colnames(smData) <- names(t)[1:14]
smData

tempf <- Vectorize(function(x) {
  stopifnot(length(x) == 1 && !is.na(x))
  
  q <- srbtw$getQForX(x)
  sm <- srbtw$getSubModel(q = q)
  t <- sm$asTuple()
  t$mqc(x)
})

# We have a new plot-function!
srbtw$plot_warp()
```

```{r echo=FALSE}
objF <- function(x, isGrad = FALSE) {
  # 'x' is our vartheta_l [, +b[, +e]]
  b <- if (srbtw$isOpenBegin()) tail(x, 2)[1] else NULL
  e <- if (srbtw$isOpenEnd()) tail(x, 1) else NULL
  
  srbtw$setParams(vartheta_l = x[1:max(srbtw$getQ())])
  if (!is.null(b)) srbtw$setParams(begin = b)
  if (!is.null(e)) srbtw$setParams(end = e)
  
  tempf <- Vectorize(function(x_) {
    srbtw$M(x = x_)
  })
  
  loss <- 0
  # X <- seq(srbtw$getBeta_l(), srbtw$getBeta_u(), length.out = 1e3)
  # y <- sapply(X, r)
  # y_hat <- sapply(X, tempf)
  
  # RSS:
  # loss <- log(1 + sum((y - y_hat)^2))
  
  # KL:
  # y <- y - min(y) + .1
  # y <- y / max(y)
  # y_hat <- y_hat - min(y_hat) + .1
  # y_hat <- y_hat / max(y_hat)
  # loss <- sum(na.omit(y * log(y / y_hat)))
  
  # Correlation:
  # eps <- .Machine$double.eps
  # loss <- loss - log(eps + max(0, stat_diff_2_functions_cor_score()(r, tempf) - eps))
  
  # # Area:
  # loss <- loss - log(area_diff_2_functions_score()(r, tempf))
  
  # JSD:
  loss <- loss - log(stat_diff_2_functions_symmetric_JSD_score()(r, tempf))
  
  
  # R: Open begin/end:
  if (srbtw$isOpenBegin() || srbtw$isOpenEnd()) {
    loss <- loss - .5 * log(max(.Machine$double.eps, srbtw$getBeta_u() - srbtw$getBeta_l()))
  }
  
  if (!isGrad) {
    print(loss)
  }
  if (is.na(loss) || is.infinite(loss)) {
    stop(paste(x, collapse = ", "))
  }
  
  loss
}
```

Let's try some optimization using box-bounds and closed open and begin:

```{r echo=FALSE}
# Optimization Goal 1, test 1, parallel:
cow_og1_test1p <- loadResultsOrCompute(file = "../results/cow_og1_test1p.rds", computeExpr = {
  library(optimParallel)
  
  cl <- parallel::makePSOCKcluster(parallel::detectCores())
  parallel::clusterExport(cl, varlist = c(
    "srbtw", "r", "f", "objF", "stat_diff_2_functions", "stat_diff_2_functions_cor",
    "stat_diff_2_functions_cor_score", "area_diff_2_functions", "area_diff_2_functions_score",
    "stat_diff_2_functions_symmetric_JSD_score",
    "stat_diff_2_functions_symmetric_JSD_sampled",
    "stat_diff_2_functions_philentropy_sampled"))

  doWithParallelClusterExplicit(cl = cl, expr = {
    set.seed(1337)
    optRp <- optimParallel::optimParallel(
      par = rep(1/length(vartheta_l), length(vartheta_l)),
      fn = objF,
      lower = rep(0, length(vartheta_l)),
      upper = rep(1, length(vartheta_l)),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
})

cow_og1_test1p
srbtw$setAllParams(cow_og1_test1p$par)
srbtw$plot_warp()
```

In the meantime, I have implemented some new models as __R6-classes__. We should test that we can reach the same optimization result with these new structures. Some rationale for the architecture's design are the following:

Self-Regularizing Boundary Time Warping (__SRBTW__) is implicitly multi-variate and multi-dataseries capable. When defining a loss that concerns a variable, we will refer to it as a __Data-loss__. A loss that concerns the choice of model parameters will be called __Regularizer__. Since we have potentially many variables and intervals, a fully configured model may have many of either kind, and each of these are __objectives__; SRBTW, by design, hence is a __Multilevel-Model__ and poses a __Multi-objective__ optimization problem.

This is comparable to the output-layer of a neural network, so we can also view a fully-configured SRBTW as a vector-valued function, where each entry in the vector is a loss. The gradient then would be the __Jacobian__-matrix, i.e.,

$$
\begin{aligned}
  \mathcal{O}(\operatorname{SRBTW})=&\;f:\mathbb{R}^m\to\mathbb{R}^n\;\text{,}
  \\[1ex]
  &\;\text{where}\;m\;\text{is the number of parameters (begin, end, interval-lengths etc.),}
  \\[0ex]
  &\;\text{and}\;n\;\text{is the number of defined singular objectives,}
  \\[1ex]
  \mathbf{J}_f=&\;\begin{bmatrix}
    \frac{\partial\,f_1}{\partial\,x_1} & \cdots & \frac{\partial\,f_1}{\partial\,x_m} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial\,f_n}{\partial\,x_1} & \cdots & \frac{\partial\,f_n}{\partial\,x_m}
  \end{bmatrix}
\end{aligned}
$$

Consider our Fire Drill example: The Warping Pattern has four variables and four intervals. If we want to fit a single project to it, we may want to define a single model and weight per variable and interval, which would result in already 16 objectives here. I wrote that SRBTW is implicitly capable of handling multiple variables and/or dataseries. Implicit because this complexity is reduced to having one SRBTW handle exactly two signals, the Warping Pattern and -Candidate, and then defining a _singular_ loss per each. It is perfectly fine to reuse an SRBTW model across intervals of the same variable, but in practice the overhead of doing so may be reduced by having each objective use its own instance. We provide a __linear scalarizer__ to handle one or more objectives, each with their own weight. Since the linear scalarizer reduces any number of objectives to a single objective, computing it results in a scalar loss, and the Jacobian becomes a gradient again.

$$
\begin{aligned}
  \mathcal{O}^{(\text{LS})}(\operatorname{SRBTW})=&\;w_1\mathcal{L}_1(\cdot)+\;\dots\;+w_n\mathcal{L}_n(\cdot)\;\text{, (linear scalarization)}
  \\[1ex]
  =&\;\mathbf{w}^\top\bm{\mathcal{L}}\;\text{, with gradient (for any parameter in}\;\mathcal{L}_i\text{)}
  \\[1ex]
  \nabla\,\mathcal{O}=&\;w_1\mathcal{L}'_1(\cdot)+\;\dots\;+w_n\mathcal{L}'_n(\cdot)\;\text{.}
\end{aligned}
$$

In practice, each weight is some combination of multiple other weights, in our case it will most likely be the product of the weights for the current data series, variable and interval. The linear scalarizer also makes it apparent that each single objective (as well as its gradient) can be computed __independently__ and simultaneously, meaning that we can arbitrarily parallelize the computation.

Back to our example that captures only the __A__-variable, let's set up this scenario with the new classes:

```{r}
dataLoss <- SRBTW_DataLoss$new(
  srbtw = srbtw, intervals = 1:4, # note how we cover 4 intervals
  weight = 1, # should be 4 but we need to check that our results
  # of cow_og1_test1p are identical to cow_og1_test3p later.
  continuous = TRUE, params = rep(1/4, length(vartheta_l)))

dataLoss_RSS <- function(loss, listOfSms) {
  # 'loss' is a reference to the data-loss itself.
  continuous <- loss$isContinuous()
  err <- 0
  
  for (sm in listOfSms) {
    t <- sm$asTuple()
    if (continuous) {
      tempf <- function(x) (t$wp(x) - t$mqc(x))^2
      err <- err + cubature::cubintegrate(
        f = tempf, lower = t$tb_q, upper = t$te_q)$integral
    } else {
      X <- seq(from = t$tb_q, to = t$te_q, length.out = 250)
      y <- sapply(X = X, FUN = t$wp)
      y_hat <- sapply(X = X, FUN = t$mqc)
      err <- err + sum((y - y_hat)^2)
    }
  }
  
  log(1 + err)
}

dataLoss$setLossFunc(lossFunc = dataLoss_RSS)

soo <- SRBTW_SingleObjectiveOptimization$new(srbtw = srbtw)
soo$addObjective(obj = dataLoss)
soo$setParams(params = rep(1/4, length(vartheta_l)))
soo$compute()
```

Let's attempt to optimize this:

```{r echo=FALSE}
cow_og1_test2p <- loadResultsOrCompute(file = "../results/cow_og1_test2p.rds", computeExpr = {
  library(optimParallel)
  
  cl <- parallel::makePSOCKcluster(parallel::detectCores())
  parallel::clusterExport(cl, varlist = c(
    "srbtw", "r", "f", "objF", "stat_diff_2_functions", "stat_diff_2_functions_cor",
    "stat_diff_2_functions_cor_score", "area_diff_2_functions", "area_diff_2_functions_score",
    "stat_diff_2_functions_symmetric_JSD_score",
    "stat_diff_2_functions_symmetric_JSD_sampled",
    "stat_diff_2_functions_philentropy_sampled", "SRBTW_Loss", "SRBTW_DataLoss",
    "SRBTW_SingleObjectiveOptimization", "soo"))

  doWithParallelClusterExplicit(cl = cl, expr = {
    set.seed(1337)
    optRp <- optimParallel::optimParallel(
      par = rep(1/length(vartheta_l), length(vartheta_l)),
      fn = function(x) {
        soo$setParams(params = x)
        soo$compute()
      },
      lower = rep(0, length(vartheta_l)),
      upper = rep(1, length(vartheta_l)),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
})

cow_og1_test2p
srbtw$setAllParams(cow_og1_test2p$par)
srbtw$plot_warp()
```

While this fit looks subjectively worse compared to the previous fit using JSD, it is actually better w.r.t. the RSS-loss used here. The loss here is `r round(cow_og1_test2p$value, 6)`. If we use the parameters from the JSD-fit and compute the loss for the RSS, it becomes:

```{r}
srbtw$setAllParams(params = cow_og1_test1p$par)
dataLoss$setLossFunc(lossFunc = dataLoss_RSS)
soo$compute()
```

This is a clear demonstration of __SRBTW__'s strength, that it can use any arbitrary loss or (weighted) combinations thereof, to optimize for distinct goals simultaneously. Here is the test that should yield the exact same results as we got with `cow_og1_test1p`:

```{r}
dataLoss_JSD <- function(loss, listOfSms) {
  
  tempf <- Vectorize(function(x_) {
    stopifnot(length(x_) == 1 && !is.na(x_))
    q <- srbtw$getQForX(x_)
    sm <- listOfSms[[q]]
    t <- sm$asTuple()
    t$mqc(x_)
  })
  
  -log(stat_diff_2_functions_symmetric_JSD_score()(listOfSms[[1]]$asTuple()$wp, tempf))
}

dataLoss$setLossFunc(lossFunc = dataLoss_JSD)
```


```{r echo=FALSE}
cow_og1_test3p <- loadResultsOrCompute(file = "../results/cow_og1_test3p.rds", computeExpr = {
  library(optimParallel)
  
  cl <- parallel::makePSOCKcluster(parallel::detectCores())
  parallel::clusterExport(cl, varlist = c(
    "srbtw", "r", "f", "objF", "stat_diff_2_functions", "stat_diff_2_functions_cor",
    "stat_diff_2_functions_cor_score", "area_diff_2_functions", "area_diff_2_functions_score",
    "stat_diff_2_functions_symmetric_JSD_score",
    "stat_diff_2_functions_symmetric_JSD_sampled",
    "stat_diff_2_functions_philentropy_sampled", "SRBTW_Loss", "SRBTW_DataLoss",
    "SRBTW_SingleObjectiveOptimization", "soo"))
  
  doWithParallelClusterExplicit(cl = cl, expr = {
    set.seed(1337)
    optRp <- optimParallel::optimParallel(
      par = rep(1/length(vartheta_l), length(vartheta_l)),
      fn = function(x) {
        soo$setParams(params = x)
        soo$compute()
      },
      lower = rep(0, length(vartheta_l)),
      upper = rep(1, length(vartheta_l)),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
})

cow_og1_test3p
srbtw$setAllParams(cow_og1_test3p$par)
srbtw$plot_warp()
```

These are the exact same results as in `cow_og1_test1p`!


### Half- and full-open tests

Let's do some tests where we leave either the begin or end open (or both) and see what happens.

```{r}
srbtw$setOpenBegin(ob = TRUE)
srbtw$setOpenEnd(oe = TRUE)
```

```{r echo=FALSE}
cow_og1_test4p <- loadResultsOrCompute(file = "../results/cow_og1_test4p.rds", computeExpr = {
  library(optimParallel)
    
  cl <- parallel::makePSOCKcluster(parallel::detectCores())
  parallel::clusterExport(cl, varlist = c(
    "srbtw", "r", "f", "objF", "stat_diff_2_functions", "stat_diff_2_functions_cor",
    "stat_diff_2_functions_cor_score", "area_diff_2_functions", "area_diff_2_functions_score",
    "stat_diff_2_functions_symmetric_JSD_score",
    "stat_diff_2_functions_symmetric_JSD_sampled",
    "stat_diff_2_functions_philentropy_sampled", "SRBTW_Loss", "SRBTW_DataLoss",
    "SRBTW_SingleObjectiveOptimization", "soo"))
  
  doWithParallelClusterExplicit(cl = cl, expr = {
    set.seed(1337)
    optRp <- optimParallel::optimParallel(
      par = c(rep(1/length(vartheta_l), length(vartheta_l)), 0, 1),
      fn = function(x) {
        soo$setParams(params = x)
        soo$compute()
      },
      lower = rep(0, 2 + length(vartheta_l)),
      upper = rep(1, 2 + length(vartheta_l)),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
})

cow_og1_test4p
srbtw$setAllParams(cow_og1_test4p$par)
srbtw$plot_warp() +
  ggplot2::geom_vline(xintercept = cow_og1_test4p$par[5]) +
  ggplot2::geom_vline(xintercept = cow_og1_test4p$par[6])
```

This is still using the Jenson-Shannon divergence, and apparently it is a better fit. The black vertical lines indicate the used begin and end of the Warping Candidate, and `M` was sliced and laid over `WP`. The begin and end were `r range(tail(cow_og1_test4p$par, 2))`.

```{r eval=FALSE}
optextras::kktchk(cow_og1_test4p$par, fn = function(x) {
  soo$setParams(params = x)
  soo$compute()
}, gr = function(x) {
  soo$setParams(params = x)
  soo$computeGrad()
})
```


Let's also try this with the RSS data-loss:

```{r}
dataLoss$setLossFunc(lossFunc = dataLoss_RSS)
```

```{r echo=FALSE}
cow_og1_test5p <- loadResultsOrCompute(file = "../results/cow_og1_test5p.rds", computeExpr = {
  library(optimParallel)
    
  cl <- parallel::makePSOCKcluster(parallel::detectCores())
  parallel::clusterExport(cl, varlist = c(
    "srbtw", "r", "f", "objF", "stat_diff_2_functions", "stat_diff_2_functions_cor",
    "stat_diff_2_functions_cor_score", "area_diff_2_functions", "area_diff_2_functions_score",
    "stat_diff_2_functions_symmetric_JSD_score",
    "stat_diff_2_functions_symmetric_JSD_sampled",
    "stat_diff_2_functions_philentropy_sampled", "SRBTW_Loss", "SRBTW_DataLoss",
    "SRBTW_SingleObjectiveOptimization", "soo"))
  
  doWithParallelClusterExplicit(cl = cl, expr = {
    set.seed(1337)
    optRp <- optimParallel::optimParallel(
      par = c(rep(1/length(vartheta_l), length(vartheta_l)), 0, 1),
      fn = function(x) {
        soo$setParams(params = x)
        soo$compute()
      },
      lower = rep(0, 2 + length(vartheta_l)),
      upper = rep(1, 2 + length(vartheta_l)),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
})

cow_og1_test5p
srbtw$setAllParams(cow_og1_test5p$par)
srbtw$plot_warp() +
  ggplot2::geom_vline(xintercept = cow_og1_test5p$par[5]) +
  ggplot2::geom_vline(xintercept = cow_og1_test5p$par[6])
```

That worked, too. I tested this with the discrete version, and it takes about twice as many iterations. However, we can also observe that the target-extent is smaller than in the previous example, which brings us to the next important matter: __Regularization__. But before that, let's make two half-open tests quickly, using JSD again:


```{r}
srbtw$setOpenBegin(ob = FALSE)
srbtw$setOpenEnd(oe = TRUE)

dataLoss$setLossFunc(lossFunc = dataLoss_JSD)
```

```{r echo=FALSE}
cow_og1_test6p <- loadResultsOrCompute(file = "../results/cow_og1_test6p.rds", computeExpr = {
  library(optimParallel)
    
  cl <- parallel::makePSOCKcluster(parallel::detectCores())
  parallel::clusterExport(cl, varlist = c(
    "srbtw", "r", "f", "objF", "stat_diff_2_functions", "stat_diff_2_functions_cor",
    "stat_diff_2_functions_cor_score", "area_diff_2_functions", "area_diff_2_functions_score",
    "stat_diff_2_functions_symmetric_JSD_score",
    "stat_diff_2_functions_symmetric_JSD_sampled",
    "stat_diff_2_functions_philentropy_sampled", "SRBTW_Loss", "SRBTW_DataLoss",
    "SRBTW_SingleObjectiveOptimization", "soo"))
  
  doWithParallelClusterExplicit(cl = cl, expr = {
    set.seed(1337)
    optRp <- optimParallel::optimParallel(
      par = c(rep(1/length(vartheta_l), length(vartheta_l)), 1),
      fn = function(x) {
        soo$setParams(params = x)
        soo$compute()
      },
      lower = rep(0, 1 + length(vartheta_l)),
      upper = rep(1, 1 + length(vartheta_l)),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
})

cow_og1_test6p
srbtw$setAllParams(cow_og1_test6p$par)
srbtw$plot_warp() +
  ggplot2::geom_vline(xintercept = cow_og1_test6p$par[5])
```

Now we have a late cut-out, that basically ignores `1/6` of the signal at the end. This fit is better than the closed/closed one we made earlier, the loss here is `r round(cow_og1_test6p$value, 6)` and previously it was `r round(cow_og1_test1p$value, 6)`. Let's do this once more, open begin, closed end this time:

```{r}
srbtw$setOpenBegin(ob = TRUE)
srbtw$setOpenEnd(oe = FALSE)
```

```{r echo=FALSE}
cow_og1_test7p <- loadResultsOrCompute(file = "../results/cow_og1_test7p.rds", computeExpr = {
  library(optimParallel)
    
  cl <- parallel::makePSOCKcluster(parallel::detectCores())
  parallel::clusterExport(cl, varlist = c(
    "srbtw", "r", "f", "objF", "stat_diff_2_functions", "stat_diff_2_functions_cor",
    "stat_diff_2_functions_cor_score", "area_diff_2_functions", "area_diff_2_functions_score",
    "stat_diff_2_functions_symmetric_JSD_score",
    "stat_diff_2_functions_symmetric_JSD_sampled",
    "stat_diff_2_functions_philentropy_sampled", "SRBTW_Loss", "SRBTW_DataLoss",
    "SRBTW_SingleObjectiveOptimization", "soo"))
  
  doWithParallelClusterExplicit(cl = cl, expr = {
    set.seed(1337)
    optRp <- optimParallel::optimParallel(
      par = c(rep(1/length(vartheta_l), length(vartheta_l)), 0),
      fn = function(x) {
        soo$setParams(params = x)
        soo$compute()
      },
      lower = rep(0, 1 + length(vartheta_l)),
      upper = rep(1, 1 + length(vartheta_l)),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
})

cow_og1_test7p
srbtw$setAllParams(cow_og1_test7p$par)
srbtw$plot_warp() +
  ggplot2::geom_vline(xintercept = cow_og1_test7p$par[5])
```

Previously, I had wrongly set the begin-parameter to start with $1$, and the final result did not move this parameter, which is the first obvious problem that warrants for regularization. However, after correction, I had let the parameter start from $0$ and the result as can be seen is quite good again -- it finds a nice cut in while it has to keep the remainder of the signal. The loss here is the lowest thus far using JSD (`r round(cow_og1_test7p$value, 6)`).


### Swapping Warping Pattern and Candidate

This is something we should try as well. Remember that the difference between Warping Pattern and Warping Candidate is, that the former has to be matched wholly. Let's swap them and make an open/open test-fit. This test will tell us, which sub-support of the Warping Pattern actually matches the data we have.

```{r}
vartheta_l_swap <- rep(1/15, 15)

srbtw_swap <- SRBTW$new(
  wp = f,
  wc = r,
  theta_b = seq(0, 1, length.out = 16),
  gamma_bed = c(0, 1, 0),
  lambda = rep(0, length(vartheta_l_swap)),
  begin = 0,
  end = 1,
  openBegin = TRUE,
  openEnd = TRUE
)

srbtw_swap$setParams(vartheta_l = vartheta_l_swap)

dataLoss_swap <- SRBTW_DataLoss$new(
  srbtw = srbtw_swap, intervals = 1:4, # note how we cover 4 intervals
  weight = 1, # should be 4 but we need to check that our results
  # of cow_og1_test1p are identical to cow_og1_test3p later.
  continuous = TRUE, params = c(vartheta_l_swap, 0, 1))

soo_swap <- SRBTW_SingleObjectiveOptimization$new(srbtw = srbtw_swap)
soo_swap$addObjective(obj = dataLoss_swap)
dataLoss_swap$setLossFunc(lossFunc = dataLoss_RSS)
```

```{r echo=FALSE}
cow_og1_test8p <- loadResultsOrCompute(file = "../results/cow_og1_test8p.rds", computeExpr = {
  library(optimParallel)
    
  cl <- parallel::makePSOCKcluster(parallel::detectCores())
  parallel::clusterExport(cl, varlist = c(
    "srbtw", "r", "f", "objF", "stat_diff_2_functions", "stat_diff_2_functions_cor",
    "stat_diff_2_functions_cor_score", "area_diff_2_functions", "area_diff_2_functions_score",
    "stat_diff_2_functions_symmetric_JSD_score",
    "stat_diff_2_functions_symmetric_JSD_sampled",
    "stat_diff_2_functions_philentropy_sampled", "SRBTW_Loss", "SRBTW_DataLoss",
    "SRBTW_SingleObjectiveOptimization", "SRBTW_SubModel", "soo_swap"))
  
  doWithParallelClusterExplicit(cl = cl, expr = {
    set.seed(1337)
    optRp <- optimParallel::optimParallel(
      par = c(vartheta_l_swap, 0, 1),
      fn = function(x) {
        soo_swap$setParams(params = x)
        soo_swap$compute()
      },
      lower = rep(0, 2 + length(vartheta_l_swap)),
      upper = rep(1, 2 + length(vartheta_l_swap)),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
})

cow_og1_test8p
srbtw_swap$setAllParams(cow_og1_test8p$par)
srbtw_swap$plot_warp()
```

This works great, too, but we have a somewhat unfavorable example here. The project data, which has now become the reference (WP, blue) is zero between $\approx[0,0.26]$. We have attempted open/open optimization, but since the reference (now WC, black) does not have this at the beginning, the optimization finds a way around this by mapping the first three intervals of the blue WP to three times the length of zero, effectively moving the reference by that. There is some cut-off at the end, however ;) These zero-length intervals are still required, because only through those we learn the offset in the blue line, where the black/red data series start to match. For this example, the reference as WC matches (given the loss we used) the data as WP almost entirely (a small cut-off at the end). This means that the first three intervals are technically _not_ contained in the reference signal (now as WC). The result is a _padding_ in the beginning, and could have happened also at the end. Padding thus occurs whenever one or more consecutive lengths at the beginning or end are $0$. In some cases, we would probably also get intermittent intervals with a zero length (zero-length intervals preceded or followed by non-zero-length intervals), which means that the match is _partial_, and some interval of the WP has no counterpart in the WC, i.e., a section in the WP is missing in the WC.


## Regularization

Regularization-losses are those that impose penalties on extreme parameters. SRBTW is self-regularizing w.r.t. interval-lengths and begin- and end-offsets. No interval can be of negative length (or smaller than a user-defined, yet positive, parameter). Begin and end are always ordered and always keep a minimum distance to each other ($\gamma_d$), while guaranteed not to extend beyond some user-defined lower and upper boundary. The tests up until now for the first optimization goal have shown these built-in regularizations alone are suitable for achieving very good fits. We also chose to build in these, so that no logarithmic barriers or (in-)equality constraints need to be used, as these require exponentially many additional iterations, especially when using a non-analytic gradient, leading to infeasibe large problems with already few ($<\approx200$) boundaries.

However, it may still be desirable to _guide_ the optimization process using some user-preferences. For example, one may want to avoid extreme interval-lengths. Or it may not be desirable to have the begin or end cut in too far, decimating the Warping Candidate too much. We formulate these two regularizers below but any other user-defined regularization is possible, too. Also, a regularizer is not strictly limited to compute a loss based on the parameters only, but it is more common, so we have made the distinction of _data-loss_ and _regularizer_ thus far, and we are going to continue to do so.

The nice thing about the architecture so far is, that SRBTW is already a _multi-objective_ optimization problem (using the linear scalarizer). We can just go ahead and add any regularizer as an additional objective to the overall loss. Each of these specifies its own weight, too.


### Regularize extreme supports

An extreme support is one that is much shorter than what would be allowed (given by $\gamma_e-\gamma_b-\gamma_d$). We can keep the regularizer and its gradient very simple.

$$
\begin{aligned}
  \mathcal{R}^{(\text{supp})}=&\;\frac{\beta_u-\beta_l}{\gamma_e-\gamma_b-\gamma_d}
  \\[1ex]
  =&\;\frac{\overbrace{\max{\Big(\gamma_b+\gamma_d,\min{\big(\gamma_e, \max{(b,e)}\big)}\Big)}}^{\beta_u}-\overbrace{\min{\Big(\gamma_e-\gamma_d,\max{\big(\gamma_b, \min{(b,e)}\big)}\Big)}}^{\beta_l}}{\gamma_e-\gamma_b-\gamma_d}\;\text{.}
\end{aligned}
$$

We had previously worked out the gradient for the sub-expression of $\beta_u-\beta_l$, and can reuse it in this regularizer. Here, we showed the complete regularizer for both, open begin and -end BTW. If either of these is not open, $\beta_l$ or $\beta_u$ can be replaced with a constant. In that case, it is recommended to already account for the difference of that constant and its maximum allowed value ($\gamma_b$ or $\gamma_e$). Note that its fraction is always between zero and one:

$$
\begin{aligned}
  \text{supp}\big(\mathcal{R}^{(\text{supp})}\big)=&\;\big\{\;x\in\mathbb{R}\;\rvert\;0\leq x\leq 1\;\big\}\;\text{, where values closer to zero}
  \\[0ex]
  &\;\text{mean more extreme intervals, so we propose a}\;\log\text{-loss:}
  \\[1ex]
  \mathcal{L}_{\mathcal{R}^{(\text{supp})}}=&\;-\log{\Big(\mathcal{R}^{(\text{supp})}\Big)}\mapsto[+0,\infty]\;\text{.}
\end{aligned}
$$

Until now I had not mentioned that I usually attempt to design data- and regularization-losses using logarithms, and, if possible, such that the result of a loss is __always positive__, usually with an upper bound of $\infty$, that approaches $+0$ as lower bound. Furthermore, and that depends on the objective, we most often deal with two kinds of losses. Either, the loss is some kind of ratio (or we can formulate it as such) or has a lower and upper bound. In this case, I usually scale the raw loss into the interval $[0,1]$ and use it in a negative logarithm (like the above regularizer). In the other case, the raw loss has only one bound (usually the lower bound, which is $0$). In that case, we can use the absolute raw loss as $\log{(1+[\text{raw loss}])}$.


### Regularize extreme intervals

This is something that I had also done in some of the previous notebooks, but it is not compatible with SRBTW. Still, we can come up with something similar. First we need to answer the question, what is an extreme interval? This is subjective, and probably different from case to case. I want to suggest a quite general notion here that probably works in many cases.

We will assume that an extreme interval is one that deviates from its _expected length_. A priori we do not know what amount of time time warping is required, so we assume none. That means, each interval's length is expected to be equal to or close to its corresponding Warping Pattern's counterpart (not the length but the ratio). Or, one could say that the expected length of each interval is the mean, which is the allowed extent divided by the number of intervals. Or, the user can define their own preference. Then for each interval, we could sum up the difference from the expected value.

$$
\begin{aligned}
  \bm{\kappa}\;\dots&\;\text{vector with expected lengths (ratios) for each interval,}
  \\[1ex]
  \mathcal{R}^{(\text{exint})}=&\;\sum_{q=1}^{\max{(Q)}}\,\Big(\bm{\vartheta}_q^{(l)}-\bm{\kappa}_q\Big)^2\;\text{, with gradient}
  \\[1ex]
  \frac{\partial\,\mathcal{R}^{(\text{exint})}}{\partial\,\bm{\vartheta}_q^{(l)}}=&\;2\sum_{q=1}^{\max{(Q)}}\,\Big(\bm{\vartheta}_q^{(l)}-\bm{\kappa}_q\Big)\;\text{and}\;\log\text{-loss}
  \\[1ex]
  \mathcal{L}_{\mathcal{R}^{(\text{exint})}}=&\;\log{\Big(1+\mathcal{R}^{(\text{exint})}\Big)}\mapsto[+0,\infty]\;\text{, with gradient}
  \\[1ex]
  \nabla\,\mathcal{L}_{\mathcal{R}^{(\text{exint})}}=\frac{\partial\,\mathcal{L}_{\mathcal{R}^{(\text{exint})}}}{\partial\,\bm{\vartheta}_q^{(l)}}=&\;\frac{\nabla\,\mathcal{R}^{(\text{exint})}}{1+\mathcal{R}^{(\text{exint})}}\;\text{.}
\end{aligned}
$$

In practice, this regularizer is never zero, and it should probably not have a large weight, as otherwise, it would actually suppress time warping. Also, one may ask why use $\bm{\vartheta}_q^{(l)}$ and not $l_q$, $l'_q$ or $l_q^{(c)}$ instead. Internally, SRBTW converts boundaries to interval-lengths, and then finally to ratios that are then scaled back using the actual extent given by $\beta_u-\beta_l$. $\bm{\vartheta}_q^{(l)}$. It is best if the regularizer directly concerns the parameters. Ideally, the lengths passed are close to ratios. If no box-bounds regularizer is used, those might be far off.

### Regularize box-bounds

This is something that is not explicitly required for SRBTW, as the box-bounds for all intervals' lengths are adhered to through the self-regularization process. The same goes for begin and end (if open). So far, we have however always used box-bounds, and the only reason for that was that this usually comes with a speedup, because of the vastly reduced feasible region. When box-constrained optimization cannot be used, it may thus be advisable to add some (mild) regularization. In the following definition, the loss for each parameter that is outside the bounds is strictly positive. Hint: This regularizer assumes there is a lower- and upper boundary for each parameter. Should that not be the case for some or all parameters, their specific $i$-th lower or upper bound can be replaced with $-\infty,\infty$, respectively.

$$
\begin{aligned}
  \mathcal{R}^{(\text{bb})}=&\;\sum_{i=1}^{\norm{\bm{\theta}}}\;\mathcal{H}\Big(b_i^{(l)}-\bm{\theta}_i\Big)*\big(b_i^{(l)}-\bm{\theta}_i\big) + \mathcal{H}\Big(\bm{\theta}_i-b_i^{(u)}\Big)*\big(\bm{\theta}_i-b_i^{(u)}\big)\;\text{, where}
  \\[1ex]
  b_i^{(l)},b_i^{(u)}\dots&\;\text{are the lower and upper bound for parameter}\;\bm{\theta}_i\text{, with gradient}
  \\[1ex]
  \frac{\partial\,\mathcal{R}^{(\text{bb})}}{\partial\,\bm{\theta}_i}=&\;\mathcal{D}(b_i^{(l)}-\bm{\theta}_i)\times(\bm{\theta}_i-b_i^{(l)})+\mathcal{D}(\bm{\theta}_i-b_i^{(u)})\times(\bm{\theta}_i-b_i^{(u)})
  \\[0ex]
  &\;+\mathcal{H}(\bm{\theta}_i-b_i^{(u)})-\mathcal{H}(b_i^{(l)}-\bm{\theta}_i)\;\text{, where}
  \\[1ex]
  \mathcal{D}(x)=&\;\begin{cases}
    1,&\text{if}\;x=0\text{,}
    \\
    0,&\text{otherwise,}
  \end{cases}\;\text{the Dirac-delta function, and suggested loss}
  \\[1ex]
  \mathcal{L}_{\mathcal{R}^{(\text{bb})}}=&\;\log{\Big(1+\mathcal{R}^{(\text{bb})}\Big)}\mapsto[+0,\infty]\;\text{, with gradient}
  \\[1ex]
  \nabla\,\mathcal{L}_{\mathcal{R}^{(\text{bb})}}=\frac{\partial\,\mathcal{L}_{\mathcal{R}^{(\text{bb})}}}{\partial\,\bm{\theta}_i}=&\;\frac{\nabla\,\mathcal{R}^{(\text{bb})}}{1+\mathcal{R}^{(\text{bb})}}\;\text{.}
\end{aligned}
$$

Also, instead of or additionally to this regularizer, we can sum up the number of violations. This can then be used, e.g., as base, exponent or multiplier. At this point I would recommend to use this regularizer for all parameters, that is all lengths ($\bm{\vartheta}^{(l)}$) as well as $b,e$. Additionally, I recommend adding another instance of this regularizer for $b,e$ that uses $e-\gamma_d$ as upper boundary for $b$, and $b+\gamma_d$ as lower boundary for $e$. This regularizer should be able to help the model figure out good values more quickly, and it should speed up the process if $b>e$ (which is handled by the model but still not OK).


## Extension: Non-linear time warping

We can easily make sr-BTW use non-linear time warping. Given the model $m^c_q$, we replace the linear scaling function with an arbitrary function.

$$
\begin{aligned}
	\delta_q^{(t)}=&\;t_e^{(q)}-t_b^{(q)}\;\text{and}
	\\[1ex]
	\delta_q^{(s)}\equiv&\;l_q^{(c)}\;\text{,}
	\\[1ex]
	m^c_q(f,x,\dots)=&\;f\Bigg(\frac{\Big(x-t_b^{(q)}\Big)\times l_q^{(c)}}{\delta_q^{(t)}}+s_b^{(q)}\Bigg)\;\text{.}
\end{aligned}
$$

As we can see, the target-$x$ is transformed in three (four) steps:

* 1. -- Translation to $0$ ($x-t_b^{(q)}$),
* 2.(a) -- Normalization according to the target-interval/-extent ($\times{\delta_q^{(t)}}^{-1}$),
* 2.(b) -- Scaling to the source-interval/-extent ($\times l_q^{(c)}$),
* 3. -- Translation to offset in source-interval/-extent ($+s_b^{(q)}$).

So, really, the time warping happens in step __2__, when $x$ is scaled _within_ the source-interval: Currently, this is a linear function of the form $y=a*x+b$, where $b=0$, as the function goes through the origin, thanks to steps $1$/$3$. $a$ results from putting the source- and target-extents into relation, so we have no additional parameters to learn.

The linear function hence maps a target-$x$ to a source-$x$, where the support is $\Big[0\;,\;t_e^{(q)}-t_b^{(q)}\Big]$, and the co-domain is $\Big[0\;,\;s_e^{(q)}-s_b^{(q)}\Big]$. The linear function is also strictly monotonically increasing, which is a requirement as we will show later. If the function had saddle points, i.e., a slope of $0$, then this would mean no time warping is applied. A negative slope would result in negative time warping (i.e., while the target-$x$ moves forward in time, the source-$x$ would move backward), which is prohibited.

If we want to introduce arbitrary scaling functions, even non-linear functions, then it is helpful to introduce a few additional notions first:

$$
\begin{aligned}
	x'=&\;\Big(x-t_b^{(q)}\Big)\;\text{, pre-translation of}\;x\text{,}
	\\[1ex]
	s^{(\text{lin})}(x')=&\;x'\times\frac{\delta_q^{(s)}}{\delta_q^{(t)}}\;\text{, linear scaling used thus far,}
	\\[1ex]
	s^{(\text{nonlin})}(x')=&\;\dots\;\text{(some non-linear and strictly increasing scaling function),}
	\\[1ex]
	s'^{(\text{nonlin})}(x')=&\;\delta_q^{(s)}\times\frac{s^{(\text{nonlin})}(x')-s^{(\text{nonlin})}(0)}{s^{(\text{nonlin})}\Big(\underbrace{t_e^{(q)}-t_b^{(q)}}_{\delta_q^{(t)}}\Big)-s^{(\text{nonlin})}(0)}\;\text{, constrained version.}
\end{aligned}
$$

Any arbitrary scaling function $s$ needs to be constrained, such that its constrained version $s'$ satisfies the same properties as does the linear scaling function:

* The function is continuous,
* same support and co-domain as the linear function,
* goes through $[0,0]$ and $\Big[t_e^{(q)}-t_b^{(q)},\;s_e^{(q)}-s_b^{(q)}\Big]$,
* is strictly increasing.

Technically, it is not required that any scaling function is strictly increasing, only that $s(0)\prec s(x),\,\forall\,x>0$, so the function may have saddle-points. Otherwise, its constrained version would potentially divide by $0$.

If we now introduce arbitrary scaling functions (here using a constrained non-linear example) into our submodel formulation, we get:

$$
\begin{aligned}
	m_q^{c}(f,x',\dots)=&\;f\Big(s'^{(\text{nonlin})}\Big(x-t_b^{(q)}\Big)+s_b^{(q)}\Big)
\end{aligned}\;\text{.}
$$

Note that, while omitted here, any scaling function would be passed the pre-translated $x$ as $x'=x-t_b^{(q)}$, as well as all extent-parameters, namely $s_b^{(q)},s_e^{(q)},t_b^{(q)},t_e^{(q)}$. If the scaling function introduces parameters that should be optimized for, e.g., if it is a polynomial, then its gradient must reflect that, and hence the submodel's gradient does change, too. It would be perfectly fine to use a different kind of scaling function in each interval.

Example with a non-linear polynomial, where we want to capture the portion of the function in the interval $[0,1]$, which is then scaled to the current target-extent of $\Big[0,\delta_q^{(t)}\Big]$:

$$
\begin{aligned}
	s(x')=&\;\frac{mx'}{\delta_q^{(t)}}+\frac{nx'^2}{\delta_q^{(t)}}\;\text{,}
	\\[1ex]
	=&\;\frac{x'\times(m+nx')}{\delta_q^{(t)}}\;\text{, with constrained version}\;s'\text{:}
	\\[1ex]
	s'(x')=&\;\delta_q^{(s)}\times\frac{s\big(x'\big)-s(0)}{s\Big(\delta_q^{(t)}\Big)-s(0)}\;\text{,}
	\\[1ex]
	=&\;\frac{\delta_q^{(s)}\times s\big(x'\big)}{s\Big(\delta_q^{(t)}\Big)}\;\text{, as}\;s(0)=0\text{, fully expanded as}
	\\[1ex]
	=&\;\frac{\delta_q^{(s)}\times x'\times\big(m+nx'\big)}{\delta_q^{(t)}\times\Big(m+n\times\delta_q^{(t)}\Big)}\;\text{.}
\end{aligned}
$$

It is important that the scaling function covers some characteristic interval $[0,k]$, where $k>0$. The value of $k$ is not important, only that the interval starts with $0$. In other words, any non-constrained scaling function $s(x)$ needs to satisfy these criteria:

* The function is continuous,
* has a support of $[0,k]$, where $k>0$,
* is monotonically increasing and satisifies $s(0)\prec s(x),\,\forall\,x>0$.

The co-domain may arbitrary, as the constraining function re-maps it to $\Big[0,\delta_q^{(s)}\Big]$.

In the previous example, we chose a scaling function that has two learnable parameters, $m,n$. Also, the parameter $\delta_q^{(s)}\equiv\,l_q^{(c)}$, which depends on other parameters of the model. So, the complete gradient requires deriving for these three kinds of parameters, and we will omit the set of partial derivatives for the last one here:

$$
\begin{aligned}
	\nabla\,s'(x')=&\;\Bigg[\frac{\partial\,s}{\partial\,m}\;,\;\frac{\partial\,s}{\partial\,n}\;,\;\Bigg(\frac{\partial\,s}{\partial\,\delta_q^{(s)}}\Bigg)\Bigg]\;\text{,}
	\\[1ex]
	=&\;\Bigg[\frac{\delta_q^{(s)}\times n\times x'\times\Big(\delta_q^{(t)}-x'\Big)}{\delta_q^{(t)}\times\Big(m+n\times\delta_q^{(t)}\Big)^2}\;,\;\frac{\delta_q^{(s)}\times m\times x'\times\Big(x'-\delta_q^{(t)}\Big)}{\delta_q^{(t)}\times\Big(m+n\times\delta_q^{(t)}\Big)^2}\;,\;\dots\Bigg]\;\text{.}
\end{aligned}
$$



# Optimization goal II

This goal is actually specific to the current case, but in general the purpose is to a) verify, that the choice of data- and regularization-loss is apt for matching data that is expected to be somewhat similar to a given pattern, and b) to find upper- and/or lower-bounds (best/worst possible match) such that we can use the model for __scoring__ when fitting to real-world data.


An important step is to validate whether the selected sub-models and scores are suitable to detect and score the reference pattern. This can be verified by setting the query signal equal to the reference signal, i.e., $\text{WC} = \text{WP}$, and then having the multilevel model use starting intervals that are different from the reference intervals. Everything else is identical to the first optimization problem. If the selected data- and regularization-losses were chosen appropriately, the MLM converges to the reference intervals, i.e., each sub-model matches best what it should match. Ideally, this test is repeated a number of times, with randomly initialized query intervals.

## Scoring a loss

If we have the upper and lower bound for some loss $\mathcal{L}(\cdot)$, we can define a __score-operator__, $\mathsf{S}^{\oplus}$, as:

$$
\begin{aligned}
  \mathcal{L}(\cdot)\;\dots&\;\text{data- or regularization-loss, and also its gradient}\;\nabla\,\mathcal{L}(\cdot)\text{,}
  \\[1ex]
  \omega,\beta^{(l)},\beta^{(u)}\;\dots&\;\text{weight, lower- and upper bound for loss}\;\mathcal{L}(\cdot)\text{,}
  \\[1ex]
  \mathsf{S}^{\oplus}(\cdot)=&\;\omega\Bigg(1-\frac{\mathcal{L}(\cdot)-\beta^{(l)}}{\beta^{(u)}-\beta^{(l)}}\Bigg)\;\mapsto[+0,1]\;\text{,}
  \\[0ex]
  &\;\text{score-operator, where larger scores are better}\;(\oplus)\text{,}
  \\[1ex]
  \nabla\,\mathsf{S}^{\oplus}\big(\mathcal{L}_q(\cdot)\big)=&\;\frac{\omega\nabla\,\mathcal{L}_q(\cdot)}{\beta^{(l)}-\beta^{(u)}}\;\text{, gradient of this operator.}
\end{aligned}
$$

The score of some loss really just is a scaling of it into the range $[0,1]$, where, depending on the definition, zero or one is the best (in the previous definition $1$ would be the best, hence we wrote $\mathsf{S}^{\oplus}$ -- otherwise, we would use the notion $\mathsf{S}^{\ominus}$ for an operator where lower scores are better). If we choose not to negate the score by leaving out the $1-$, the gradient is almost the same, except for that we subtract the lower bound from the upper bound, i.e., $\beta^{(u)}-\beta^{(l)}$.


__Scoring?__ --- The goal is to align (to time-warp) some data to a given pattern. If we attempt to align the pattern to itself, using some unfavorable starting values, we would expect it to perfectly match when the optimization is done. But a match depends on how the loss is defined, and what is its weight. If a single variable in a single interval is captured by a singular loss, and that loss is appropriate for matching the variable, then it should ideally be $0$ (or whatever is the lowest possible value). Recall that __SRBTW__ is just a model for aligning two data series, it is up to the user chose an appropriate loss that is _adequate to pursue the user's objectives_. Such an adequate loss is one that reaches its global minimum in case of fitting the pattern to itself.

If two or more losses are combined by, e.g., a linear scalarizer, and if these losses pursue objectives that are somewhat conflicting with each other (not necessarily orthogonal), then the lower bound for that aggregation is larger than or equal to the sum of the singular minimal possible losses. It may not be trivial to determine the new lower bound for an aggregated loss. We can find this lower bound through optimization, and this is described in subsection \ref{ssec:match_wp}. In practice, the lowest possible aggregated loss lies on a __Pareto__-boundary and there may be many optimal losses on that boundary.

As for the upper bound of an aggregated loss, we too will have a Pareto-boundary. However, we cannot determine it computationally. The compromise is to sum up the worst possible losses of all singular losses. There are some losses that have a finite upper bound, and some that only have $\infty$ as upper bound. However, our goal is to come up with losses that have a well-defined upper bound for our problem, and these are presented in the next sub-section.


## Aggregation of scores\label{ssec:score_agg}

The mean aggregation operator for scores calculates a weighted sum. However, the average score is achieved simply by dividing by the number of scores. We suggest a slightly altered version which normalizes w.r.t. the _magnitude_ of the weights, and not just their amount.

$$
\begin{aligned}
  \mathsf{A}\Big(\bm{\mathsf{S}}^{\oplus},\bm{\omega}\Big)=&\;\Bigg(\bigg[\prod_{i=1}^{\norm{\bm{\mathsf{S}}^{\oplus}}}\,1+\bm{\omega}_i\bm{\mathsf{S}}^{\oplus}_i(\cdot)\bigg]-1\Bigg)\times\underbrace{\Bigg(\bigg[\prod_{i=1}^{\norm{\bm{\omega}}}\,1+\bm{\omega}_i\bigg]-1\Bigg)^{-1}}_{k}\;\text{,}
  \\[0ex]
  &\;\text{where}\;0<\bm{\omega}_i\leq1\;\land\;\norm{\bm{\mathsf{S}}^{\oplus}}=\norm{\bm{\omega}}\;\text{, i.e., one weight for each score,}
  \\[1ex]
  =&\;k\times\bigg[\prod_{i=1}^{\norm{\bm{\mathsf{S}}^{\oplus}}}\,1+\bm{\omega}_i\bm{\mathsf{S}}^{\oplus}_i(\cdot)\bigg]-k\;\text{, with gradient}
  \\[1ex]
  \nabla\,\mathsf{A}\Big(\bm{\mathsf{S}}^{\oplus},\bm{\omega}\Big)=&\;k\times\Bigg[\sum_{i=1}^{\norm{\bm{\mathsf{S}}^{\oplus}}}\,\bm{\omega}_i\nabla\,\bm{\mathsf{S}}^{\oplus}_i(\cdot)\times\bigg[\prod_{j=1}^{\norm{\bm{\mathsf{S}}^{\oplus}}}\,1+\bm{\omega}_j\bm{\mathsf{S}}^{\oplus}_j(\cdot),\;\forall\,j\neq i\,\bigg]\Bigg]\;\text{.}
\end{aligned}
$$


## Losses with finite upper bounds

Early on I had defined losses (back then called metrics) that all had one thing in common: their finite upper bound was either global, or we could pose our problem in a way such that a local finite upper bound could be specified. For many such metrics, this was done by scaling the data within an interval into the __unit-square__. In the following, we present some of these metrics, redefined as losses, together with their upper and lower local bounds. In general, we exploit the fact that each interval of the Warping Pattern is delimited by some rectangular area, and within that area, which not necessarily needs to be a (unit-)square, many definable losses have a clear upper bound.

Some of the scores will exploit the fact that in time-warping, the degrees of freedom are on the time- or x-axis only. When we define an optimization problem using SRBTW, we usually know the lower- and upper bounds of the y-axis. Even later for optimization goal III, when we allow the y-values to adjust, this will happen within __previously specified box-bounds__. Some of the losses described below will exploit these facts.

### Area between curves

This was one of the very first metrics I had implemented. The area between curves (ABC) within an interval has been proven to be quite useful. It is easy to compute, both for the continuous and discrete case. The discrete case is equivalent to the mean absolute error and approaches the continuous loss with sufficiently large number of samples. The ABC exploits the lower and upper bound for the y-axis (known a priori), and scales the rectangle of an interval using the current Warping Pattern's interval's extent ($\delta_q$ below), such that the loss is always computed in a rectangle that always has the same size.

$$
\begin{aligned}
  y_q^{\min},y_q^{\max}\;\dots&\;\text{lower and upper bounds for the }y\text{-axis in the }q\text{-th WP-interval,}
  \\[1ex]
  \mathbf{x}_q^{(\text{WP})}\;\dots&\;\text{its support (going from}\;\bm{\theta}_{q+1}\;\text{to}\;\bm{\theta}_{q}\text{),}
  \\[1ex]
  \delta_q=&\;\bm{\theta}_{q+1}-\bm{\theta}_{q}\equiv\max{\Big(\mathbf{x}_q^{(\text{WP})}\Big)}-\min{\Big(\mathbf{x}_q^{(\text{WP})}\Big)}\;\text{, the extent of the support,}
  \\[1ex]
  \rho_q=&\;\delta_q\times\big(y_q^{\max}-y_q^{\min}\big)\;\text{, the total area of that interval,}
  \\[1ex]
  \rho_q^{(\text{WP})}=&\;\int\displaylimits_{\mathbf{x}_q^{(\text{WP})}}\,r(x)-y_q^{\min}\,dx\;\text{, note that}\;\forall\,x\to y_q^{\min}\leq r(x)\leq y_q^{\max}\text{, or}
  \\[1ex]
  \rho_q^{(\text{WP}_d)}=&\;\frac{1}{N}\sum_{i=1}^{N}\,r\Big(\mathbf{x}_{q,i}^{(\text{WP})}\Big)\;\text{,}
  \\[0ex]
  &\;\text{the (discrete) area between the reference signal (WP) and the x-/time-axis (at}\;y_q^{\min}\text{),}
  \\[1ex]
  \beta_l=&\;0\;\text{, the metric's lower bound, which is always}\;0\text{,}
  \\[1ex]
  \beta_u=&\;\max{\bigg(\rho_q^{\big(\text{WP}_{(d)}\big)},\rho_q-\rho_q^{\big(\text{WP}_{(d)}\big)}\bigg)}\;\text{, the metric's upper bound,}
  \\[1em]
  \mathcal{L}_q^{(\text{ABC}_c)}=&\;\beta_u^{-1}\times\int\displaylimits_{\mathbf{x}_q^{(\text{WP})}}\,\norm{r(x_q)-m_q^{(c)}\Big(x_q\Big)}\;\text{, the continous ABC-loss,}
  \\[1ex]
  \mathcal{L}_q^{(\text{ABC}_d)}=&\;\frac{1}{N\times\beta_u}\sum_{i=1}^{N}\,\norm{r\Big(\mathbf{x}_{q,i}^{(\text{WP})}\Big)-m_q^{(c)}\Big(\mathbf{x}_{q,i}^{(\text{WC})}\Big)}\;\text{, (discrete case), with gradient}
  \\[1ex]
  \nabla\,\mathcal{L}_q^{(\text{ABC}_d)}=&\;\frac{1}{N\times\beta_u}\sum_{i=1}^{N}\,\frac{\nabla\,m_q^{(c)}\Big(\mathbf{x}_{q,i}^{(\text{WC})}\Big)\times\bigg(m_q^{(c)}\Big(\mathbf{x}_{q,i}^{(\text{WC})}\Big)-r\Big(\mathbf{x}_{q,i}^{(\text{WP})}\Big)\bigg)}{\norm{m_q^{(c)}\Big(\mathbf{x}_{q,i}^{(\text{WC})}\Big)-r\Big(\mathbf{x}_{q,i}^{(\text{WP})}\Big)}}\;\text{.}
\end{aligned}
$$

Note that no scaling back of the intervals is required, because the discrete version samples equidistantly with equally many samples ($N$) from both, the WP and the WC, within their current intervals. In the continuous case, the supports of WP and WC are identical, as that is what SRBTW does (move a candidate-interval to the pattern-interval). The discrete ABC loss is also known as Sum of absolute differences (SAD), $L_1$-norm, _Manhattan_- or Taxicab-norm.


### Residual sum of squares

RSS, also known as sum of squared distance (SSD) and probably as many more synonyms, is usually always the first loss I use, as it is computationally cheap, easy to compute, fast and robust (delivers good results in many cases). Until now I had not been thinking about upper boundaries for it. However, with regard to that, it should be very similar to the ABC-loss, as the RSS' behavior is asymptotic. When, as in our case, we know the maximum y-extent, together with the (constant) support.

$$
\begin{aligned}
  \rho_q=&\;\delta_q\times\big(y_q^{\max}-y_q^{\min}\big)^2\;\text{, the maximum possible loss in the WP-interval's area,}
  \\[1ex]
  \rho_q^{(\text{WP})}=&\;\int\displaylimits_{\mathbf{x}_q^{(\text{WP})}}\,\Big(r(x)-y_q^{\min}\Big)^2\;\text{, note that}\;\forall\,x\to y_q^{\min}\leq r(x)\leq y_q^{\max}\text{, or}
  \\[1ex]
  \rho_q^{(\text{WP}_d)}=&\;\frac{1}{N}\sum_{i=1}^{N}\,\bigg(r\Big(\mathbf{x}_{q,i}^{(\text{WP})}\Big)-y_q^{\min}\bigg)^2\;\text{,}
  \\[0ex]
  &\;\text{the (discrete) squared loss between the reference signal (WP) and the x-/time-axis (at}\;y_q^{\min}\text{),}
  \\[1ex]
  \beta_l,\beta_u\;\dots&\;\text{(no change),}
  \\[1em]
  \mathcal{L}_q^{(\text{RSS}_c)}=&\;\beta_u^{-1}\times\int\displaylimits_{\mathbf{x}_q^{(\text{WP})}}\,\bigg(r(x_q)-m_q^{(c)}\Big(x_q\Big)\bigg)^2\;\text{, the continous RSS-loss,}
  \\[1ex]
  \mathcal{L}_q^{(\text{RSS}_d)}=&\;\frac{1}{N\times\beta_u}\sum_{i=1}^{N}\,\bigg(r\Big(\mathbf{x}_{q,i}^{(\text{WP})}\Big)-m_q^{(c)}\Big(\mathbf{x}_{q,i}^{(\text{WC})}\Big)\bigg)^2\;\text{, (discrete case), with gradient}
  \\[1ex]
  \nabla\,\mathcal{L}_q^{(\text{RSS}_d)}=&\;-\frac{2}{N\times\beta_u}\sum_{i=1}^{N}\,\nabla\,m_q^{(c)}\Big(\mathbf{x}_{q,i}^{(\text{WC})}\Big)\times\bigg(r\Big(\mathbf{x}_{q,i}^{(\text{WP})}\Big)-m_q^{(c)}\Big(\mathbf{x}_{q,i}^{(\text{WC})}\Big)\bigg)\;\text{.}
\end{aligned}
$$

The difference really just is to replace the norm with a square, and the then-simplified gradient.

### More simple metrics

For more simple metrics see [^1]. However, we will focus on metrics that we have upper bounds for.

[^1]: https://web.archive.org/web/20200701202102/https://numerics.mathdotnet.com/distance.html

$$
\begin{aligned}
  {\text{SSD/RSS}} &: (x, y) \mapsto \|x-y\|_2^2 = \langle x-y, x-y\rangle = \sum_{i=1}^{n} (x_i-y_i)^2
  \\[0ex]
  {\text{MAE}} &: (x, y) \mapsto \frac{d_{\mathbf{SAD}}}{n} = \frac{\|x-y\|_1}{n} = \frac{1}{n}\sum_{i=1}^{n} \norm{x_i-y_i}
  \\[0ex]
  {\text{MSE}} &: (x, y) \mapsto \frac{d_{\mathbf{SSD}}}{n} = \frac{\|x-y\|_2^2}{n} = \frac{1}{n}\sum_{i=1}^{n} (x_i-y_i)^2
  \\[0ex]
  {\text{Euclidean}} &: (x, y) \mapsto \|x-y\|_2 = \sqrt{d_{\mathbf{SSD}}} = \sqrt{\sum_{i=1}^{n} (x_i-y_i)^2}
  \\[0ex]
  {\infty\text{/Chebyshev}} &: (x, y) \mapsto \|x-y\|_\infty = \lim_{p \rightarrow \infty}\bigg(\sum_{i=1}^{n} |x_i-y_i|^p\bigg)^\frac{1}{p} = \max_{i} \norm{x_i-y_i}
  \\[0ex]
  {\text{p/Minkowski}} &: (x, y) \mapsto \|x-y\|_p = \bigg(\sum_{i=1}^{n} \norm{x_i-y_i}^p\bigg)^\frac{1}{p}
  \\[0ex]
  {\text{Canberra}} &: (x, y) \mapsto \sum_{i=1}^{n} \frac{\norm{x_i-y_i}}{\norm{x_i}+\norm{y_i}}
  \\[0ex]
  {\text{cosine}} &: (x, y) \mapsto 1-\frac{\langle x, y\rangle}{\|x\|_2\|y\|_2} = 1-\frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2}\sqrt{\sum_{i=1}^{n} y_i^2}}
  \\[0ex]
  {\text{Pearson}} &: (x, y) \mapsto 1 - \text{Corr}(x, y)
\end{aligned}
$$

I have copy-pasted these here, and we should check later for which of these there may be an upper bound.


### Correlation between curves

This is also one I have used early on, and also as a score. The correlation is an intuitively elegant loss for estimating whether two curves have a similar shape. If one curve was to follow the shape of the other, the correlation would be high, a strongly negative correlation would mean one curve exhibits the opposite behavior. The correlation is between $[1,-1]$. While in some cases we would not be interested in negative correlation (and hence treat any correlation $\leq 0$ as no correlation), I think that in many cases it is better for the _training_ of our model to not do that. The correlation is a good candidate for combination with other losses that, for example, measure the difference in magnitude (like ABC or RSS), as it itself does not care for that. The correlation is a __ratio-loss__, and does not depend on local finite upper bounds, it thus has __global__ bounds.

When I say correlation, I mean _Pearson_'s __sample__-correlation, just to be clear. In the following, we define the discrete version only, and we do it in a way that this loss is strictly positive, like the others presented so far.

$$
\begin{aligned}
  \operatorname{cor}(\mathbf{a},\mathbf{b})=&\;\frac{\sum_{i=1}^N\,(\mathbf{a}_i-\mean{\mathbf{a}})\times(\mathbf{b}_i-\mean{\mathbf{b}})}{\sqrt{\sum_{i=1}^{N}\,(\mathbf{a}_i-\mean{\mathbf{a}})^2}\times\sqrt{\sum_{i=1}^{N}\,(\mathbf{b}_i-\mean{\mathbf{b}})^2}}
  \\[1ex]
  =&\;\operatorname{cor}\bigg(r\Big(\mathbf{x}_{q}^{(\text{WP})}\Big)\;,\;m_q^{(c)}\Big(\mathbf{x}_{q}^{(\text{WC})}\Big)\bigg)
  \\[1ex]
  =&\;\operatorname{cor}\Bigg(\frac{\sum_{i=1}^N\,\bigg(r\Big(\mathbf{x}_{q,i}^{(\text{WP})}\Big)-\mean{r\Big(\mathbf{x}_{q}^{(\text{WP})}\Big)}\bigg)\times\bigg(m_q^{(c)}\Big(\mathbf{x}_{q,i}^{(\text{WC})}\Big)-\mean{m_q^{(c)}\Big(\mathbf{x}_{q}^{(\text{WC})}\Big)}\bigg)}{\sqrt{\sum_{i=1}^{N}\,\bigg(r\Big(\mathbf{x}_{q,i}^{(\text{WP})}\Big)-\mean{r\Big(\mathbf{x}_{q}^{(\text{WP})}\Big)}\bigg)^2}\times\sqrt{\sum_{i=1}^{N}\,\bigg(m_q^{(c)}\Big(\mathbf{x}_{q,i}^{(\text{WC})}\Big)-\mean{m_q^{(c)}\Big(\mathbf{x}_{q}^{(\text{WC})}\Big)}\bigg)^2}}\Bigg)\;\text{.}
\end{aligned}
$$

The gradient of this, using some substitutions for the _constant_ Warping Pattern, is:

$$
\begin{aligned}
  \bm{\tau}=&\;\sum_{i=1}^N\,\bigg(r\Big(\mathbf{x}_{q,i}^{(\text{WP})}\Big)-\mean{r\Big(\mathbf{x}_{q}^{(\text{WP})}\Big)}\bigg)\;\text{, and}
  \\[1ex]
  \bm{\gamma}_i=&\;m_q^{(c)}\Big(\mathbf{x}_{q,i}^{(\text{WC})}\Big)\;\text{,}
  \\[1ex]
  \bm{\hat{\gamma}}_i=&\;\nabla\,m_q^{(c)}\Big(\mathbf{x}_{q,i}^{(\text{WC})}\Big)\;\text{,}
  \\[1ex]
  \rho=&\;\mean{m_q^{(c)}\Big(\mathbf{x}_{q}^{(\text{WC})}\Big)}
  \\[1ex]
  \hat{\rho}=&\;\mean{\nabla\,m_q^{(c)}\Big(\mathbf{x}_{q}^{(\text{WC})}\Big)}
  \\[1ex]
  \nabla\,\operatorname{cor}(\cdot)=&\;\Bigg(\frac{\Big[\sum_{i=1}^N\,\bm{\tau}\times\big(\bm{\hat{\gamma}}_i-\hat{\rho}\big)\Big]\times\sqrt{\sum_{i=1}^N(\bm{\gamma}_i-\rho)^2}}{\sqrt{(\bm{\tau}^\top\bm{\tau})}\times\Big[\sum_{i=1}^N\,\big(\bm{\gamma}_i-\rho\big)\times\big(\bm{\hat{\gamma}}_i-\bm{\hat{\gamma}}_i\times\hat{\rho}\big)\Big]}\Bigg)\;\text{.}
\end{aligned}
$$

The actual loss shall map to $\mathcal{L}^{(\text{Corr})}\to[0+,2]$, so we define it as:

$$
\begin{aligned}
  \mathcal{L}_q^{(\text{RSS})}=&\;1-\operatorname{cor}(\cdot)\;\text{, with gradient}
  \\[1ex]
  \nabla\,\mathcal{L}_q^{(\text{RSS})}=&\;-\nabla\operatorname{cor}(\cdot)\;\text{.}
\end{aligned}
$$

### Ratio between curves' arc-lengths

Comparing the arc-lengths of two curves is the next loss that we can define such that it has a global upper bound. This is true for any loss that is a __ratio__. First, we obtain two measurements, one from the WP and one from the WC (here: the arc-length). We then set both measurements into a relation that we subtract from $1$. We make sure that each ratio ideally is $1$, and goes to $0$ the worse it gets. This is guaranteed by always dividing the minimum of both measurements by the maximum.

The arc-length can be obtained continuously and discretely.

$$
\begin{aligned}
  \operatorname{arclen}_{(c)} f(x)=&\;\int\displaylimits_{\mathbf{x}_q^{(\text{WP})}}\,\sqrt{1+\bigg[\frac{\partial}{\partial\,x}f(x)\bigg]^2}dx\;\text{,}
  \\[0ex]
  &\;\text{continuous arc-length of some function}\;f\;\text{with support}\;\mathbf{x}_q^{(\text{WP})}\text{,}
  \\[1ex]
  \operatorname{arclen}_{(d)} f(x)=&\;\lim_{N\to\infty}\,\sum_{i=1}^N\,\norm{f(\delta_i)-f(\delta_{i-1})}\;\text{, where}
  \\[0ex]
  \delta_i=&\;\min{\big(\operatorname{supp}(f)\big)}+i\times\Big(\max{\big(\operatorname{supp}(f)\big)}-\min{\big(\operatorname{supp}(f)\big)}\Big)\times N^{-1}\;\text{, with gradient}
  \\[1ex]
  \nabla\,\operatorname{arclen}_{(d)}f(x)=&\;\lim_{N\to\infty}\,\sum_{i=1}^N\,\frac{\Big(f(\delta_i)-f(\delta_{i-1})\Big)\times\Big(f'(\delta_i)-f'(\delta_{i-1})\Big)}{\norm{f(\delta_i)-f(\delta_{i-1})}}\;\text{.}
\end{aligned}
$$

The arc-length is the first __ratio__-loss, so we will define a generic ratio-loss where we can plug in any other loss that measures the same property of two signals and puts them into relation. The actual loss is then defined as:

$$
\begin{aligned}
  \mathcal{L}^{(\text{ratio})}=&\;1-\frac{\min{\Big(\mathcal{L}^{(\text{WP})}(\cdot)}\;,\;\mathcal{L}^{(\text{WC})}(\cdot)\Big)}{\max{\Big(\mathcal{L}^{(\text{WP})}(\cdot)}\;,\;\mathcal{L}^{(\text{WC})}(\cdot)\Big)}\;\mapsto[+0,1]\;\text{,}
  \\[0ex]
  &\;\text{where}\;\mathcal{L}^{(\text{WP})}(\cdot)>0\;,\;\mathcal{L}^{(\text{WC})}(\cdot)>0\;\text{measure some property/loss of the}
  \\[0ex]
  &\;\text{Warping Pattern and -Candidate, with gradient}
  \\[1ex]
  \nabla\,\mathcal{L}_q^{(\text{ratio})}=&\;\mathcal{H}\Big(\mathcal{L}^{(\text{WP})}(\cdot)-\mathcal{L}^{(\text{WC})}(\cdot)\Big)\times\frac{\mathcal{L}^{(\text{WC})}(\cdot)\times \nabla\,\mathcal{L}^{(\text{WP})}(\cdot)-\mathcal{L}^{(\text{WP})}(\cdot)\times \nabla\,\mathcal{L}^{(\text{WC})}(\cdot)}{\Big[\mathcal{L}^{(\text{WP})}(\cdot)\Big]^2}
  \\[1ex]
  &\;+\mathcal{H}\Big(\mathcal{L}^{(\text{WC})}(\cdot)-\mathcal{L}^{(\text{WP})}(\cdot)\Big)\times\frac{\mathcal{L}^{(\text{WP})}(\cdot)\times\nabla\,\mathcal{L}^{(\text{WC})}(\cdot)-\mathcal{L}^{(\text{WC})}(\cdot)\times \nabla\,\mathcal{L}^{(\text{WP})}(\cdot)}{\Big[\mathcal{L}^{(\text{WC})}(\cdot)\Big]^2}\;\text{.}
\end{aligned}
$$

We used the Heaviside step function to check the condition $\mathcal{L}^{(\text{WP})}(\cdot)>\mathcal{L}^{(\text{WC})}(\cdot)$ (and vice versa). It is now straightforward to see that we can effortlessly plug in the arc-length operator defined previously. We have defined the ratio-loss such that a perfect ratio of $\frac{1}{1}$ results in a loss of $0$. As the denominator approaches $\infty$, the loss approaches $1$, i.e., $\lim_{\text{denom}\to\infty}\mathcal{L}^{(\text{ratio})}=1$. The ratio-loss is __only defined for strictly positive__ losses.

Here is a short list with more properties that may be useful for a ratio comparison:

* Standard deviation, Variance
* Signal measures: RMS, Kurtosis, Impulse-factor, Peak (Peak for for strictly positive signals)
* Any other unary aggregation that is positive (or an absolute value can be obtained), for example $\min,\max$, mean, median etc.
* discrete/differential Entropy (but we can define binary entropy-based losses, such as KL- and JSD-divergence, joint-/cross-entropy, mutual information etc.)

### Jensen--Shannon divergence

Most scores so far were _low-level_ or mid-level, meaning that they capture usually just a single simple property of a signal. Especially losses that discard many properties are low-level, such as $\min,\max$ for example. The area between curves, correlation and arc-length ratio already are somewhat mid-level, as they aggregate more than one (or a few) properties into a loss. Early on I had tried to use entropy-based losses and -divergences. Coming across the Kullback-Leibler divergence I early saw the need for such losses that have an upper bound. The (symmetric) KL-divergence only has application specific bounds, and I cannot see how to develop one in our case. However, the Jenson--Shannon divergence will compare two probability distributions, and does so with a __global upper bound__ of $\ln{(2)}$ (or $1$ when using the base-2 logarithm). Since it is a divergence, the global lower bound is $0$ in any case, i.e., no divergence.

Using entropy-based measures technically requires the variables compared to be probability distributions (or functions thereof). This means that the sum/integral of these must be $1$. As of our examples, we have some modeled some variables using probability densities, and some are just metrics (univariate events). Strictly speaking, such measurements are not applicable at all for the latter type. Also, for the former type, we would strictly speaking still need to consider any variable in question as a partial probability distribution, as we are looking at a specific interval of it. In practice so far, however, it turns out that for both kind of variables we can obtain excellent results by ignoring the nature of the variable and simply treating it as discrete or continuous probability distribution, as long as we ascertain that for any realization it is strictly positive and that it sums up/integrates to $1$. That means that even if we look at a sub-support (interval) of a variable, we treat that current interval as the entire probability distribution of the variable in question. This approach is probably further justified by how SRBTW works: Given a Warping Pattern, we sub-divide it using come __constant__ boundaries into __independent__ intervals. For the scope of any single loss, the other intervals do not exist, so that we can view the captured variable as the entire probability distribution of it.

I had previously also used entropy-based measures with success, esp. the mutual information (MI). It may be worth looking more into such available measures. However, those can be quite similar, and with the JSD we already have a well-working measure. The JSD has been used previously in Generative Adversarial Nets to minimize the divergence between probability distributions [@goodfellow2014generative].

In the following, we will first define how to normalize two continuous or discrete signals (note that mixed mode is not supported) such that they sum up/integrate to $1$. Then we will define the Jensen--Shannon divergence for either type.

$$
\begin{aligned}
  \mathsf{N}^{(\text{c})}(f,\mathbf{x}^{(\text{supp})},a)=&\;f(a)\times\bigg[\int\displaylimits_{\mathbf{x}^{(\text{supp})}}\,f(x)\bigg]^{-1}\;\text{, where}\;\min{\big(\mathbf{x}^{(\text{supp})}\big)}\leq a\leq\max{\big(\mathbf{x}^{(\text{supp})}\big)}\;\land\;\forall\,f(a)\geq 0\;\text{,}
  \\[1ex]
  &\;\text{(continuous to-probability transform operator),}
  \\[1ex]
  \mathsf{N}^{(\text{d})}(\mathbf{y})=&\;\frac{\mathbf{y}-\min{(\mathbf{y})}+1}{\sum_{i=1}^{\norm{\mathbf{y}}}\,\big[\mathbf{y}-\min{(\mathbf{y})}+1\big]_i}\;\text{,}
  \\[1ex]
  &\;\text{(discrete to-probability transform operator).}
\end{aligned}
$$

In practice, so far, we get more robust results with discrete vectors (typically using between $1e3$ to $1e4$ samples), even when those were sampled from the same functions we would use in the continuous case. Therefore, we will (for now at least) only show the discrete JSD.

$$
\begin{aligned}
  \mathbf{y}^{(\text{n})}=&\;\mathsf{N}^{(\text{d})}(\mathbf{y})\;\text{, and}
  \\[0ex]
  \mathbf{\hat{y}}^{(\text{n})}=&\;\mathsf{N}^{(\text{d})}(\mathbf{\hat{y}})\;\text{, to-probability normalized samples from two signals,}
  \\[1ex]
  \operatorname{KL}_{(\text{d,symm})}(\infdiv{\mathbf{y}}{\mathbf{\hat{y}}})=&\;\sum_{i=1}^{\norm{\mathbf{y}}}\,\mathbf{y}_i\times\log{\bigg(\frac{\mathbf{y}_i}{\mathbf{\hat{y}}_i}\bigg)}+\mathbf{\hat{y}}_i\times\log{\bigg(\frac{\mathbf{\hat{y}}_i}{\mathbf{y}_i}\bigg)}
  \\[1ex]
  \operatorname{JSD}_{(\text{d})}(\infdiv{\mathbf{y}}{\mathbf{\hat{y}}})=&\;\frac{1}{2}\operatorname{KL}_{(\text{d,symm})}\Big(\infdiv{\mathbf{y}}{\frac{\mathbf{y}+\mathbf{\hat{y}}}{2}}\Big)+\frac{1}{2}\operatorname{KL}_{(\text{d,symm})}\Big(\infdiv{\mathbf{\hat{y}}}{\frac{\mathbf{y}+\mathbf{\hat{y}}}{2}}\Big)\;\text{.}
\end{aligned}
$$


## Match WP against itself\label{ssec:match_wp}

The WP can be matched best by the reference pattern itself, i.e., nothing matches the reference better than it itself. A side-effect of this optimization goal thus is to obtain the maximum possible score, given the selected sub-models and their losses. The maximum score may then later be used to normalize scores when fitting the then \emph{calibrated} MLM to actual project data, as now we have upper bounds for all scores.

```{r echo=FALSE}
# Let's define a function that we use to test:
cow_og2 <- function(vtl, tb = seq(0, 1, length.out = length(vtl) + 1), b = 0, e = 1, ob = FALSE, oe = FALSE, lossFunc = dataLoss_RSS) {
  srbtw <- SRBTW$new(
    wp = r, wc = r,
    theta_b = tb,
    gamma_bed = c(0, 1, 0),
    lambda = rep(0, length(vtl)),
    begin = b, end = e,
    openBegin = ob, openEnd = oe)
  
  params <- c(vtl, if (ob) b else c(), if (oe) e else c())
  srbtw$setAllParams(params = params)
  
  dataLoss <- SRBTW_DataLoss$new(
    srbtw = srbtw, intervals = 1:length(vtl),
    weight = 1, continuous = FALSE, params = params)
  
  soo <- SRBTW_SingleObjectiveOptimization$new(srbtw = srbtw)
  soo$addObjective(obj = dataLoss)
  dataLoss$setLossFunc(lossFunc = lossFunc)
  
  
  library(optimParallel)
    
  cl <- parallel::makePSOCKcluster(parallel::detectCores())
  parallel::clusterExport(cl, varlist = c(
    "srbtw", "r", "f", "objF", "stat_diff_2_functions", "stat_diff_2_functions_cor",
    "stat_diff_2_functions_cor_score", "area_diff_2_functions", "area_diff_2_functions_score",
    "stat_diff_2_functions_symmetric_JSD_score",
    "stat_diff_2_functions_symmetric_JSD_sampled",
    "stat_diff_2_functions_philentropy_sampled", "SRBTW_Loss", "SRBTW_DataLoss",
    "SRBTW_SingleObjectiveOptimization", "SRBTW_SubModel", "soo"))
  
  optRp <- doWithParallelClusterExplicit(cl = cl, expr = {
    set.seed(1337)
    optimParallel::optimParallel(
      par = params,
      fn = function(x) {
        soo$setParams(params = x)
        loss <- soo$compute()
        if (is.na(loss) || !is.finite(loss)) {
          stop(paste0(x, collapse = ", "))
        }
        loss
      },
      lower = rep(0, length(vtl) + (if (ob) 1 else 0) + (if (oe) 1 else 0)),
      upper = rep(1, length(vtl) + (if (ob) 1 else 0) + (if (oe) 1 else 0)),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
  
  list(
    opt = optRp,
    srbtw = srbtw,
    vtl = vtl,
    tb = tb,
    b = b,
    e = e,
    ob = ob,
    oe = oe
  )
}
```

As I wrote before, let's make some tests where we use random values as starting parameters and check whether the model can converge to the pattern. We make tests using equidistantly-spaced and randomized lengths.

```{r}
cow_og2_test1p <- loadResultsOrCompute(file = "../results/cow_og2_test1p.rds", computeExpr = {
  set.seed(1337)
  useTests <- list(
    list(p = rep(1/20, 20), n = "eq-20"),
    list(p = rep(1/40, 40), n = "eq-40"),
    list(p = runif(20), n = "unif-20"),
    list(p = runif(40), n = "unif-40"),
    list(p = rbeta(20, 1, 2), n = "beta_1_2-20"),
    list(p = rbeta(40, 1, 2), n = "beta_1_2-40"),
    
    list(p = rep(1/30, 30), b = runif(1), n = "eq_ob-30"),
    list(p = rep(1/20, 20), e = runif(1), n = "eq_oe-20"),
    list(p = rep(1/20, 20), b = runif(1, max = .5), e = runif(1, min = .5), n = "eq_ob_oe-20"),
    list(p = rbeta(40, 1, 2), b = runif(1, max = .5), e = runif(1, min = .5), n = "beta_1_2_ob_oe-40")
  )
  
  for (i in seq_len(length.out = length(useTests))) {
    u <- useTests[[i]]
    print(paste("Calculating", u$n, "..."))
    params <- list(vtl = u$p)
    params["b"] <- if ("b" %in% names(u)) u$b else 0
    params["e"] <- if ("e" %in% names(u)) u$e else 1
    params["ob"] <- "b" %in% names(u)
    params["oe"] <- "e" %in% names(u)
    
    u$r <- do.call(what = cow_og2, args = params)
    useTests[[i]] <- u
  }
  
  useTests
})
```

```{r echo=FALSE}
library(ggplot2)
library(ggpubr)

cow_og2_test1p_plots <- lapply(cow_og2_test1p, function(u) {
  # We do this here because the previous tests were
  # conducted before the plot-methods existed.
  p <- u$r$srbtw$.__enclos_env__$private
  s <- SRBTW$new(
    wp = p$wp, wc = p$wc, theta_b = p$thetaB, gamma_bed = p$gamma_bed,
    lambda = p$lambda, begin = p$begin, end = p$end,
    openBegin = p$openBegin, openEnd = p$openEnd)
  
  s$setAllParams(params = u$r$opt$par)
  pd <- s$plot_dual_original(vOffset = 2)
  pd + labs(subtitle = u$n)
})

ggarrange(
  ncol = 2,
  nrow = 2,
  plotlist = head(cow_og2_test1p_plots, 4),
  common.legend = TRUE
)

ggarrange(
  ncol = 2,
  nrow = 2,
  plotlist = tail(head(cow_og2_test1p_plots, 8), 4),
  common.legend = FALSE
)

ggarrange(
  ncol = 2,
  nrow = 1,
  plotlist = tail(cow_og2_test1p_plots, 2),
  common.legend = TRUE
)
```

In all of the above cases, I used the discrete RSS data-loss, which itself uses a constant number of samples of $250$ per interval. Also I did __not__ use any regularization. This means, for example, that even a very short interval will be sampled from $250$ times, and if, for example, the WP interval was in the Long Stretch, and the WC interval is too, but it is much shorter, the loss will be equally low because of this setup. Especially when using open begin and/or end, regularization is strongly suggested

The first two equidistantly-spaced examples resulted in perfect fits, no real surprise there. Examples 3 and 4 are quite OK, but they have a couple of intervals that are very short. Within the Long Stretch, it is virtually impossible for any of the intervals there to find a good match, as the function looks the same to the left and to the right (i.e., the gradient). Where the WP is not flat, the intervals actually find a good match again (i.e., we observe vertical lines in the non-flat phases). Examples 5 and 6 show the same behavior (`beta_1_2-20` and `beta_1_2-40`). It is worth noting that these two achieved quite a low error, which again is a hint at how ill-posed our setup here was (see previous paragraph).

In examples 7 and 8 we use an open begin and end, respectively. In example 7, we see a perfect beginning with the _Fire Drill_ phase. Before that, almost all intervals have a loss of zero (except for the first ~3-4). This is because of the peculiarity I mentioned earlier ($250$ samples no matter the length). The begin was set initially at `r round(cow_og2_test1p[[7]]$b, 4)` and did not move much -- I suppose it was just too far off for the optimization to find the real beginning __without__ regularization. I suppose the same is true for example number 8, although that the starting-parameter for the end was `r round(cow_og2_test1p[[8]]$e, 4)`. However, that starting point is already slightly in the Fire Drill phase, and mapped initially to the last interval, which is flat. So the optimization probably moved it farther to the left to decrease that loss, as it would have gotten much stronger otherwise.

If we divide the loss by the number of intervals, we can actually make apples-to-apples comparisons.

```{r echo=FALSE}
cow_og2_test1p_stats <- NULL

for (i in seq_len(length.out = length(cow_og2_test1p))) {
  u <- cow_og2_test1p[[i]]
  u$r$srbtw$setAllParams(params = u$r$opt$par)
  cow_og2_test1p_stats <- rbind(cow_og2_test1p_stats, data.frame(
    Type = u$n,
    ob = if ("b" %in% names(u)) "Y" else "-",
    oe = if ("e" %in% names(u)) "Y" else "-",
    b_init = if ("b" %in% names(u)) round(u$b, 3) else 0,
    e_init = if ("e" %in% names(u)) round(u$e, 3) else 1,
    b = round(u$r$srbtw$getBegin(), 3),
    e = round(u$r$srbtw$getEnd(), 3),
    loss = formatC(u$r$opt$value, format = "e", digits = 2),
    meanLoss = formatC(u$r$opt$value / max(u$r$srbtw$getQ()), format = "e", digits = 2),
    optFn = u$r$opt$counts[[1]],
    optGr = u$r$opt$counts[[2]]
  ))
}
```

```{r echo=FALSE}
if (interactive()) {
  cow_og2_test1p_stats
} else {
  knitr::kable(
    x = cow_og2_test1p_stats,
    booktabs = TRUE,
    caption = "Overview of incurred alignment losses over various setups.",
    label = "cow-og2-losses"
  )
}
```

The type in above table reveals some details: 'eq' means starting lengths of equal length, 'unif' mean they wer uniformly sampled from $[0,1]$ (with out scaling them back), 'beta' means they were drawn from a Beta-distribution with $\alpha=1$ and $\beta=2$ (also, no scaling back). 'ob' and 'oe' in the name indicate that the begin and/or end was open. Finally, the number of intervals is at the end of the type.

__Conclusion__: Yes, if the loss is well defined and mechanism for taking interval-lengths, begin and end into account (extreme value regularization), then this will absolutely work well.


# Optimization goal III

sr-BTW and sr-BAW allow to interchange Warping Pattern and Warping Candidate. Recall that only the former is treated as constant and needs to be used wholly (the WC only needs to be used wholly in closed/closed matching).

* __Pattern as WP, Data as WC__: This is the "ordinary" approach. It allows us to set boundaries on the WP that we think make sense for the pattern, it will thus preserve what was in the original intervals and apply warping to them. This approach is suitable for checking how well the data matches the pattern. If the goal were to adjust the WP, we could still use this approach and eventually inverse the warping. I.e., if we know how to warp the Data to the Pattern, we know how to warp the Pattern to the Data. That is not implemented yet but should be straightforward.
* __Data as WP, Pattern AS WC__: This means that we still can choose boundaries, but they must be chosen on the data. This makes less sense as we do not know how to subdivide the data into intervals, so generally, you probably just want equidistantly-spaced boundaries. So, if we would choose our original Fire Drill AP as WC, we could not preserve the original segmentation into the 4 characteristic intervals, as the optimization would highly likely choose a different segmentation. It would be possible to restore the original segmentation after optimization, however. This would only work if we choose to have 4 intervals.

A note on highly multivariate optimization: In section \ref{ssec:srbtaw} we introduce the Multilevel-model (__MLM__) that supports arbitrary many variables for either, Warping Pattern or Warping Candidate. The algorithms introduced here, namely `sr-BTW` and `sr-BAW`, are only univariate: they support exactly one variable for the WP and one for the WC. In the suggested MLM, we will use one instance of an algorithm per __unique__ pair of variables. All of the instances __share the same__ set of parameters, so that the MLM appears as a single model. This is a prerequisite for multivariate fitting and illustrated with a practical example of ours: Suppose we want to adjust the __A__-variable of our Fire Drill AP, such that the distance between it and the A-variables of, say, 5 other projects is minimized. The MLM will have 5 instances of the algorithm that share the same parameters during optimization. The data from the 5 projects will be adjusted simultaneously. Once we are finished, we inverse the learned parameters so that we can modify the Fire Drill AP's A-variable. If we had used different parameters for each algorithm, there would be no clear path for the inversion of the parameters.

## Ideas


* Use the AP as WP with the 4 intervals that we have defined and warp all data to it. If we know how to warp the data to the AP, we can inverse the results to apply warping to the pattern instead (see above). This approach also allows us to give low or no weight/loss to, e.g., the Long Stretch phase. If we were to do that, the other phases would extent into the Long Stretch, leaving is with a gap in each project that we can afterwards analyze. This will help us to learn about the Long Stretch phases!
* Use the AP as query instead. This means that the project data is constant, and that we subdivide it into some intervals by some strategy or just equidistantly-spaced. Using, e.g., AIC/BIC, we find the right amount of boundaries. This approach cannot consider the original intervals of the AP as we defined them, as the optimization chooses new intervals (also, it would most likely end up using a different number of intervals). This approach could result in a vastly different pattern that does not preserve too many shapes from the original pattern, but it may still worth testing. In the case where we use exactly 4 intervals, it would be possible to restore the original segmentation.
* A third approach: Start with an empty Warping Candidate: A straight horizontal line located at $0.5$ for each variable and variably many equidistantly-spaced intervals (use information criterion to find best number). This is the approach where we throw away our definition of the Fire Drill AP and create one that is solely based on data. The resulting warped candidate is a new pattern then.


## Approach

The approach that I suggest is to learn a __piece-wise linear__ function, that will be overlaid with the signal. For each interval, we will learn a separate slope. However, instead of learning a slope directly, we will learn the terminal y-translation at the end of the interval (with the initial translation being $0$). A linear function is given by $y=ax+b$, and we will only learn $a$. $b$, the intercept, is for each interval implicitly assumed to be $0$, but in reality is the sum of all preceding terminal y-translations. This is required because the resulting piece-wise linear function needs to be continuous (otherwise, when adding it to the signal, the signal would also become discontinuous). The first interval has no such preceding y-translation, so we will learn one using the additional parameter, $v$, that represents this. The advantage is, that we can simply __add__ this piece-wise function to each interval, in our case, to each $m_q^c(x,\dots)$. We can think of $v$ and the ordered set of terminal translations as a set of perpendicular vectors, so that for each $q$-th interval, we simply sum up $v$ and the set up to and including the preceding interval, to get the intercept.

I call this approach __self-regularizing boundary amplitude warping__ (sr-BAW). In the style of the sub-model definition for sr-BTW, $m_q^c$, we will call the sub-models for amplitude warping $n_q^c$.

## Sub-Model formulation

In each interval, we will call this function $t_q$:

$$
\begin{aligned}
  \bm{\vartheta}^{(y)}\;\dots&\;\text{ordered set of terminal}\;y\text{-translations for each interval,}
  \\[1ex]
  a_q=&\;\frac{\bm{\vartheta}_q^{(y)}}{\iota_q}\;\text{, the slope for the}\;q\text{-th interval,}
  \\[0ex]
  &\;\text{where}\;\iota_q\;\text{is the length of the}\;q\text{-th \textbf{target}-interval,}
  \\[1ex]
  v\;\dots&\;\text{the global translation for the first interval,}
  \\[1ex]
  \phi_q^{(y)}=&\;\begin{cases}
    0,&\text{if}\;q=1,
    \\
    \sum_{i=1}^{q-1}\,\bm{\vartheta}^{(y)}_i,&\text{otherwise,}
  \end{cases}
  \\[0ex]
  &\;\text{(the intercept for the}\;q\text{-th interval), and}
  \\[1ex]
  \rho_q\;\dots&\;\text{starting}\;x\text{-offset of the}\;q\text{-th \textbf{target}-interval,}
  \\[1ex]
  t_q(x,\dots)=&\;a_q(x-\rho_q)+v+\phi_q^{(y)}\;\text{, linear amplitude warping in}\;q\text{-th interval.}
\end{aligned}
$$

The amount of parameters for the y-translations is $1$ plus the amount of intervals (or the same as the amount of boundaries). For SRBTW, we had previously used the notion of $\phi_q$ to calculate the offset for each $q$-th interval. We use a similar notion here, $\phi_q^{(y)}$, to denote the intercept of each $q$-th interval.

### Self-regularization

Again, we would like this approach to be __self-regularizing__. That means, that any resulting final y-coordinate should never be less or more than the defined box-bounds. In DTW, the degrees of freedom are limited to warping time, i.e., the x-axis. Time can (linearly) compress and stretch, but never is the __amplitude__ of the signal altered. This would not make much sense for DTW, either, as it is a purely discrete algorithm, that manipulates __each__ discrete point of a query-signal. If DTW would alter the magnitude, the result would simply be the reference signal. Boundary Time Warping, on the other hand, instead of manipulating points, manipulates everything between the points (intervals) through linear affine transformation. This is the reason why the degrees of freedom are limited to time in DTW. That also means, for DTW, that time warping only works, when the query signal is within the bounds (y) of the reference signal. For that reason, time warping problems often have the application-specific y-limits that directly come from the reference signal, which is known a priori. It is also a possibility that the bounds are specified by the user.

In SRBAW it is possible to specify limits on a per-interval basis, so we will have two vectors, one with lower- and one with upper bounds for each interval. It would also be possible to specify the limits as a function (per interval). But at this point it is not required so we will not make that extra effort.

It is important to notice that in Boundary Time/Amplitude Warping, we cannot (must not) make any assumptions about the signal within an interval. For example, we cannot know its interval-global minimum or maximum. At the same time, we want to enforce box-bounds on the warped signal. The solution is to enforce these only after the warped signal has been computed. That means, that we need to wrap the warping function. We do this below by wrapping $n_q(\dots)$ into the $\min$/$\max$ expression $n_q^c(\dots)$.

$$
\begin{aligned}
  f(x,\dots)\;\dots&\;\text{some signal function, could be, e.g.,}\;m_q^c(x,\dots)\text{,}
  \\[1ex]
  n_q(x,\dots)=&\;f(x,\dots)+\overbrace{a_q(x-\rho_q)+v+\phi_q^{(y)}}^{t_q(x,\dots)}\;\text{, sub-model that adds amplitude-warping to}\;f(x,\dots)\text{,}
  \\[1ex]
  \bm{\lambda}^{(\text{ymin})},\bm{\lambda}^{(\text{ymax})}\;\dots&\;\text{vectors with lower and upper}\;y\text{-bounds for each interval}
  \\[0ex]
  &\;\text{(which could also be functions of}\;q\;\text{and/or}\;x\text{),}
  \\[1ex]
  n_q^c(x,\dots)=&\;\max{\bigg(\bm{\lambda}_q^{(\text{ymin})}, \;\min{\Big(n_q(x,\dots),\bm{\lambda}_q^{(\text{ymax})}\Big)}\bigg)}\;\text{, the}\;\min\text{/}\max\text{-constrained sub-model.}
\end{aligned}
$$

### Gradient of the sub-model

We need to pay attention when defining the gradient, for cases when some parameters depend on parameters of the underlying model. For example, $l_q^{(c)}$ depends on $\forall\,l\in\bm{\vartheta}^{(l)}$ as well as $b,e$ (if open begin and/or end), while $\phi_q$ depends on $\forall\,l_i^{(c)},i<q$. If we derive w.r.t. any of the sr-BAW parameters, i.e., $v$ or any of the terminal translations in $\bm{\vartheta}^{(y)}$, then this will eliminate any $m_q^c$, but we need to be careful when deriving for parameters that affect any $l_q^{(c)}$.


### Testing

We will define some piece-wise linear function and use the signal of the __A__-variable, then we warp the amplitude and see how that looks.

```{r echo=FALSE}
baw_intervals <- 5
theta_baw <- c(0, .1, .3, .5, .8, 1)
vartheta_l_baw <- rep(1 / baw_intervals, baw_intervals)
baw_v <- .05
baw_vartheta_y <- c(.025, .1, -.1, .05, .025)

baw_tq <- function(q, x) {
  a_q <- baw_vartheta_y[q] / (theta_baw[q + 1] - theta_baw[q])
  phi_q_y <- if (q == 1) 0 else sum(baw_vartheta_y[1:(q - 1)])
  phi_q <- theta_baw[q]
  a_q * (x - phi_q) + baw_v + phi_q_y
}

baw <- Vectorize(function(x) {
  q <- 0
  for (i in seq_len(baw_intervals)) {
    q <- i
    if (x >= theta_baw[i] && x < theta_baw[i + 1]) {
      break
    }
  }
  baw_tq(q = q, x = x)
})

n <- function(x) {
  f(x) + baw(x)
}
```

```{r}
print(baw_v)
print(theta_baw)
print(vartheta_l_baw)
print(baw_vartheta_y)

ggplot() +
  xlim(0, 1) +
  stat_function(fun = f, color = "red") +
  stat_function(fun = baw, color = "black") +
  stat_function(fun = n, color = "blue")
```

What we see in the above plot is the following:

* __Red__: The __A__-variable, as it appeared in the dataset,
* __Black__: the piece-wise linear function, with initial offset `baw_v` (`r baw_v`), and per-interval terminal translations of `r baw_vartheta_y`. The interval-boundaries were equidistantly placed at `r theta_baw`,
* __Blue__: Red+Black, i.e., the piece-wise linear function applied to the signal from the dataset.


## Architecture of sr-BTW + sr-BAW

So far, I had designed sr-BTW as an R6-class, and also a sub-model of it. This design was not made to be very performant, but to be very expressive and verbose, and I tried to be as close to the terms I use in this notebook as possible. BTW and BAW are stand-alone models, and they can be used independently from each other. If used together, the most straightforward way, however, is to __wrap__ BTW in BAW, as I had shown it using the $n_q^c$-model, where the signal is actually $m_q^c$. This is how we are going to do it here for optimization goal III.


### Current architecture

Our architecture here so far has a few drawbacks and does not exploit some commonalities. However, for the scope of the development of sr-BTW and sr-BAW, we will continue with this approach and just organically implement things as we need them. I plan to release everything as an R-package on CRAN, and for that purpose I will make a new repository where I implement it using the new architecture described below. I already took some minor parts of the new architecture and implemented it.


### Future architecture

In the future, we still want to use R6-classes. One important goal with the future architecture is extensibility. It must be possible for 3rd-party users to extend the code, especially with custom (data-)losses and regularizers. In the following, I show a class hierarchy of how it is planned to be eventually.

* `abstract class Differentiable` -- this will be the mother-class of everything that is differentiable, i.e., models, objectives etc. The Differentiable is just a functional piece that itself cannot be computed.
  * Main methods: `get0Function()` (returns the function that can be called), `get1stOrderPd(name)` and `get2ndOrderPd(name)` return functions for the the first- resp. second-order partial derivatives for the parameters specified by `name` (or all if `name=NULL`). The idea behind this class is to provide a standardized interface for any function that has zero or arbitrary many parameters that can be derived for once or twice. If we think of a specific loss, let's say the RSS, it needs to obtain the function for the warped candidate, and its gradient needs the partial derivatives of the WC. I.e., we need the actual functions and not just the results. This class will also allow us to provide some helper classes for, e.g., encapsulating a signal-generating function more robustly.
  * More methods: `getParamNames()`, `getNumParams()`, `getNumOutputs()`, `getOutputNames()` -- note how there is no setter for the actual parameters, as the `Differentiable`'s purpose is to describe the encapsulated function. It is important to know what the parameters are one can derive by, especially later when we combine many objectives and not each and every has the same parameters.
* `abstract class Model extends Differentiable` -- A model is a mathematical formulation of how to, e.g., transform some data using some parameters. So that means we will have 1st- and 2nd-order partial derivatives next to the actual model function. The `Model` also encompasses methods for getting statistical properties, such as residual or the likelihood (after fitting).
  * Main methods: `setParams(p)`, `getParams()`/`coefficients()` for setting concrete values for some or all of the model's non-constant parameters (those that are altered during optimization).
  * Statistical methods: `AIC()`/`BIC()`, `coefficients()`, `residuals()`, `likelihood()`
  * More methods: `plot()` for example (this method is inherited from R6)
  * Sub-classes: ___`srBTAW`___ -- Our main __multilevel-model__ (described below)
* `abstract class Objective extends Differentiable` -- An objective is something that can be computed; however, it does not yet have a notion of a model or concrete parameters. Instead, it introduces methods that allow (numeric) computation of the underlying `Differentiable`. An `Objective` has no notion of a weight, or whether it should be minimized or maximized, for example.
  * Main methods: `compute(x)`, `compute1stOrderPd(x, name)`, `compute2ndOrderPd(x, name)` -- given some $\mathbf{x}$, calls the methods of the underlying `Differentiable`. The `name`-arguments allow to specify specific partial derivatives (return all if null). Those can then be used to construct, e.g., a Jacobian. These methods return lists of functions, and the idea is that the name of the index represents the index of the output-parameter, and then which variable it was derived for first, then second.
  * Additionally: `compute1stOrderPd_numeric(x, name)` and `compute2ndOrderPd_numeric(x, name)` -- these methods compute the 1st- and 2nd-order partial derivatives numerically. We will use the same naming scheme as previously. Note that 1st-order PDs would result in a 2-dimensional matrix for vector-valued functions, and in a 3-dimensional tensor for the second order.
  * Sub-classes (all extend `Objective`):
    * `Loss`: Adds a weight, and also it knows whether we minimize or maximize,
    * `LogLoss`: Meta-class that takes another `Loss` and returns its result in a logarithmic way. For our project at hand, we usually return strictly positive logarithmic losses.
    * `DataLoss`: Specialized objective that computes a loss solely based on the data. This is more of a semantic class.
    * `Regularizer`: Like `DataLoss`, this is more of a semantic class that computes a loss solely based on the model's parameters.
    * `Score`: Another meta-class that takes another `Objective` (or, e.g., `LogLoss`), together with some lower and upper bounds, and transforms it into a score.
    * `abstract class MultiObjective`: Generic meta-class that can combine multiple other objectives, losses, regularizers etc. It is abstract and generic because A) it does not define an implementation on how to do that and B) because it provides some common facilities to do so, for example it will allow to compute objectives in parallel.
      * `LinearScalarizer extends MultiObjective`: A concrete example of a `MultiObjective` that adds together all defined objectives using their weight. This is mostly what we will be using for our purposes.
      * `ScoreMagnitudeScalarizer extends MultiObjective`: Previously described in section \ref{ssec:score_agg}, this is a score-aggregator that can be used to train the MLM with one or more `Score`s.

## `srBTAW extends Model` -- a Multilevel-model\label{ssec:srbtaw}

sr-BTW and sr-BAW are mere algorithms, and not quite models. Either can take a reference- and a query-signal (Warping Pattern and Warping Candidate) and compute a warped version of the candidate using the boundaries and terminal y-translations. Instead of inheriting from `Objective`, we chose to have the actual model called ___`srBTAW`___ to inherit from `Differentiable`, and to encapsulate a single `Objective` that can be computed. The abstract methods of `Differentiable` are forwarded to the encapsulated `Objective`. This has the advantage of the encapsulated objective actually representing the entire model, also we can use a single objective or, e.g., a `MultiObjective`, making this de facto a __multilevel-model__, that supports arbitrary many variables (one or more) for either, Warping Pattern and Warping Candidate.

The main purpose of `srBTAW` is to encapsulate all available data (variables) and intervals, and to provide helper methods (factories) for creating losses. It will allow us to reuse instances of `SRBTW` and `SRBAW`/`SRBTWBAW`, as any of these create a pair of one WP and one WC.


## Testing

As I wrote in the previous section, we will just wrap sr-BTW with sr-BAW. So I sub-classes it and its sub-model and made the required extensions. In the following, we will conduct very similar tests to the ones I did with sr-BTW, except for the fact that we allow the y-intercepts to adjust now, too.


```{r echo=FALSE, message=FALSE}
source(file = "../models/SRBTW-R6.R")

vartheta_l_baw <- c(.25,.275,.25,.225)
# vartheta_y <- c(.1, .1, .1, .1)
v <- .125
vartheta_y <- c(0,0,.65,-.75)

srbtwbaw <- SRBTWBAW$new(
  wp = r,
  wc = f,
  theta_b = c(0, bounds, 1),
  gamma_bed = c(0, 1, 0),
  lambda = rep(0, length(vartheta_l_baw)),
  begin = 0,
  end = 1,
  openBegin = FALSE,
  openEnd = FALSE,
  lambda_ymin = rep(0, length(vartheta_l_baw)),
  lambda_ymax = rep(1, length(vartheta_l_baw))
)

srbtwbaw$setParams(vartheta_l = vartheta_l_baw, v = v, vartheta_y = vartheta_y)

# We have a new plot-function!
srbtwbaw$plot_warp()
```

I tried to put in values for the y-translation that would make the WC approximately close to the WP. It is still off quite a bit, but it should be already much closer (lower loss) than we had achieved in the first optimization where amplitude warping was not allowed. Now if we carefully check all of the values and the plot, we'll see that it works. Now what we are going to do is to repeat some of the optimizations, using the same losses as earlier, and then we are going to compare those fits.

```{r echo=FALSE}
dataLoss_baw <- SRBTW_DataLoss$new(
  srbtw = srbtwbaw, intervals = 1:4,
  weight = 1, continuous = TRUE,
  params = c(vartheta_l_baw, v, vartheta_y))

dataLoss_RSS_baw <- function(loss, listOfSms) {
  # 'loss' is a reference to the data-loss itself.
  continuous <- loss$isContinuous()
  err <- 0
  
  for (sm in listOfSms) {
    t <- sm$asTuple()
    if (continuous) {
      tempf <- function(x) (t$wp(x) - t$nqc(x))^2 # change mqc to nqc
      err <- err + cubature::cubintegrate(
        f = tempf, lower = t$tb_q, upper = t$te_q)$integral
    } else {
      X <- seq(from = t$tb_q, to = t$te_q, length.out = 250)
      y <- sapply(X = X, FUN = t$wp)
      y_hat <- sapply(X = X, FUN = t$nqc) # change mqc to nqc
      err <- err + sum((y - y_hat)^2)
    }
  }
  
  log(1 + err)
}

dataLoss_baw$setLossFunc(lossFunc = dataLoss_RSS_baw)

soo_baw <- SRBTW_SingleObjectiveOptimization$new(srbtw = srbtwbaw)
soo_baw$addObjective(obj = dataLoss_baw)
soo_baw$setParams(params = c(vartheta_l_baw, v, vartheta_y))
soo_baw$compute()
```

Previously, the error at this point was $\approx 0.0709$, it is already much lower. What we should attempt now, is to find even better parameters using optimization:


```{r echo=FALSE}
# Optimization Goal 3, test 1, parallel:
cow_og3_test1p <- loadResultsOrCompute(file = "../results/cow_og3_test1p.rds", computeExpr = {
  library(optimParallel)
  
  cl <- parallel::makePSOCKcluster(parallel::detectCores())
  parallel::clusterExport(cl, varlist = c("soo_baw"))

  doWithParallelClusterExplicit(cl = cl, expr = {
    set.seed(1337)
    optRp <- optimParallel::optimParallel(
      par = c(vartheta_l_baw, v, vartheta_y),
      # par = c(vartheta_l_baw, rep(0, 1 + length(vartheta_y))),
      # par = c(vartheta_l_baw, 1/2, runif(n = length(vartheta_y), min = 0, max = 1)),
      fn = function(x) {
        soo_baw$setParams(params = x)
        soo_baw$compute()
      },
      lower = c(rep(0, length(vartheta_l_baw)), rep(-.Machine$double.xmax, 1 + length(vartheta_y))),
      upper = c(rep(1, length(vartheta_l_baw)), rep( .Machine$double.xmax, 1 + length(vartheta_y))),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
})

cow_og3_test1p
srbtwbaw$setAllParams(cow_og3_test1p$par)
srbtwbaw$plot_warp()
srbtwbaw$plot_dual_original()
```

Ok, that worked extraordinarily well. However, we also gave it quite good starting values to be fair. Using just zeros as starting values for $v$ and all terminal y-translations worked also well. If we look at the learned parameters we notice a peculiarity of this model: The last terminal y-translation is almost minus three -- this allows the model in the last interval to decline steeply, and to actually make this "flat foot" at the end that the Warping Pattern has, because the signal now effectively becomes zero in the last section. That is why I had formulated the $\min,\max$ self-regularization on the final signal, and not the terminal y-translations, as these might be rather extreme in order to achieve a good fit.

Let's also make an open/open test and see what happens:

```{r}
srbtwbaw$setOpenBegin(ob = TRUE)
srbtwbaw$setOpenEnd(oe = TRUE)
```


```{r echo=FALSE}
cow_og3_test2p <- loadResultsOrCompute(file = "../results/cow_og3_test2p.rds", computeExpr = {
  library(optimParallel)
  
  cl <- parallel::makePSOCKcluster(parallel::detectCores())
  parallel::clusterExport(cl, varlist = c("soo_baw"))

  doWithParallelClusterExplicit(cl = cl, expr = {
    set.seed(1337)
    optRp <- optimParallel::optimParallel(
      par = c(vartheta_l_baw, 0, 1, v, vartheta_y),
      fn = function(x) {
        soo_baw$setParams(params = x)
        soo_baw$compute()
      },
      lower = c(rep(0, length(vartheta_l_baw)), 0, 0, rep(-.Machine$double.xmax, 1 + length(vartheta_y))),
      upper = c(rep(1, length(vartheta_l_baw)), 1, 1, rep( .Machine$double.xmax, 1 + length(vartheta_y))),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
})

cow_og3_test2p
srbtwbaw$setAllParams(cow_og3_test2p$par)

ggpubr::ggarrange(
  srbtwbaw$plot_warp() +
    ggplot2::geom_vline(xintercept = cow_og3_test2p$par[5]) +
    ggplot2::geom_vline(xintercept = cow_og3_test2p$par[6]) +
    theme(legend.position = "bottom"),
  srbtwbaw$plot_dual_original() + theme(legend.position = "bottom"),
  nrow = 1
)
```

Ok, cut-in is still at $0$, but cut-out is at `r round(cow_og3_test2p$par[6], 4)`. The error is as low as in the previous test.


## Regularization

Regularization for sr-BAW is probably more difficult, as we cannot impose regularization directly on the parameters as I see it. Well, we could, but as I just wrote before this, it may be necessary for $v$ and the terminal y-translations to assume extreme values (well beyond $\bm{\lambda}^{(\text{ymin})},\bm{\lambda}^{(\text{ymax})}$) to achieve a good fit. In other words, I would not interfere with these too much at the moment. In fact, we did above optimization with the minimum/maximum machine double, effectively alleviating box-bounds for these parameters. I suppose that it may be an idea to regularize all y-translations together, and to look at them as a distribution. Then, extreme values could be penalized. However, this is probably application specific.

What seems to be more promising is to impose regularization on the output of the model. Using $\bm{\lambda}^{(\text{ymin})},\bm{\lambda}^{(\text{ymax})}$, we can limit the __output__ of the model to be within box-bounds. However, if the model were to push too much of the signal onto either boundary, we would get a less optimal fit in many cases. An approach to alleviating this is described in the next subsection.

### Regularize extreme outputs

Another reason that makes regularization of the terminal y-translations difficult, is the fact that we cannot (and must not) make any assumptions about the signal in an interval -- we simply do not know how it behaves. Therefore, I suggest a regularizer that penalizes the output of the model instead. The more of its output is close to either box-bound, the higher the cost. This would require a somewhat _hybrid_ regularizer, as we need to consider the box-bounds, the y-translations and the data. A simple first version could take the __average__ of the output signal and score that:

$$
\begin{aligned}
  \mathbf{\hat{y}},f(x)\;\dots&\;\text{complete discrete output of some model, or the model itself as function,}
  \\[1ex]
  \mu^{(c)}=&\;\frac{1}{\max{\big(\operatorname{supp}(f)\big)}-\min{\big(\operatorname{supp}(f)\big)}}\times\int\displaylimits_{\operatorname{supp}(f)}\,f(x)\;dx\;\text{, mean of function(model), or}
  \\[1ex]
  \mu^{(d)}=&\;\frac{1}{N}\sum_{i=1}^{N}\,\mathbf{\hat{y}}_i\;\text{, mean of some discrete output,}
  \\[1ex]
  \beta_l,\beta_u\;\dots&\;\text{lower- and upper bounds for the output,}
  \\[1ex]
  \mathcal{R}^{(\text{avgout})}=&\;1-\frac{\min{\big(\mu-\beta_l,\beta_u-\mu\big)}}{\frac{1}{2}\big(\beta_u-\beta_l\big)}\mapsto[0+,1]\;\text{, or, alternatively:}
  \\[1ex]
  \mathcal{R}^{(\text{avgout})}=&\;2\,\sqrt{\bigg(\frac{1}{2}\big(\beta_u-\beta_l\big)-\mu\bigg)^2}\mapsto[0+,1]\;\text{, and the log-loss:}
  \\[1ex]
  \mathcal{L}_{\mathcal{R}^{(\text{avgout})}}=&\;-\log{\Big(1-\mathcal{R}^{(\text{avgout})}\Big)}\mapsto[0+,\infty]\;\text{.}
\end{aligned}
$$

Note that both versions of the regularizer are defined as ratios, and that, at most, their loss can be $1$ (and the least possible loss would be $0$). Also, $\frac{1}{2}\big(\beta_u-\beta_l\big)$ can be replaced with some user-defined, arbitrary expectation, i.e., it does not necessarily need to be the mean of the upper and lower bound (but it needs to be within them).


### Trapezoidal-overlap regularization

Using the suggested y-translations results in a rectangular interval becoming a __trapezoid__. Even more specific, the trapezoid is a __parallelogram__, which has the __same__ area as the original interval, as the base of corresponds to the interval's length. I suggest using another kind of regularizer that measures the overlap of these skewed and translated parallelograms with their original interval (as a ratio).

$$
\begin{aligned}
  \mathbf{x}_q\;\dots&\;\text{support for the}\;q\text{-th interval},
  \\[1ex]
  \beta_l,\beta_u\;\dots&\;\text{lower and upper boundary for}\;y\;\text{(coming from}\;\bm{\lambda}^{(\text{ymin})},\bm{\lambda}^{(\text{ymax})}\text{),}
  \\[1ex]
  b_{\beta_l}(x),b_{\beta_u}(x)\;\dots&\;\text{functions over the lower- and upper boundaries,}
  \\[1ex]
  o_q=&\;v+\phi_q^{(y)}\;\text{, initial translation in the interval,}
  \\[1ex]
  s,t\;\dots&\;\text{intersections of}\;t_q(x,\dots)\;\text{with lower- and upper boundary,}
  \\[0ex]
  &\;\text{(}s=\min{(\mathbf{x}_q)}\;\text{and}\;t=\max{(\mathbf{x}_q)}\;\text{if no intersetion),}
  \\[1ex]
  \delta_q^{(\max)}\;\dots&\;\text{maximum possible difference in interval}\;q\text{, i.e., no overlap,}
  \\[1ex]
  \delta_q=&\;\begin{cases}
    \text{if}\;o_q<\beta_l,&\begin{cases}
      \text{if}\;o_q+\bm{\vartheta}_q^{(y)}>\beta_l,&\int_s^{\max{(\mathbf{x}_q)}}\,\min{\big(b_{\beta_u}(x),t_q(x)\big)}-b_{\beta_l}(x)\,dx,
      \\
      \text{else}&\min{\Big(\delta_q^{(\max)},\int_s^t\,b_{\beta_l}(x)-t_q(x)\Big)},
    \end{cases}
    \\
    \text{else if}\;o_q>\beta_u,&\begin{cases}
      \text{if}\;o_q+\bm{\vartheta}_q^{(y)}<\beta_u,&\int_{\min{(\mathbf{x}_q)}}^{t}\,\min{\big(b_{\beta_u}(x),t_q(x)\big)-b_{\beta_l}(x)}\,dx,
      \\
      \text{else}&0,
    \end{cases}
    \\
    \text{else}&\begin{cases}
      \text{if}\;o_q+\bm{\vartheta}_q^{(y)}>\beta_u,&\int_s^t\,\min{\big(b_{\beta_u}(x),t_q(x)\big)}-b_{\beta_l}(x)\,dx,
      \\
      \text{else}&\int_s^t\,t_q(x)-b_{\beta_l}(x)\,dx,
    \end{cases}
  \end{cases}
  \\[1ex]
  \mathcal{R}^{(\text{trapez})}=&\;{\max{(Q)}}^{-1}\times\sum_{i=1}^{\max{(Q)}}\,\frac{\delta_q}{\delta_q^{(\max)}}\mapsto[0+,1]\;\text{, and the log-loss:}
  \\[1ex]
  \mathcal{L}_{\mathcal{R}^{(\text{trapez})}}=&\;-\log{\Big(1-\mathcal{R}^{(\text{trapez})}\Big)}\mapsto[0+,\infty]\;\text{.}
\end{aligned}
$$

### Tikhonov/$L_2$ regularization of translations

A somewhat effortless way to regularize extreme y-translations is to use the (squared) $L_2$-norm for the parameters $v$ and $\bm{\vartheta}^{(y)}$, as we would prefer less extreme values for these translations. However, during optimization we also would prefer allowing these parameters to cover a wider range than others, to allow more adapted solutions. This regularization has to be used carefully therefore.

$$
\begin{aligned}
  \bm{\omega}=&\;\{v\,\frown\,\bm{\vartheta}^{(y)}\}\;\text{, vector containing all y-translations,}
  \\
  \lVert \bm{\omega}\rVert_2=&\;\sqrt{\sum_{i=1}^{\norm{\bm{\omega}}}\,\bm{\omega}_i^2}\;\text{, the}\;L_2\text{-norm, alternatively}
  \\[1ex]
  \lVert \bm{\omega}\rVert_2^2=&\;\sum_{i=1}^{\norm{\bm{\omega}}}\,\bm{\omega}_i^2\;\text{, the squared}\;L_2\text{-norm, with regularizer/loss}
  \\[1ex]
  \mathcal{L}_{\mathcal{R}^{(L_2)}}=&\;\log{\Big(1+\lVert \bm{\omega}\rVert_2^2\Big)}\;\text{.}
\end{aligned}
$$


## Testing new `srBTAW` model

I did implement the new architecture and wrapped some of the older classes, and it's finally time to make a test.

```{r echo=FALSE, message=FALSE}
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

# Let's use the same as in the previous example and see, if the result is the same.
vartheta_l_mlm <- c(.25,.275,.25,.225)
#vartheta_y_mlm <- c(0, 0, 0.5, -0.6)
vartheta_y_mlm <- c(.0001, .0001, .0001, .0001)

mlm <- srBTAW$new(
  theta_b = c(0, bounds, 1),
  gamma_bed = c(0, 1, sqrt(.Machine$double.eps)),
  lambda = rep(sqrt(.Machine$double.eps), length(vartheta_l_mlm)),
  begin = 0, end = 1,
  openBegin = FALSE, openEnd = FALSE,
  useAmplitudeWarping = TRUE,
  lambda_ymin = rep(0, length(vartheta_l_mlm)),
  lambda_ymax = rep(1, length(vartheta_l_mlm)))

# Next, we do the signals; here we'll only do 1 WP and 1 WC:
sig_wp_mlm <- Signal$new(name = "A_WP", isWp = TRUE, support = c(0, 1), func = r)
sig_wc_mlm <- Signal$new(name = "A_WC", isWp = FALSE, support = c(0, 1), func = f)

ggarrange(
  sig_wp_mlm$plot(),
  sig_wc_mlm$plot(),
  nrow = 1
)
```


Finally, we can set the parameters and start fitting!

```{r echo=FALSE}
mlm$setParams(params = c(
  # The order is important!
  c(v = 0.001),
  `names<-`(vartheta_l_mlm, paste0("vtl_", seq_len(length.out = length(vartheta_l_mlm)))),
  `names<-`(vartheta_y_mlm, paste0("vty_", seq_len(length.out = length(vartheta_y_mlm))))
))
```


We got our MLM and the two signals (one WP/WC each). The next steps are adding the signal to the model, as well as initializing an appropriate loss (objective).

```{r echo=FALSE}
mlm$setSignal(signal = sig_wp_mlm)
mlm$setSignal(signal = sig_wc_mlm)

mlm_obj <- srBTAW_LossLinearScalarizer$new(
  computeParallel = TRUE, gradientParallel = TRUE,
  progressCallback = function(what, step, total) {
    # print(paste(what, step, total))
    # We can also update, e.g., a progress-bar here..
  })

# Let's add 4 singular losses, one for each interval,
# so that we can exploit parallelism.
for (i in 1:4) {
  temp <- srBTAW_Loss_Rss$new(
    wpName = sig_wp_mlm$getName(), wcName = sig_wc_mlm$getName(),
    weight = 1, intervals = i, continuous = TRUE)#, numSamples = 250)
  # register loss for variable-pair:
  mlm$addLoss(loss = temp)
  
  # add loss to MultiObjective:
  mlm_obj$setObjective(name = paste0("Loss_q_", i), obj = temp)
}

mlm$setObjective(obj = mlm_obj)
```

```{r echo=FALSE}
# Currently disabled:
# temp <- YTransRegularization$new(
#   wpName = sig_wp_mlm$getName(), wcName = sig_wc_mlm$getName(),
#   intervals = 1:4, weight = .05, use = "trapezoid")
# mlm$addLoss(loss = temp)
# mlm_obj$setObjective(name = "Loss_vty", obj = temp)
# 
# temp <- TimeWarpRegularization$new(
#   wpName = sig_wp_mlm$getName(), wcName = sig_wc_mlm$getName(), intervals = 1:4, weight = .1, use = "exint")
# mlm$addLoss(loss = temp)
# mlm_obj$setObjective(name = "Loss_exint", obj = temp)
```


The following optimization works equally well with a __continuous__ version of the same loss, but computing is approx. ten times slower and needs about 50% more iterations. However, it converges much faster to its final minimum, whereas we see a rather gradual improvement for the discrete RSS. In the following, we plot the parameters' course over all optimization steps. Note that values for $\bm{\vartheta}^{(l)}$ (`vtl_1` etc.) are not normalized as to sum up to $1$.

```{r echo=FALSE}
mlm_fr <- loadResultsOrCompute(file = "../results/cow_og3_mlm_fr.rds", computeExpr = {
  cl <- parallel::makeCluster(parallel::detectCores())
  doSNOW::registerDoSNOW(cl)
  
  temp <- mlm$fit(verbose = TRUE)
  
  parallel::stopCluster(cl)
  foreach::registerDoSEQ()
  temp
})

mlm$setParams(params = mlm_fr$optResult$par)

mlm_fr$optResult

ggpubr::ggarrange(
  mlm_fr$plot(c("vtl_1", "vtl_2", "vtl_3", "vtl_4"), FALSE) + coord_flip() + scale_x_reverse() + theme(legend.position = "bottom") + theme(axis.title.y = element_text(angle = 270)),
  mlm_fr$plot(c("v", "vty_1", "vty_2", "vty_3", "vty_4"), FALSE) + coord_flip() + scale_x_reverse() + theme(legend.position = "bottom") + theme(axis.title.y = element_text(angle = 270)),
  nrow = 1
)
```

The warping- and dual-original plots of this fit are in the top of the following figure. We can see that without regularization we obtain a very good fit. However, the first interval  has a length of almost $0$, which might not be desirable in practice.

Therefore, I have added very mild (weight of $0.1$) regularization for extreme intervals and ran it again, the results are in the second row of below plot. I suppose it is fair to say that this fit is the overall best so far: It selects the flat beginning to map to the first interval, then the slight fluctuations to the Long Stretch, before mapping the begin of the next interval to the signal's local minimum shortly before the peak, which is perfectly mapped to the reference's peak.

In the third row, I have added very mild trapezoidal-overlap regularization (weight of $0.05$). The fit is very good, too, but the first two intervals were selected even slightly better with extreme-interval regularization only. Also the peak appears to be selected slightly better there.

```{r echo=FALSE, fig.height=8}
g_noReg <- annotate_figure(p = ggpubr::ggarrange(
  mlm$getInstance(wpName = sig_wp_mlm$getName(), wcName = sig_wc_mlm$getName())$plot_warp() + theme(legend.position = "bottom"),
  mlm$getInstance(wpName = sig_wp_mlm$getName(), wcName = sig_wc_mlm$getName())$plot_dual_original() + theme(legend.position = "bottom"),
  nrow = 1
), top = text_grob("No Regularization", size = 16))

temp <- readRDS(file = "../results/cow_og3_mlm_fr_exint.rds")
mlm$setParams(params = temp$optResult$par)
g_exint <- annotate_figure(p = ggpubr::ggarrange(
  mlm$getInstance(wpName = sig_wp_mlm$getName(), wcName = sig_wc_mlm$getName())$plot_warp() + theme(legend.position = "bottom"),
  mlm$getInstance(wpName = sig_wp_mlm$getName(), wcName = sig_wc_mlm$getName())$plot_dual_original() + theme(legend.position = "bottom"),
  nrow = 1
), top = text_grob("Using extreme-intervals regularization", size = 15))

temp <- readRDS(file = "../results/cow_og3_mlm_fr_both.rds")
mlm$setParams(params = temp$optResult$par)
g_both <- annotate_figure(p = ggpubr::ggarrange(
  mlm$getInstance(wpName = sig_wp_mlm$getName(), wcName = sig_wc_mlm$getName())$plot_warp() + theme(legend.position = "bottom"),
  mlm$getInstance(wpName = sig_wp_mlm$getName(), wcName = sig_wc_mlm$getName())$plot_dual_original() + theme(legend.position = "bottom"),
  nrow = 1
), top = text_grob("Using extreme-intervals and trapezoidal-overlap regularization", size = 16))

ggarrange(
  g_noReg, g_exint, g_both,
  nrow = 3
)
```

This is (within margin of error) the same result as we got previously (first row), so this means that our new architecture indeed works! Like in the previous result, we have one interval with length $\approx0$, and generally, the chosen lengths could align better with the reference. However, this is a human error, as the model did exactly what it was supposed to. In other words, what is missing, is __regularization__. The red `M`-function could probably not be closer to the blue warping pattern (`WP`). It is an extremely good solution that also requires extreme parameters, like the almost zero-length first interval and the y-translations for the last two intervals. If, for the use-case, the goal is to obtain `M` such that it is most close to `WP`, regardless of how extreme the parameters may be, then this is our winner. In practice, however, such extreme parameters are often less desirable, so I have tested two additional cases with added regularization.

My advice is the following: Since `srBTAW` is self-regularizing to some degree, always try first __without__ any additional regularization. Regularizations should be added one-by-one to better observe their impact, and they should be balanced. A regularizer is always an additional optimization goal, do not forget that. If very short (almost or equal to zero length) intervals are acceptable, additional regularization is not necessary. To avoid these, extreme-interval regularization may be added. It should have a low weight compared to the data-losses to begin with (start with an order of magnitude smaller) and then may be adjusted from there.

For most cases it is probably advisable to start with regularizations for the interval-lengths, and not the y-translations. Looking at the first plot in the group above, the third and fourth intervals require quite extreme y-translations, esp. since the absolute peak is mapped before it happens. Imposing just y-translations regularizations leads to a result similar as in the first plot. However, excessive y-translations are costly quickly, and can be avoided by aligning the intervals better in the first place. Y-translation regularization appears to have a stronger guiding-effect, and higher weights can lead to the signal not being able to properly translate. Also, and that depends on the regularizer used, penalizing y-translations should not be necessary in most cases, as we need/want the process to assume more extreme values in order to get a good fit. However, adding this kind of regularization is specific to the use-case and might still be beneficial. The three kinds of regularizers I had introduced earlier (Tikhonov, Average-output and Trapezoidal-overlap) are targeting three different properties and should cover any eventual needs.

# Other thoughts

Short unsorted list of things to consider.

Check progress of pattern: SRBTW allows for open-end matching, so that it can be used to detect by how much (time wise into the project) Fire Drill matches. Or, we can inverse the process and match the AP to the project, to see which phases exist and score them.

In optimization goal III, we should also try to adapt the reference AP without actually defining a loss for the Long Stretch. With such an adapted pattern, we will find out what goes actually on in that phase in the projects we have, as the other intervals will match, leaving a gap for the LS.

Choosing a good number of boundaries _a priori_: This depends on the problem, e.g., classification, and on whether a search can be performed for it. A search can be done using some information criterion (IC). Using some IC, then computing a best fit using $1\dots n$ boundaries, we should get a somewhat convex function, and can choose the number of boundaries using the minimum of that function. Sometimes, however, we cannot do this and need to make a decision a priori. I have done some tests using the __entropy__ of the signals, by treating them as probability density functions (which we showed works quite well for fitting a model). I have tried to compute it continuously (differential entropy) and discretely, by sampling from the signal. The latter case gives more usable values, yet it is much more volatile w.r.t. the number of samples. For example, a typical entropy (Shannon-bits) for $1e3$ samples is $\approx10$, and for $1e5$ it is already $\approx16$. Given the entropies of two signals, we could calculate the starting-number $n$ of boundaries like $n=(H^{(\text{WP})}\times H^{(\text{WC})})^{1-\frac{1}{e}}$. This gives OK-values for entropies in the range of approx. $\approx[5,20]$ and is just one idea. Eventually, we probably would need to do an empirical evaluation with many examples, where we employ a full search using some IC. Then, we could attempt to come up with some rule that best approximates this value.

Choosing a priori: One idea could be to select some number of equidistant boundaries/intervals and then replace the signal in each interval with a linear function that goes from start to end. Then, we would compute a loss and AIC between this function and the original interval to find the best number of intervals.

Another idea: Calculate arc length over full support, then divide it by a constant number of intervals. Then, start from the begin of the support and go a partial arc length (calculate the end coordinate as we know the begin and the arc length already). Repeat to the end. This method will subdivide the function into parts of equal arc length.

Classification: One additional way for classification would be an open/open approach using only one interval. If the selected section is much shorter than the original signal, the match would be bad (for non-repetitive signals).


# References {-}

<div id="refs"></div>






















