---
title: "Detecting the Fire Drill anti-pattern using Source Code -- Technical Report"
author: "Sebastian HÃ¶nel"
bibliography: ../inst/REFERENCES.bib
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  md_document:
    toc: yes
    toc_depth: 6
    df_print: kable
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: kable
  word_document: default
  pdf_document:
    toc: yes
    toc_depth: '6'
header-includes:
- \usepackage{bm}
- \usepackage{mathtools}
- \usepackage{xurl}
---

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\norm}[1]{\left\lvert#1\right\rvert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}

```{r echo=FALSE, warning=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

library(ggplot2)
library(ggpubr)
```

# Introduction

This is the complementory technical report for the paper/article tentatively entitled "Detection of Project Management Anti-Patterns through Source Code". Here, we import the ground truth as well as all projects' data, and instantiate our model based on _self-regularizing Boundary Time Warping and Boundary Amplitude Warping_. Given a few patterns that represent the __Fire Drill__ anti-pattern (AP), the goal is evaluate these patterns and their aptitude for detecting the AP in concordance with the ground truth.

All complementary data and results can be found at Zenodo. This notebook was written in a way that it can be run without any additional efforts to reproduce the results. The canonical URL to this notebook is[^1], with a rendered markdown version at[^2]. All code can be found in this repository, too.

[^1]: https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/fire-drill-technical-report.Rmd
[^2]: https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/fire-drill-technical-report.md

# Data

We have $9$ projects conducted by students, and two raters have __independently__, i.e., without prior communication, assessed to what degree the AP is present in each project. This was done using a scale from zero to ten, where zero means that the AP was not present, and ten would indicate a strong manifestation.

## The Ground Truth

```{r}
ground_truth <- read.csv(file = "../data/ground-truth.csv", sep = ";")
```

```{r echo=FALSE}
knitr::kable(
  x = ground_truth,
  booktabs = TRUE,
  caption = "Entire ground truth as of both raters",
  label = "groundtruth"
)
```


Using the _quadratic weighted Kappa_ [@cohen1968weighted], we can report an unadjusted agreement of __`r round(Metrics::ScoreQuadraticWeightedKappa(rater.a = ground_truth$rater.a, rater.b = ground_truth$rater.b, min.rating = 0, max.rating = 10), 3)`__ for both raters. A Kappa value in the range $[0.6,0.8]$ is considered _substantial_, and values beyond that as _almost perfect_ [@landis1977application]. As for the Pearson-correlation, we report a slightly higher value of __`r round(cor(x = ground_truth$rater.a, y = ground_truth$rater.b), 3)`__. The entire ground truth is shown in table \ref{tab:groundtruth}. The final consensus was reached after both raters exchanged their opinions, and it is the consensus that we will use as the actual ground truth from here on and out.


## The Student Projects

The ground truth was extracted from nine student-conducted projects. Seven of these were implemented simultaneously between March and June 2020, and two the year before in a similar timeframe.

```{r echo=FALSE, eval=FALSE}
# TODO: deleteme -- this MUST NOT END UP IN THE FINAL NOTEBOOK
temp <- (function() {
  repos <- c("Anonymous", "BHVS", "HKMM", "HOANG", "Horky", "Medici", "Merlot", "QWERTY", "VLDC")
  
  idx <- 1
  all <- NULL
  for (repo in repos) {
    temp <- read.csv(file = paste0("D:/repos/", repo, ".csv"))
    temp <- cbind(data.frame(project = rep(paste0("project_", idx), nrow(temp))), temp)
    all <- rbind(all, temp) 
    idx <- idx + 1
  }
  
  # Remove cols:
  all <- all[, colnames(all)[!(colnames(all) %in% c("SHA1", "RepoPathOrUrl", "AuthorName", "CommitterName", "Message", "AuthorEmail", "CommitterEmail"))]]
  
  # Predict labels:
  p <- final_model$predict(data = all, type = "both")
  colnames(p) <- c("label", paste0("prob_", colnames(p)[2:4]))
  all <- as.data.frame(cbind(all, p))
  
  all
})()
write.table(x = temp, file = "../data/student-projects.csv", row.names = FALSE, fileEncoding = "utf-8", sep = ";")
```

```{r}
student_projects <- read.csv(file = "../data/student-projects.csv", sep = ";")
```

We have a total of:

* Nine projects,
* `r length(unique(student_projects$AuthorNominalLabel))` authors that authored `r nrow(student_projects)` commits total which are of type
* Adaptive / Corrective / Perfective (`a/c/p`) commits: `r sum(student_projects$label == "a")` / `r sum(student_projects$label == "c")` / `r sum(student_projects$label == "p")`

We have a complete breakdown of all activities across all projects in figure \ref{fig:project-activity}.

```{r echo=FALSE}
student_projects_info <- NULL

for (pId in unique(student_projects$project)) {
  temp <- student_projects[student_projects$project == pId, ]
  student_projects_info <- rbind(student_projects_info, data.frame(
    project = pId,
    authors = length(unique(temp$AuthorNominalLabel)),
    commits = nrow(temp),
    a = nrow(temp[temp$label == "a", ]),
    c = nrow(temp[temp$label == "c", ]),
    p = nrow(temp[temp$label == "p", ]),
    avgDens = round(mean(temp$Density), 3)
  ))
}

if (!interactive()) {
  knitr::kable(
    x = student_projects_info,
    booktabs = TRUE,
    caption = "Per-project overview of the student projects",
    label = "studentprojects"
  )
}
```

```{r project-activity, echo=FALSE, fig.cap="Commit activities across projects", fig.align="top", fig.pos="ht!"}
ggplot(data = student_projects, aes(x = length(label), fill = label)) +
  geom_bar() + facet_grid(label ~ project) +
  theme_light() +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"),
    legend.position = "none")
```

We have slightly different begin- and end-times in each project. However, the data for all projects was previously cropped, so that each project's extent marks the absolute begin and end of it -- it starts with the first commit and ends with the last. As for our methods here, we only need to make sure that we scale the timestamps into a relative $[0,1]$-range, where $1$ marks the project's end.

For each project, we model __four__ variables: The activities __adaptive__ (__`A`__), __corrective+perfective__ (__`CP`__), the frequency of all activities, regardless of their type (__`FREQ`__), and the __source code density__ (__`SCD`__). While for the first three variables we estimate a Kernel density, the last variable is a metric collected with each commit. The data for it is mined using `Git-Density` [@honel2020gitdens], and we use a highly efficient commit classification model[^3] ($\approx83.6\%$ accuracy, $\approx0.745$ Kappa) [@honel2020using] to attach maintenance activity labels to each commit, based on size- and keyword-data only.

[^3]: https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/comm-class-models.md

Technically, we will compose each variable into an instance of our `Signal`-class. Before we start, we will do some normalizations and conversions, like converting the timestamps. This has to be done on a per-project basis.

```{r}
student_projects$label <- as.factor(student_projects$label)
student_projects$project <- as.factor(student_projects$project)
student_projects$AuthorTimeNormalized <- NA_real_
```

```{r echo=FALSE}
for (pId in levels(student_projects$project)) {
  student_projects[student_projects$project == pId, ]$AuthorTimeNormalized <-
    (student_projects[student_projects$project == pId, ]$AuthorTimeUnixEpochMilliSecs -
      min(student_projects[student_projects$project == pId, ]$AuthorTimeUnixEpochMilliSecs))
  student_projects[student_projects$project == pId, ]$AuthorTimeNormalized <-
    (student_projects[student_projects$project == pId, ]$AuthorTimeNormalized /
      max(student_projects[student_projects$project == pId, ]$AuthorTimeNormalized))
}
```

And now for the actual signals: Since the timestamps have been normalized for each project, we model each variable to actually start at $0$ and end at $1$ (the support). We will begin with activity-related variables before we model the source code density, as the process is different. When using Kernel density estimation (KDE), we obtain an empirical probability density function (PDF) that integrates to $1$. This is fine when looking at all activities combined (__`FREQ`__). However, when we are interested in a specific fraction of the activities, say __`A`__, then we should scale its activities according to its overall ratio. Adding all scaled activities together should again integrate to $1$. When this is done, we scale one last time such that no empirical PDF has a co-domain larger than $1$.

```{r}
project_signals <- list()

# passed to stats::density
use_kernel <- "gauss" # "rect"

for (pId in levels(student_projects$project)) {
  temp <- student_projects[student_projects$project == pId, ]
  
  # We'll need these for the densities:
  acp_ratios <- table(temp$label) / sum(table(temp$label))
  
  dens_a <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp[temp$label == "a", ]$AuthorTimeNormalized,
    ratio = acp_ratios[["a"]], kernel = use_kernel)
  
  dens_cp <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp[temp$label == "c" | temp$label == "p", ]$AuthorTimeNormalized,
    ratio = acp_ratios[["c"]] + acp_ratios[["p"]], kernel = use_kernel)
  
  dens_freq <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp$AuthorTimeNormalized, ratio = 1, kernel = use_kernel)
  
  # All densities need to be scaled together once more, by dividing
  # for the maximum value of the FREQ-variable.
  ymax <- max(c(attr(dens_a, "ymax"), attr(dens_cp, "ymax"), attr(dens_freq, "ymax")))
  dens_a <- stats::approxfun(
    x = attr(dens_a, "x"), y = sapply(X = attr(dens_a, "x"), FUN = dens_a) / ymax)
  dens_cp <- stats::approxfun(
    x = attr(dens_cp, "x"), y = sapply(X = attr(dens_cp, "x"), FUN = dens_cp) / ymax)
  dens_freq <- stats::approxfun(
    x = attr(dens_freq, "x"), y = sapply(X = attr(dens_freq, "x"), FUN = dens_freq) / ymax)
  
  project_signals[[pId]] <- list(
    A = Signal$new(name = paste(pId, "A", sep = "_"),
                   func = dens_a, support = c(0, 1), isWp = FALSE),
    CP = Signal$new(name = paste(pId, "CP", sep = "_"),
                    func = dens_cp, support = c(0, 1), isWp = FALSE),
    FREQ = Signal$new(name = paste(pId, "FREQ", sep = "_"),
                      func = dens_freq, support = c(0, 1), isWp = FALSE)
  )
}
```

Now, for each project, we estimate the variable for the source code density as follows:

```{r warning=FALSE}
for (pId in levels(student_projects$project)) {
  temp <- data.frame(
    x = student_projects[student_projects$project == pId, ]$AuthorTimeNormalized,
    y = student_projects[student_projects$project == pId, ]$Density)
  temp <- temp[with(temp, order(x)), ]
  
  # Using a polynomial with maximum possible degree, we smooth the
  # SCD-data, as it can be quite "peaky"
  temp_poly <- poly_autofit_max(x = temp$x, y = temp$y, startDeg = 13)
  
  dens_scd <- Vectorize((function() {
    rx <- range(temp$x)
    ry <- range(temp$y)
    poly_y <- stats::predict(temp_poly, x = temp$x)
    tempf <- stats::approxfun(x = temp$x, y = poly_y, ties = "ordered")
    function(x) {
      if (x < rx[1] || x > rx[2]) {
        return(NA_real_)
      }
      max(ry[1], min(ry[2], tempf(x)))
    }
  })())
  
  project_signals[[pId]][["SCD"]] <- Signal$new(
    name = paste(pId, "SCD", sep = "_"), func = dens_scd,
    support = c(0, 1), isWp = FALSE)
}
```

Let's plot all the projects:

```{r echo=FALSE}
tempdf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(tempdf) <- c("x", "y", "p", "v")

n <- 500
x <- seq(from = 0, to = 1, length.out = n)
for (pId in levels(student_projects$project)) {
  for (v in c("A", "CP", "FREQ", "SCD")) {
    tempdf <- rbind(tempdf, data.frame(
      x = x,
      y = sapply(X = x, FUN = project_signals[[pId]][[v]]$get0Function()),
      p = pId,
      v = v
    ))
  }
}
```


```{r project-vars, echo=FALSE, fig.cap="All variables over each project's time span", fig.align="top", fig.pos="ht!"}
ggplot(data = tempdf, aes(x = x, y = y, color = v)) +
  geom_line() +
  facet_wrap(p ~.) +
  theme_light() +
  labs(color = "Variable") + xlab("Relative Time") + ylab("Value") +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"))

```

# Patterns for scoring the projects

Our overall goal is to propose a single model that is able to detect the presence of the Fire Drill AP, and how strong its manifestation is. In order to do that, we require a pattern that defines how a Fire Drill looks in practice. Any real-world project can never follow such a pattern perfectly, because of, e.g., time dilation and compression. Even after correcting these, some distance between the project and the pattern will remain. The projects from figure \ref{fig:project-vars} indicate that certain phases occur, but that their occurrence happens at different points in time, and lasts for various durations.

Given some pattern, we first attempt to remove any distortions in the data, by using our new model _self-regularizing Boundary Time Warping_ (sr-BTW). This model takes a pattern that is subdivided into one or more intervals, and aligns the project data such that the loss in each interval is minimized. After alignment, we calculate a score that quantifies the remaining differences. Ideally, we hope to find a (strong) positive correlation of these scores with the ground truth.


## Pattern I: Initial best guess

```{r}
fd_data_concat <- readRDS("../data/fd_data_concat.rds")
```

This pattern was created based on all available literature, __without__ inspecting any of the projects. It is subdivided into four intervals:

1. Begin -- Short project warm-up phase
2. Long Stretch -- The longest phase in the project, about which we do not know much about, except for that there should be a rather constant amount of activities over time.
3. Fire Drill -- Characteristic is a sudden and steep increase of adaptive activities. This phase is over once these activities reached their apex.
4. Aftermath -- Everything after the apex. We should see even steeper declines.


@brown1998refactoring describe a typical scenario where about six months are spent on non-developmental activities, and the actual software is then developed in less than four weeks. If we were to include some of the aftermath, the above first guess would describe a project of about eight weeks.

We define the boundaries as follows (there are three boundaries to split the pattern into four intervals):

```{r}
fd_data_boundaries <- c("b1" = 0.085, "b2" = 0.625, "b3" = 0.875)
```

The pattern and its boundaries look like this:

```{r pattern-1, fig.cap="The pattern that was our initial best guess", fig.align="top", fig.pos="ht!"}
plot_project_data(data = fd_data_concat, boundaries = fd_data_boundaries)
```

### Initialize the pattern

The pattern as shown in \ref{fig:pattern-1} is just a collection of x/y coordinate-data, and for us being able to use it, we need to instantiate it. We do this by storing each variable in an instance of `Signal`.

```{r}
p1_signals <- list(
  A = Signal$new(name = "p1_A", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "A", ]$x,
      y = fd_data_concat[fd_data_concat$t == "A", ]$y)),
  CP = Signal$new(name = "p1_CP", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "CP", ]$x,
      y = fd_data_concat[fd_data_concat$t == "CP", ]$y)),
  FREQ = Signal$new(name = "p1_FREQ", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "FREQ", ]$x,
      y = fd_data_concat[fd_data_concat$t == "FREQ", ]$y)),
  SCD = Signal$new(name = "p1_SCD", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "SCD", ]$x,
      y = fd_data_concat[fd_data_concat$t == "SCD", ]$y))
)
```

```{r echo=FALSE, fig.height=1.8}
ggarrange(
  p1_signals$A$plot() + ylim(0,1), p1_signals$CP$plot() + ylim(0,1),
  p1_signals$FREQ$plot() + ylim(0,1), p1_signals$SCD$plot() + ylim(0,1),
  nrow = 1
)
```



## Pattern II: Adaptation of best guess

The second pattern is a compromise between the first and the third: While we want to keep as much of the initial best guess, we also want to adjust the pattern based on the projects and the ground truth. Adjusting means, that we will keep what is in each interval, but we allow each interval to stretch and compress, and we allow each interval to impose a vertical translation both at then begin and end (a somewhat trapezoidal translation). In any case, each such alteration is a linear affine transformation. Additionally to sr-BTW, we will also apply __sr-BAW__ (self-regularizing Boundary Amplitude Warping) to accomplish this. This model is called __`srBTAW`__ and the process is the following:

* The pattern is decomposed into its four variables first, as we can adapt these (almost) independently from each other.
* Then, for each type of variable, an instance of `srBTAW` is created. As __Warping Candidates__ (WC) we add all of the projects' corresponding variables. The __Warping Pattern__ (WP) is the single variable from the pattern in this case -- again, we warp the project data, however, eventually the learned warping gets inversed and applied to the WC.
* All four `srBTAW` instances are then fitted simultaneously: While we allow the y-translations to adapt independently for each type of variable, all instances share the same intervals, as eventually we have to assemble the variables back into a common pattern.

### Preparation

We already have the `srBTAW` __Multilevel model__, which can keep track of arbitrary many variables and losses. The intention behind this however was, to track variables of the __same type__, i.e., signals that are logically of the same type. In our case this means that any single instance should only track variables that are either `A`, `CP`, `FREQ` or `SCD`. For this pattern, the WP is a single signal per variable, and the WC is the corresponding signal from each of the nine projects. This is furthermore important to give different weights to different variables. In our case, we want to give a lower weight to the `SCD`-variable.

As for the loss, we will first test a combined loss that measures __`3`__ properties: The area between curves (or alternatively the residual sum of squares), the correlation between the curves, and the arc-length ratio between the curves. We will consider any of these to be equally important, i.e., no additional weights. Each loss shall cover all intervals with weight $=1$, except for the Long Stretch interval, where we will use a reduced weight.

There are $4$ types of variables, $7$ projects (two projects have consensus $=0$, i.e., no weight) and $2\times 3$ single losses, resulting in $168$ losses to compute. The final weight for each loss is computed as: $\omega_i=\omega^{(\text{project})}\times\omega^{(\text{vartype})}\times\omega^{(\text{interval})}$. For the phase Long Stretch, the weight for any loss will $\frac{1}{2}$, and for the source code density we will chose $\frac{1}{2}$, too. The weight of each project is based on the consensus of the ground truth. The ordinal scale for that is $[0,10]$, so that we will divide the score by $10$ and use that as weight. Examples:

* __A__ in Fire Drill in project $p3$: $\omega=0.6\times 1\times 1=0.6$ (consensus is $6$ in project $p3$)
* __FREQ__ in Long Stretch in project $p7$: $\omega=0.3\times 0.5\times 1=0.15$ and
* __SCD__ in Long Stretch in project $p4$: $\omega=0.8\times 0.5\times 0.5=0.2$.

In table \ref{tab:groundtruth-score} we show all projects with a consensus-score $>0$, projects $2$ and $8$ are not included any longer.

```{r}
ground_truth$consensus_score <- ground_truth$consensus / 10
weight_vartype <- c("A" = 1, "CP" = 1, "FREQ" = 1, "SCD" = 0.5)
weight_interval <- c("Begin" = 1, "Long Stretch" = 0.5, "Fire Drill" = 1, "Aftermath" = 1)
```

```{r echo=FALSE}
temp <- expand.grid(weight_interval, weight_vartype, ground_truth$consensus_score)
temp$p <- temp$Var1 * temp$Var2 * temp$Var3
weight_total <- sum(temp$p)
```

The sum of all weights combined is `r weight_total`.

```{r echo=FALSE}
knitr::kable(
  x = ground_truth[ground_truth$consensus > 0, c("project", "consensus", "consensus_score")],
  booktabs = TRUE,
  caption = "Entire ground truth as of both raters",
  label = "groundtruth-score"
)
```

### Defining the losses

For the optimization we will use mainly __`5`__ classes:

* `srBTAW_MultiVartype`: One instance globally, that manages all parameters across all instances of `srBTAW`.
* `srBTAW`: One instance per variable-type, so here we'll end up with four instances.
* `srBTAW_LossLinearScalarizer`: A linear scalarizer that will take on all of the defined singular losses and compute and add them together according to their weight.
* `srBTAW_Loss2Curves`: Used for each of the $168$ singular losses, and configured using a specific loss function, weight, and set of intervals where it ought to be used.
* `TimeWarpRegularization`: One global instance for all `srBTAW` instances, to regularize extreme intervals. We chose a mild weight for this of just $1$, which is small compared to the sum of all other weights (`r weight_total`).

```{r}
p2_smv <- srBTAW_MultiVartype$new()

p2_vars <- c("A", "CP", "FREQ", "SCD")
p2_inst <- list()
for (name in p2_vars) {
  p2_inst[[name]] <- srBTAW$new(
    theta_b = c(0, fd_data_boundaries, 1),
    gamma_bed = c(0, 1, sqrt(.Machine$double.eps)),
    lambda = rep(sqrt(.Machine$double.eps), length(p2_vars)),
    begin = 0, end = 1, openBegin = FALSE, openEnd = FALSE,
    useAmplitudeWarping = TRUE,
    lambda_ymin = rep(0, length(p2_vars)),
    lambda_ymax = rep(1, length(p2_vars)),
    isObjectiveLogarithmic = TRUE,
    paramNames = c("v",
                   paste0("vtl_", seq_len(length.out = length(p2_vars))),
                   paste0("vty_", seq_len(length.out = length(p2_vars)))))
  
  # We can already add the WP:
  p2_inst[[name]]$setSignal(signal = p1_signals[[name]])
  p2_smv$setSrbtaw(varName = name, srbtaw = p2_inst[[name]])
  
  # .. and also all the projects' signals:
  for (project in ground_truth[ground_truth$consensus > 0, ]$project) {
    p2_inst[[name]]$setSignal(signal = project_signals[[project]][[name]])
  }
}

# We call this there so there are parameters present.
p2_smv$setParams(params =
                   `names<-`(x = runif(n = p2_smv$getNumParams()),
                             value = p2_smv$getParamNames()))
```

We can already initialize the linear scalarizer. This includes also to set up some progress-callback. Even with massive parallelization, this process will take its time so it will be good to know where we are approximately.

```{r}
p2_lls <- srBTAW_LossLinearScalarizer$new(
  returnRaw = FALSE,
  computeParallel = TRUE, progressCallback = function(what, step, total) {
    if (step == total) {
      print(paste(what, step, total))
    }
  })

for (name in names(p2_inst)) {
  p2_inst[[name]]$setObjective(obj = p2_lls)
}
```

The basic infrastructure stands, so now it's time to instantiate all of the singular losses. First we define a helper-function to do the bulk-work, then we iterate all projects, variables and intervals.

```{r}
#' This function creates a singular loss that is a linear combination
#' of an area-, correlation- and arclength-loss (all with same weight).
p2_attach_combined_loss <- function(project, vartype, intervals) {
  weight_p <- ground_truth[ground_truth$project == project, ]$consensus_score
  weight_v <- weight_vartype[[vartype]]
  temp <- weight_interval[intervals]
  stopifnot(length(unique(temp)) == 1)
  weight_i <- unique(temp)
  weight <- weight_p * weight_v * weight_i
  
  lossRss <- srBTAW_Loss_Rss$new(
    wpName = paste0("p1_", vartype), wcName = paste(project, vartype, sep = "_"),
    weight = weight, intervals = intervals, continuous = FALSE,
    numSamples = rep(500, length(intervals)), returnRaw = TRUE)
  
  p2_inst[[vartype]]$addLoss(loss = lossRss)
  p2_lls$setObjective(
    name = paste(project, vartype, paste(intervals, collapse = "_"),
                 "rss", sep = "_"), obj = lossRss)
}
```

Let's call our helper in iteratively:

```{r}
interval_types <- list(A = c(1,3,4), B = 2)


for (vartype in p2_vars) {
  for (project in ground_truth[ground_truth$consensus > 0, ]$project) {
    for (intervals in interval_types) {
      p2_attach_combined_loss(
        project = project, vartype = vartype, intervals = intervals)
    }
  }
  
  # Add one per variable-pair:
  lossYtrans <- YTransRegularization$new(
    wpName = paste0("p1_", vartype), wcName = paste(project, vartype, sep = "_"),
    intervals = seq_len(length.out = 4), returnRaw = TRUE,
    weight = 1, use = "tikhonov")
    
  p2_inst[[vartype]]$addLoss(loss = lossYtrans)
  p2_lls$setObjective(
    name = paste(vartype, "p2_reg_output", sep = "_"),
    obj = lossYtrans)
}
```


Finally, we add the regularizer for extreme intervals:

```{r}
p2_lls$setObjective(name = "p2_reg_exint2", obj = TimeWarpRegularization$new(
  weight = p2_lls$getNumObjectives() / 20, use = "exint2", returnRaw = TRUE,
  wpName = p1_signals$A$getName(), wcName = project_signals$project_1$A$getName(),
  intervals = seq_len(length.out = length(p2_vars))
)$setSrBtaw(srbtaw = p2_inst$A))
```


### Fitting the pattern

```{r p2-params}
p2_params <- loadResultsOrCompute(file = "../results/p2_params.rds", computeExpr = {
  cl <- parallel::makePSOCKcluster(min(64, parallel::detectCores()))
  tempf <- tempfile()
  saveRDS(object = list(a = p2_smv, b = p2_lls), file = tempf)
  parallel::clusterExport(cl, varlist = list("tempf"))
  
  res <- doWithParallelClusterExplicit(cl = cl, expr = {
    optimParallel::optimParallel(
      par = p2_smv$getParams(),
      method = "L-BFGS-B",
      lower = c(
        rep(0, length(p2_vars)), # v_[vartype]
        rep(sqrt(.Machine$double.eps), length(p2_vars)), # vtl
        rep(-.Machine$double.xmax, length(p2_vars) * length(weight_vartype))), # vty for each
      upper = c(
        rep(1, length(p2_vars)),
        rep(1, length(p2_vars)),
        rep(.Machine$double.xmax, length(p2_vars) * length(weight_vartype))),
      fn = function(x) {
        temp <- readRDS(file = tempf)
        temp$a$setParams(params = x)
        temp$b$compute0()
      },
      parallel = list(cl = cl, forward = FALSE, loginfo = TRUE)
    )
  })
})
```

### Inversing the parameters


## Pattern III: Evidence-based

A third kind of pattern is produced by starting with an empty pattern and having it adapt to the available ground truth. Here we inverse the relation WP-WC: All data is becoming the constant Warping Pattern, and we warp an empty pattern as candidate to it. Empty means that we will start with a flat line located at $0.5$ for each pattern. To find the optimum amount of intervals, we try all values in a certain range and compute a fit, and then use an information criterion to decide which of the produced patterns provides the best trade-off between number of parameters and goodness-of-fit.

The process is very similar to the one for pattern II: Using a set of `srBTAW` instances (one instance per variable type) that all share the same intervals, we allow each to produce individual amplitude warpings.

TODO: Remember to always use constant amount of samples if discrete, i.e., amount per interval = total amount / intervals!


# Scoring of projects

The true main-purpose of our work is to take a pattern and check it against any project, with the goal of obtaining a score, or goodness-of-match so that we can determine if the AP in the pattern is present in the project. In the previous sections we have introduced a number of patterns that we are going to apply here.

How it works: Given some pattern that consists of one or arbitrary many signals, the pattern is added to a single instance of `srBTAW` as __Warping Pattern__. The project's signals are added as __Warping Candidates__ to the same instance.

To compute a score, we need to define how to measure the distance between the WP and the WC (between each pair of signals and each interval). In the notebooks for sr-BTW we have previously defined some suitable losses with either __global__ or __local__ finite upper bounds. Currently, only the Jenson--Shannon divergence has a global upper bound of $\ln{2}$. Losses with local finite upper bound are, for example, the area between curves, the residual sum of squares, the correlation, and the arc-length ratio between two curves. For some of the patterns, we have used a combination of such losses with local bounds. In general, it is not necessary to fit a pattern with the same kinds of losses that are later on used for scoring, but it is recommended to avoid confusing may results.

For scoring a single project, we first warp it to the pattern, then we measure the remaining distance. We only do time-warping.

## Pattern I

```{r}
#' This function creates a new instance of \code{srBTAW} and adds all
#' the signals of pattern 1 and the given project to it. Then it creates
#' a single RSS-loss per variable pair and adds it to the model and a
#' linear scalarizer that is used as objective.
time_warp_project_p1 <- function(project) {
  stopifnot(is.logical(all.equal(names(p1_signals), names(project))))
  
  obj <- srBTAW_LossLinearScalarizer$new(
    computeParallel = TRUE, gradientParallel = TRUE, returnRaw = FALSE)
  
  inst <- srBTAW$new(
    theta_b = c(0, fd_data_boundaries, 1),
    gamma_bed = c(0, 1, sqrt(.Machine$double.eps)),
    lambda = rep(sqrt(.Machine$double.eps), 4),
    begin = 0, end = 1, openBegin = FALSE, openEnd = FALSE,
    useAmplitudeWarping = FALSE)
  inst$setParams(params = `names<-`(rep(1/4, 4), inst$getParamNames()))
  
  for (vartype in names(p1_signals)) {
    inst$setSignal(signal = p1_signals[[vartype]])
    inst$setSignal(signal = project[[vartype]])
    
    loss <- srBTAW_Loss_Rss$new(
      intervals = seq_len(length.out = 4), returnRaw = TRUE, # NOT logarithmic!
      wpName = p1_signals[[vartype]]$getName(), wcName = project[[vartype]]$getName(),
      weight = 1, continuous = FALSE, numSamples = rep(1e3, length(names(project))))
    inst$addLoss(loss = loss)
    obj$setObjective(name = paste("rss", vartype, sep = "_"), obj = loss)
  }
  
  # Add a time-warp regularizer:
  reg <- TimeWarpRegularization$new(
    weight = 1/2, use = "exint2", wpName = p1_signals$A$getName(), wcName = project$A$getName(),
    returnRaw = TRUE, intervals = seq_len(length.out = length(names(project))))
  inst$addLoss(loss = reg)
  obj$setObjective(name = "reg_exint2", obj = reg)

  inst$setObjective(obj = obj)
}
```

```{r echo=FALSE, eval=FALSE}
# doWithParallelCluster({
#   temp <- time_warp_project_p1(project = project_signals$project_1)
#   temp$fit(verbose = TRUE)
# })
```


```{r scores-p1}
library(foreach)

scores_p1 <- loadResultsOrCompute(file = "../results/p1_scores.rds", computeExpr = {
  # Let's compute all projects in parallel!
  cl <- parallel::makePSOCKcluster(length(project_signals))
  unlist(doWithParallelClusterExplicit(cl = cl, expr = {
    foreach::foreach(
      projectName = names(project_signals),
      .inorder = FALSE,
      .packages = c("parallel")
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      
      # There are 5 objectives that can be computed in parallel!
      cl_nested <- parallel::makePSOCKcluster(5)
      `names<-`(list(doWithParallelClusterExplicit(cl = cl_nested, expr = {
        temp <- time_warp_project_p1(project = project_signals[[projectName]])
        temp$fit(verbose = TRUE)
        temp # return the instance, it includes the FitResult
      })), projectName)
    }
  }))
})
```



# References {-}

<div id="refs"></div>






















