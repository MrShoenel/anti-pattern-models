---
title: "Technical Report: Detecting the Fire Drill anti-pattern using Source Code"
author: "Sebastian Hönel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ../inst/REFERENCES.bib
urlcolor: blue
output:
  md_document:
    toc: yes
    toc_depth: 6
    df_print: kable
    variant: gfm
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: kable
  word_document: default
header-includes:
- \usepackage{bm}
- \usepackage{mathtools}
- \usepackage{xurl}
---

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\norm}[1]{\left\lvert#1\right\rvert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}

```{r setoptions, echo=FALSE, warning=FALSE, message=FALSE}
Sys.setenv(LANG = "en_US.UTF-8")
Sys.setenv(LC_ALL = "en_US.UTF-8")
Sys.setenv(LC_CTYPE = "en_US.UTF-8")
library(knitr)
opts_chunk$set(tidy = TRUE, tidy.opts = list(indent=2))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

library(ggplot2)
library(ggpubr)
```

# Introduction\label{tr:fire-drill-technical-report}

This is the complementary technical report for the paper/article tentatively entitled "Multivariate Continuous Processes: Modeling, Instantiation, Goodness-of-fit, Forecasting". Here, we import the ground truth as well as all projects' data, and instantiate our model based on _self-regularizing Boundary Time Warping and Boundary Amplitude Warping_. Given a few patterns that represent the __Fire Drill__ anti-pattern (AP), the goal is evaluate these patterns and their aptitude for detecting the AP in concordance with the ground truth.

All complementary data and results can be found at Zenodo [@honel_picha_2021]. This notebook was written in a way that it can be run without any additional efforts to reproduce the outputs (using the pre-computed results). This notebook has a canonical URL^[[Link]](https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/fire-drill-technical-report.Rmd)^ and can be read online as a rendered markdown^[[Link]](https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/fire-drill-technical-report.md)^ version. All code can be found in this repository, too.

# Fire Drill - anti-pattern

We describe the Fire Drill (FD) anti-pattern for usage in models that are based on the source code (i.e., not from a managerial or project management perspective). The purpose also is to start with a best guess, and then to iteratively improve the description when new evidence is available.

FD is described now both from a managerial and a technical perspective^[[Link]](https://github.com/ReliSA/Software-process-antipatterns-catalogue/pull/13/commits/78c06c30b1880795e6c1dd0f20f146c548212675?short_path=01589ac#diff-01589ac85c3fc29739823b5a41ab1bbfba7fbc2579aaf63de0f1ce31713689ab)^. The technical description is limited to variables we can observe, such as the amount (frequency) of commits and source code, the density of source code, and maintenance activities (a/c/p).

In literature, FD is described in [@silva2015software] and [@brown1998refactoring], as well as at^[[Link]](https://web.archive.org/web/20210414150555/https://sourcemaking.com/antipatterns/fire-drill)^.

Currently, FD is defined to have these symptoms and consequences:

*   Rock-edge burndown (especially when viewing implementation tasks only)
*   Long period at project start where activities connected to requirements, analysis and planning prevale, and design and implementation activities are rare
*   Only analytical or documentational artefacts for a long time
*   Relatively short period towards project end with sudden increase in development efforts
*   Little testing/QA and project progress tracking activities during development period
*   Final product with poor code quality, many open bug reports, poor or patchy documentation
*   Stark contrast between inter-level communication in project hierarchy (management - developers) during the first period (close to silence) and after realizing the problem (panic and loud noise

From these descriptions, we have attempted to derive the following symptoms and consequences in source code:

*   Rock-edge burndown of esp. implementation tasks mean there are no or just very few adaptive maintenance activities before the burning down starts
*   The long period at project start translates to few modifications made to the source code, resulting in fewer commits (lower overall relative frequency)
*   Likewise, documentational artifacts have a lower source code density, as less functionality is delivered; this density should increase as soon as adaptive activities are registered
*   The short period at project end is characterized by a higher frequency of higher-density implementation tasks, with little to no perfective or corrective work
*   At the end of the project, code quality is comparatively lower, while complexity is probably higher, due to pressure exerted on developers in the burst phase

## Prerequisites

Through source code, we can observe the following variables (and how they change over time). We have the means to model and match complex behavior for each variable over time. By temporally subdividing the course of a random variable, we can introduce additional measures for a pattern, that are based on comparing the two intervals (e.g., mean, steepest slope, comparisons of the shape etc.).

*   Amount of commits over interval of time (frequency) -- We can observe any commit, both at when it was authored first, and when it was added to the repository (author-/committer-date).
*   Amount/Frequency of each maintenance activity separately
*   Density of the source code (also possibly per activity if required)
Any other metric, if available (e.g., total Quality or Complexity of project at each commit) -- however, we need to distinguish random variables by whether their relative frequency (when they are changing or simply when they are observed) changes the shape of the function, or whether it only leads to different sample rates. In case of metrics, the latter is the case. In other words, for some variables their occurrence is important, while for others it is the observed value.

It is probably the most straightforward way to decompose a complex pattern such as Fire Drill into sub-processes, one for each random variable. That has several advantages:

*   We are not bound/limited to only one global aggregated match, which could hide alignment details.
*   We can quantify the goodness of match for each variable separately, including details such as the interval in which it matched, and how well it matched in it.
*   Matching separately allows us to come up with our own scoring methods; e.g., it could be that the matching-score of one variable needs to be differently computed than the score of another, or simply the weights between variables are different.
*   If a larger process was temporally subdivided, we may want to score a variable in one of the intervals differently, or not at all. This is useful for when we cannot make sufficient assumptions.

## Modeling the Fire Drill

In this section, we will collect attempts to model the Fire Drill anti-pattern. The first attempt is our initial guess, and subsequent attempts are based on new input (evidence, opinions/discussions etc.).

### Initial Best Guess

Our initial best guess is solely based on the literature and descriptions from above, and no dataset was inspected yet. We chose to model our initial best guess using a visual process. Figure \ref{fig:initial-best-guess} must be understood as a simplified and idealized approximation. While we could add a confidence interval for each variable represented, we will later show how align (fit) a project (process) to this pattern (process model), and then measure its deviation from it. The modeled pattern is a **continuous-time stochastic process model**, and we will demonstrate means to quantify the difference between this **process model** and a **process**, which is shown in section \ref{ssec:model-metrics-kde}.

```{r initial-best-guess, echo=FALSE, out.width="75%", fig.cap="Modeling of the Fire Drill anti-pattern over the course of an entire project, according to what we know from literature.", fig.align="center", fig.pos="ht!"}
knitr::include_graphics(
  path = "../figures/Fire-Drill_first-best-guess.png",
  )
```

The pattern is divided into four intervals (five if one counts $t_{\text{x-over}}$ as delimiter, but it is more like an additional point of interest). These intervals are:

1.    $[0, t_1)$ -- Begin
2.    $[t_1, t_2)$ -- Long Stretch
3.    $[t_2, t_{\text{end}}]$ -- Fire Drill
4.    $(t_{\text{end}}, t_3]$ -- Aftermath

In each interval and for each of the random variables modeled, we can perform matching. This also means that a) we do not have to attempt matching in each interval, b) we do not have to perform matching for each variable, and c) that we can select a different set of appropriate measures for each variable in each interval (this is useful if, e.g., we do not have much information for some of them).

Each variable is its own sub-pattern. As of now, we track the maintenance activities, and their frequency over time. A higher accumulation results in a higher peak. One additional variable, the source code density (blue), is not measured by its frequency (occurrences), but rather by its value. We may include and model additional metrics, such as complexity or quality.

Whenever we temporally subdivide the pattern into two intervals, we can take these measurements:

*   Compute the goodness-of-fit of the curve of a variable, compared to its behavior in the data. As of now, that includes a rich set of metrics, all of which can quantify these differences, and for all of which we have developed scores. While we can compute scores for the actual match, we do also have the means to compute the score of the dynamic time warping. Among others, we have these metrics:
    *   Reference and Query signal: Start/end (cut-in/-out) for both absolute & relative
About the DTW match: (relative) monotonicity (continuity of the warping function), residuals of (relative) warping function (against its LM or optimized own version (fitted using RSS))
    *   Between two curves (all of these are normalized as they are computed in the unit square, see R notebook): area (by integration), generic statistics by sampling (mae, rmse, correlation (pearson, kendall, spearman), covariance, standard-deviation, variance, symmetric Kullback-Leibler divergence, symmetric Jenson-Shannon Divergence)
*   Sub-division allows for comparisons of properties of the two intervals, e.g.,
    *   Compare averages of the variable in each interval. This can be easily implemented as a score, as we know the min/max and also do have expectations (lower/higher).
    *   Perform a linear regression/create a linear model (LM) over the variable in each interval, so that we can compare slopes, residuals etc.
*   Cross-over of two variables: This means that a) the two slopes of their respective LM converge and b) that there is a crossover within the interval.

#### Description of Figure \ref{fig:initial-best-guess}

*   The frequency of activities (purple) is the sum of all activities, i.e., this curve is green (corrective + perfective) plus red (adaptive).
*   The frequency is the only variable that may be modeled with its actual maximum of 1, as we expect it to reach its maximum at $t_{\text{end}}$. The frequency also has to be actually 0, before the first commit is made.
    *   Some of our metrics can measure how well one curve resembles the other, regardless of their “vertical difference”. Other metrics can describe the distance better. What I want to say is, it is not very important what the value of a variable in our modeled pattern actually is. But it is important however if it touches 0 or 1. A variable should only be modeled as touching 0 or 1 if it is a) monotonically increasing over the course of the project and b) actually assumes its theoretical min/max.
*   Corrective and Perfective have been aggregated into one variable, as according to the description of Fire Drill, there is only a distinction between adding features and performing other, such as Q/A related, tasks.
*   The source code density (blue) should, with the first commit, jump to its expected value, which is the average over all commits in the project. A steep but short increase is expected in $[t_2, t_{\text{end}}]$ as less focus is spent on documentational artifacts.
*   We do not know much about the activities’ frequency and relative distribution up till $t_2$. That leaves us with two options: either, we make only very modest assumptions about the progression of a variable, such as the slope of its LM in that interval or the residuals. Otherwise, we can also choose not to assess the variable in that interval. It is not yet clear how useful the results from the DTW would be, as we can only model a straight line (that’s why I am suggesting LMs instead). For Fire Drill, the interval $[t_2, t_3]$ is most characteristic. We can however extract some constraints for our expectations between the intervals $[t_2, t_3]$ and everything that happens before. For example, in [BRO’98] it is described that $[0, t_2)$ is about 5-7x longer than $[t_2, t_{\text{end}}]$.
*   $t_{\text{end}}$ is not a delimiter we set manually, but it is rather discovered by the sub-patterns’ matches. However, it needs to be sufficiently close to the project’s actual end (or there needs to be a viable explanation for the difference, such as holidays in between etc.)
*   $t_{\text{x-over}}$ must happen; but other than that, there is not much we can assume about it. We could specify properties as to where approximately we’d expect it to happen (I suppose in the first half of the interval) or how steep the crossover actually is but it is probably hard to rely on.


#### Description Phase-by-Phase

**Begin**: A short phase of a few days, probably not more than a week. While there will be few commits, $t_1$ does not really start until the frequency stabilizes. We expect the maintenance activities’ relative frequency to decrease towards the end of Begin, before they become rather constant in the next phase. In this phase, the source code density is expected to be close to its average, as initial code as well as documentation are added.

**Long Stretch**: This is the phase we do not know much about, except for that the amount of adaptive activities is comparatively lower, especially when compared to the aggregated corrective and perfective activities (approx. less than half of these two). While the activities’ variables will most likely not be perfectly linear, the LM over these should show rather small residuals. Also the slope of the LM is expected to be rather flat (probably less than +/-10°). The source code density is expected to fall slightly below its average after Begin, as less code is shipped.

**Fire Drill**: The actual Fire Drill happens in $[t_2, t_{\text{end}}]$, and we detect $t_{\text{end}}$ by finding the apex of the frequency. However, we choose to extend this interval to include $(t_{\text{end}}, t_3]$, as by doing so, we can craft more characteristics of the anti-pattern and impose more assumptions. These are a) that the steep decline in that last phase has a more extreme slope than its increase before $t_{\text{end}}$ (because after shipping/project end, probably no activity is performed longer). B) This last sub-phase should be shorter than the phase before (probably just up to a few days; note that the phase $[t_2, t_{\text{end}}]$ is described to be approximately one week long in literature).

With somewhat greater confidence, we can define the following:

*   The source code density will rise suddenly and approach its maximum of 1 (however we should not model it with its maximum value to improve matching). It is expected to last until $t_{\text{end}}$, with a sudden decline back to its average from Begin. We do not have more information for after $t_{\text{end}}$, so the average is the expected value.
*   Perfective and corrective activities will vanish quickly and together become the least frequent activity in the project. The average of these activities is expected to be less than half compared to the Long Stretch. Until $t_3$ (the very end), the amount of these activities keeps monotonically decreasing.
*   At the same time, we will see a steep increase of adaptive activity. The increase is expected to be greater than or equal to the decrease of perfective and corrective activities. In other words, the average of adaptive activities is expected to be more than double, compared to what it was in the Long Stretch. Also, adaptive activities will reach their maximum frequency over the course of the project here.
*   The nature of a Fire Drill is a frantic and desperate phase. While adaptive approaches its maximum, the commit frequency also approaches its maximum, even though perfective and corrective activities decline (that is why the purple curve is less steep than the adaptive one but still goes to its global maximum).
*   There will be a sharp crossover between perfective+corrective and adaptive activities. It is expected to happen sooner than later in the phase $[t_2, t_{\text{end}}]$.

**Aftermath**: Again, we do not know how this phase looks, but it will help us to more confidently identify the Fire Drill, as the curves of the activities, frequencies and other metrics have very characteristic curves that we can efficiently match. All metrics that are invariant to the frequency are expected to approach their expected value, without much variance (rather constant slope of their resp. LMs). Any of the maintenance activities will continue to fall. In case of adaptive activities we will see an extraordinary steep decline, as after project end/shipping, no one adds functionality. It should probably even fall below all other activities, resulting in another crossover. We do not set any of the activities to be exactly zero however, to allow more efficient matching.


# Data

We have $9$ projects conducted by students, and two raters have __independently__, i.e., without prior communication, assessed to what degree the AP is present in each project. This was done using a scale from zero to ten, where zero means that the AP was not present, and ten would indicate a strong manifestation.

## The Ground Truth

```{r}
ground_truth <- read.csv(file = "../data/ground-truth.csv", sep = ";")
```

```{r echo=FALSE}
knitr::kable(
  x = ground_truth,
  booktabs = TRUE,
  caption = "Entire ground truth as of both raters",
  label = "groundtruth"
)
```


Using the _quadratic weighted Kappa_ [@cohen1968weighted], we can report an unadjusted agreement of __`r round(Metrics::ScoreQuadraticWeightedKappa(rater.a = ground_truth$rater.a, rater.b = ground_truth$rater.b, min.rating = 0, max.rating = 10), 3)`__ for both raters. A Kappa value in the range $[0.6,0.8]$ is considered _substantial_, and values beyond that as _almost perfect_ [@landis1977application]. As for the Pearson-correlation, we report a slightly higher value of __`r round(cor(x = ground_truth$rater.a, y = ground_truth$rater.b), 3)`__. The entire ground truth is shown in table \ref{tab:groundtruth}. The final consensus was reached after both raters exchanged their opinions, and it is the consensus that we will use as the actual ground truth from here on and out.


## The Student Projects

The ground truth was extracted from nine student-conducted projects. Seven of these were implemented simultaneously between March and June 2020, and two the year before in a similar timeframe.

```{r}
student_projects <- read.csv(file = "../data/student-projects.csv", sep = ";")
```

In the first batch, we have a total of:

* Nine projects,
* `r length(unique(student_projects$AuthorNominalLabel))` authors that authored `r nrow(student_projects)` commits total which are of type
* Adaptive / Corrective / Perfective (`a/c/p`) commits: `r sum(student_projects$label == "a")` / `r sum(student_projects$label == "c")` / `r sum(student_projects$label == "p")`

We have a complete breakdown of all activities across all projects in figure \ref{fig:project-activity}.

```{r}
student_projects_info <- NULL

for (pId in unique(student_projects$project)) {
  temp <- student_projects[student_projects$project == pId, ]
  student_projects_info <- rbind(student_projects_info, data.frame(
    project = pId,
    authors = length(unique(temp$AuthorNominalLabel)),
    commits = nrow(temp),
    a = nrow(temp[temp$label == "a", ]),
    c = nrow(temp[temp$label == "c", ]),
    p = nrow(temp[temp$label == "p", ]),
    avgDens = round(mean(temp$Density), 3)
  ))
}
```


```{r echo=FALSE}
if (interactive()) {
  student_projects_info
} else {
  knitr::kable(
    x = student_projects_info,
    booktabs = TRUE,
    caption = "Per-project overview of the student projects",
    label = "studentprojects"
  )
}
```

```{r project-activity, echo=FALSE, fig.cap="Commit activities across projects", fig.align="top", fig.pos="ht!"}
ggplot(data = student_projects, aes(x = length(label), fill = label)) +
  geom_bar() + facet_grid(label ~ project) +
  theme_light() +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"),
    legend.position = "none")
```

We have slightly different begin- and end-times in each project. However, the data for all projects was previously cropped, so that each project's extent marks the absolute begin and end of it -- it starts with the first commit and ends with the last. As for our methods here, we only need to make sure that we scale the timestamps into a relative $[0,1]$-range, where $1$ marks the project's end.

For each project, we model __four__ variables: The activities __adaptive__ (__`A`__), __corrective+perfective__ (__`CP`__), the frequency of all activities, regardless of their type (__`FREQ`__), and the __source code density__ (__`SCD`__). While for the first three variables we estimate a Kernel density, the last variable is a metric collected with each commit. The data for it is mined using `Git-Density` [@honel2020gitdens], and we use a highly efficient commit classification model[^1] ($\approx83.6\%$ accuracy, $\approx0.745$ Kappa) [@honel2020using] to attach maintenance activity labels to each commit, based on size- and keyword-data only.

[^1]: https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/comm-class-models.Rmd

Technically, we will compose each variable into an instance of our `Signal`-class. Before we start, we will do some normalizations and conversions, like converting the timestamps. This has to be done on a per-project basis.

```{r}
student_projects$label <- as.factor(student_projects$label)
student_projects$project <- as.factor(student_projects$project)
student_projects$AuthorTimeNormalized <- NA_real_
```

```{r}
for (pId in levels(student_projects$project)) {
  student_projects[student_projects$project == pId, ]$AuthorTimeNormalized <-
    (student_projects[student_projects$project == pId, ]$AuthorTimeUnixEpochMilliSecs -
      min(student_projects[student_projects$project == pId, ]$AuthorTimeUnixEpochMilliSecs))
  student_projects[student_projects$project == pId, ]$AuthorTimeNormalized <-
    (student_projects[student_projects$project == pId, ]$AuthorTimeNormalized /
      max(student_projects[student_projects$project == pId, ]$AuthorTimeNormalized))
}
```

And now for the actual signals: Since the timestamps have been normalized for each project, we model each variable to actually start at $0$ and end at $1$ (the support). We will begin with activity-related variables before we model the source code density, as the process is different. When using Kernel density estimation (KDE), we obtain an empirical probability density function (PDF) that integrates to $1$. This is fine when looking at all activities combined (__`FREQ`__). However, when we are interested in a specific fraction of the activities, say __`A`__, then we should scale its activities according to its overall ratio. Adding all scaled activities together should again integrate to $1$. When this is done, we scale one last time such that no empirical PDF has a co-domain larger than $1$.

```{r}
project_signals <- list()

# passed to stats::density
use_kernel <- "gauss" # "rect"

for (pId in levels(student_projects$project)) {
  temp <- student_projects[student_projects$project == pId, ]
  
  # We'll need these for the densities:
  acp_ratios <- table(temp$label) / sum(table(temp$label))
  
  dens_a <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp[temp$label == "a", ]$AuthorTimeNormalized,
    ratio = acp_ratios[["a"]], kernel = use_kernel)
  
  dens_cp <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp[temp$label == "c" | temp$label == "p", ]$AuthorTimeNormalized,
    ratio = acp_ratios[["c"]] + acp_ratios[["p"]], kernel = use_kernel)
  
  dens_freq <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp$AuthorTimeNormalized, ratio = 1, kernel = use_kernel)
  
  # All densities need to be scaled together once more, by dividing
  # for the maximum value of the FREQ-variable.
  ymax <- max(c(attr(dens_a, "ymax"), attr(dens_cp, "ymax"), attr(dens_freq, "ymax")))
  dens_a <- stats::approxfun(
    x = attr(dens_a, "x"), y = sapply(X = attr(dens_a, "x"), FUN = dens_a) / ymax)
  dens_cp <- stats::approxfun(
    x = attr(dens_cp, "x"), y = sapply(X = attr(dens_cp, "x"), FUN = dens_cp) / ymax)
  dens_freq <- stats::approxfun(
    x = attr(dens_freq, "x"), y = sapply(X = attr(dens_freq, "x"), FUN = dens_freq) / ymax)
  
  project_signals[[pId]] <- list(
    A = Signal$new(name = paste(pId, "A", sep = "_"),
                   func = dens_a, support = c(0, 1), isWp = FALSE),
    CP = Signal$new(name = paste(pId, "CP", sep = "_"),
                    func = dens_cp, support = c(0, 1), isWp = FALSE),
    FREQ = Signal$new(name = paste(pId, "FREQ", sep = "_"),
                      func = dens_freq, support = c(0, 1), isWp = FALSE)
  )
}
```

Now, for each project, we estimate the variable for the source code density as follows:

```{r warning=FALSE}
for (pId in levels(student_projects$project)) {
  temp <- data.frame(
    x = student_projects[student_projects$project == pId, ]$AuthorTimeNormalized,
    y = student_projects[student_projects$project == pId, ]$Density)
  temp <- temp[with(temp, order(x)), ]
  
  # Using a polynomial with maximum possible degree, we smooth the
  # SCD-data, as it can be quite "peaky"
  temp_poly <- poly_autofit_max(x = temp$x, y = temp$y, startDeg = 13)
  
  dens_scd <- Vectorize((function() {
    rx <- range(temp$x)
    ry <- range(temp$y)
    poly_y <- stats::predict(temp_poly, x = temp$x)
    tempf <- stats::approxfun(x = temp$x, y = poly_y, ties = "ordered")
    function(x) {
      if (x < rx[1] || x > rx[2]) {
        return(NA_real_)
      }
      max(ry[1], min(ry[2], tempf(x)))
    }
  })())
  
  project_signals[[pId]][["SCD"]] <- Signal$new(
    name = paste(pId, "SCD", sep = "_"), func = dens_scd,
    support = c(0, 1), isWp = FALSE)
}
```

Let's plot all the projects:

```{r echo=FALSE}
tempdf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(tempdf) <- c("x", "y", "p", "v")

n <- 500
x <- seq(from = 0, to = 1, length.out = n)
for (pId in levels(student_projects$project)) {
  for (v in c("A", "CP", "FREQ", "SCD")) {
    tempdf <- rbind(tempdf, data.frame(
      x = x,
      y = sapply(X = x, FUN = project_signals[[pId]][[v]]$get0Function()),
      p = pId,
      v = v
    ))
  }
}
```


```{r project-vars, echo=FALSE, fig.cap="All variables over each project's time span (first batch of projects).", fig.align="top", fig.pos="ht!"}
plot_all_acp <- ggplot(data = tempdf, aes(x = x, y = y, color = v)) +
  geom_line() +
  facet_wrap(p ~.) +
  theme_light() +
  labs(color = "Variable") + xlab("Relative Time") + ylab("Value") +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"))
plot_all_acp
```

```{r echo=FALSE, eval=FALSE}
# Let's save this as tikz:
tikzDevice::tikz(file = "../figures/all_acp.tex", width = 3.4, height = 1.8)
plot_all_acp +
  theme_light(base_size = 10) +
  theme(
    axis.title.x.bottom = element_text(margin = margin(b=5), size = 10),
    axis.title.y.left = element_text(margin = margin(r=3), size = 10),
    legend.position = "bottom",
    legend.margin = margin(-7.5, 0, 0, 0),
    strip.background = element_rect(fill="#dfdfdf"))
dev.off()
```


## Modeling of metrics and events using KDE\label{ssec:model-metrics-kde}

We need to make an important distinction between events and metrics. An event does not carry other information, other than that it occurred. One could thus say that such an event is _nulli_-variate. If an event were to carry extra information, such as a measurement that was taken, it would be _uni_-variate. That is the case for many metrics in software: the time of their measurement coincides with an event, such as a commit that was made. On the time-axis we thus know __when__ it occurred and __what__ was its value. Such a metric could be easily understood as a _bivariate x/y_ variable and be plotted in a two-dimensional Cartesian coordinate system.

An event however does not have any associated y-value we could plot. Given a time-axis, we could make a mark whenever it occurred. Some of the markers would probably be closer to each other or be more or less accumulated. The y-value could express these accumulations relative to each other. These are called _densities_. This is exactly what KDE does: it expresses the relative accumulations of data on the x-axis as density on the y-axis. For KDE, the actual values on the x-axis have another meaning, and that is to compare the relative likelihoods of the values on it, since the axis is ordered. For our case however, the axis is linear time and carries no such meaning. The project data we analyze is a kind of sampling over the project's events. We subdivide the gathered project data hence into these two types of data series:

* __Events__: They do not carry any extra information or measurements. As for the projects we analyze, events usually are occurrences of specific types of commits, for example. The time of occurrence is the x-value on the time-axis, and the y-value is obtained through KDE. We model all maintenance activities as such variables.
* __Metrics__: Extracted from the project at specific times, for example at every commit. We can extract any number or type of metric, but each becomes its own variable, where the x-value is on the time-axis, and the y-value is the metric's value. We model the source code density as such a variable.


# Patterns for scoring the projects

Our overall goal is to propose a single model that is able to detect the presence of the Fire Drill AP, and how strong its manifestation is. In order to do that, we require a pattern that defines how a Fire Drill looks in practice. Any real-world project can never follow such a pattern perfectly, because of, e.g., time dilation and compression. Even after correcting these, some distance between the project and the pattern will remain. The projects from figure \ref{fig:project-vars} indicate that certain phases occur, but that their occurrence happens at different points in time, and lasts for various durations.

Given some pattern, we first attempt to remove any distortions in the data, by using our new model _self-regularizing Boundary Time Warping_ (sr-BTW). This model takes a pattern that is subdivided into one or more intervals, and aligns the project data such that the loss in each interval is minimized. After alignment, we calculate a score that quantifies the remaining differences. Ideally, we hope to find a (strong) positive correlation of these scores with the ground truth.


## Pattern I: Initial best guess

```{r}
fd_data_concat <- readRDS("../data/fd_data_concat.rds")
```

This pattern was created based on all available literature, __without__ inspecting any of the projects. It is subdivided into four intervals:

1. Begin -- Short project warm-up phase
2. Long Stretch -- The longest phase in the project, about which we do not know much about, except for that there should be a rather constant amount of activities over time.
3. Fire Drill -- Characteristic is a sudden and steep increase of adaptive activities. This phase is over once these activities reached their apex.
4. Aftermath -- Everything after the apex. We should see even steeper declines.


@brown1998refactoring describe a typical scenario where about six months are spent on non-developmental activities, and the actual software is then developed in less than four weeks. If we were to include some of the aftermath, the above first guess would describe a project of about eight weeks.

We define the boundaries as follows (there are three boundaries to split the pattern into four intervals):

```{r}
fd_data_boundaries <- c("b1" = 0.085, "b2" = 0.625, "b3" = 0.875)
```

The pattern and its boundaries look like this:

```{r pattern-1, fig.cap="The pattern that was our initial best guess", fig.align="top", fig.pos="ht!"}
plot_project_data(data = fd_data_concat, boundaries = fd_data_boundaries)
```

```{r echo=FALSE, eval=FALSE}
tikzDevice::tikz(file = "../figures/pattern-I.tex", width = 3.4, height = 1.8)
plot_project_data(data = fd_data_concat, boundaries = fd_data_boundaries) +
  xlab("Normalized time") + ylab("Magnitude") +
  scale_x_continuous(breaks = seq(0, 1, by = 0.1), position = "top") +
  theme_light(base_size = 10) +
  theme(
    axis.title.x.top = element_text(margin = margin(b=5), size = 10),
    axis.title.y = element_text(margin = margin(r=3), size = 10),
    legend.position = "bottom",
    legend.margin = margin(-7.5, 0, 0, 0),
    strip.background = element_rect(fill="#dfdfdf"))
dev.off()
```


### Initialize the pattern

The pattern as shown in \ref{fig:pattern-1} is just a collection of x/y coordinate-data, and for us being able to use it, we need to instantiate it. We do this by storing each variable in an instance of `Signal`.

```{r}
p1_signals <- list(
  A = Signal$new(name = "p1_A", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "A", ]$x,
      y = fd_data_concat[fd_data_concat$t == "A", ]$y)),
  CP = Signal$new(name = "p1_CP", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "CP", ]$x,
      y = fd_data_concat[fd_data_concat$t == "CP", ]$y)),
  FREQ = Signal$new(name = "p1_FREQ", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "FREQ", ]$x,
      y = fd_data_concat[fd_data_concat$t == "FREQ", ]$y)),
  SCD = Signal$new(name = "p1_SCD", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "SCD", ]$x,
      y = fd_data_concat[fd_data_concat$t == "SCD", ]$y))
)
```

```{r echo=FALSE, fig.height=1.8, fig.cap="The separate signals of pattern I.", fig.align="top", fig.pos="ht!"}
ggarrange(
  p1_signals$A$plot() + ylim(0,1), p1_signals$CP$plot() + ylim(0,1),
  p1_signals$FREQ$plot() + ylim(0,1), p1_signals$SCD$plot() + ylim(0,1),
  nrow = 1
)
```



## Pattern II: Adaptation of best guess

The second pattern is a compromise between the first and the third: While we want to keep as much of the initial best guess, we also want to adjust the pattern based on the projects and the ground truth. Adjusting means, that we will keep what is in each interval, but we allow each interval to stretch and compress, and we allow each interval to impose a vertical translation both at then begin and end (a somewhat trapezoidal translation). In any case, each such alteration is a linear affine transformation. Additionally to sr-BTW, we will also apply __sr-BAW__ (self-regularizing Boundary Amplitude Warping) to accomplish this. This model is called __`srBTAW`__ and the process is the following:

* The pattern is decomposed into its four variables first, as we can adapt these (almost) independently from each other.
* Then, for each type of variable, an instance of `srBTAW` is created. As __Warping Candidates__ (WC) we add all of the projects' corresponding variables. The __Warping Pattern__ (WP) is the single variable from the pattern in this case -- again, we warp the project data, however, eventually the learned warping gets inversed and applied to the WC.
* All four `srBTAW` instances are then fitted simultaneously: While we allow the y-translations to adapt independently for each type of variable, all instances share the same intervals, as eventually we have to assemble the variables back into a common pattern.

### Preparation

We already have the `srBTAW` __Multilevel model__, which can keep track of arbitrary many variables and losses. The intention behind this however was, to track variables of the __same type__, i.e., signals that are logically of the same type. In our case this means that any single instance should only track variables that are either `A`, `CP`, `FREQ` or `SCD`. For this pattern, the WP is a single signal per variable, and the WC is the corresponding signal from each of the nine projects. This is furthermore important to give different weights to different variables. In our case, we want to give a lower weight to the `SCD`-variable.

As for the loss, we will first test a combined loss that measures __`3`__ properties: The area between curves (or alternatively the residual sum of squares), the correlation between the curves, and the arc-length ratio between the curves. We will consider any of these to be equally important, i.e., no additional weights. Each loss shall cover all intervals with weight $=1$, except for the Long Stretch interval, where we will use a reduced weight.

There are $4$ types of variables, $7$ projects (two projects have consensus $=0$, i.e., no weight) and $2\times 3$ single losses, resulting in $168$ losses to compute. The final weight for each loss is computed as: $\omega_i=\omega^{(\text{project})}\times\omega^{(\text{vartype})}\times\omega^{(\text{interval})}$. For the phase Long Stretch, the weight for any loss will $\frac{1}{2}$, and for the source code density we will chose $\frac{1}{2}$, too. The weight of each project is based on the consensus of the ground truth. The ordinal scale for that is $[0,10]$, so that we will divide the score by $10$ and use that as weight. Examples:

* __A__ in Fire Drill in project $p3$: $\omega=0.6\times 1\times 1=0.6$ (consensus is $6$ in project $p3$)
* __FREQ__ in Long Stretch in project $p7$: $\omega=0.3\times 0.5\times 1=0.15$ and
* __SCD__ in Long Stretch in project $p4$: $\omega=0.8\times 0.5\times 0.5=0.2$.

In table \ref{tab:groundtruth-score} we show all projects with a consensus-score $>0$, projects $2$ and $8$ are not included any longer.

```{r}
ground_truth$consensus_score <- ground_truth$consensus / 10
weight_vartype <- c("A" = 1, "CP" = 1, "FREQ" = 1, "SCD" = 0.5)
weight_interval <- c("Begin" = 1, "Long Stretch" = 0.5, "Fire Drill" = 1, "Aftermath" = 1)
```

```{r}
temp <- expand.grid(weight_interval, weight_vartype, ground_truth$consensus_score)
temp$p <- temp$Var1 * temp$Var2 * temp$Var3
weight_total <- sum(temp$p)
```

The sum of all weights combined is `r weight_total`.

```{r echo=FALSE}
if (interactive()) {
  ground_truth[ground_truth$consensus > 0, c("project", "consensus", "consensus_score")]
} else {
  knitr::kable(
    x = ground_truth[ground_truth$consensus > 0, c("project", "consensus", "consensus_score")],
    booktabs = TRUE,
    caption = "Entire ground truth as of both raters",
    label = "groundtruth-score"
  )
}
```

### Defining the losses

For the optimization we will use mainly __`5`__ classes:

* `srBTAW_MultiVartype`: One instance globally, that manages all parameters across all instances of `srBTAW`.
* `srBTAW`: One instance per variable-type, so here we'll end up with four instances.
* `srBTAW_LossLinearScalarizer`: A linear scalarizer that will take on all of the defined singular losses and compute and add them together according to their weight.
* `srBTAW_Loss2Curves`: Used for each of the $168$ singular losses, and configured using a specific loss function, weight, and set of intervals where it ought to be used.
* `TimeWarpRegularization`: One global instance for all `srBTAW` instances, to regularize extreme intervals. We chose a mild weight for this of just $1$, which is small compared to the sum of all other weights (`r weight_total`).

```{r}
p2_smv <- srBTAW_MultiVartype$new()

p2_vars <- c("A", "CP", "FREQ", "SCD")
p2_inst <- list()
for (name in p2_vars) {
  p2_inst[[name]] <- srBTAW$new(
    theta_b = c(0, fd_data_boundaries, 1),
    gamma_bed = c(0, 1, sqrt(.Machine$double.eps)),
    lambda = rep(sqrt(.Machine$double.eps), length(p2_vars)),
    begin = 0, end = 1, openBegin = FALSE, openEnd = FALSE,
    useAmplitudeWarping = TRUE,
    # We allow these to be larger; however, the final result should be within [0,1]
    lambda_ymin = rep(-10, length(p2_vars)),
    lambda_ymax = rep( 10, length(p2_vars)),
    isObjectiveLogarithmic = TRUE,
    paramNames = c("v",
                   paste0("vtl_", seq_len(length.out = length(p2_vars))),
                   paste0("vty_", seq_len(length.out = length(p2_vars)))))
  
  # We can already add the WP:
  p2_inst[[name]]$setSignal(signal = p1_signals[[name]])
  p2_smv$setSrbtaw(varName = name, srbtaw = p2_inst[[name]])
  
  # .. and also all the projects' signals:
  for (project in ground_truth[ground_truth$consensus > 0, ]$project) {
    p2_inst[[name]]$setSignal(signal = project_signals[[project]][[name]])
  }
}

# We call this there so there are parameters present.
set.seed(1337)
p2_smv$setParams(params =
  `names<-`(x = runif(n = p2_smv$getNumParams()), value = p2_smv$getParamNames()))
```

We can already initialize the linear scalarizer. This includes also to set up some progress-callback. Even with massive parallelization, this process will take its time so it will be good to know where we are approximately.

```{r}
p2_lls <- srBTAW_LossLinearScalarizer$new(
  returnRaw = FALSE,
  computeParallel = TRUE, progressCallback = function(what, step, total) {
    # if (step == total) {
    #   print(paste(what, step, total))
    # }
  })

for (name in names(p2_inst)) {
  p2_inst[[name]]$setObjective(obj = p2_lls)
}
```

The basic infrastructure stands, so now it's time to instantiate all of the singular losses. First we define a helper-function to do the bulk-work, then we iterate all projects, variables and intervals.

```{r}
#' This function creates a singular loss that is a linear combination
#' of an area-, correlation- and arclength-loss (all with same weight).
p2_attach_combined_loss <- function(project, vartype, intervals) {
  weight_p <- ground_truth[ground_truth$project == project, ]$consensus_score
  weight_v <- weight_vartype[[vartype]]
  temp <- weight_interval[intervals]
  stopifnot(length(unique(temp)) == 1)
  weight_i <- unique(temp)
  weight <- weight_p * weight_v * weight_i
  
  lossRss <- srBTAW_Loss_Rss$new(
    wpName = paste0("p1_", vartype), wcName = paste(project, vartype, sep = "_"),
    weight = weight, intervals = intervals, continuous = FALSE,
    numSamples = rep(500, length(intervals)), returnRaw = TRUE)
  
  p2_inst[[vartype]]$addLoss(loss = lossRss)
  p2_lls$setObjective(
    name = paste(project, vartype, paste(intervals, collapse = "_"),
                 "rss", sep = "_"), obj = lossRss)
}
```

Let's call our helper iteratively:

```{r}
interval_types <- list(A = c(1,3,4), B = 2)

for (vartype in p2_vars) {
  for (project in ground_truth[ground_truth$consensus > 0, ]$project) {
    for (intervals in interval_types) {
      p2_attach_combined_loss(
        project = project, vartype = vartype, intervals = intervals)
    }
  }
  
  # Add one per variable-type:
  lossYtrans <- YTransRegularization$new(
    wpName = paste0("p1_", vartype), wcName = paste(project, vartype, sep = "_"),
    intervals = seq_len(length.out = 4), returnRaw = TRUE,
    weight = 1, use = "tikhonov")

  p2_inst[[vartype]]$addLoss(loss = lossYtrans)
  p2_lls$setObjective(
    name = paste(vartype, "p2_reg_output", sep = "_"),
    obj = lossYtrans)
}
```


Finally, we add the regularizer for extreme intervals:

```{r}
p2_lls$setObjective(name = "p2_reg_exint2", obj = TimeWarpRegularization$new(
  weight = 0.25 * p2_lls$getNumObjectives(), use = "exint2", returnRaw = TRUE,
  wpName = p1_signals$A$getName(), wcName = project_signals$project_1$A$getName(),
  intervals = seq_len(length.out = length(p2_vars))
)$setSrBtaw(srbtaw = p2_inst$A))
```


### Fitting the pattern

```{r p2-params}
p2_params <- loadResultsOrCompute(file = "../results/p2_params.rds", computeExpr = {
  cl <- parallel::makePSOCKcluster(min(64, parallel::detectCores()))
  tempf <- tempfile()
  saveRDS(object = list(a = p2_smv, b = p2_lls), file = tempf)
  parallel::clusterExport(cl, varlist = list("tempf"))
  
  res <- doWithParallelClusterExplicit(cl = cl, expr = {
    optimParallel::optimParallel(
      par = p2_smv$getParams(),
      method = "L-BFGS-B",
      lower = c(
        rep(-.Machine$double.xmax, length(p2_vars)), # v_[vartype]
        rep(sqrt(.Machine$double.eps), length(p2_vars)), # vtl
        rep(-.Machine$double.xmax, length(p2_vars) * length(weight_vartype))), # vty for each
      upper = c(
        rep(.Machine$double.xmax, length(p2_vars)),
        rep(1, length(p2_vars)),
        rep(.Machine$double.xmax, length(p2_vars) * length(weight_vartype))),
      fn = function(x) {
        temp <- readRDS(file = tempf)
        temp$a$setParams(params = x)
        temp$b$compute0()
      },
      parallel = list(cl = cl, forward = FALSE, loginfo = TRUE)
    )
  })
})

p2_fr <- FitResult$new("a")
p2_fr$fromOptimParallel(p2_params)
format(p2_fr$getBest(paramName = "loss")[
  1, !(p2_fr$getParamNames() %in% c("duration", "begin", "end"))],
  scientific = FALSE, digits = 4, nsmall = 4)
```

```{r p2-params-fig, echo=FALSE, fig.cap="Neg. Log-loss of fitting pattern type II.", fig.align="top", fig.pos="ht!"}
p2_fr$plot_logloss()
```

### Inversing the parameters

For this pattern, we have warped all the projects to the pattern, while the ultimate goal is to warp the pattern to all the projects (or, better, to warp each type of variable of the WP to the group of variables of the same type of all projects, according to their weight, which is determined by the consensus of the ground truth). So, if we know how to go from A to B, we can inverse the learned parameters and go from B to A, which means in our case that we have to apply the inverse parameters to the WP in order to obtain WP-prime.

As for y-translations (that is, $v$, as well as all $\bm{\vartheta}^{(y)}$), the inversion is simple: we multiply these parameters with $-1$. The explanation for that is straightforward: If, for example, we had to go down by $-0.5$, to bring the data closer to the pattern, then that means that we have to lift the pattern by $+0.5$ to achieve the inverse effect.

Inversing the the boundaries is simple, too, and is explained by how we take some portion of the WC (the source) and warp it to the corresponding interval of the WP (the target).

That's how we do it:

* Given are the WP's __original__ boundaries, $\bm{\theta}^{(b)}$, and the learned $\bm{\vartheta}^{(l)}$. The goal is, for each $q$-th interval, to take what is in the WP's interval and warp it according to the learned length.
* Given the boundaries-to-lengths operator, $\mathsf{T}^{(l)}$, and the lengths-to-boundaries operator, $\mathsf{T}^{(b)}$, we can convert between $\bm{\theta}$ and $\bm{\vartheta}$.
* Start with a new instance of `SRBTW` (or `SRBTWBAW` for also warping y-translations) and set as $\bm{\theta}^{(b)}=\mathsf{T}^{(b)}(\bm{\vartheta}^{(l)})$. The learned lengths will become the __target__ intervals.
* Add the variable that ought to be transformed as __WC__, and set $\bm{\vartheta}^{(l)}=\mathsf{T}^{(l)}(\bm{\theta}^{(b)})$.
* That will result in that we are taking what was _originally_ in each interval, and warp it to a new length.
* The warped signal is then the `M`-function of the `SRBTW`/`SRBTWBAW`-instance.

Short example: Let's take the `SCD`-variable from the first pattern and warp it!

```{r inverse-example}
# Transforming some learned lengths to new boundaries:
p2_ex_thetaB <- c(0, .3, .5, .7, 1)
# Transforming the original boundaries to lengths:
p2_ex_varthetaL <- unname(c(
  fd_data_boundaries[1], fd_data_boundaries[2] - fd_data_boundaries[1],
  fd_data_boundaries[3] - fd_data_boundaries[2], 1 - fd_data_boundaries[3]))

p2_ex_srbtw <- SRBTW$new(
  theta_b = p2_ex_thetaB, gamma_bed = c(0, 1, 0),
  wp = p1_signals$SCD$get0Function(), wc = p1_signals$SCD$get0Function(),
  lambda = rep(0, 4), begin = 0, end = 1)

p2_ex_srbtw$setParams(vartheta_l = p2_ex_varthetaL)
```

In figure \ref{fig:inverse-example-fig} we can quite clearly see how the pattern warped from the blue intervals into the orange intervals

```{r inverse-example-fig, echo=FALSE, fig.cap="Warping the variable from within the blue to the orange intervals.", fig.align="top", fig.pos="ht!"}
plot_2_functions(
  fReference = p1_signals$SCD$get0Function(),
  fQuery = Vectorize(function(x) p2_ex_srbtw$M(x = x))) +
  geom_vline(xintercept = fd_data_boundaries[1], color = "blue") +
  geom_vline(xintercept = fd_data_boundaries[2], color = "blue") +
  geom_vline(xintercept = fd_data_boundaries[3], color = "blue") +
  geom_vline(xintercept = p2_ex_thetaB[2], color = "orange") +
  geom_vline(xintercept = p2_ex_thetaB[3], color = "orange") +
  geom_vline(xintercept = p2_ex_thetaB[4], color = "orange")
```

We have learned the following parameters from our optimization for pattern II:

```{r}
p2_best <- p2_fr$getBest(paramName = "loss")[
  1, !(p2_fr$getParamNames() %in% c("begin", "end", "duration"))]
p2_best
```

All of the initial translations ($v$) are zero. The learned lengths converted to boundaries are:

```{r}
# Here, we transform the learned lengths to boundaries.
p2_best_varthetaL <- p2_best[names(p2_best) %in% paste0("vtl_", 1:4)] /
  sum(p2_best[names(p2_best) %in% paste0("vtl_", 1:4)])
p2_best_varthetaL
p2_best_thetaB <- unname(c(
  0, p2_best_varthetaL[1], sum(p2_best_varthetaL[1:2]),
  sum(p2_best_varthetaL[1:3]), 1))
p2_best_thetaB
```

The first two intervals are rather short, while the last two are comparatively long. Let's transform all of the pattern's variables according to the parameters:

```{r}
p2_signals <- list()

for (vartype in names(weight_vartype)) {
  temp <- SRBTWBAW$new(
    theta_b = unname(p2_best_thetaB), gamma_bed = c(0, 1, 0),
    wp = p1_signals[[vartype]]$get0Function(), wc = p1_signals[[vartype]]$get0Function(),
    lambda = rep(0, 4), begin = 0, end = 1,
    lambda_ymin = rep(0, 4), lambda_ymax = rep(1, 4)) # not important here
  # That's still the same ('p2_ex_varthetaL' is the original
  # boundaries of Pattern I transformed to lengths):
  temp$setParams(vartheta_l = p2_ex_varthetaL,
                 v = -1 * p2_best[paste0("v_", vartype)],
                 vartheta_y = -1 * p2_best[paste0("vty_", 1:4, "_", vartype)])
  
  p2_signals[[vartype]] <- Signal$new(
    name = paste0("p2_", vartype), support = c(0, 1), isWp = TRUE, func = Vectorize(temp$M))
}
```

```{r echo=FALSE}
p2_data_concat <- NULL

for (vartype in names(weight_vartype)) {
  f <- p2_signals[[vartype]]$get0Function()
  x <- seq(from = 0, to = 1, length.out = 1e3)
  y <- f(x)
  li <- levels(fd_data_concat$interval)
  
  p2_data_concat <- rbind(p2_data_concat, data.frame(
    x = x,
    y = y,
    t = vartype,
    interval = sapply(X = x, FUN = function(x) {
      if (x <= p2_best_thetaB[2]) li[1] else if (x <= p2_best_thetaB[3]) li[2] else if (x <= p2_best_thetaB[4]) li[3] else li[4]
    })
  ))
}
```

The 2nd pattern, as derived from the ground truth, is shown in figure \ref{fig:p2-signals}.

```{r p2-signals, echo=FALSE, fig.cap="Second pattern as aligned by the ground truth.", fig.align="top", fig.pos="ht!"}
plot_project_data(data = p2_data_concat, boundaries = p2_best_thetaB[2:4])
```

While this worked I suppose it is fair to say that our initial pattern is hardly recognizable. Since we expected this, we planned for a third kind of pattern in section \ref{sec:pattern3}, that is purely evidence-based. It appears that, in order to match the ground truths we have at our disposal, projects register some kind of weak initial peak for the maintenance activities, that is followed by a somewhat uneventful second and third interval. Interestingly, the optimization seemed to have used to mostly straight lines in the Long Stretch phase to model linear declines and increases. The new Aftermath phase is the longest, so it is clear that the original pattern and its subdivision into phases is not a good mapping any longer. Instead of a sharp decline in the Aftermath, we now see an increase of all variables, without the chance of any decline before the last observed commit. We will check how this adapted pattern fares in section \ref{ssec:score-pattern2}.


## Pattern III: Averaging the ground truth

We can produce a pattern by computing a weighted average over all available ground truth. As weight, we can use either rater's score, their mean or consensus (default).

```{r}
gt_weighted_avg <- function(vartype, wtype = c("consensus", "rater.a", "rater.b", "rater.mean"), use_signals = project_signals, use_ground_truth = ground_truth) {
  wtype <- match.arg(wtype)
  gt <- use_ground_truth[use_ground_truth[[wtype]] > 0, ]
  wTotal <- sum(gt[[wtype]])
  proj <- gt$project
  weights <- `names<-`(gt[[wtype]], gt$project)
  
  funcs <- lapply(
    use_signals, function(ps) ps[[vartype]]$get0Function())
  
  Vectorize(function(x) {
    val <- 0
    for (p in proj) {
      val <- val + weights[[p]] * funcs[[p]](x)
    }
    val / wTotal
  })
}
```

Now we can easily call above function to produce a weighted average of each signal:

```{r}
p3_avg_signals <- list()

for (vartype in names(weight_vartype)) {
  p3_avg_signals[[vartype]] <- Signal$new(
    name = paste0("p3_avg_", vartype), support = c(0, 1), isWp = TRUE,
    func = gt_weighted_avg(vartype = vartype))
}
```

```{r echo=FALSE}
p3_avg_data_concat <- NULL

for (vartype in names(weight_vartype)) {
  f <- p3_avg_signals[[vartype]]$get0Function()
  x <- seq(from = 0, to = 1, length.out = 1e3)
  y <- f(x)
  
  p3_avg_data_concat <- rbind(p3_avg_data_concat, data.frame(
    x = x,
    y = y,
    t = vartype
  ))
}
```

The 2nd pattern, as derived from the ground truth, is shown in figure \ref{fig:p3-avg-signals}.

```{r p3-avg-signals, echo=FALSE, fig.cap="The third kind of pattern as weighted average over all ground truth.", fig.align="top", fig.pos="ht!"}
plot_project_data(data = p3_avg_data_concat)
```




## Pattern III (b): Evidence-based\label{sec:pattern3}

A third kind of pattern is produced by starting with an empty warping pattern and having all available ground truth adapt to it. Empty means that we will start with a flat line located at $0.5$ for each variable. Finally, the parameters are inversed. While we could do this the other way round, we have two reasons to do it this way, which is the same as we used for pattern II. First of all if the warping candidate was a perfectly flat line, it would be very difficult for the gradient to converge towards some alignment. Secondly, we want to use equidistantly-spaced boundaries (resulting in equal-length intervals) and using this approach, we can guarantee the interval lengths. To find the optimum amount of intervals, we try all values in a certain range and compute a fit, and then use an information criterion to decide which of the produced patterns provides the best trade-off between number of parameters and goodness-of-fit.

The process is the same as for pattern II: Using an instance of `srBTAW_MultiVartype` that holds one instance of an `srBTAW` per variable-type. We will choose equidistantly-spaced boundaries over the WP, and start with just $1$ interval, going up to some two-digit number. The best amount of parameters (intervals) is then determined using the Akaike Information Criterion [@akaike1981likelihood], which is directly implemented in `srBTAW`. We either have to use continuous losses or make sure to __always__ use the exact same amount of samples total. The amount per interval is determined by dividing by the number of intervals. This is important, as otherwise the information criterion will not work. We will do a single RSS-loss that covers all intervals. We will also use an instance of `TimeWarpRegularization` with the `exint2`-regularizer, as it scales with arbitrary many intervals (important!). I do not suppose that regularization for the y-values is needed, so we will not have this. This means that the resulting objective has just two losses.

For a set of equal-length number of intervals, we will fit such a multiple variable-type model. This also means we can do this in parallel. However, models with more intervals and hence more parameters will considerable take longer during gradient iterations. The more parameters, the fewer of these models should be fit simultaneously. We have access to 128-thread machine (of which about 125 thread can be used). Gradients are computed in parallel as well.


### Preparation

We define a single function that encapsulates the multiple variable-type model, losses and objectives and returns them, so that we can just fit them in a loop. The only configurable parameters is the amount of intervals.

```{r}
p3_prepare_mvtypemodel <- function(numIntervals) {
  eps <- sqrt(.Machine$double.eps)
  p3_smv <- srBTAW_MultiVartype$new()
  
  p3_vars <- c("A", "CP", "FREQ", "SCD")
  p3_inst <- list()
  
  # The objective:
  p3_lls <- srBTAW_LossLinearScalarizer$new(
    returnRaw = FALSE, computeParallel = TRUE, gradientParallel = TRUE)
  
  for (name in p3_vars) {
    p3_inst[[name]] <- srBTAW$new(
      # Always includes 0,1 - just as we need it! Works for values >= 1
      theta_b = seq(from = 0, to = 1, by = 1 / numIntervals),
      gamma_bed = c(0, 1, eps),
      lambda = rep(eps, numIntervals),
      begin = 0, end = 1, openBegin = FALSE, openEnd = FALSE,
      useAmplitudeWarping = TRUE,
      # We allow these to be larger; however, the final result should be within [0,1]
      lambda_ymin = rep(-10, numIntervals),
      lambda_ymax = rep( 10, numIntervals),
      isObjectiveLogarithmic = TRUE,
      paramNames = c("v",
        paste0("vtl_", seq_len(length.out = length(p3_vars))),
        paste0("vty_", seq_len(length.out = length(p3_vars)))))
    
    # The WP is a flat line located at 0.5:
    p3_inst[[name]]$setSignal(signal = Signal$new(
      func = function(x) .5, isWp = TRUE, support = c(0, 1), name = paste0("p3_", name)))
    
    # Set the common objective:
    p3_inst[[name]]$setObjective(obj = p3_lls)
    
    # .. and also all the projects' signals:
    for (project in ground_truth[ground_truth$consensus > 0, ]$project) {
      p3_inst[[name]]$setSignal(signal = project_signals[[project]][[name]])
    }
    
    p3_smv$setSrbtaw(varName = name, srbtaw = p3_inst[[name]])
  }

  # We call this there so there are parameters present.
  set.seed(1337 * numIntervals)
  p3_smv$setParams(params =
    `names<-`(x = runif(n = p3_smv$getNumParams()), value = p3_smv$getParamNames()))
  
  for (name in p3_vars) {
    # Add RSS-loss per variable-pair:
    for (project in ground_truth[ground_truth$consensus > 0, ]$project) {
      # The RSS-loss:
      lossRss <- srBTAW_Loss_Rss$new(
        wpName = paste0("p3_", name), wcName = paste(project, name, sep = "_"),
        weight = 1, intervals = seq_len(length.out = numIntervals), continuous = FALSE,
        numSamples = rep(round(5000 / numIntervals), numIntervals), returnRaw = TRUE)
      p3_inst[[name]]$addLoss(loss = lossRss)
      p3_lls$setObjective(
        name = paste(project, name, "rss", sep = "_"), obj = lossRss)
    }
  }
  
  # This has a much higher weight than we had for pattern II
  # because we are using many more samples in the RSS-loss.
  p3_lls$setObjective(name = "p3_reg_exint2", obj = TimeWarpRegularization$new(
    weight = p3_lls$getNumObjectives(), use = "exint2", returnRaw = TRUE,
    wpName = "p3_A", wcName = project_signals$project_1$A$getName(),
    intervals = seq_len(numIntervals)
  )$setSrBtaw(srbtaw = p3_inst$A))
  
  list(smv = p3_smv, lls = p3_lls)
}
```


Now we can compute these in parallel:

```{r p3-compute}
for (numIntervals in c(1:16)) {
  loadResultsOrCompute(
    file = paste0("../results/p3-compute/i_", numIntervals, ".rds"),
    computeExpr =
  {
    p3_vars <- c("A", "CP", "FREQ", "SCD")
    temp <- p3_prepare_mvtypemodel(numIntervals = numIntervals)
    tempf <- tempfile()
    saveRDS(object = temp, file = tempf)
    
    # It does not scale well beyond that.
    cl <- parallel::makePSOCKcluster(min(32, parallel::detectCores()))
    parallel::clusterExport(cl = cl, varlist = list("tempf"))
    
    optR <- doWithParallelClusterExplicit(cl = cl, expr = {
      optimParallel::optimParallel(
        par = temp$smv$getParams(),
        method = "L-BFGS-B",
        lower = c(
          rep(-.Machine$double.xmax, length(p3_vars)), # v_[vartype]
          rep(sqrt(.Machine$double.eps), length(p3_vars)), # vtl
          rep(-.Machine$double.xmax, length(p3_vars) * length(weight_vartype))), # vty for each
        upper = c(
          rep(.Machine$double.xmax, length(p3_vars)),
          rep(1, length(p3_vars)),
          rep(.Machine$double.xmax, length(p3_vars) * length(weight_vartype))),
        fn = function(x) {
          temp <- readRDS(file = tempf)
          temp$smv$setParams(params = x)
          temp$lls$compute0()
        },
        parallel = list(cl = cl, forward = FALSE, loginfo = TRUE)
      )
    })
    list(optR = optR, smv = temp$smv, lls = temp$lls)
  })
}
```

### Finding the best fit

We will load all the results previously computed and compute an information criterion to compare fits, and then choose the best model.

```{r}
p3_params <- NULL
for (tempPath in gtools::mixedsort(
  Sys.glob(paths = paste0(getwd(), "/../results/p3-compute/i*.rds")))
) {
  temp <- readRDS(file = tempPath)
  p3_params <- rbind(p3_params, data.frame(
    numInt = (temp$lls$getNumParams() - 1) / 2,
    numPar = temp$smv$getNumParams(),
    numParSrBTAW = temp$lls$getNumParams(),
    # This AIC would the original one!
    AIC = 2 * temp$smv$getNumParams() - 2 * log(1 / exp(temp$optR$value)),
    # This AIC is based on the number of intervals, not parameters!
    AIC1 = (temp$lls$getNumParams() - 1) - 2 * log(1 / exp(temp$optR$value)),
    # This AIC is based on the amount of parameters per srBTAW instance:
    AIC2 = 2 * temp$lls$getNumParams() - 2 * log(1 / exp(temp$optR$value)),
    logLoss = temp$optR$value,
    loss = exp(temp$optR$value)
  ))
}
```

In table \ref{tab:p3-params}, we show computed fits for various models, where the only difference is the number of intervals. Each interval comes with two degrees of freedom: its length and terminal y-translation. Recall that each computed fit concerns four variables. For example, the first model with just one interval per variable has nine parameters: All of the variables share the interval's length, the first parameter. Then, each variable has one $v$-parameter, the global y-translation. For each interval, we have one terminal y-translation. For example, the model with $7$ intervals has $7 + 4 + (4\times7)=39$ parameters.

We compute the AIC for each fit, which is formulated as in the following. The parameter $k$ is the number of parameters in the model, i.e., as described, it refers to all the parameters in the `srBTAW_MultiVartype`-model. The second AIC-alternative uses the parameter $p$ instead, which refers to the number of variables per `srBTAW`-instance.

$$
\begin{aligned}
  \operatorname{AIC}=&\;2\times k - 2\times\log{(\mathcal{\hat{L}})}\;\text{, where}\;\mathcal{\hat{L}}\;\text{is the maximum log-likelihood of the model,}
  \\[1ex]
  \mathcal{\hat{L}}=&\;\frac{1}{\exp{\big(\;\text{lowest loss of the model}\;\big)}}\;\text{, since we use logarithmic losses.}
  \\[1em]
  \text{The alternatives}&\;\operatorname{AIC^1}\;\text{and}\;\operatorname{AIC^2}\;\text{ are defined as:}
  \\[1ex]
  \operatorname{AIC^1}=&\;k-2\times\log{(\mathcal{\hat{L}})}-1\;\text{, which is based on the number of intervals, and}
  \\[1ex]
  \operatorname{AIC^2}=&\;2\times p - 2\times\log{(\mathcal{\hat{L}})}\;\text{, where}\;p\;\text{is the amount of params per}\;\operatorname{srBTAW}\text{-instance.}
\end{aligned}
$$

```{r echo=FALSE}
if (interactive()) {
  p3_params
} else {
  knitr::kable(
    x = round(p3_params, 3),
    booktabs = TRUE,
    caption = "Likelihood and Akaike information criteria (AIC) for computed models.",
    label = "p3-params"
  )
}
```

Comparing the results from table \ref{tab:p3-params}, it appears that no matter how we define the AIC, it is increasing with the number of parameters, and it does so faster than the loss reduces. So, picking a model by AIC is not terribly useful, as the results suggest we would to go with the $1$-interval model. The model with the lowest loss is the one with `r p3_params[which.min(p3_params$loss), ]$numInt` intervals.


### Create pattern from best fit

This is the same process as for pattern II, as the parameters need inversion. We will reconstruct the warped signals according to the inversed parameters to produce the third pattern. According to the overview above, the best model (lowest loss, __not__ AIC) is the one with __`r p3_params[which.min(p3_params$loss), ]$numInt`__ intervals. Its parameters are the following:

```{r}
# Let's first define a function that inverses the params and reconstructs the pattern.
p3_pattern_from_fit <- function(whichNumIntervals) {
  res <- list()
  p3_i <- readRDS(file = paste0(getwd(), "/../results/p3-compute/i_", whichNumIntervals, ".rds"))
  
  # FitResult:
  fr <- FitResult$new("foo")
  fr$fromOptimParallel(optR = p3_i$optR)
  res$fr <- fr
  
  # Inversion:
  lambda <- p3_i$smv$.__enclos_env__$private$instances$A$.__enclos_env__$private$instances$`p3_A|project_1_A`$getLambda()
  p3_i_varthetaL <- p3_i$optR$par[grepl(pattern = "^vtl_", x = names(p3_i$optR$par))]
  for (q in seq_len(length.out = whichNumIntervals)) {
    if (p3_i_varthetaL[q] < lambda[q]) {
      p3_i_varthetaL[q] <- lambda[q]
    }
  }
  p3_i_varthetaL <- p3_i_varthetaL / sum(p3_i_varthetaL)
  
  p3_i_thetaB <- c(0)
  for (idx in seq_len(length.out = length(p3_i_varthetaL))) {
    p3_i_thetaB <- c(p3_i_thetaB, sum(p3_i_varthetaL[1:idx]))
  }
  p3_i_thetaB[length(p3_i_thetaB)] <- 1 # numeric stability
  
  p3_i_varthetaL
  p3_i_thetaB
  res$varthetaL <- p3_i_varthetaL
  res$thetaB <- p3_i_thetaB
  
  # Signals:
  p3_i_numInt <- length(p3_i_varthetaL)
  p3_i_signals <- list()
  
  for (vartype in names(weight_vartype)) {
    emptySig <- Signal$new(
      isWp = TRUE, # does not matter here
        func = function(x) .5, support = c(0, 1), name = paste0("p3_", vartype))
    
    temp <- SRBTWBAW$new(
      theta_b = unname(p3_i_thetaB), gamma_bed = c(0, 1, 0),
      wp = emptySig$get0Function(), wc = emptySig$get0Function(),
      lambda = rep(0, p3_i_numInt), begin = 0, end = 1,
      lambda_ymin = rep(0, p3_i_numInt), lambda_ymax = rep(1, p3_i_numInt))
    
    # Recall that originally we used equidistantly-spaced boundaries:
    temp$setParams(vartheta_l = rep(1 / p3_i_numInt, p3_i_numInt),
                   v = -1 * p3_i$optR$par[paste0("v_", vartype)],
                   vartheta_y = -1 * p3_i$optR$par[paste0("vty_", 1:p3_i_numInt, "_", vartype)])
    
    p3_i_signals[[vartype]] <- Signal$new(
      name = paste0("p3_", vartype), support = c(0, 1), isWp = TRUE, func = Vectorize(temp$M))
  }
  res$signals <- p3_i_signals
  
  # Data:
  temp <- NULL
  for (vartype in names(weight_vartype)) {
    f <- p3_i_signals[[vartype]]$get0Function()
    x <- seq(from = 0, to = 1, length.out = 1e3)
    y <- f(x)
    
    temp <- rbind(temp, data.frame(
      x = x,
      y = y,
      t = vartype,
      numInt = whichNumIntervals
    ))
  }
  res$data <- temp
  res
}
```


```{r}
p3_best <- readRDS(file = paste0(getwd(), "/../results/p3-compute/i_",
                     p3_params[which.min(p3_params$loss), ]$numInt, ".rds"))
p3_best$optR$par
```

First we have to inverse the parameters before we can reconstruct the signals:

```{r}
p3_best_varthetaL <- p3_best$optR$par[
  grepl(pattern = "^vtl_", x = names(p3_best$optR$par))]
p3_best_varthetaL <- p3_best_varthetaL / sum(p3_best_varthetaL)

p3_best_thetaB <- c(0)
for (idx in seq_len(length.out = length(p3_best_varthetaL))) {
  p3_best_thetaB <- c(p3_best_thetaB, sum(p3_best_varthetaL[1:idx]))
}
p3_best_thetaB[length(p3_best_thetaB)] <- 1 # numeric stability

p3_best_varthetaL
p3_best_thetaB
```


```{r}
p3_best_numInt <- length(p3_best_varthetaL)
p3_signals <- list()

for (vartype in names(weight_vartype)) {
  emptySig <- Signal$new(
    isWp = TRUE, # does not matter here
      func = function(x) .5, support = c(0, 1), name = paste0("p3_", vartype))
  
  temp <- SRBTWBAW$new(
    theta_b = unname(p3_best_thetaB), gamma_bed = c(0, 1, 0),
    wp = emptySig$get0Function(), wc = emptySig$get0Function(),
    lambda = rep(0, p3_best_numInt), begin = 0, end = 1,
    lambda_ymin = rep(0, p3_best_numInt), lambda_ymax = rep(1, p3_best_numInt))
  
  # Recall that originally we used equidistantly-spaced boundaries:
  temp$setParams(vartheta_l = rep(1 / p3_best_numInt, p3_best_numInt),
                 v = -1 * p3_best$optR$par[paste0("v_", vartype)],
                 vartheta_y = -1 * p3_best$optR$par[paste0("vty_", 1:p3_best_numInt, "_", vartype)])
  
  p3_signals[[vartype]] <- Signal$new(
    name = paste0("p3_", vartype), support = c(0, 1), isWp = TRUE, func = Vectorize(temp$M))
}
```


```{r echo=FALSE}
p3_best_data_concat <- NULL

for (vartype in names(weight_vartype)) {
  f <- p3_signals[[vartype]]$get0Function()
  x <- seq(from = 0, to = 1, length.out = 1e3)
  y <- f(x)
  
  p3_best_data_concat <- rbind(p3_best_data_concat, data.frame(
    x = x,
    y = y,
    t = vartype
  ))
}
```

The 2nd pattern, as derived from the ground truth, is shown in figure \ref{fig:p3-signals}.

```{r p3-signals, echo=FALSE, fig.cap="Pattern type III (b) pattern as aligned by the ground truth only.", fig.align="top", fig.pos="ht!"}
plot_project_data(data = p3_best_data_concat, boundaries = p3_best_thetaB[2:(length(p3_best_thetaB) - 1)])
```

Let's show all computed patterns in a grid:

```{r echo=FALSE}
p3b_all <- loadResultsOrCompute(file = "../results/p3b_all.rds", computeExpr = {
  unlist(doWithParallelCluster(numCores = 16, expr = {
    foreach::foreach(
      numInt = 1:16,
      .inorder = FALSE
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      `names<-`(list(p3_pattern_from_fit(whichNumIntervals = numInt)), paste0("i_", numInt))
    }
  }), recursive = FALSE)
})
```


In figure \ref{fig:p3-all} we can clearly observe how the pattern evolves with growing number or parameters. Almost all patterns with sufficiently many degrees of freedom have some crack at about one quarter of the projects' time, a second crack is observed at about three quarter's time. In all patterns, it appears that adaptive activities are the least common. All patterns started with randomized coefficients, and something must have gone wrong for pattern $8$. From five and more intervals we can observe growing similarities with the weighted-average pattern, although it never comes really close. Even though we used a timewarp-regularizer with high weight, we frequently get extreme intervals.


```{r p3-all, echo=FALSE, fig.height=6, fig.cap="Computed pattern by number of intervals.", fig.align="top", fig.pos="ht!"}
temp <- NULL
for (res in p3b_all) {
  temp <- rbind(temp, res$data)
}

ggplot(data = temp, mapping = aes(x = x, y = y, color = t)) +
  geom_line() +
  facet_wrap(numInt ~., nrow = 4) +
  theme_light() +
  labs(color = "Variable") + xlab("Relative Time") + ylab("Value") +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"))
```


In figure \ref{fig:p3b-all-fr} we can clearly see that all but the eighth pattern converged nicely (this was already visible in \ref{fig:p3-all}). The loss is logarithmic, so the progress is rather substantial. For example, going from $\log{(14)}\approx1.2e6$ to $\log{(8)}\approx3e3$ is a reduction by $3$ (!) orders of magnitude.


```{r p3b-all-fr, echo=FALSE, fig.height=6, fig.cap="Losses for all computed pattern by number of intervals.", fig.align="top", fig.pos="ht!"}
ggarrange(
  plotlist = lapply(p3b_all, function(p) p$fr$plot_loss() + theme(axis.title.x.bottom = element_blank(), axis.title.y.left = element_blank()) + labs(subtitle = paste0("numInt=", length(p$varthetaL)))),
  nrow = 4, ncol = 4, common.legend = TRUE
)
```




# Scoring of projects

The true main-purpose of our work is to take a pattern and check it against any project, with the goal of obtaining a score, or goodness-of-match so that we can determine if the AP in the pattern is present in the project. In the previous sections we have introduced a number of patterns that we are going to apply here.

How it works: Given some pattern that consists of one or arbitrary many signals, the pattern is added to a single instance of `srBTAW` as __Warping Pattern__. The project's signals are added as __Warping Candidates__ to the same instance.

To compute a score, we need to define how to measure the distance between the WP and the WC (between each pair of signals and each interval). In the notebooks for sr-BTW we have previously defined some suitable losses with either __global__ or __local__ finite upper bounds. Currently, the Jensen--Shannon divergence (JSD), as well as the ratio-metrics (correlation, arc-lengths) have global upper bounds. For the JSD, it is $\ln{(2)}$. Losses with local finite upper bound are, for example, the area between curves, the residual sum of squares, the Euclidean distance etc., basically any metric that has a limit within the rectangle demarcated by one or more intervals. For some of the patterns, we have used a combination of such losses with local bounds. In general, it is not necessary to fit a pattern with the same kinds of losses that are later on used for scoring, but it is recommended to avoid confusing may results.

## The cost of alignment

When aligning a project to a pattern using boundary time warping, a deviation between the sections' lengths is introduced. Ideally, if the project would align with the pattern perfectly, there would be perfect agreement. The less good a project aligns with a pattern, the more time warping is required. However, the entire alignment needs to be assessed in conjunction with the scores -- the amount of required time warping alone is not sufficient to assess to overall goodness of fit.

During the optimization, we already used a regularizer for extreme intervals (`TimeWarpRegularization` with regularizer `exint2`).

## Scoring mechanisms\label{sssec:score-mech}

For scoring a single project, we first warp it to the pattern, then we measure the remaining distance. We only do time-warping of the projects to the pattern. We could compute a score for each interval. However, the ground truth does not yield this, so we will only compute a scores for entire signals, i.e., over all intervals. Once aligned, computing scores is cheap, so we will try a variety of scores and see what works best.

```{r}
# Function score_variable_alignment(..) has been moved to common-funcs.R!
```

We define a parallelized function to compute all scores of a project:

```{r}
compute_all_scores <- function(alignment, patternName) {
  useScores <- c("area", "corr", "jsd", "kl", "arclen", "sd", "var",
                 "mae", "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")
  
  `rownames<-`(doWithParallelCluster(numCores = length(alignment), expr = {
    foreach::foreach(
      projectName = names(alignment),
      .inorder = TRUE,
      .combine = rbind,
      .export = c("score_variable_alignment", "weight_vartype")
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      
      scores <- c()
      for (score in useScores) {
        temp <- score_variable_alignment(
          patternName = patternName, projectName = projectName,
          alignment = alignment[[projectName]], use = score)
        scores <- c(scores, `names<-`(
          c(mean(temp), prod(temp)), c(paste0(score, c("_m", "_p")))))
      }
      `colnames<-`(matrix(data = scores, nrow = 1), names(scores))
    }
  }), sort(names(alignment)))
}
```


We also need to define a function for warping a project to the pattern:

```{r}
# Function time_warp_project(..) has been moved to common-funcs.R!
```

## Pattern I

First we compute the alignment for all projects, then all scores.

```{r p1-align}
library(foreach)

p1_align <- loadResultsOrCompute(file = "../results/p1_align.rds", computeExpr = {
  # Let's compute all projects in parallel!
  cl <- parallel::makePSOCKcluster(length(project_signals))
  unlist(doWithParallelClusterExplicit(cl = cl, expr = {
    foreach::foreach(
      projectName = names(project_signals),
      .inorder = FALSE,
      .packages = c("parallel")
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      
      # There are 5 objectives that can be computed in parallel!
      cl_nested <- parallel::makePSOCKcluster(5)
      `names<-`(list(doWithParallelClusterExplicit(cl = cl_nested, expr = {
        temp <- time_warp_project(
          pattern = p1_signals, project = project_signals[[projectName]])
        temp$fit(verbose = TRUE)
        temp # return the instance, it includes the FitResult
      })), projectName)
    }
  }))
})
```

```{r p1-scores}
p1_scores <- loadResultsOrCompute(file = "../results/p1_scores.rds", computeExpr = {
  as.data.frame(compute_all_scores(alignment = p1_align, patternName = "p1"))
})
```

Recall that we are obtaining scores for each interval. To aggregate them we build the product and the mean in the following table, there is no weighing applied.

```{r echo=FALSE}
if (interactive()) {
  round(t(p1_scores), 4)
} else {
  knitr::kable(
    x = round(`colnames<-`(t(p1_scores), paste0("pr_", 1:9)), 2),
    booktabs = TRUE,
    caption = "Scores for the aligned projects with pattern I (p=product, m=mean).",
    label = "p1-scores"
  )
}
```

In table \ref{tab:p1-corr} the correlations of the scores with the ground truth as computed against the first pattern are shown.

```{r echo=FALSE}
corr <- stats::cor(ground_truth$consensus, p1_scores)[1, ]

if (interactive()) {
  corr
} else {
  perCol <- ceiling(length(corr) / 3)
  temp <- data.frame(matrix(ncol = 6, nrow = perCol))
  for (idx in 1:3) {
    off <- (idx - 1) * perCol
    temp[, idx * 2 - 1] <- names(corr)[(1 + off):(perCol + off)]
    temp[, idx * 2] <- corr[(1 + off):(perCol + off)]
  }
  colnames(temp) <- rep(c("Score", "Value"), 2)
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Correlation of the ground truth with all other scores for pattern I.",
    label = "p1-corr")
}
```

Let's show a correlation matrix in figure \ref{fig:p1-corr-mat}:

```{r p1-corr-mat, echo=FALSE, fig.cap="Correlation matrix for scores using pattern I.", fig.align="top", fig.pos="ht!"}
temp <- cbind(data.frame(gt_consensus = ground_truth$consensus_score), p1_scores[,])
temp <- temp[, colSums(is.na(temp)) < nrow(temp)]

corrplot::corrplot(corr = stats::cor(temp), type = "upper",# order = "hclust",
                   tl.col = "black", tl.srt = 90)
```

We appear to have mostly strongly negative correlations -- note that the measure using Kullback-Leibler is a divergence, not a similarity. Area and RMSE have a strong negative correlation, which suggests that whenever their score is high, the ground truth's consensus score is low. A high score for area or RMSE however means, that the distance between the signals is comparatively low, so we should have a good alignment, so how is this explained then?

Going back to table \ref{tab:groundtruth}, we will notice that projects 2, 3 and 5 have somewhat similar courses for their variables, yet their consensus scores are 0, 6 and 1, respectively. In other words, only project 3 has had a Fire Drill. We can hence conclude that the visual distance and the ground truth consensus are __not__ proportional (at least not for the variables we chose to model). The visual similarity of a project to a pattern is just that; the score quantifies the deviation from the pattern, but it does not necessarily correlate with the ground truth. This however was our underlying assumption all along, hence the initial pattern. We deliberately chose to design patterns _without_ investigating any of the projects. Also, while we had access to the projects for some time now, the ground truth became available only very recently, after all modeling was done.

Nevertheless, it does not hurt to check out patterns II and III, as we would like to achieve better matches. Eventually, the goal with this approach is to improve correlations and to get more accurate scores. The final stage then could be to compute, for example, a weighted consensus, based on the projects that we have, or to create a linear model that can regress to a value close to the ground truth by considering all the different scores.

## Pattern II\label{ssec:score-pattern2}

The second pattern was produced by having it warp to all the ground truths simultaneously, using their weight.

```{r p2-align}
library(foreach)

p2_align <- loadResultsOrCompute(file = "../results/p2_align.rds", computeExpr = {
  # Let's compute all projects in parallel!
  cl <- parallel::makePSOCKcluster(length(project_signals))
  unlist(doWithParallelClusterExplicit(cl = cl, expr = {
    foreach::foreach(
      projectName = names(project_signals),
      .inorder = FALSE,
      .packages = c("parallel")
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      
      # There are 5 objectives that can be computed in parallel!
      cl_nested <- parallel::makePSOCKcluster(5)
      `names<-`(list(doWithParallelClusterExplicit(cl = cl_nested, expr = {
        temp <- time_warp_project(
          pattern = p2_signals, project = project_signals[[projectName]])
        temp$fit(verbose = TRUE)
        temp # return the instance, it includes the FitResult
      })), projectName)
    }
  }))
})
```


```{r score-p2}
p2_scores <- loadResultsOrCompute(file = "../results/p2_scores.rds", computeExpr = {
  as.data.frame(compute_all_scores(alignment = p2_align, patternName = "p2"))
})
```


```{r echo=FALSE}
if (interactive()) {
  round(t(p2_scores), 4)
} else {
  knitr::kable(
    x = round(`colnames<-`(t(p2_scores), paste0("pr_", 1:9)), 2),
    booktabs = TRUE,
    caption = "Scores for the aligned projects with pattern II (p=product, m=mean).",
    label = "p2-scores"
  )
}
```

The correlation of just the ground truth with all scores is in table \ref{tab:p2-corr}.

```{r echo=FALSE}
corr <- stats::cor(ground_truth$consensus, p2_scores)[1, ]

if (interactive()) {
  corr
} else {
  perCol <- ceiling(length(corr) / 3)
  temp <- data.frame(matrix(ncol = 6, nrow = perCol))
  for (idx in 1:3) {
    off <- (idx - 1) * perCol
    temp[, idx * 2 - 1] <- names(corr)[(1 + off):(perCol + off)]
    temp[, idx * 2] <- corr[(1 + off):(perCol + off)]
  }
  colnames(temp) <- rep(c("Score", "Value"), 3)
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Correlation of the ground truth with all other scores for pattern II.",
    label = "p2-corr")
}
```

The correlation matrix looks as in figure \ref{fig:p2-corr-mat}.

```{r p2-corr-mat, echo=FALSE, fig.cap="Correlation matrix for scores using pattern II.", fig.align="top", fig.pos="ht!"}
temp <- cbind(data.frame(gt_consensus = ground_truth$consensus_score), p2_scores[,])

corrplot::corrplot(corr = stats::cor(temp), type = "upper",# order = "hclust",
                   tl.col = "black", tl.srt = 90)
```

With the second pattern we get much stronger correlations on average, meaning that the alignment of each project to the second pattern is better. While some correlations with the ground truth remain similar, we get strong positive correlations with the signal-measures _Impulse-factor_ (`r round(stats::cor(ground_truth$consensus_score, p2_scores[, "ImpulseFactor_m"]), 3)`) and _Peak_ (`r round(stats::cor(ground_truth$consensus_score, p2_scores[, "Peak_m"]), 3)`) [@xi2000bearing]. This however is explained by the double warping: First the pattern II was produced by time- and amplitude-warping it to the ground truth. Then, each project was time-warped to the that pattern. Simply put, this should result in some good alignment of all of the signals' peaks, explaining the high correlations.


## Pattern II (without alignment)

Since pattern II was computed such that it warps to all projects, it already should correct for time- and amplitude warping. This gives us incentive to compute scores against the non-aligned projects:


```{r}
p2_no_align <- list()

for (project in ground_truth$project) {
  temp <- p2_align[[project]]$clone()
  temp$setParams(params = `names<-`(p2_ex_varthetaL, temp$getParamNames()))
  p2_no_align[[project]] <- temp
}
```

```{r score-p2_no}
p2_no_scores <- loadResultsOrCompute(file = "../results/p2_no_scores.rds", computeExpr = {
  as.data.frame(compute_all_scores(alignment = p2_no_align, patternName = "p2"))
})
```


```{r echo=FALSE}
if (interactive()) {
  round(t(p2_no_scores), 4)
} else {
  knitr::kable(
    x = round(`colnames<-`(t(p2_no_scores), paste0("pr_", 1:9)), 2),
    booktabs = TRUE,
    caption = "Scores for the non-aligned projects with pattern II (p=product, m=mean).",
    label = "p2-no-scores"
  )
}
```

The correlation of just the ground truth with all scores is in table \ref{tab:p2-no-corr}.

```{r echo=FALSE}
corr <- stats::cor(ground_truth$consensus, p2_no_scores)[1, ]

if (interactive()) {
  corr
} else {
  perCol <- ceiling(length(corr) / 3)
  temp <- data.frame(matrix(ncol = 6, nrow = perCol))
  for (idx in 1:3) {
    off <- (idx - 1) * perCol
    temp[, idx * 2 - 1] <- names(corr)[(1 + off):(perCol + off)]
    temp[, idx * 2] <- corr[(1 + off):(perCol + off)]
  }
  colnames(temp) <- rep(c("Score", "Value"), 3)
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Correlation of the ground truth with all other non-aligned scores for pattern II.",
    label = "p2-no-corr")
}
```

The correlation matrix looks as in figure \ref{fig:p2-corr-mat}.

```{r p2-no-corr-mat, echo=FALSE, fig.cap="Correlation matrix for non-aligned scores using pattern II.", fig.align="top", fig.pos="ht!"}
temp <- cbind(data.frame(gt_consensus = ground_truth$consensus_score), p2_no_scores[,])

corrplot::corrplot(corr = stats::cor(temp), type = "upper",# order = "hclust",
                   tl.col = "black", tl.srt = 90)
```

While some correlations are weaker now, we observe more agreement between the scores, i.e., more correlations tend to be positive. _Peak_ and _Impulse-factor_ however have negative correlations now.

## Pattern III (average)

The 3rd pattern that was produced as weighted average over all ground truth is scored in this section. __Note!__ There is one important difference here: the weighted-average pattern does not have the same intervals as our initial pattern -- in fact, we cannot make any assumptions about any of the intervals. Therefore, we will compute this align with __ten equally-long__ intervals. This amount was chosen arbitrarily as a trade-off between the time it takes to compute, and the resolution of the results. Adding more intervals increases both, computing time and resolution exponentially, however the latter much less.


```{r p3-avg-align}
library(foreach)

p3_avg_align <- loadResultsOrCompute(file = "../results/p3_avg_align.rds", computeExpr = {
  # Let's compute all projects in parallel!
  cl <- parallel::makePSOCKcluster(length(project_signals))
  unlist(doWithParallelClusterExplicit(cl = cl, expr = {
    foreach::foreach(
      projectName = names(project_signals),
      .inorder = FALSE,
      .packages = c("parallel")
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      
      cl_nested <- parallel::makePSOCKcluster(5)
      `names<-`(list(doWithParallelClusterExplicit(cl = cl_nested, expr = {
        temp <- time_warp_project(
          thetaB = seq(from = 0, to = 1, by = 0.1), # important!
          pattern = p3_avg_signals, project = project_signals[[projectName]])
        temp$fit(verbose = TRUE)
        temp # return the instance, it includes the FitResult
      })), projectName)
    }
  }))
})
```


```{r score-p3_avg}
p3_avg_scores <- loadResultsOrCompute(file = "../results/p3_avg_scores.rds", computeExpr = {
  as.data.frame(compute_all_scores(alignment = p3_avg_align, patternName = "p3_avg"))
})
```


```{r echo=FALSE}
if (interactive()) {
  round(t(p3_avg_scores), 4)
} else {
  knitr::kable(
    x = round(`colnames<-`(t(p3_avg_scores), paste0("pr_", 1:9)), 2),
    booktabs = TRUE,
    caption = "Scores for the aligned projects with pattern III (average ground truth).",
    label = "p3-avg-scores"
  )
}
```

The correlation of just the ground truth with all scores is in table \ref{tab:p3-avg-corr}.

```{r echo=FALSE}
corr <- stats::cor(ground_truth$consensus, p3_avg_scores)[1, ]

if (interactive()) {
  corr
} else {
  perCol <- ceiling(length(corr) / 3)
  temp <- data.frame(matrix(ncol = 6, nrow = perCol))
  for (idx in 1:3) {
    off <- (idx - 1) * perCol
    temp[, idx * 2 - 1] <- names(corr)[(1 + off):(perCol + off)]
    temp[, idx * 2] <- corr[(1 + off):(perCol + off)]
  }
  colnames(temp) <- rep(c("Score", "Value"), 3)
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Correlation of the ground truth with all other scores for pattern II.",
    label = "p3-avg-corr")
}
```

The correlation matrix looks as in figure \ref{fig:p3-avg-corr-mat}.

```{r p3-avg-corr-mat, echo=FALSE, fig.cap="Correlation matrix for scores using pattern III (average).", fig.align="top", fig.pos="ht!"}
temp <- cbind(data.frame(gt_consensus = ground_truth$consensus_score), p3_avg_scores[,])

corrplot::corrplot(corr = stats::cor(temp), type = "upper",# order = "hclust",
                   tl.col = "black", tl.srt = 90)
```

I suppose that the most significant result here is the positive Jensen--Shannon divergence score correlation.

## Pattern III (average, no alignment)

Before we go any further, I would also like to compute the scores for this data-driven pattern __without__ having the projects aligned. After manually inspecting some of these alignments, it turns out that some are quite extreme. Since this pattern is a weighted average over all ground truth, not much alignment should be required.

```{r}
# We'll have to mimic an 'aligned' object, which is a list
# of srBTAW_MultiVartype instances. We can clone it and just
# undo the time warping.
p3_avg_no_align <- list()

for (project in ground_truth$project) {
  temp <- p3_avg_align[[project]]$clone()
  temp$setParams(params = `names<-`(rep(1 / temp$getNumParams(),
                  temp$getNumParams()), temp$getParamNames()))
  p3_avg_no_align[[project]] <- temp
}
```

```{r score-p3_avg_no}
p3_avg_no_scores <- loadResultsOrCompute(file = "../results/p3_avg_no_scores.rds", computeExpr = {
  as.data.frame(compute_all_scores(alignment = p3_avg_no_align, patternName = "p3_avg"))
})
```


```{r echo=FALSE}
if (interactive()) {
  round(t(p3_avg_no_scores), 4)
} else {
  knitr::kable(
    x = round(`colnames<-`(t(p3_avg_no_scores), paste0("pr_", 1:9)), 2),
    booktabs = TRUE,
    caption = "Scores for the non-aligned projects with pattern III (average ground truth).",
    label = "p3-avg-no-scores"
  )
}
```

The correlation of just the ground truth with all scores is in table \ref{tab:p3-avg-corr}.

```{r echo=FALSE}
corr <- stats::cor(ground_truth$consensus, p3_avg_no_scores)[1, ]

if (interactive()) {
  corr
} else {
  perCol <- ceiling(length(corr) / 3)
  temp <- data.frame(matrix(ncol = 6, nrow = perCol))
  for (idx in 1:3) {
    off <- (idx - 1) * perCol
    temp[, idx * 2 - 1] <- names(corr)[(1 + off):(perCol + off)]
    temp[, idx * 2] <- corr[(1 + off):(perCol + off)]
  }
  colnames(temp) <- rep(c("Score", "Value"), 3)
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Correlation of the ground truth with all other scores for pattern II.",
    label = "p3-avg-no-corr")
}
```

The correlation matrix looks as in figure \ref{fig:p3-avg-no-corr-mat}.

```{r p3-avg-no-corr-mat, echo=FALSE, fig.cap="Correlation matrix for non-aligned scores using pattern III (average).", fig.align="top", fig.pos="ht!"}
temp <- cbind(data.frame(gt_consensus = ground_truth$consensus_score), p3_avg_no_scores[,])

corrplot::corrplot(corr = stats::cor(temp), type = "upper",# order = "hclust",
                   tl.col = "black", tl.srt = 90)
```

And here in figure \ref{fig:p3-avg-no-corr-mat} we got the result that was the most-expected. We get almost always positive correlations, and most of them range between medium and significant strength. If we look at the correlation for `sd_p`, it is almost perfect with `r round(corr["sd_p"], 3)`. The Jensen--Shannon divergence score (note: while called divergence, this is a score, the higher the better) of `r round(corr["jsd_m"], 3)` is at a high level now. I mention this because this measure is less primitive than the others and tends to capture more properties of the signals. If this is high, it means we get a robust measure that ought to be usable stand-alone. The other low-level scores probably would need to be combined instead.

### Linear combination of scores

So far we have tested whether the calculated scores correlate with the scores of the ground truth, and we find some good examples. However, these scores are not scaled or translated in any way, so it is probably best to A) take multiple scores into account and B) create a regression model that makes these adjustments. We will test some linear combination of the scores `corr_p`, `jsd_m`, `RMS_m` and `sd_p`.

```{r}
p3_avg_lm <- stats::glm(formula = gt_consensus ~ corr_p + jsd_m + RMS_m + sd_p, data = temp)
stats::coef(p3_avg_lm)
plot(p3_avg_lm, ask = FALSE, which = 1:2)
```

Of course, this model should not be used to predict on the training data, but what we wanted to learn here is simply how to linearly combine the scores in order to get scores that are in the same range as the ground truth, we learn a re-scale so to say.

```{r}
p3_avg_lm_scores <- stats::predict(p3_avg_lm, temp)
round(p3_avg_lm_scores * 10, 3)
stats::cor(p3_avg_lm_scores, ground_truth$consensus_score)
```

This also increased the correlation to `r round(stats::cor(p3_avg_lm_scores, ground_truth$consensus_score), 3)`.


### Finding the most important scores\label{sssec:var-imp}

Previously, we have combined some hand-picked scores into a linear model, in order to mainly scale and translate them, in order to be able to obtain predictions in the range of the actual ground truth. However, this approach is not suitable for a model that shall generalize.

We have shown in figure \ref{fig:p3-avg-no-corr-mat} that most scores capture different properties of the alignment. It therefore makes sense to choose scores for a regression that, once combined, have the most predictive power in terms of accuracy (precision), and generalizability. There are quite many techniques and approaches to _feature selection_. Here, we will attempt a __recursive feature elimination__ (RFE), that uses partial least squares as estimator, a measure of __variable importance__, and using bootstrapped data (for example, @darst_using_2018).

Before we start, a few things are of importance. First, a feature selection should be done whenever a new model with prediction and generalization capabilities is to be trained, i.e., it is specific to the case (here process model and observed processes). Therefore, the features we select here are only a valid selection for a model that is to make predictions on scores as obtained from the pattern type III (average, no align). Secondly, the amount of data we have only suffices for running the suggested RFE, but the data is too scarce to obtain a robust model. Therefore, the results we will obtain here are practically only usable for giving an indication as to the relative importance of features (here: scores), but we must not deduce a genuine truth from them, nor can we conclude an absolute ordering that will hold for future runs of this approach, given different data. Thirdly, and that is specific to our case, we have separate scores for each interval (previously aggregated once using the mean, and once using the product), but the score is an aggregation across all variables (activities). Having one score represent the deviation over only one contiguous, aggregated interval, should be avoided in a real-world setting, and scores should be localized to each interval, to become their own feature. However, that requires even more data. Also, one would probably not aggregate scores across variables. We will examine this scenario more closely for issue-tracking data.

Let's start, we'll use the scores from `p3_avg_no_scores`:

```{r}
rfe_data <- cbind(
  p3_avg_no_scores,
  data.frame(gt = ground_truth$consensus))
```

The RFE is done via caret, with 3-times repeated, 10-fold cross validation as outer resampling method[^2].

[^2]: https://web.archive.org/web/20211120164401/https://topepo.github.io/caret/recursive-feature-elimination.html


```{r warning=FALSE}
library(caret, quietly = TRUE)

set.seed(1337)

control <- caret::trainControl(method="repeatedcv", number=10, repeats=3)
modelFit <- caret::train(gt ~., data=rfe_data, method="pls", trControl=control)

imp <- caret::varImp(object = modelFit)
```

```{r p3-avg-no-var-imp-plot, warning=FALSE, echo=FALSE, fig.height=5, fig.cap="Plot of the relative variable importances of scores as computed against pattern III (average).", fig.align="top", fig.pos="ht!"}

plot_var_imp(imp)
```


The relative variable importances are shown in table \ref{tab:p3-avg-no-varimp}. Unsurprisingly, correlation is the most important feature (score), as it expresses the degree to which two curves resemble each other (regardless of their absolute difference). This is then captured by the next most important score, the RMSE, closely followed by the standard deviation and area between curves. Note that area, RMSE, sd and variance are all highly correlated, so this does not come as a surprise. The next two places is the Impulsefactor and Jenson--Shannon divergence. We observe a somewhat healthy mix between mean- and product-measures. For values that tend to be comparatively tiny, such as the JSD or KL divergence, the product reduces variance too extreme (should have used log-sums), such that the score forfeits too much of its importance.


```{r echo=FALSE}
temp <- imp$importance[order(-imp$importance$Overall), , drop=FALSE]

if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Relative variable importances of scores as computed from all projects against pattern type III (average, no align).",
    label = "p3-avg-no-varimp")
}
```

The final model as selected by the RFE approach has these properties:

```{r}
modelFit
```

Now if we were to make predictions with this model, the results would be these:

```{r}
temp <- predict(object = modelFit, newdata = p3_avg_no_scores)
round(x = temp, digits = 2)
ground_truth$consensus
```

The correlation with the ground truth would then be:

```{r}
cor.test(ground_truth$consensus, temp)
```



## Pattern III (b)\label{ssec:score-pattern3}

The third and last pattern is based on the ground truth only. Starting with straight lines and equally long intervals, a pattern was generated and selected using a best trade-off between number of parameters and highest _likelihood_, or by the lowest loss. Like pattern II, all of these patterns were produced with time warping applied, so that we do not need to align the projects to it. We have produced 16 patterns, so let's compute the scores for all of them.


```{r p3b-no-scores}
p3b_no_scores <- loadResultsOrCompute(file = "../results/p3b_no_scores.rds", computeExpr = {
  unlist(doWithParallelCluster(numCores = min(8, parallel::detectCores()), expr = {
    foreach::foreach(
      numIntervals = seq_len(length.out = 16),
      .inorder = FALSE
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      
      temp_no_align <- list()
      
      for (project in ground_truth$project) {
        inst <- srBTAW$new(
          theta_b = seq(0, 1, length.out = 2),
          gamma_bed = c(0, 1, .Machine$double.eps),
          lambda = .Machine$double.eps,
          begin = 0, end = 1, openBegin = FALSE, openEnd = FALSE,
          useAmplitudeWarping = FALSE)
        
        for (vartype in names(weight_vartype)) {
          # Set WP and WC:
          wp <- p3b_all[[paste0("i_", numIntervals)]]$signals[[vartype]]
          wc <- project_signals[[project]][[vartype]]
            
          inst$setSignal(signal = wp)
          inst$setSignal(signal = wc)
          
          srbtwbaw <- inst$.__enclos_env__$private$createInstance(
            wp = wp$get0Function(), wc = wc$get0Function())
          inst$.__enclos_env__$private$addInstance(
            instance = srbtwbaw, wpName = wp$getName(), wcName = wc$getName())
        }
        
        inst$setParams(params = c(vtl_1 = 1))
        temp_no_align[[project]] <- inst
      }
      
      res <- `names<-`(list(tryCatch(expr = {
        # compute_all_scores is already parallel!
        temp <-as.data.frame(compute_all_scores(alignment = temp_no_align, patternName = "p3"))
        saveRDS(object = temp, file = paste0("../results/p3-compute/p3b_no_", numIntervals, ".rds"))
        temp
      }, error = function(cond) {
        list(cond = cond, tb = traceback())
      })), paste0("i_", numIntervals))
    }
  }), recursive = FALSE)
})
```

We will have a lot of results. In order to give an overview, we will show the correlation of the ground truth with the scores as obtained by scoring against each of the 16 patterns. The correlation plot in figure \ref{fig:p3b-no-scores-corr} should give us a good idea of how the scores changes with increasing number of parameters.

```{r p3b-no-scores-corr, echo=FALSE, fig.cap="Correlation-scores for all 16 patterns (type III, b).", fig.align="top", fig.pos="ht!"}
# dummy:
p3b_corr <- NULL

for (idx in seq_len(length.out = length(p3b_no_scores))) {
  temp <- p3b_no_scores[[paste0("i_", idx)]]
  if (!is.data.frame(temp)) {
    next
  }
  p3b_corr <- rbind(
    p3b_corr, `rownames<-`(stats::cor(ground_truth$consensus, temp), paste0("P III (b) [numInt=", idx, "]")))
}

corrplot::corrplot(corr = p3b_corr)
```

If `RMS` was to score to use, then the even the $1$-interval pattern will do, as the correlation for it is always strongly positive. In general, we can observe how some correlations get weaker, and some get stronger for patterns with higher number of parameters. There is no clear winner here, and the results are quite similar. What can say clearly is, that it is likely not worth to use highly parameterized models to detect the Fire Drill as it was present in our projects, as the manifestation is just not strong enough to warrant for patterns with high degrees of freedom. It is probably best, to use one of the other pattern types.

Let's also show an overview of the correlation with the ground truth for all of the other patterns:

```{r pall-corr, echo=FALSE, fig.cap="Overview of correlation-scores for all other types of patterns.", fig.align="top", fig.pos="ht!"}
pAll_corr <- rbind(
  `rownames<-`(stats::cor(ground_truth$consensus, p1_scores), "Pattern I"),
  `rownames<-`(stats::cor(ground_truth$consensus, p2_scores), "Pattern II"),
  `rownames<-`(stats::cor(ground_truth$consensus, p2_no_scores), "Pattern II (no align)"),
  `rownames<-`(stats::cor(ground_truth$consensus, p3_avg_scores), "Pattern III (avg)"),
  `rownames<-`(stats::cor(ground_truth$consensus, p3_avg_no_scores), "Pattern III (avg, no align)"),
  `rownames<-`(p3b_corr["P III (b) [numInt=4]",, drop = FALSE], "Pattern III (b, numInt=4)"),
  `rownames<-`(p3b_corr["P III (b) [numInt=11]",, drop = FALSE], "Pattern III (b, numInt=11)")
)

corrplot::corrplot(corr = pAll_corr)
```

The correlation overview in figure \ref{fig:pall-corr} suggests that the no-alignment patterns have most of the strong positive-correlated scores. However, it appears that it was still worth adapting our initial pattern using the ground truth, as it is the only pattern with very high positive correlations for Peak and Impulse-factor. These however disappear, if we do not do the double warping.



### Linear combination of scores

The pattern that incurred the lowest loss was number __`r p3_params[which.min(p3_params$loss), ]$numInt`__. We should however also test the pattern that had the highest correlations (positive _or_ negative) on average:

```{r}
p3b_corr_all <- apply(X = p3b_corr, MARGIN = 1,
                      FUN = function(row) mean(abs(row)))
```

```{r echo=FALSE}
temp <- `rownames<-`(data.frame(corr = p3b_corr_all), names(p3b_corr_all))

if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Mean absolute correlation for all patterns of type III (b).",
    label = "p3b-corr-all"
  )
}
```

In table \ref{tab:p3b-corr-all} we show the mean absolute correlation for the scores of all projects as computed against each pattern of type III (b).

```{r}
p3b_highest_corr <- names(which.max(p3b_corr_all))
p3b_highest_corr
```

The pattern using __`r gsub("[^0-9]", "", p3b_highest_corr)`__ has the highest correlation.

Taking the best pattern of type III (b), we can also do a linear combination again. "Best" may refer to the pattern with the highest correlations over all scores or the one with the lowest loss when fitting to all projects.

```{r}
# Take the project with the highest mean absolute correlation.
temp <- cbind(
  data.frame(gt_consensus = ground_truth$consensus_score),
  p3b_no_scores[[paste0("i_", gsub("[^0-9]", "", p3b_highest_corr))]])
p3b_lm <- stats::lm(
  formula = gt_consensus ~ area_p + corr_p + jsd_m + sd_p + rmse_p + RMS_p,
  data = temp)
stats::coef(p3b_lm)
plot(p3b_lm, ask = FALSE, which = 1:2)
```

```{r}
p3b_lm_scores <- stats::predict(p3b_lm, temp)
round(p3b_lm_scores * 10, 3)
```

```{r}
stats::cor(p3b_lm_scores, ground_truth$consensus_score)
```

With this linear model, we can report a high correlation with the consensus, too.



# References {-}

<div id="refs"></div>





```{r echo=FALSE}
# This is the last invisible section where we produce some plots
# that are then used in the paper(s).
```



```{r echo=FALSE, warning=FALSE, eval=FALSE}
setEPS()
postscript("../figures/pattern_all-corr.eps")

temp <- t(pAll_corr[1:5,]) # skip last pattern (3b[1-16]) for the paper
colnames(temp) <- c("Pattern I", "Pattern II", "Pattern II (no align)", "Pattern III", "Pattern III (no align)")
temp[is.na(temp)] <- 0
# rownames(temp) <- gsub("_", "\\\\_", rownames(temp))
#temp <- temp[, rev(colnames(temp))]

ggcorrplot::ggcorrplot(corr = temp, ggtheme = theme_light(base_size = 12) + theme(
    axis.text.x.top = element_text(angle = 90, vjust = 1/3, hjust = 0, size = 12),
    legend.position = "bottom",
    legend.text = element_text(vjust = 0),
    legend.title = element_text(margin = margin(r = 15, b = 3.5)),
    legend.key.width = unit(25, 'pt'),
    legend.key.height = unit(12.5, 'pt')
  )
) +#, colors = c("#0000ee", "#ffffff", "#ee0000")) +
scale_x_discrete(position = "top") +
# lims(fill = c(-1,1)) +
scale_fill_gradient2(low = "#0037AA", mid = "#eeeeee", high = "#AA0037", guide = "colorbar", limits = c(-1,1), breaks = seq(-1, 1, by = .5)) +
#scale_fill_gradient(low = "#eeeeee", high = "#111111") +
#scale_fill_gradient2()+#(low = "blue", high = "red") +
#scale_fill_continuous(guide = "colourbar", type = ) +
labs(fill = "Pearson Correlation")

dev.off()
```

```{r echo=FALSE, warning=FALSE, eval=FALSE}
tikzDevice::tikz(file = "../figures/pattern-123.tex", width = 3.4, height = 3.4)
temp <- rbind(
  cbind(fd_data_concat[, 1:3], data.frame(Pattern = rep("Pattern I", nrow(fd_data_concat)))),
  cbind(p2_data_concat[, 1:3], data.frame(Pattern = rep("Pattern II", nrow(p2_data_concat)))),
  cbind(p3_avg_data_concat, data.frame(Pattern = rep("Pattern III", nrow(p3_avg_data_concat))))#,
  #cbind(p3_best_data_concat, data.frame(Pattern = rep("Pattern III (b, numInt=6)", nrow(p3_best_data_concat))))
)

ggplot(data = temp, mapping = aes(x = x, y = y, color = t)) +
  geom_line(size = 1) +
  xlab("Normalized time") + ylab("Magnitude") +
  scale_x_continuous(breaks = seq(0, 1, by = 0.1), position = "top") +
  # scale_color_brewer(palette = 3) +
  scale_color_manual(values = c("#F8766D", "#7CAE00", "#00BFC4", "#C77CFF")) +
  facet_wrap(Pattern ~., nrow = 3) +
  theme_light() +
  labs(color = "Activity") + xlab("Normalized Time") + ylab("Magnitude") +
  theme(
    axis.title.x.top = element_text(margin = margin(b=5), size = 10),
    axis.title.y = element_text(margin = margin(r=3), size = 10),
    legend.position = "bottom",
    legend.margin = margin(-7.5, 0, 0, 0),
    strip.text = element_text(colour = "#000000", margin = ggplot2::margin(t = 1.5, b = 1.5)),
    strip.background = element_rect(fill="#dfdfdf"))
dev.off()
```















