---
title: "Technical Report: Detecting the Fire Drill anti-pattern using Source Code"
author: "Sebastian Hönel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ../inst/REFERENCES.bib
urlcolor: blue
output:
  md_document:
    toc: yes
    toc_depth: 6
    df_print: kable
    variant: gfm
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: kable
  word_document: default
header-includes:
- \usepackage{bm}
- \usepackage{mathtools}
- \usepackage{xurl}
---

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\abs}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand{\norm}[1]{\left\lVert\,#1\,\right\rVert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}

```{r setoptions, echo=FALSE, warning=FALSE, message=FALSE}
Sys.setenv(LANG = "en_US.UTF-8")
Sys.setenv(LC_ALL = "en_US.UTF-8")
Sys.setenv(LC_CTYPE = "en_US.UTF-8")
library(knitr)
opts_chunk$set(tidy = TRUE, tidy.opts = list(indent=2))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

library(ggplot2)
library(ggpubr)
library(tidyr)
```

# Introduction\label{tr:fire-drill-technical-report}

This is the complementary technical report for the paper/article tentatively entitled "Multivariate Continuous Processes: Modeling, Instantiation, Goodness-of-fit, Forecasting". Here, we import the ground truth as well as all projects' data, and instantiate our model based on _self-regularizing Boundary Time Warping and Boundary Amplitude Warping_. Given a few patterns that represent the __Fire Drill__ anti-pattern (AP), the goal is evaluate these patterns and their aptitude for detecting the AP in concordance with the ground truth.

All complementary data and results can be found at Zenodo [@honel_picha_2021]. This notebook was written in a way that it can be run without any additional efforts to reproduce the outputs (using the pre-computed results). This notebook has a canonical URL^[[Link]](https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/fire-drill-technical-report.Rmd)^ and can be read online as a rendered markdown^[[Link]](https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/fire-drill-technical-report.md)^ version. All code can be found in this repository, too.

# Fire Drill - anti-pattern

We describe the Fire Drill (FD) anti-pattern for usage in models that are based on the source code (i.e., not from a managerial or project management perspective). The purpose also is to start with a best guess, and then to iteratively improve the description when new evidence is available.

FD is described now both from a managerial and a technical perspective^[[Link]](https://github.com/ReliSA/Software-process-antipatterns-catalogue/pull/13/commits/78c06c30b1880795e6c1dd0f20f146c548212675?short_path=01589ac#diff-01589ac85c3fc29739823b5a41ab1bbfba7fbc2579aaf63de0f1ce31713689ab)^. The technical description is limited to variables we can observe, such as the amount (frequency) of commits and source code, the density of source code, and maintenance activities (a/c/p).

In literature, FD is described in [@silva2015software] and [@brown1998refactoring], as well as at^[[Link]](https://web.archive.org/web/20210414150555/https://sourcemaking.com/antipatterns/fire-drill)^.

Currently, FD is defined to have these symptoms and consequences:

*   Rock-edge burndown (especially when viewing implementation tasks only)
*   Long period at project start where activities connected to requirements, analysis and planning prevale, and design and implementation activities are rare
*   Only analytical or documentational artefacts for a long time
*   Relatively short period towards project end with sudden increase in development efforts
*   Little testing/QA and project progress tracking activities during development period
*   Final product with poor code quality, many open bug reports, poor or patchy documentation
*   Stark contrast between inter-level communication in project hierarchy (management - developers) during the first period (close to silence) and after realizing the problem (panic and loud noise

From these descriptions, we have attempted to derive the following symptoms and consequences in source code:

*   Rock-edge burndown of esp. implementation tasks mean there are no or just very few adaptive maintenance activities before the burning down starts
*   The long period at project start translates to few modifications made to the source code, resulting in fewer commits (lower overall relative frequency)
*   Likewise, documentational artifacts have a lower source code density, as less functionality is delivered; this density should increase as soon as adaptive activities are registered
*   The short period at project end is characterized by a higher frequency of higher-density implementation tasks, with little to no perfective or corrective work
*   At the end of the project, code quality is comparatively lower, while complexity is probably higher, due to pressure exerted on developers in the burst phase

## Prerequisites

Through source code, we can observe the following variables (and how they change over time). We have the means to model and match complex behavior for each variable over time. By temporally subdividing the course of a random variable, we can introduce additional measures for a pattern, that are based on comparing the two intervals (e.g., mean, steepest slope, comparisons of the shape etc.).

*   Amount of commits over interval of time (frequency) -- We can observe any commit, both at when it was authored first, and when it was added to the repository (author-/committer-date).
*   Amount/Frequency of each maintenance activity separately
*   Density of the source code (also possibly per activity if required)
Any other metric, if available (e.g., total Quality or Complexity of project at each commit) -- however, we need to distinguish random variables by whether their relative frequency (when they are changing or simply when they are observed) changes the shape of the function, or whether it only leads to different sample rates. In case of metrics, the latter is the case. In other words, for some variables their occurrence is important, while for others it is the observed value.

It is probably the most straightforward way to decompose a complex pattern such as Fire Drill into sub-processes, one for each random variable. That has several advantages:

*   We are not bound/limited to only one global aggregated match, which could hide alignment details.
*   We can quantify the goodness of match for each variable separately, including details such as the interval in which it matched, and how well it matched in it.
*   Matching separately allows us to come up with our own scoring methods; e.g., it could be that the matching-score of one variable needs to be differently computed than the score of another, or simply the weights between variables are different.
*   If a larger process was temporally subdivided, we may want to score a variable in one of the intervals differently, or not at all. This is useful for when we cannot make sufficient assumptions.

## Modeling the Fire Drill

In this section, we will collect attempts to model the Fire Drill anti-pattern. The first attempt is our initial guess, and subsequent attempts are based on new input (evidence, opinions/discussions etc.).

### Initial Best Guess

Our initial best guess is solely based on the literature and descriptions from above, and no dataset was inspected yet. We chose to model our initial best guess using a visual process. Figure \ref{fig:initial-best-guess} must be understood as a simplified and idealized approximation. While we could add a confidence interval for each variable represented, we will later show how align (fit) a project (process) to this pattern (process model), and then measure its deviation from it. The modeled pattern is a **continuous-time stochastic process model**, and we will demonstrate means to quantify the difference between this **process model** and a **process**, which is shown in section \ref{ssec:model-metrics-kde}.

```{r initial-best-guess, echo=FALSE, out.width="75%", fig.cap="Modeling of the Fire Drill anti-pattern over the course of an entire project, according to what we know from literature.", fig.align="center", fig.pos="ht!"}
knitr::include_graphics(
  path = "../figures/Fire-Drill_first-best-guess.png",
  )
```

The pattern is divided into four intervals (five if one counts $t_{\text{x-over}}$ as delimiter, but it is more like an additional point of interest). These intervals are:

1.    $[0, t_1)$ -- Begin
2.    $[t_1, t_2)$ -- Long Stretch
3.    $[t_2, t_{\text{end}}]$ -- Fire Drill
4.    $(t_{\text{end}}, t_3]$ -- Aftermath

In each interval and for each of the random variables modeled, we can perform matching. This also means that a) we do not have to attempt matching in each interval, b) we do not have to perform matching for each variable, and c) that we can select a different set of appropriate measures for each variable in each interval (this is useful if, e.g., we do not have much information for some of them).

Each variable is its own sub-pattern. As of now, we track the maintenance activities, and their frequency over time. A higher accumulation results in a higher peak. One additional variable, the source code density (blue), is not measured by its frequency (occurrences), but rather by its value. We may include and model additional metrics, such as complexity or quality.

Whenever we temporally subdivide the pattern into two intervals, we can take these measurements:

*   Compute the goodness-of-fit of the curve of a variable, compared to its behavior in the data. As of now, that includes a rich set of metrics, all of which can quantify these differences, and for all of which we have developed scores. While we can compute scores for the actual match, we do also have the means to compute the score of the dynamic time warping. Among others, we have these metrics:
    *   Reference and Query signal: Start/end (cut-in/-out) for both absolute & relative
About the DTW match: (relative) monotonicity (continuity of the warping function), residuals of (relative) warping function (against its LM or optimized own version (fitted using RSS))
    *   Between two curves (all of these are normalized as they are computed in the unit square, see R notebook): area (by integration), generic statistics by sampling (mae, rmse, correlation (pearson, kendall, spearman), covariance, standard-deviation, variance, symmetric Kullback-Leibler divergence, symmetric Jenson-Shannon Divergence)
*   Sub-division allows for comparisons of properties of the two intervals, e.g.,
    *   Compare averages of the variable in each interval. This can be easily implemented as a score, as we know the min/max and also do have expectations (lower/higher).
    *   Perform a linear regression/create a linear model (LM) over the variable in each interval, so that we can compare slopes, residuals etc.
*   Cross-over of two variables: This means that a) the two slopes of their respective LM converge and b) that there is a crossover within the interval.

#### Description of Figure \ref{fig:initial-best-guess}

*   The frequency of activities (purple) is the sum of all activities, i.e., this curve is green (corrective + perfective) plus red (adaptive).
*   The frequency is the only variable that may be modeled with its actual maximum of 1, as we expect it to reach its maximum at $t_{\text{end}}$. The frequency also has to be actually 0, before the first commit is made.
    *   Some of our metrics can measure how well one curve resembles the other, regardless of their “vertical difference”. Other metrics can describe the distance better. What I want to say is, it is not very important what the value of a variable in our modeled pattern actually is. But it is important however if it touches 0 or 1. A variable should only be modeled as touching 0 or 1 if it is a) monotonically increasing over the course of the project and b) actually assumes its theoretical min/max.
*   Corrective and Perfective have been aggregated into one variable, as according to the description of Fire Drill, there is only a distinction between adding features and performing other, such as Q/A related, tasks.
*   The source code density (blue) should, with the first commit, jump to its expected value, which is the average over all commits in the project. A steep but short increase is expected in $[t_2, t_{\text{end}}]$ as less focus is spent on documentational artifacts.
*   We do not know much about the activities’ frequency and relative distribution up till $t_2$. That leaves us with two options: either, we make only very modest assumptions about the progression of a variable, such as the slope of its LM in that interval or the residuals. Otherwise, we can also choose not to assess the variable in that interval. It is not yet clear how useful the results from the DTW would be, as we can only model a straight line (that’s why I am suggesting LMs instead). For Fire Drill, the interval $[t_2, t_3]$ is most characteristic. We can however extract some constraints for our expectations between the intervals $[t_2, t_3]$ and everything that happens before. For example, in [BRO’98] it is described that $[0, t_2)$ is about 5-7x longer than $[t_2, t_{\text{end}}]$.
*   $t_{\text{end}}$ is not a delimiter we set manually, but it is rather discovered by the sub-patterns’ matches. However, it needs to be sufficiently close to the project’s actual end (or there needs to be a viable explanation for the difference, such as holidays in between etc.)
*   $t_{\text{x-over}}$ must happen; but other than that, there is not much we can assume about it. We could specify properties as to where approximately we’d expect it to happen (I suppose in the first half of the interval) or how steep the crossover actually is but it is probably hard to rely on.


#### Description Phase-by-Phase

**Begin**: A short phase of a few days, probably not more than a week. While there will be few commits, $t_1$ does not really start until the frequency stabilizes. We expect the maintenance activities’ relative frequency to decrease towards the end of Begin, before they become rather constant in the next phase. In this phase, the source code density is expected to be close to its average, as initial code as well as documentation are added.

**Long Stretch**: This is the phase we do not know much about, except for that the amount of adaptive activities is comparatively lower, especially when compared to the aggregated corrective and perfective activities (approx. less than half of these two). While the activities’ variables will most likely not be perfectly linear, the LM over these should show rather small residuals. Also the slope of the LM is expected to be rather flat (probably less than +/-10°). The source code density is expected to fall slightly below its average after Begin, as less code is shipped.

**Fire Drill**: The actual Fire Drill happens in $[t_2, t_{\text{end}}]$, and we detect $t_{\text{end}}$ by finding the apex of the frequency. However, we choose to extend this interval to include $(t_{\text{end}}, t_3]$, as by doing so, we can craft more characteristics of the anti-pattern and impose more assumptions. These are a) that the steep decline in that last phase has a more extreme slope than its increase before $t_{\text{end}}$ (because after shipping/project end, probably no activity is performed longer). B) This last sub-phase should be shorter than the phase before (probably just up to a few days; note that the phase $[t_2, t_{\text{end}}]$ is described to be approximately one week long in literature).

With somewhat greater confidence, we can define the following:

*   The source code density will rise suddenly and approach its maximum of 1 (however we should not model it with its maximum value to improve matching). It is expected to last until $t_{\text{end}}$, with a sudden decline back to its average from Begin. We do not have more information for after $t_{\text{end}}$, so the average is the expected value.
*   Perfective and corrective activities will vanish quickly and together become the least frequent activity in the project. The average of these activities is expected to be less than half compared to the Long Stretch. Until $t_3$ (the very end), the amount of these activities keeps monotonically decreasing.
*   At the same time, we will see a steep increase of adaptive activity. The increase is expected to be greater than or equal to the decrease of perfective and corrective activities. In other words, the average of adaptive activities is expected to be more than double, compared to what it was in the Long Stretch. Also, adaptive activities will reach their maximum frequency over the course of the project here.
*   The nature of a Fire Drill is a frantic and desperate phase. While adaptive approaches its maximum, the commit frequency also approaches its maximum, even though perfective and corrective activities decline (that is why the purple curve is less steep than the adaptive one but still goes to its global maximum).
*   There will be a sharp crossover between perfective+corrective and adaptive activities. It is expected to happen sooner than later in the phase $[t_2, t_{\text{end}}]$.

**Aftermath**: Again, we do not know how this phase looks, but it will help us to more confidently identify the Fire Drill, as the curves of the activities, frequencies and other metrics have very characteristic curves that we can efficiently match. All metrics that are invariant to the frequency are expected to approach their expected value, without much variance (rather constant slope of their resp. LMs). Any of the maintenance activities will continue to fall. In case of adaptive activities we will see an extraordinary steep decline, as after project end/shipping, no one adds functionality. It should probably even fall below all other activities, resulting in another crossover. We do not set any of the activities to be exactly zero however, to allow more efficient matching.


# Data

We have $9$ projects conducted by students, and two raters have __independently__, i.e., without prior communication, assessed to what degree the AP is present in each project. This was done using a scale from zero to ten, where zero means that the AP was not present, and ten would indicate a strong manifestation.

## The Ground Truth

```{r}
ground_truth <- read.csv(file = "../data/ground-truth.csv", sep = ";")
```

```{r echo=FALSE}
knitr::kable(
  x = ground_truth,
  booktabs = TRUE,
  caption = "Entire ground truth as of both raters",
  label = "groundtruth"
)
```


Using the _quadratic weighted Kappa_ [@cohen1968weighted], we can report an unadjusted agreement of __`r round(Metrics::ScoreQuadraticWeightedKappa(rater.a = ground_truth$rater.a, rater.b = ground_truth$rater.b, min.rating = 0, max.rating = 10), 3)`__ for both raters. A Kappa value in the range $[0.6,0.8]$ is considered _substantial_, and values beyond that as _almost perfect_ [@landis1977application]. As for the Pearson-correlation, we report a slightly higher value of __`r round(cor(x = ground_truth$rater.a, y = ground_truth$rater.b), 3)`__. The entire ground truth is shown in table \ref{tab:groundtruth}. The final consensus was reached after both raters exchanged their opinions, and it is the consensus that we will use as the actual ground truth from here on and out.


## The Student Projects

The ground truth was extracted from nine student-conducted projects. Seven of these were implemented simultaneously between March and June 2020, and two the year before in a similar timeframe.

```{r}
student_projects <- read.csv(file = "../data/student-projects.csv", sep = ";")
```

In the first batch, we have a total of:

* Nine projects,
* `r length(unique(student_projects$AuthorNominalLabel))` authors that authored `r nrow(student_projects)` commits total which are of type
* Adaptive / Corrective / Perfective (`a/c/p`) commits: `r sum(student_projects$label == "a")` / `r sum(student_projects$label == "c")` / `r sum(student_projects$label == "p")`

We have a complete breakdown of all activities across all projects in figure \ref{fig:project-activity}.

```{r}
student_projects_info <- NULL

for (pId in unique(student_projects$project)) {
  temp <- student_projects[student_projects$project == pId, ]
  student_projects_info <- rbind(student_projects_info, data.frame(
    project = pId,
    authors = length(unique(temp$AuthorNominalLabel)),
    commits = nrow(temp),
    a = nrow(temp[temp$label == "a", ]),
    c = nrow(temp[temp$label == "c", ]),
    p = nrow(temp[temp$label == "p", ]),
    avgDens = round(mean(temp$Density), 3)
  ))
}
```


```{r echo=FALSE}
if (interactive()) {
  student_projects_info
} else {
  knitr::kable(
    x = student_projects_info,
    booktabs = TRUE,
    caption = "Per-project overview of the student projects",
    label = "studentprojects"
  )
}
```

```{r project-activity, echo=FALSE, fig.cap="Commit activities across projects", fig.align="top", fig.pos="ht!"}
ggplot(data = student_projects, aes(x = length(label), fill = label)) +
  geom_bar() + facet_grid(label ~ project) +
  theme_light() +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"),
    legend.position = "none")
```

We have slightly different begin- and end-times in each project. However, the data for all projects was previously cropped, so that each project's extent marks the absolute begin and end of it -- it starts with the first commit and ends with the last. As for our methods here, we only need to make sure that we scale the timestamps into a relative $[0,1]$-range, where $1$ marks the project's end.

For each project, we model __four__ variables: The activities __adaptive__ (__`A`__), __corrective+perfective__ (__`CP`__), the frequency of all activities, regardless of their type (__`FREQ`__), and the __source code density__ (__`SCD`__). While for the first three variables we estimate a Kernel density, the last variable is a metric collected with each commit. The data for it is mined using `Git-Density` [@honel2020gitdens], and we use a highly efficient commit classification model[^1] ($\approx83.6\%$ accuracy, $\approx0.745$ Kappa) [@honel2020using] to attach maintenance activity labels to each commit, based on size- and keyword-data only.

[^1]: https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/comm-class-models.Rmd

Technically, we will compose each variable into an instance of our `Signal`-class. Before we start, we will do some normalizations and conversions, like converting the timestamps. This has to be done on a per-project basis.

```{r}
student_projects$label <- as.factor(student_projects$label)
student_projects$project <- as.factor(student_projects$project)
student_projects$AuthorTimeNormalized <- NA_real_
```

```{r}
for (pId in levels(student_projects$project)) {
  student_projects[student_projects$project == pId, ]$AuthorTimeNormalized <-
    (student_projects[student_projects$project == pId, ]$AuthorTimeUnixEpochMilliSecs -
      min(student_projects[student_projects$project == pId, ]$AuthorTimeUnixEpochMilliSecs))
  student_projects[student_projects$project == pId, ]$AuthorTimeNormalized <-
    (student_projects[student_projects$project == pId, ]$AuthorTimeNormalized /
      max(student_projects[student_projects$project == pId, ]$AuthorTimeNormalized))
}
```

And now for the actual signals: Since the timestamps have been normalized for each project, we model each variable to actually start at $0$ and end at $1$ (the support). We will begin with activity-related variables before we model the source code density, as the process is different. When using Kernel density estimation (KDE), we obtain an empirical probability density function (PDF) that integrates to $1$. This is fine when looking at all activities combined (__`FREQ`__). However, when we are interested in a specific fraction of the activities, say __`A`__, then we should scale its activities according to its overall ratio. Adding all scaled activities together should again integrate to $1$. When this is done, we scale one last time such that no empirical PDF has a co-domain larger than $1$.

```{r}
project_signals <- list()

# passed to stats::density
use_kernel <- "gauss" # "rect"

for (pId in levels(student_projects$project)) {
  temp <- student_projects[student_projects$project == pId, ]
  
  # We'll need these for the densities:
  acp_ratios <- table(temp$label) / sum(table(temp$label))
  
  dens_a <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp[temp$label == "a", ]$AuthorTimeNormalized,
    ratio = acp_ratios[["a"]], kernel = use_kernel)
  
  dens_cp <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp[temp$label == "c" | temp$label == "p", ]$AuthorTimeNormalized,
    ratio = acp_ratios[["c"]] + acp_ratios[["p"]], kernel = use_kernel)
  
  dens_freq <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp$AuthorTimeNormalized, ratio = 1, kernel = use_kernel)
  
  # All densities need to be scaled together once more, by dividing
  # for the maximum value of the FREQ-variable.
  ymax <- max(c(attr(dens_a, "ymax"), attr(dens_cp, "ymax"), attr(dens_freq, "ymax")))
  dens_a <- stats::approxfun(
    x = attr(dens_a, "x"), y = sapply(X = attr(dens_a, "x"), FUN = dens_a) / ymax)
  dens_cp <- stats::approxfun(
    x = attr(dens_cp, "x"), y = sapply(X = attr(dens_cp, "x"), FUN = dens_cp) / ymax)
  dens_freq <- stats::approxfun(
    x = attr(dens_freq, "x"), y = sapply(X = attr(dens_freq, "x"), FUN = dens_freq) / ymax)
  
  project_signals[[pId]] <- list(
    A = Signal$new(name = paste(pId, "A", sep = "_"),
                   func = dens_a, support = c(0, 1), isWp = FALSE),
    CP = Signal$new(name = paste(pId, "CP", sep = "_"),
                    func = dens_cp, support = c(0, 1), isWp = FALSE),
    FREQ = Signal$new(name = paste(pId, "FREQ", sep = "_"),
                      func = dens_freq, support = c(0, 1), isWp = FALSE)
  )
}
```

Now, for each project, we estimate the variable for the source code density as follows:

```{r warning=FALSE}
for (pId in levels(student_projects$project)) {
  temp <- data.frame(
    x = student_projects[student_projects$project == pId, ]$AuthorTimeNormalized,
    y = student_projects[student_projects$project == pId, ]$Density)
  temp <- temp[with(temp, order(x)), ]
  
  # Using a polynomial with maximum possible degree, we smooth the
  # SCD-data, as it can be quite "peaky"
  temp_poly <- poly_autofit_max(x = temp$x, y = temp$y, startDeg = 13)
  
  dens_scd <- Vectorize((function() {
    rx <- range(temp$x)
    ry <- range(temp$y)
    poly_y <- stats::predict(temp_poly, x = temp$x)
    tempf <- stats::approxfun(x = temp$x, y = poly_y, ties = "ordered")
    function(x) {
      if (x < rx[1] || x > rx[2]) {
        return(NA_real_)
      }
      max(ry[1], min(ry[2], tempf(x)))
    }
  })())
  
  project_signals[[pId]][["SCD"]] <- Signal$new(
    name = paste(pId, "SCD", sep = "_"), func = dens_scd,
    support = c(0, 1), isWp = FALSE)
}
```

Let's plot all the projects:

```{r echo=FALSE}
tempdf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(tempdf) <- c("x", "y", "p", "v")

n <- 500
x <- seq(from = 0, to = 1, length.out = n)
for (pId in levels(student_projects$project)) {
  for (v in c("A", "CP", "FREQ", "SCD")) {
    tempdf <- rbind(tempdf, data.frame(
      x = x,
      y = sapply(X = x, FUN = project_signals[[pId]][[v]]$get0Function()),
      p = pId,
      v = v
    ))
  }
}
```


```{r project-vars, echo=FALSE, fig.cap="All variables over each project's time span (first batch of projects).", fig.align="top", fig.pos="ht!"}
plot_all_acp <- ggplot(data = tempdf, aes(x = x, y = y, color = v)) +
  geom_line() +
  facet_wrap(p ~.) +
  theme_light() +
  labs(color = "Variable") + xlab("Relative Time") + ylab("Value") +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"))
plot_all_acp
```

```{r echo=FALSE, eval=FALSE}
# Let's save this as tikz:
tikzDevice::tikz(file = "../figures/all_acp.tex", width = 3.4, height = 1.8)
plot_all_acp +
  theme_light(base_size = 10) +
  theme(
    axis.title.x.bottom = element_text(margin = margin(b=5), size = 10),
    axis.title.y.left = element_text(margin = margin(r=3), size = 10),
    legend.position = "bottom",
    legend.margin = margin(-7.5, 0, 0, 0),
    strip.background = element_rect(fill="#dfdfdf"))
dev.off()
```


## Modeling of metrics and events using KDE\label{ssec:model-metrics-kde}

We need to make an important distinction between events and metrics. An event does not carry other information, other than that it occurred. One could thus say that such an event is _nulli_-variate. If an event were to carry extra information, such as a measurement that was taken, it would be _uni_-variate. That is the case for many metrics in software: the time of their measurement coincides with an event, such as a commit that was made. On the time-axis we thus know __when__ it occurred and __what__ was its value. Such a metric could be easily understood as a _bivariate x/y_ variable and be plotted in a two-dimensional Cartesian coordinate system.

An event however does not have any associated y-value we could plot. Given a time-axis, we could make a mark whenever it occurred. Some of the markers would probably be closer to each other or be more or less accumulated. The y-value could express these accumulations relative to each other. These are called _densities_. This is exactly what KDE does: it expresses the relative accumulations of data on the x-axis as density on the y-axis. For KDE, the actual values on the x-axis have another meaning, and that is to compare the relative likelihoods of the values on it, since the axis is ordered. For our case however, the axis is linear time and carries no such meaning. The project data we analyze is a kind of sampling over the project's events. We subdivide the gathered project data hence into these two types of data series:

* __Events__: They do not carry any extra information or measurements. As for the projects we analyze, events usually are occurrences of specific types of commits, for example. The time of occurrence is the x-value on the time-axis, and the y-value is obtained through KDE. We model all maintenance activities as such variables.
* __Metrics__: Extracted from the project at specific times, for example at every commit. We can extract any number or type of metric, but each becomes its own variable, where the x-value is on the time-axis, and the y-value is the metric's value. We model the source code density as such a variable.


# Patterns for scoring the projects

Our overall goal is to propose a single model that is able to detect the presence of the Fire Drill AP, and how strong its manifestation is. In order to do that, we require a pattern that defines how a Fire Drill looks in practice. Any real-world project can never follow such a pattern perfectly, because of, e.g., time dilation and compression. Even after correcting these, some distance between the project and the pattern will remain. The projects from figure \ref{fig:project-vars} indicate that certain phases occur, but that their occurrence happens at different points in time, and lasts for various durations.

Given some pattern, we first attempt to remove any distortions in the data, by using our new model _self-regularizing Boundary Time Warping_ (sr-BTW). This model takes a pattern that is subdivided into one or more intervals, and aligns the project data such that the loss in each interval is minimized. After alignment, we calculate a score that quantifies the remaining differences. Ideally, we hope to find a (strong) positive correlation of these scores with the ground truth.


## Pattern I: Initial best guess

```{r}
fd_data_concat <- readRDS("../data/fd_data_concat.rds")
```

This pattern was created based on all available literature, __without__ inspecting any of the projects. It is subdivided into four intervals:

1. Begin -- Short project warm-up phase
2. Long Stretch -- The longest phase in the project, about which we do not know much about, except for that there should be a rather constant amount of activities over time.
3. Fire Drill -- Characteristic is a sudden and steep increase of adaptive activities. This phase is over once these activities reached their apex.
4. Aftermath -- Everything after the apex. We should see even steeper declines.


@brown1998refactoring describe a typical scenario where about six months are spent on non-developmental activities, and the actual software is then developed in less than four weeks. If we were to include some of the aftermath, the above first guess would describe a project of about eight weeks.

We define the boundaries as follows (there are three boundaries to split the pattern into four intervals):

```{r}
fd_data_boundaries <- c("b1" = 0.085, "b2" = 0.625, "b3" = 0.875)
```

The pattern and its boundaries look like this:

```{r pattern-1, fig.cap="The pattern that was our initial best guess", fig.align="top", fig.pos="ht!"}
plot_project_data(data = fd_data_concat, boundaries = fd_data_boundaries)
```

```{r echo=FALSE, eval=FALSE}
tikzDevice::tikz(file = "../figures/pattern-I.tex", width = 3.4, height = 1.8)
plot_project_data(data = fd_data_concat, boundaries = fd_data_boundaries) +
  xlab("Normalized time") + ylab("Magnitude") +
  scale_x_continuous(breaks = seq(0, 1, by = 0.1), position = "top") +
  theme_light(base_size = 10) +
  theme(
    axis.title.x.top = element_text(margin = margin(b=5), size = 10),
    axis.title.y = element_text(margin = margin(r=3), size = 10),
    legend.position = "bottom",
    legend.margin = margin(-7.5, 0, 0, 0),
    strip.background = element_rect(fill="#dfdfdf"))
dev.off()
```


### Initialize the pattern

The pattern as shown in \ref{fig:pattern-1} is just a collection of x/y coordinate-data, and for us being able to use it, we need to instantiate it. We do this by storing each variable in an instance of `Signal`.

```{r}
p1_signals <- list(
  A = Signal$new(name = "p1_A", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "A", ]$x,
      y = fd_data_concat[fd_data_concat$t == "A", ]$y)),
  CP = Signal$new(name = "p1_CP", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "CP", ]$x,
      y = fd_data_concat[fd_data_concat$t == "CP", ]$y)),
  FREQ = Signal$new(name = "p1_FREQ", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "FREQ", ]$x,
      y = fd_data_concat[fd_data_concat$t == "FREQ", ]$y)),
  SCD = Signal$new(name = "p1_SCD", support = c(0, 1), isWp = TRUE, func = 
    stats::approxfun(
      x = fd_data_concat[fd_data_concat$t == "SCD", ]$x,
      y = fd_data_concat[fd_data_concat$t == "SCD", ]$y))
)
```

```{r echo=FALSE, fig.height=1.8, fig.cap="The separate signals of pattern I.", fig.align="top", fig.pos="ht!"}
ggarrange(
  p1_signals$A$plot() + ylim(0,1), p1_signals$CP$plot() + ylim(0,1),
  p1_signals$FREQ$plot() + ylim(0,1), p1_signals$SCD$plot() + ylim(0,1),
  nrow = 1
)
```



## Pattern II: Adaptation of best guess

The second pattern is a compromise between the first and the third: While we want to keep as much of the initial best guess, we also want to adjust the pattern based on the projects and the ground truth. Adjusting means, that we will keep what is in each interval, but we allow each interval to stretch and compress, and we allow each interval to impose a vertical translation both at then begin and end (a somewhat trapezoidal translation). In any case, each such alteration is a linear affine transformation. Additionally to sr-BTW, we will also apply __sr-BAW__ (self-regularizing Boundary Amplitude Warping) to accomplish this. This model is called __`srBTAW`__ and the process is the following:

* The pattern is decomposed into its four variables first, as we can adapt these (almost) independently from each other.
* Then, for each type of variable, an instance of `srBTAW` is created. As __Warping Candidates__ (WC) we add all of the projects' corresponding variables. The __Warping Pattern__ (WP) is the single variable from the pattern in this case -- again, we warp the project data, however, eventually the learned warping gets inversed and applied to the WC.
* All four `srBTAW` instances are then fitted simultaneously: While we allow the y-translations to adapt independently for each type of variable, all instances share the same intervals, as eventually we have to assemble the variables back into a common pattern.

### Preparation

We already have the `srBTAW` __Multilevel model__, which can keep track of arbitrary many variables and losses. The intention behind this however was, to track variables of the __same type__, i.e., signals that are logically of the same type. In our case this means that any single instance should only track variables that are either `A`, `CP`, `FREQ` or `SCD`. For this pattern, the WP is a single signal per variable, and the WC is the corresponding signal from each of the nine projects. This is furthermore important to give different weights to different variables. In our case, we want to give a lower weight to the `SCD`-variable.

As for the loss, we will first test a combined loss that measures __`3`__ properties: The area between curves (or alternatively the residual sum of squares), the correlation between the curves, and the arc-length ratio between the curves. We will consider any of these to be equally important, i.e., no additional weights. Each loss shall cover all intervals with weight $=1$, except for the Long Stretch interval, where we will use a reduced weight.

There are $4$ types of variables, $7$ projects (two projects have consensus $=0$, i.e., no weight) and $2\times 3$ single losses, resulting in $168$ losses to compute. The final weight for each loss is computed as: $\omega_i=\omega^{(\text{project})}\times\omega^{(\text{vartype})}\times\omega^{(\text{interval})}$. For the phase Long Stretch, the weight for any loss will $\frac{1}{2}$, and for the source code density we will chose $\frac{1}{2}$, too. The weight of each project is based on the consensus of the ground truth. The ordinal scale for that is $[0,10]$, so that we will divide the score by $10$ and use that as weight. Examples:

* __A__ in Fire Drill in project $p3$: $\omega=0.6\times 1\times 1=0.6$ (consensus is $6$ in project $p3$)
* __FREQ__ in Long Stretch in project $p7$: $\omega=0.3\times 0.5\times 1=0.15$ and
* __SCD__ in Long Stretch in project $p4$: $\omega=0.8\times 0.5\times 0.5=0.2$.

In table \ref{tab:groundtruth-score} we show all projects with a consensus-score $>0$, projects $2$ and $8$ are not included any longer.

```{r}
ground_truth$consensus_score <- ground_truth$consensus / 10
weight_vartype <- c("A" = 1, "CP" = 1, "FREQ" = 1, "SCD" = 0.5)
weight_interval <- c("Begin" = 1, "Long Stretch" = 0.5, "Fire Drill" = 1, "Aftermath" = 1)
```

```{r}
temp <- expand.grid(weight_interval, weight_vartype, ground_truth$consensus_score)
temp$p <- temp$Var1 * temp$Var2 * temp$Var3
weight_total <- sum(temp$p)
```

The sum of all weights combined is `r weight_total`.

```{r echo=FALSE}
if (interactive()) {
  ground_truth[ground_truth$consensus > 0, c("project", "consensus", "consensus_score")]
} else {
  knitr::kable(
    x = ground_truth[ground_truth$consensus > 0, c("project", "consensus", "consensus_score")],
    booktabs = TRUE,
    caption = "Entire ground truth as of both raters",
    label = "groundtruth-score"
  )
}
```

### Defining the losses

For the optimization we will use mainly __`5`__ classes:

* `srBTAW_MultiVartype`: One instance globally, that manages all parameters across all instances of `srBTAW`.
* `srBTAW`: One instance per variable-type, so here we'll end up with four instances.
* `srBTAW_LossLinearScalarizer`: A linear scalarizer that will take on all of the defined singular losses and compute and add them together according to their weight.
* `srBTAW_Loss2Curves`: Used for each of the $168$ singular losses, and configured using a specific loss function, weight, and set of intervals where it ought to be used.
* `TimeWarpRegularization`: One global instance for all `srBTAW` instances, to regularize extreme intervals. We chose a mild weight for this of just $1$, which is small compared to the sum of all other weights (`r weight_total`).

```{r}
p2_smv <- srBTAW_MultiVartype$new()

p2_vars <- c("A", "CP", "FREQ", "SCD")
p2_inst <- list()
for (name in p2_vars) {
  p2_inst[[name]] <- srBTAW$new(
    theta_b = c(0, fd_data_boundaries, 1),
    gamma_bed = c(0, 1, sqrt(.Machine$double.eps)),
    lambda = rep(sqrt(.Machine$double.eps), length(p2_vars)),
    begin = 0, end = 1, openBegin = FALSE, openEnd = FALSE,
    useAmplitudeWarping = TRUE,
    # We allow these to be larger; however, the final result should be within [0,1]
    lambda_ymin = rep(-10, length(p2_vars)),
    lambda_ymax = rep( 10, length(p2_vars)),
    isObjectiveLogarithmic = TRUE,
    paramNames = c("v",
                   paste0("vtl_", seq_len(length.out = length(p2_vars))),
                   paste0("vty_", seq_len(length.out = length(p2_vars)))))
  
  # We can already add the WP:
  p2_inst[[name]]$setSignal(signal = p1_signals[[name]])
  p2_smv$setSrbtaw(varName = name, srbtaw = p2_inst[[name]])
  
  # .. and also all the projects' signals:
  for (project in ground_truth[ground_truth$consensus > 0, ]$project) {
    p2_inst[[name]]$setSignal(signal = project_signals[[project]][[name]])
  }
}

# We call this there so there are parameters present.
set.seed(1337)
p2_smv$setParams(params =
  `names<-`(x = runif(n = p2_smv$getNumParams()), value = p2_smv$getParamNames()))
```

We can already initialize the linear scalarizer. This includes also to set up some progress-callback. Even with massive parallelization, this process will take its time so it will be good to know where we are approximately.

```{r}
p2_lls <- srBTAW_LossLinearScalarizer$new(
  returnRaw = FALSE,
  computeParallel = TRUE, progressCallback = function(what, step, total) {
    # if (step == total) {
    #   print(paste(what, step, total))
    # }
  })

for (name in names(p2_inst)) {
  p2_inst[[name]]$setObjective(obj = p2_lls)
}
```

The basic infrastructure stands, so now it's time to instantiate all of the singular losses. First we define a helper-function to do the bulk-work, then we iterate all projects, variables and intervals.

```{r}
#' This function creates a singular loss that is a linear combination
#' of an area-, correlation- and arclength-loss (all with same weight).
p2_attach_combined_loss <- function(project, vartype, intervals) {
  weight_p <- ground_truth[ground_truth$project == project, ]$consensus_score
  weight_v <- weight_vartype[[vartype]]
  temp <- weight_interval[intervals]
  stopifnot(length(unique(temp)) == 1)
  weight_i <- unique(temp)
  weight <- weight_p * weight_v * weight_i
  
  lossRss <- srBTAW_Loss_Rss$new(
    wpName = paste0("p1_", vartype), wcName = paste(project, vartype, sep = "_"),
    weight = weight, intervals = intervals, continuous = FALSE,
    numSamples = rep(500, length(intervals)), returnRaw = TRUE)
  
  p2_inst[[vartype]]$addLoss(loss = lossRss)
  p2_lls$setObjective(
    name = paste(project, vartype, paste(intervals, collapse = "_"),
                 "rss", sep = "_"), obj = lossRss)
}
```

Let's call our helper iteratively:

```{r}
interval_types <- list(A = c(1,3,4), B = 2)

for (vartype in p2_vars) {
  for (project in ground_truth[ground_truth$consensus > 0, ]$project) {
    for (intervals in interval_types) {
      p2_attach_combined_loss(
        project = project, vartype = vartype, intervals = intervals)
    }
  }
  
  # Add one per variable-type:
  lossYtrans <- YTransRegularization$new(
    wpName = paste0("p1_", vartype), wcName = paste(project, vartype, sep = "_"),
    intervals = seq_len(length.out = 4), returnRaw = TRUE,
    weight = 1, use = "tikhonov")

  p2_inst[[vartype]]$addLoss(loss = lossYtrans)
  p2_lls$setObjective(
    name = paste(vartype, "p2_reg_output", sep = "_"),
    obj = lossYtrans)
}
```


Finally, we add the regularizer for extreme intervals:

```{r}
p2_lls$setObjective(name = "p2_reg_exint2", obj = TimeWarpRegularization$new(
  weight = 0.25 * p2_lls$getNumObjectives(), use = "exint2", returnRaw = TRUE,
  wpName = p1_signals$A$getName(), wcName = project_signals$project_1$A$getName(),
  intervals = seq_len(length.out = length(p2_vars))
)$setSrBtaw(srbtaw = p2_inst$A))
```


### Fitting the pattern

```{r p2-params}
p2_params <- loadResultsOrCompute(file = "../results/p2_params.rds", computeExpr = {
  cl <- parallel::makePSOCKcluster(min(64, parallel::detectCores()))
  tempf <- tempfile()
  saveRDS(object = list(a = p2_smv, b = p2_lls), file = tempf)
  parallel::clusterExport(cl, varlist = list("tempf"))
  
  res <- doWithParallelClusterExplicit(cl = cl, expr = {
    optimParallel::optimParallel(
      par = p2_smv$getParams(),
      method = "L-BFGS-B",
      lower = c(
        rep(-.Machine$double.xmax, length(p2_vars)), # v_[vartype]
        rep(sqrt(.Machine$double.eps), length(p2_vars)), # vtl
        rep(-.Machine$double.xmax, length(p2_vars) * length(weight_vartype))), # vty for each
      upper = c(
        rep(.Machine$double.xmax, length(p2_vars)),
        rep(1, length(p2_vars)),
        rep(.Machine$double.xmax, length(p2_vars) * length(weight_vartype))),
      fn = function(x) {
        temp <- readRDS(file = tempf)
        temp$a$setParams(params = x)
        temp$b$compute0()
      },
      parallel = list(cl = cl, forward = FALSE, loginfo = TRUE)
    )
  })
})

p2_fr <- FitResult$new("a")
p2_fr$fromOptimParallel(p2_params)
format(p2_fr$getBest(paramName = "loss")[
  1, !(p2_fr$getParamNames() %in% c("duration", "begin", "end"))],
  scientific = FALSE, digits = 4, nsmall = 4)
```

```{r p2-params-fig, echo=FALSE, fig.cap="Neg. Log-loss of fitting pattern type II.", fig.align="top", fig.pos="ht!"}
p2_fr$plot_logloss()
```

### Inversing the parameters

For this pattern, we have warped all the projects to the pattern, while the ultimate goal is to warp the pattern to all the projects (or, better, to warp each type of variable of the WP to the group of variables of the same type of all projects, according to their weight, which is determined by the consensus of the ground truth). So, if we know how to go from A to B, we can inverse the learned parameters and go from B to A, which means in our case that we have to apply the inverse parameters to the WP in order to obtain WP-prime.

As for y-translations (that is, $v$, as well as all $\bm{\vartheta}^{(y)}$), the inversion is simple: we multiply these parameters with $-1$. The explanation for that is straightforward: If, for example, we had to go down by $-0.5$, to bring the data closer to the pattern, then that means that we have to lift the pattern by $+0.5$ to achieve the inverse effect.

Inversing the the boundaries is simple, too, and is explained by how we take some portion of the WC (the source) and warp it to the corresponding interval of the WP (the target).

That's how we do it:

* Given are the WP's __original__ boundaries, $\bm{\theta}^{(b)}$, and the learned $\bm{\vartheta}^{(l)}$. The goal is, for each $q$-th interval, to take what is in the WP's interval and warp it according to the learned length.
* Given the boundaries-to-lengths operator, $\mathsf{T}^{(l)}$, and the lengths-to-boundaries operator, $\mathsf{T}^{(b)}$, we can convert between $\bm{\theta}$ and $\bm{\vartheta}$.
* Start with a new instance of `SRBTW` (or `SRBTWBAW` for also warping y-translations) and set as $\bm{\theta}^{(b)}=\mathsf{T}^{(b)}(\bm{\vartheta}^{(l)})$. The learned lengths will become the __target__ intervals.
* Add the variable that ought to be transformed as __WC__, and set $\bm{\vartheta}^{(l)}=\mathsf{T}^{(l)}(\bm{\theta}^{(b)})$.
* That will result in that we are taking what was _originally_ in each interval, and warp it to a new length.
* The warped signal is then the `M`-function of the `SRBTW`/`SRBTWBAW`-instance.

Short example: Let's take the `SCD`-variable from the first pattern and warp it!

```{r inverse-example}
# Transforming some learned lengths to new boundaries:
p2_ex_thetaB <- c(0, .3, .5, .7, 1)
# Transforming the original boundaries to lengths:
p2_ex_varthetaL <- unname(c(
  fd_data_boundaries[1], fd_data_boundaries[2] - fd_data_boundaries[1],
  fd_data_boundaries[3] - fd_data_boundaries[2], 1 - fd_data_boundaries[3]))

p2_ex_srbtw <- SRBTW$new(
  theta_b = p2_ex_thetaB, gamma_bed = c(0, 1, 0),
  wp = p1_signals$SCD$get0Function(), wc = p1_signals$SCD$get0Function(),
  lambda = rep(0, 4), begin = 0, end = 1)

p2_ex_srbtw$setParams(vartheta_l = p2_ex_varthetaL)
```

In figure \ref{fig:inverse-example-fig} we can quite clearly see how the pattern warped from the blue intervals into the orange intervals

```{r inverse-example-fig, echo=FALSE, fig.cap="Warping the variable from within the blue to the orange intervals.", fig.align="top", fig.pos="ht!"}
plot_2_functions(
  fReference = p1_signals$SCD$get0Function(),
  fQuery = Vectorize(function(x) p2_ex_srbtw$M(x = x))) +
  geom_vline(xintercept = fd_data_boundaries[1], color = "blue") +
  geom_vline(xintercept = fd_data_boundaries[2], color = "blue") +
  geom_vline(xintercept = fd_data_boundaries[3], color = "blue") +
  geom_vline(xintercept = p2_ex_thetaB[2], color = "orange") +
  geom_vline(xintercept = p2_ex_thetaB[3], color = "orange") +
  geom_vline(xintercept = p2_ex_thetaB[4], color = "orange")
```

We have learned the following parameters from our optimization for pattern II:

```{r}
p2_best <- p2_fr$getBest(paramName = "loss")[
  1, !(p2_fr$getParamNames() %in% c("begin", "end", "duration"))]
p2_best
```

All of the initial translations ($v$) are zero. The learned lengths converted to boundaries are:

```{r}
# Here, we transform the learned lengths to boundaries.
p2_best_varthetaL <- p2_best[names(p2_best) %in% paste0("vtl_", 1:4)] /
  sum(p2_best[names(p2_best) %in% paste0("vtl_", 1:4)])
p2_best_varthetaL
p2_best_thetaB <- unname(c(
  0, p2_best_varthetaL[1], sum(p2_best_varthetaL[1:2]),
  sum(p2_best_varthetaL[1:3]), 1))
p2_best_thetaB
```

The first two intervals are rather short, while the last two are comparatively long. Let's transform all of the pattern's variables according to the parameters:

```{r}
p2_signals <- list()

for (vartype in names(weight_vartype)) {
  temp <- SRBTWBAW$new(
    theta_b = unname(p2_best_thetaB), gamma_bed = c(0, 1, 0),
    wp = p1_signals[[vartype]]$get0Function(), wc = p1_signals[[vartype]]$get0Function(),
    lambda = rep(0, 4), begin = 0, end = 1,
    lambda_ymin = rep(0, 4), lambda_ymax = rep(1, 4)) # not important here
  # That's still the same ('p2_ex_varthetaL' is the original
  # boundaries of Pattern I transformed to lengths):
  temp$setParams(vartheta_l = p2_ex_varthetaL,
                 v = -1 * p2_best[paste0("v_", vartype)],
                 vartheta_y = -1 * p2_best[paste0("vty_", 1:4, "_", vartype)])
  
  p2_signals[[vartype]] <- Signal$new(
    name = paste0("p2_", vartype), support = c(0, 1), isWp = TRUE, func = Vectorize(temp$M))
}
```

```{r echo=FALSE}
p2_data_concat <- NULL

for (vartype in names(weight_vartype)) {
  f <- p2_signals[[vartype]]$get0Function()
  x <- seq(from = 0, to = 1, length.out = 1e3)
  y <- f(x)
  li <- levels(fd_data_concat$interval)
  
  p2_data_concat <- rbind(p2_data_concat, data.frame(
    x = x,
    y = y,
    t = vartype,
    interval = sapply(X = x, FUN = function(x) {
      if (x <= p2_best_thetaB[2]) li[1] else if (x <= p2_best_thetaB[3]) li[2] else if (x <= p2_best_thetaB[4]) li[3] else li[4]
    })
  ))
}
```

The 2nd pattern, as derived from the ground truth, is shown in figure \ref{fig:p2-signals}.

```{r p2-signals, echo=FALSE, fig.cap="Second pattern as aligned by the ground truth.", fig.align="top", fig.pos="ht!"}
plot_project_data(data = p2_data_concat, boundaries = p2_best_thetaB[2:4])
```

While this worked I suppose it is fair to say that our initial pattern is hardly recognizable. Since we expected this, we planned for a third kind of pattern in section \ref{sec:pattern3}, that is purely evidence-based. It appears that, in order to match the ground truths we have at our disposal, projects register some kind of weak initial peak for the maintenance activities, that is followed by a somewhat uneventful second and third interval. Interestingly, the optimization seemed to have used to mostly straight lines in the Long Stretch phase to model linear declines and increases. The new Aftermath phase is the longest, so it is clear that the original pattern and its subdivision into phases is not a good mapping any longer. Instead of a sharp decline in the Aftermath, we now see an increase of all variables, without the chance of any decline before the last observed commit. We will check how this adapted pattern fares in section \ref{ssec:score-pattern2}.


## Pattern III: Averaging the ground truth

We can produce a pattern by computing a weighted average over all available ground truth. As weight, we can use either rater's score, their mean or consensus (default).

```{r}
gt_weighted_avg <- function(vartype, wtype = c("consensus", "rater.a", "rater.b", "rater.mean"), use_signals = project_signals, use_ground_truth = ground_truth) {
  wtype <- match.arg(wtype)
  gt <- use_ground_truth[use_ground_truth[[wtype]] > 0, ]
  wTotal <- sum(gt[[wtype]])
  proj <- gt$project
  weights <- `names<-`(gt[[wtype]], gt$project)
  
  funcs <- lapply(
    use_signals, function(ps) ps[[vartype]]$get0Function())
  
  Vectorize(function(x) {
    val <- 0
    for (p in proj) {
      val <- val + weights[[p]] * funcs[[p]](x)
    }
    val / wTotal
  })
}
```

Now we can easily call above function to produce a weighted average of each signal:

```{r}
p3_avg_signals <- list()

for (vartype in names(weight_vartype)) {
  p3_avg_signals[[vartype]] <- Signal$new(
    name = paste0("p3_avg_", vartype), support = c(0, 1), isWp = TRUE,
    func = gt_weighted_avg(vartype = vartype))
}
```

```{r echo=FALSE}
p3_avg_data_concat <- NULL

for (vartype in names(weight_vartype)) {
  f <- p3_avg_signals[[vartype]]$get0Function()
  x <- seq(from = 0, to = 1, length.out = 1e3)
  y <- f(x)
  
  p3_avg_data_concat <- rbind(p3_avg_data_concat, data.frame(
    x = x,
    y = y,
    t = vartype
  ))
}
```

The 2nd pattern, as derived from the ground truth, is shown in figure \ref{fig:p3-avg-signals}.

```{r p3-avg-signals, echo=FALSE, fig.cap="The third kind of pattern as weighted average over all ground truth.", fig.align="top", fig.pos="ht!"}
plot_project_data(data = p3_avg_data_concat)
```




## Pattern III (b): Evidence-based\label{sec:pattern3}

A third kind of pattern is produced by starting with an empty warping pattern and having all available ground truth adapt to it. Empty means that we will start with a flat line located at $0.5$ for each variable. Finally, the parameters are inversed. While we could do this the other way round, we have two reasons to do it this way, which is the same as we used for pattern II. First of all if the warping candidate was a perfectly flat line, it would be very difficult for the gradient to converge towards some alignment. Secondly, we want to use equidistantly-spaced boundaries (resulting in equal-length intervals) and using this approach, we can guarantee the interval lengths. To find the optimum amount of intervals, we try all values in a certain range and compute a fit, and then use an information criterion to decide which of the produced patterns provides the best trade-off between number of parameters and goodness-of-fit.

The process is the same as for pattern II: Using an instance of `srBTAW_MultiVartype` that holds one instance of an `srBTAW` per variable-type. We will choose equidistantly-spaced boundaries over the WP, and start with just $1$ interval, going up to some two-digit number. The best amount of parameters (intervals) is then determined using the Akaike Information Criterion [@akaike1981likelihood], which is directly implemented in `srBTAW`. We either have to use continuous losses or make sure to __always__ use the exact same amount of samples total. The amount per interval is determined by dividing by the number of intervals. This is important, as otherwise the information criterion will not work. We will do a single RSS-loss that covers all intervals. We will also use an instance of `TimeWarpRegularization` with the `exint2`-regularizer, as it scales with arbitrary many intervals (important!). I do not suppose that regularization for the y-values is needed, so we will not have this. This means that the resulting objective has just two losses.

For a set of equal-length number of intervals, we will fit such a multiple variable-type model. This also means we can do this in parallel. However, models with more intervals and hence more parameters will considerable take longer during gradient iterations. The more parameters, the fewer of these models should be fit simultaneously. We have access to 128-thread machine (of which about 125 thread can be used). Gradients are computed in parallel as well.


### Preparation

We define a single function that encapsulates the multiple variable-type model, losses and objectives and returns them, so that we can just fit them in a loop. The only configurable parameters is the amount of intervals.

```{r}
p3_prepare_mvtypemodel <- function(numIntervals) {
  eps <- sqrt(.Machine$double.eps)
  p3_smv <- srBTAW_MultiVartype$new()
  
  p3_vars <- c("A", "CP", "FREQ", "SCD")
  p3_inst <- list()
  
  # The objective:
  p3_lls <- srBTAW_LossLinearScalarizer$new(
    returnRaw = FALSE, computeParallel = TRUE, gradientParallel = TRUE)
  
  for (name in p3_vars) {
    p3_inst[[name]] <- srBTAW$new(
      # Always includes 0,1 - just as we need it! Works for values >= 1
      theta_b = seq(from = 0, to = 1, by = 1 / numIntervals),
      gamma_bed = c(0, 1, eps),
      lambda = rep(eps, numIntervals),
      begin = 0, end = 1, openBegin = FALSE, openEnd = FALSE,
      useAmplitudeWarping = TRUE,
      # We allow these to be larger; however, the final result should be within [0,1]
      lambda_ymin = rep(-10, numIntervals),
      lambda_ymax = rep( 10, numIntervals),
      isObjectiveLogarithmic = TRUE,
      paramNames = c("v",
        paste0("vtl_", seq_len(length.out = length(p3_vars))),
        paste0("vty_", seq_len(length.out = length(p3_vars)))))
    
    # The WP is a flat line located at 0.5:
    p3_inst[[name]]$setSignal(signal = Signal$new(
      func = function(x) .5, isWp = TRUE, support = c(0, 1), name = paste0("p3_", name)))
    
    # Set the common objective:
    p3_inst[[name]]$setObjective(obj = p3_lls)
    
    # .. and also all the projects' signals:
    for (project in ground_truth[ground_truth$consensus > 0, ]$project) {
      p3_inst[[name]]$setSignal(signal = project_signals[[project]][[name]])
    }
    
    p3_smv$setSrbtaw(varName = name, srbtaw = p3_inst[[name]])
  }

  # We call this there so there are parameters present.
  set.seed(1337 * numIntervals)
  p3_smv$setParams(params =
    `names<-`(x = runif(n = p3_smv$getNumParams()), value = p3_smv$getParamNames()))
  
  for (name in p3_vars) {
    # Add RSS-loss per variable-pair:
    for (project in ground_truth[ground_truth$consensus > 0, ]$project) {
      # The RSS-loss:
      lossRss <- srBTAW_Loss_Rss$new(
        wpName = paste0("p3_", name), wcName = paste(project, name, sep = "_"),
        weight = 1, intervals = seq_len(length.out = numIntervals), continuous = FALSE,
        numSamples = rep(round(5000 / numIntervals), numIntervals), returnRaw = TRUE)
      p3_inst[[name]]$addLoss(loss = lossRss)
      p3_lls$setObjective(
        name = paste(project, name, "rss", sep = "_"), obj = lossRss)
    }
  }
  
  # This has a much higher weight than we had for pattern II
  # because we are using many more samples in the RSS-loss.
  p3_lls$setObjective(name = "p3_reg_exint2", obj = TimeWarpRegularization$new(
    weight = p3_lls$getNumObjectives(), use = "exint2", returnRaw = TRUE,
    wpName = "p3_A", wcName = project_signals$project_1$A$getName(),
    intervals = seq_len(numIntervals)
  )$setSrBtaw(srbtaw = p3_inst$A))
  
  list(smv = p3_smv, lls = p3_lls)
}
```


Now we can compute these in parallel:

```{r p3-compute}
for (numIntervals in c(1:16)) {
  loadResultsOrCompute(
    file = paste0("../results/p3-compute/i_", numIntervals, ".rds"),
    computeExpr =
  {
    p3_vars <- c("A", "CP", "FREQ", "SCD")
    temp <- p3_prepare_mvtypemodel(numIntervals = numIntervals)
    tempf <- tempfile()
    saveRDS(object = temp, file = tempf)
    
    # It does not scale well beyond that.
    cl <- parallel::makePSOCKcluster(min(32, parallel::detectCores()))
    parallel::clusterExport(cl = cl, varlist = list("tempf"))
    
    optR <- doWithParallelClusterExplicit(cl = cl, expr = {
      optimParallel::optimParallel(
        par = temp$smv$getParams(),
        method = "L-BFGS-B",
        lower = c(
          rep(-.Machine$double.xmax, length(p3_vars)), # v_[vartype]
          rep(sqrt(.Machine$double.eps), length(p3_vars)), # vtl
          rep(-.Machine$double.xmax, length(p3_vars) * length(weight_vartype))), # vty for each
        upper = c(
          rep(.Machine$double.xmax, length(p3_vars)),
          rep(1, length(p3_vars)),
          rep(.Machine$double.xmax, length(p3_vars) * length(weight_vartype))),
        fn = function(x) {
          temp <- readRDS(file = tempf)
          temp$smv$setParams(params = x)
          temp$lls$compute0()
        },
        parallel = list(cl = cl, forward = FALSE, loginfo = TRUE)
      )
    })
    list(optR = optR, smv = temp$smv, lls = temp$lls)
  })
}
```

### Finding the best fit

We will load all the results previously computed and compute an information criterion to compare fits, and then choose the best model.

```{r}
p3_params <- NULL
for (tempPath in gtools::mixedsort(
  Sys.glob(paths = paste0(getwd(), "/../results/p3-compute/i*.rds")))
) {
  temp <- readRDS(file = tempPath)
  p3_params <- rbind(p3_params, data.frame(
    numInt = (temp$lls$getNumParams() - 1) / 2,
    numPar = temp$smv$getNumParams(),
    numParSrBTAW = temp$lls$getNumParams(),
    # This AIC would the original one!
    AIC = 2 * temp$smv$getNumParams() - 2 * log(1 / exp(temp$optR$value)),
    # This AIC is based on the number of intervals, not parameters!
    AIC1 = (temp$lls$getNumParams() - 1) - 2 * log(1 / exp(temp$optR$value)),
    # This AIC is based on the amount of parameters per srBTAW instance:
    AIC2 = 2 * temp$lls$getNumParams() - 2 * log(1 / exp(temp$optR$value)),
    logLoss = temp$optR$value,
    loss = exp(temp$optR$value)
  ))
}
```

In table \ref{tab:p3-params}, we show computed fits for various models, where the only difference is the number of intervals. Each interval comes with two degrees of freedom: its length and terminal y-translation. Recall that each computed fit concerns four variables. For example, the first model with just one interval per variable has nine parameters: All of the variables share the interval's length, the first parameter. Then, each variable has one $v$-parameter, the global y-translation. For each interval, we have one terminal y-translation. For example, the model with $7$ intervals has $7 + 4 + (4\times7)=39$ parameters.

We compute the AIC for each fit, which is formulated as in the following. The parameter $k$ is the number of parameters in the model, i.e., as described, it refers to all the parameters in the `srBTAW_MultiVartype`-model. The second AIC-alternative uses the parameter $p$ instead, which refers to the number of variables per `srBTAW`-instance.

$$
\begin{aligned}
  \operatorname{AIC}=&\;2\times k - 2\times\log{(\mathcal{\hat{L}})}\;\text{, where}\;\mathcal{\hat{L}}\;\text{is the maximum log-likelihood of the model,}
  \\[1ex]
  \mathcal{\hat{L}}=&\;\frac{1}{\exp{\big(\;\text{lowest loss of the model}\;\big)}}\;\text{, since we use logarithmic losses.}
  \\[1em]
  \text{The alternatives}&\;\operatorname{AIC^1}\;\text{and}\;\operatorname{AIC^2}\;\text{ are defined as:}
  \\[1ex]
  \operatorname{AIC^1}=&\;k-2\times\log{(\mathcal{\hat{L}})}-1\;\text{, which is based on the number of intervals, and}
  \\[1ex]
  \operatorname{AIC^2}=&\;2\times p - 2\times\log{(\mathcal{\hat{L}})}\;\text{, where}\;p\;\text{is the amount of params per}\;\operatorname{srBTAW}\text{-instance.}
\end{aligned}
$$

```{r echo=FALSE}
if (interactive()) {
  p3_params
} else {
  knitr::kable(
    x = round(p3_params, 3),
    booktabs = TRUE,
    caption = "Likelihood and Akaike information criteria (AIC) for computed models.",
    label = "p3-params"
  )
}
```

Comparing the results from table \ref{tab:p3-params}, it appears that no matter how we define the AIC, it is increasing with the number of parameters, and it does so faster than the loss reduces. So, picking a model by AIC is not terribly useful, as the results suggest we would to go with the $1$-interval model. The model with the lowest loss is the one with `r p3_params[which.min(p3_params$loss), ]$numInt` intervals.


### Create pattern from best fit

This is the same process as for pattern II, as the parameters need inversion. We will reconstruct the warped signals according to the inversed parameters to produce the third pattern. According to the overview above, the best model (lowest loss, __not__ AIC) is the one with __`r p3_params[which.min(p3_params$loss), ]$numInt`__ intervals. Its parameters are the following:

```{r}
# Let's first define a function that inverses the params and reconstructs the pattern.
p3_pattern_from_fit <- function(whichNumIntervals) {
  res <- list()
  p3_i <- readRDS(file = paste0(getwd(), "/../results/p3-compute/i_", whichNumIntervals, ".rds"))
  
  # FitResult:
  fr <- FitResult$new("foo")
  fr$fromOptimParallel(optR = p3_i$optR)
  res$fr <- fr
  
  # Inversion:
  lambda <- p3_i$smv$.__enclos_env__$private$instances$A$.__enclos_env__$private$instances$`p3_A|project_1_A`$getLambda()
  p3_i_varthetaL <- p3_i$optR$par[grepl(pattern = "^vtl_", x = names(p3_i$optR$par))]
  for (q in seq_len(length.out = whichNumIntervals)) {
    if (p3_i_varthetaL[q] < lambda[q]) {
      p3_i_varthetaL[q] <- lambda[q]
    }
  }
  p3_i_varthetaL <- p3_i_varthetaL / sum(p3_i_varthetaL)
  
  p3_i_thetaB <- c(0)
  for (idx in seq_len(length.out = length(p3_i_varthetaL))) {
    p3_i_thetaB <- c(p3_i_thetaB, sum(p3_i_varthetaL[1:idx]))
  }
  p3_i_thetaB[length(p3_i_thetaB)] <- 1 # numeric stability
  
  p3_i_varthetaL
  p3_i_thetaB
  res$varthetaL <- p3_i_varthetaL
  res$thetaB <- p3_i_thetaB
  
  # Signals:
  p3_i_numInt <- length(p3_i_varthetaL)
  p3_i_signals <- list()
  
  for (vartype in names(weight_vartype)) {
    emptySig <- Signal$new(
      isWp = TRUE, # does not matter here
        func = function(x) .5, support = c(0, 1), name = paste0("p3_", vartype))
    
    temp <- SRBTWBAW$new(
      theta_b = unname(p3_i_thetaB), gamma_bed = c(0, 1, 0),
      wp = emptySig$get0Function(), wc = emptySig$get0Function(),
      lambda = rep(0, p3_i_numInt), begin = 0, end = 1,
      lambda_ymin = rep(0, p3_i_numInt), lambda_ymax = rep(1, p3_i_numInt))
    
    # Recall that originally we used equidistantly-spaced boundaries:
    temp$setParams(vartheta_l = rep(1 / p3_i_numInt, p3_i_numInt),
                   v = -1 * p3_i$optR$par[paste0("v_", vartype)],
                   vartheta_y = -1 * p3_i$optR$par[paste0("vty_", 1:p3_i_numInt, "_", vartype)])
    
    p3_i_signals[[vartype]] <- Signal$new(
      name = paste0("p3_", vartype), support = c(0, 1), isWp = TRUE, func = Vectorize(temp$M))
  }
  res$signals <- p3_i_signals
  
  # Data:
  temp <- NULL
  for (vartype in names(weight_vartype)) {
    f <- p3_i_signals[[vartype]]$get0Function()
    x <- seq(from = 0, to = 1, length.out = 1e3)
    y <- f(x)
    
    temp <- rbind(temp, data.frame(
      x = x,
      y = y,
      t = vartype,
      numInt = whichNumIntervals
    ))
  }
  res$data <- temp
  res
}
```


```{r}
p3_best <- readRDS(file = paste0(getwd(), "/../results/p3-compute/i_",
                     p3_params[which.min(p3_params$loss), ]$numInt, ".rds"))
p3_best$optR$par
```

First we have to inverse the parameters before we can reconstruct the signals:

```{r}
p3_best_varthetaL <- p3_best$optR$par[
  grepl(pattern = "^vtl_", x = names(p3_best$optR$par))]
p3_best_varthetaL <- p3_best_varthetaL / sum(p3_best_varthetaL)

p3_best_thetaB <- c(0)
for (idx in seq_len(length.out = length(p3_best_varthetaL))) {
  p3_best_thetaB <- c(p3_best_thetaB, sum(p3_best_varthetaL[1:idx]))
}
p3_best_thetaB[length(p3_best_thetaB)] <- 1 # numeric stability

p3_best_varthetaL
p3_best_thetaB
```


```{r}
p3_best_numInt <- length(p3_best_varthetaL)
p3_signals <- list()

for (vartype in names(weight_vartype)) {
  emptySig <- Signal$new(
    isWp = TRUE, # does not matter here
      func = function(x) .5, support = c(0, 1), name = paste0("p3_", vartype))
  
  temp <- SRBTWBAW$new(
    theta_b = unname(p3_best_thetaB), gamma_bed = c(0, 1, 0),
    wp = emptySig$get0Function(), wc = emptySig$get0Function(),
    lambda = rep(0, p3_best_numInt), begin = 0, end = 1,
    lambda_ymin = rep(0, p3_best_numInt), lambda_ymax = rep(1, p3_best_numInt))
  
  # Recall that originally we used equidistantly-spaced boundaries:
  temp$setParams(vartheta_l = rep(1 / p3_best_numInt, p3_best_numInt),
                 v = -1 * p3_best$optR$par[paste0("v_", vartype)],
                 vartheta_y = -1 * p3_best$optR$par[paste0("vty_", 1:p3_best_numInt, "_", vartype)])
  
  p3_signals[[vartype]] <- Signal$new(
    name = paste0("p3_", vartype), support = c(0, 1), isWp = TRUE, func = Vectorize(temp$M))
}
```


```{r echo=FALSE}
p3_best_data_concat <- NULL

for (vartype in names(weight_vartype)) {
  f <- p3_signals[[vartype]]$get0Function()
  x <- seq(from = 0, to = 1, length.out = 1e3)
  y <- f(x)
  
  p3_best_data_concat <- rbind(p3_best_data_concat, data.frame(
    x = x,
    y = y,
    t = vartype
  ))
}
```

The 2nd pattern, as derived from the ground truth, is shown in figure \ref{fig:p3-signals}.

```{r p3-signals, echo=FALSE, fig.cap="Pattern type III (b) pattern as aligned by the ground truth only.", fig.align="top", fig.pos="ht!"}
plot_project_data(data = p3_best_data_concat, boundaries = p3_best_thetaB[2:(length(p3_best_thetaB) - 1)])
```

Let's show all computed patterns in a grid:

```{r echo=FALSE}
p3b_all <- loadResultsOrCompute(file = "../results/p3b_all.rds", computeExpr = {
  unlist(doWithParallelCluster(numCores = 16, expr = {
    foreach::foreach(
      numInt = 1:16,
      .inorder = FALSE
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      `names<-`(list(p3_pattern_from_fit(whichNumIntervals = numInt)), paste0("i_", numInt))
    }
  }), recursive = FALSE)
})
```


In figure \ref{fig:p3-all} we can clearly observe how the pattern evolves with growing number or parameters. Almost all patterns with sufficiently many degrees of freedom have some crack at about one quarter of the projects' time, a second crack is observed at about three quarter's time. In all patterns, it appears that adaptive activities are the least common. All patterns started with randomized coefficients, and something must have gone wrong for pattern $8$. From five and more intervals we can observe growing similarities with the weighted-average pattern, although it never comes really close. Even though we used a timewarp-regularizer with high weight, we frequently get extreme intervals.


```{r p3-all, echo=FALSE, fig.height=6, fig.cap="Computed pattern by number of intervals.", fig.align="top", fig.pos="ht!"}
temp <- NULL
for (res in p3b_all) {
  temp <- rbind(temp, res$data)
}

ggplot(data = temp, mapping = aes(x = x, y = y, color = t)) +
  geom_line() +
  facet_wrap(numInt ~., nrow = 4) +
  theme_light() +
  labs(color = "Variable") + xlab("Relative Time") + ylab("Value") +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"))
```


In figure \ref{fig:p3b-all-fr} we can clearly see that all but the eighth pattern converged nicely (this was already visible in \ref{fig:p3-all}). The loss is logarithmic, so the progress is rather substantial. For example, going from $\log{(14)}\approx1.2e6$ to $\log{(8)}\approx3e3$ is a reduction by $3$ (!) orders of magnitude.


```{r p3b-all-fr, echo=FALSE, fig.height=6, fig.cap="Losses for all computed pattern by number of intervals.", fig.align="top", fig.pos="ht!"}
ggarrange(
  plotlist = lapply(p3b_all, function(p) p$fr$plot_loss() + theme(axis.title.x.bottom = element_blank(), axis.title.y.left = element_blank()) + labs(subtitle = paste0("numInt=", length(p$varthetaL)))),
  nrow = 4, ncol = 4, common.legend = TRUE
)
```




# Scoring of projects

The true main-purpose of our work is to take a pattern and check it against any project, with the goal of obtaining a score, or goodness-of-match so that we can determine if the AP in the pattern is present in the project. In the previous sections we have introduced a number of patterns that we are going to apply here.

How it works: Given some pattern that consists of one or arbitrary many signals, the pattern is added to a single instance of `srBTAW` as __Warping Pattern__. The project's signals are added as __Warping Candidates__ to the same instance.

To compute a score, we need to define how to measure the distance between the WP and the WC (between each pair of signals and each interval). In the notebooks for sr-BTW we have previously defined some suitable losses with either __global__ or __local__ finite upper bounds. Currently, the Jensen--Shannon divergence (JSD), as well as the ratio-metrics (correlation, arc-lengths) have global upper bounds. For the JSD, it is $\ln{(2)}$. Losses with local finite upper bound are, for example, the area between curves, the residual sum of squares, the Euclidean distance etc., basically any metric that has a limit within the rectangle demarcated by one or more intervals. For some of the patterns, we have used a combination of such losses with local bounds. In general, it is not necessary to fit a pattern with the same kinds of losses that are later on used for scoring, but it is recommended to avoid confusing may results.

## The cost of alignment

When aligning a project to a pattern using boundary time warping, a deviation between the sections' lengths is introduced. Ideally, if the project would align with the pattern perfectly, there would be perfect agreement. The less good a project aligns with a pattern, the more time warping is required. However, the entire alignment needs to be assessed in conjunction with the scores -- the amount of required time warping alone is not sufficient to assess to overall goodness of fit.

During the optimization, we already used a regularizer for extreme intervals (`TimeWarpRegularization` with regularizer `exint2`).

## Scoring mechanisms\label{sssec:score-mech}

For scoring a single project, we first warp it to the pattern, then we measure the remaining distance. We only do time-warping of the projects to the pattern. We could compute a score for each interval. However, the ground truth does not yield this, so we will only compute a scores for entire signals, i.e., over all intervals. Once aligned, computing scores is cheap, so we will try a variety of scores and see what works best.

```{r}
# Function score_variable_alignment(..) has been moved to common-funcs.R!
```

We define a parallelized function to compute all scores of a project:

```{r}
compute_all_scores <- function(alignment, patternName) {
  useScores <- c("area", "corr", "jsd", "kl", "arclen", "sd", "var",
                 "mae", "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")
  
  `rownames<-`(doWithParallelCluster(numCores = length(alignment), expr = {
    foreach::foreach(
      projectName = names(alignment),
      .inorder = TRUE,
      .combine = rbind,
      .export = c("score_variable_alignment", "weight_vartype")
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      
      scores <- c()
      for (score in useScores) {
        temp <- score_variable_alignment(
          patternName = patternName, projectName = projectName,
          alignment = alignment[[projectName]], use = score)
        scores <- c(scores, `names<-`(
          c(mean(temp), prod(temp)), c(paste0(score, c("_m", "_p")))))
      }
      `colnames<-`(matrix(data = scores, nrow = 1), names(scores))
    }
  }), sort(names(alignment)))
}
```


We also need to define a function for warping a project to the pattern:

```{r}
# Function time_warp_project(..) has been moved to common-funcs.R!
```

## Pattern I

First we compute the alignment for all projects, then all scores.

```{r p1-align}
library(foreach)

p1_align <- loadResultsOrCompute(file = "../results/p1_align.rds", computeExpr = {
  # Let's compute all projects in parallel!
  cl <- parallel::makePSOCKcluster(length(project_signals))
  unlist(doWithParallelClusterExplicit(cl = cl, expr = {
    foreach::foreach(
      projectName = names(project_signals),
      .inorder = FALSE,
      .packages = c("parallel")
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      
      # There are 5 objectives that can be computed in parallel!
      cl_nested <- parallel::makePSOCKcluster(5)
      `names<-`(list(doWithParallelClusterExplicit(cl = cl_nested, expr = {
        temp <- time_warp_project(
          pattern = p1_signals, project = project_signals[[projectName]])
        temp$fit(verbose = TRUE)
        temp # return the instance, it includes the FitResult
      })), projectName)
    }
  }))
})
```

```{r p1-scores}
p1_scores <- loadResultsOrCompute(file = "../results/p1_scores.rds", computeExpr = {
  as.data.frame(compute_all_scores(alignment = p1_align, patternName = "p1"))
})
```

Recall that we are obtaining scores for each interval. To aggregate them we build the product and the mean in the following table, there is no weighing applied.

```{r echo=FALSE}
if (interactive()) {
  round(t(p1_scores), 4)
} else {
  knitr::kable(
    x = round(`colnames<-`(t(p1_scores), paste0("pr_", 1:9)), 2),
    booktabs = TRUE,
    caption = "Scores for the aligned projects with pattern I (p=product, m=mean).",
    label = "p1-scores"
  )
}
```

In table \ref{tab:p1-corr} the correlations of the scores with the ground truth as computed against the first pattern are shown.

```{r echo=FALSE}
corr <- stats::cor(ground_truth$consensus, p1_scores)[1, ]

if (interactive()) {
  corr
} else {
  perCol <- ceiling(length(corr) / 3)
  temp <- data.frame(matrix(ncol = 6, nrow = perCol))
  for (idx in 1:3) {
    off <- (idx - 1) * perCol
    temp[, idx * 2 - 1] <- names(corr)[(1 + off):(perCol + off)]
    temp[, idx * 2] <- corr[(1 + off):(perCol + off)]
  }
  colnames(temp) <- rep(c("Score", "Value"), 2)
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Correlation of the ground truth with all other scores for pattern I.",
    label = "p1-corr")
}
```

Let's show a correlation matrix in figure \ref{fig:p1-corr-mat}:

```{r p1-corr-mat, echo=FALSE, fig.cap="Correlation matrix for scores using pattern I.", fig.align="top", fig.pos="ht!"}
temp <- cbind(data.frame(gt_consensus = ground_truth$consensus_score), p1_scores[,])
temp <- temp[, colSums(is.na(temp)) < nrow(temp)]

corrplot::corrplot(corr = stats::cor(temp), type = "upper",# order = "hclust",
                   tl.col = "black", tl.srt = 90)
```

We appear to have mostly strongly negative correlations -- note that the measure using Kullback-Leibler is a divergence, not a similarity. Area and RMSE have a strong negative correlation, which suggests that whenever their score is high, the ground truth's consensus score is low. A high score for area or RMSE however means, that the distance between the signals is comparatively low, so we should have a good alignment, so how is this explained then?

Going back to table \ref{tab:groundtruth}, we will notice that projects 2, 3 and 5 have somewhat similar courses for their variables, yet their consensus scores are 0, 6 and 1, respectively. In other words, only project 3 has had a Fire Drill. We can hence conclude that the visual distance and the ground truth consensus are __not__ proportional (at least not for the variables we chose to model). The visual similarity of a project to a pattern is just that; the score quantifies the deviation from the pattern, but it does not necessarily correlate with the ground truth. This however was our underlying assumption all along, hence the initial pattern. We deliberately chose to design patterns _without_ investigating any of the projects. Also, while we had access to the projects for some time now, the ground truth became available only very recently, after all modeling was done.

Nevertheless, it does not hurt to check out patterns II and III, as we would like to achieve better matches. Eventually, the goal with this approach is to improve correlations and to get more accurate scores. The final stage then could be to compute, for example, a weighted consensus, based on the projects that we have, or to create a linear model that can regress to a value close to the ground truth by considering all the different scores.

## Pattern II\label{ssec:score-pattern2}

The second pattern was produced by having it warp to all the ground truths simultaneously, using their weight.

```{r p2-align}
library(foreach)

p2_align <- loadResultsOrCompute(file = "../results/p2_align.rds", computeExpr = {
  # Let's compute all projects in parallel!
  cl <- parallel::makePSOCKcluster(length(project_signals))
  unlist(doWithParallelClusterExplicit(cl = cl, expr = {
    foreach::foreach(
      projectName = names(project_signals),
      .inorder = FALSE,
      .packages = c("parallel")
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      
      # There are 5 objectives that can be computed in parallel!
      cl_nested <- parallel::makePSOCKcluster(5)
      `names<-`(list(doWithParallelClusterExplicit(cl = cl_nested, expr = {
        temp <- time_warp_project(
          pattern = p2_signals, project = project_signals[[projectName]])
        temp$fit(verbose = TRUE)
        temp # return the instance, it includes the FitResult
      })), projectName)
    }
  }))
})
```


```{r score-p2}
p2_scores <- loadResultsOrCompute(file = "../results/p2_scores.rds", computeExpr = {
  as.data.frame(compute_all_scores(alignment = p2_align, patternName = "p2"))
})
```


```{r echo=FALSE}
if (interactive()) {
  round(t(p2_scores), 4)
} else {
  knitr::kable(
    x = round(`colnames<-`(t(p2_scores), paste0("pr_", 1:9)), 2),
    booktabs = TRUE,
    caption = "Scores for the aligned projects with pattern II (p=product, m=mean).",
    label = "p2-scores"
  )
}
```

The correlation of just the ground truth with all scores is in table \ref{tab:p2-corr}.

```{r echo=FALSE}
corr <- stats::cor(ground_truth$consensus, p2_scores)[1, ]

if (interactive()) {
  corr
} else {
  perCol <- ceiling(length(corr) / 3)
  temp <- data.frame(matrix(ncol = 6, nrow = perCol))
  for (idx in 1:3) {
    off <- (idx - 1) * perCol
    temp[, idx * 2 - 1] <- names(corr)[(1 + off):(perCol + off)]
    temp[, idx * 2] <- corr[(1 + off):(perCol + off)]
  }
  colnames(temp) <- rep(c("Score", "Value"), 3)
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Correlation of the ground truth with all other scores for pattern II.",
    label = "p2-corr")
}
```

The correlation matrix looks as in figure \ref{fig:p2-corr-mat}.

```{r p2-corr-mat, echo=FALSE, fig.cap="Correlation matrix for scores using pattern II.", fig.align="top", fig.pos="ht!"}
temp <- cbind(data.frame(gt_consensus = ground_truth$consensus_score), p2_scores[,])

corrplot::corrplot(corr = stats::cor(temp), type = "upper",# order = "hclust",
                   tl.col = "black", tl.srt = 90)
```

With the second pattern we get much stronger correlations on average, meaning that the alignment of each project to the second pattern is better. While some correlations with the ground truth remain similar, we get strong positive correlations with the signal-measures _Impulse-factor_ (`r round(stats::cor(ground_truth$consensus_score, p2_scores[, "ImpulseFactor_m"]), 3)`) and _Peak_ (`r round(stats::cor(ground_truth$consensus_score, p2_scores[, "Peak_m"]), 3)`) [@xi2000bearing]. This however is explained by the double warping: First the pattern II was produced by time- and amplitude-warping it to the ground truth. Then, each project was time-warped to the that pattern. Simply put, this should result in some good alignment of all of the signals' peaks, explaining the high correlations.


## Pattern II (without alignment)

Since pattern II was computed such that it warps to all projects, it already should correct for time- and amplitude warping. This gives us incentive to compute scores against the non-aligned projects:


```{r}
p2_no_align <- list()

for (project in ground_truth$project) {
  temp <- p2_align[[project]]$clone()
  temp$setParams(params = `names<-`(p2_ex_varthetaL, temp$getParamNames()))
  p2_no_align[[project]] <- temp
}
```

```{r score-p2_no}
p2_no_scores <- loadResultsOrCompute(file = "../results/p2_no_scores.rds", computeExpr = {
  as.data.frame(compute_all_scores(alignment = p2_no_align, patternName = "p2"))
})
```


```{r echo=FALSE}
if (interactive()) {
  round(t(p2_no_scores), 4)
} else {
  knitr::kable(
    x = round(`colnames<-`(t(p2_no_scores), paste0("pr_", 1:9)), 2),
    booktabs = TRUE,
    caption = "Scores for the non-aligned projects with pattern II (p=product, m=mean).",
    label = "p2-no-scores"
  )
}
```

The correlation of just the ground truth with all scores is in table \ref{tab:p2-no-corr}.

```{r echo=FALSE}
corr <- stats::cor(ground_truth$consensus, p2_no_scores)[1, ]

if (interactive()) {
  corr
} else {
  perCol <- ceiling(length(corr) / 3)
  temp <- data.frame(matrix(ncol = 6, nrow = perCol))
  for (idx in 1:3) {
    off <- (idx - 1) * perCol
    temp[, idx * 2 - 1] <- names(corr)[(1 + off):(perCol + off)]
    temp[, idx * 2] <- corr[(1 + off):(perCol + off)]
  }
  colnames(temp) <- rep(c("Score", "Value"), 3)
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Correlation of the ground truth with all other non-aligned scores for pattern II.",
    label = "p2-no-corr")
}
```

The correlation matrix looks as in figure \ref{fig:p2-corr-mat}.

```{r p2-no-corr-mat, echo=FALSE, fig.cap="Correlation matrix for non-aligned scores using pattern II.", fig.align="top", fig.pos="ht!"}
temp <- cbind(data.frame(gt_consensus = ground_truth$consensus_score), p2_no_scores[,])

corrplot::corrplot(corr = stats::cor(temp), type = "upper",# order = "hclust",
                   tl.col = "black", tl.srt = 90)
```

While some correlations are weaker now, we observe more agreement between the scores, i.e., more correlations tend to be positive. _Peak_ and _Impulse-factor_ however have negative correlations now.

## Pattern III (average)

The 3rd pattern that was produced as weighted average over all ground truth is scored in this section. __Note!__ There is one important difference here: the weighted-average pattern does not have the same intervals as our initial pattern -- in fact, we cannot make any assumptions about any of the intervals. Therefore, we will compute this align with __ten equally-long__ intervals. This amount was chosen arbitrarily as a trade-off between the time it takes to compute, and the resolution of the results. Adding more intervals increases both, computing time and resolution exponentially, however the latter much less.


```{r p3-avg-align}
library(foreach)

p3_avg_align <- loadResultsOrCompute(file = "../results/p3_avg_align.rds", computeExpr = {
  # Let's compute all projects in parallel!
  cl <- parallel::makePSOCKcluster(length(project_signals))
  unlist(doWithParallelClusterExplicit(cl = cl, expr = {
    foreach::foreach(
      projectName = names(project_signals),
      .inorder = FALSE,
      .packages = c("parallel")
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      
      cl_nested <- parallel::makePSOCKcluster(5)
      `names<-`(list(doWithParallelClusterExplicit(cl = cl_nested, expr = {
        temp <- time_warp_project(
          thetaB = seq(from = 0, to = 1, by = 0.1), # important!
          pattern = p3_avg_signals, project = project_signals[[projectName]])
        temp$fit(verbose = TRUE)
        temp # return the instance, it includes the FitResult
      })), projectName)
    }
  }))
})
```


```{r score-p3_avg}
p3_avg_scores <- loadResultsOrCompute(file = "../results/p3_avg_scores.rds", computeExpr = {
  as.data.frame(compute_all_scores(alignment = p3_avg_align, patternName = "p3_avg"))
})
```


```{r echo=FALSE}
if (interactive()) {
  round(t(p3_avg_scores), 4)
} else {
  knitr::kable(
    x = round(`colnames<-`(t(p3_avg_scores), paste0("pr_", 1:9)), 2),
    booktabs = TRUE,
    caption = "Scores for the aligned projects with pattern III (average ground truth).",
    label = "p3-avg-scores"
  )
}
```

The correlation of just the ground truth with all scores is in table \ref{tab:p3-avg-corr}.

```{r echo=FALSE}
corr <- stats::cor(ground_truth$consensus, p3_avg_scores)[1, ]

if (interactive()) {
  corr
} else {
  perCol <- ceiling(length(corr) / 3)
  temp <- data.frame(matrix(ncol = 6, nrow = perCol))
  for (idx in 1:3) {
    off <- (idx - 1) * perCol
    temp[, idx * 2 - 1] <- names(corr)[(1 + off):(perCol + off)]
    temp[, idx * 2] <- corr[(1 + off):(perCol + off)]
  }
  colnames(temp) <- rep(c("Score", "Value"), 3)
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Correlation of the ground truth with all other scores for pattern II.",
    label = "p3-avg-corr")
}
```

The correlation matrix looks as in figure \ref{fig:p3-avg-corr-mat}.

```{r p3-avg-corr-mat, echo=FALSE, fig.cap="Correlation matrix for scores using pattern III (average).", fig.align="top", fig.pos="ht!"}
temp <- cbind(data.frame(gt_consensus = ground_truth$consensus_score), p3_avg_scores[,])

corrplot::corrplot(corr = stats::cor(temp), type = "upper",# order = "hclust",
                   tl.col = "black", tl.srt = 90)
```

I suppose that the most significant result here is the positive Jensen--Shannon divergence score correlation.

## Pattern III (average, no alignment)

Before we go any further, I would also like to compute the scores for this data-driven pattern __without__ having the projects aligned. After manually inspecting some of these alignments, it turns out that some are quite extreme. Since this pattern is a weighted average over all ground truth, not much alignment should be required.

```{r}
# We'll have to mimic an 'aligned' object, which is a list
# of srBTAW_MultiVartype instances. We can clone it and just
# undo the time warping.
p3_avg_no_align <- list()

for (project in ground_truth$project) {
  temp <- p3_avg_align[[project]]$clone()
  temp$setParams(params = `names<-`(rep(1 / temp$getNumParams(),
                  temp$getNumParams()), temp$getParamNames()))
  p3_avg_no_align[[project]] <- temp
}
```

```{r score-p3_avg_no}
p3_avg_no_scores <- loadResultsOrCompute(file = "../results/p3_avg_no_scores.rds", computeExpr = {
  as.data.frame(compute_all_scores(alignment = p3_avg_no_align, patternName = "p3_avg"))
})
```


```{r echo=FALSE}
if (interactive()) {
  round(t(p3_avg_no_scores), 4)
} else {
  knitr::kable(
    x = round(`colnames<-`(t(p3_avg_no_scores), paste0("pr_", 1:9)), 2),
    booktabs = TRUE,
    caption = "Scores for the non-aligned projects with pattern III (average ground truth).",
    label = "p3-avg-no-scores"
  )
}
```

The correlation of just the ground truth with all scores is in table \ref{tab:p3-avg-corr}.

```{r echo=FALSE}
corr <- stats::cor(ground_truth$consensus, p3_avg_no_scores)[1, ]

if (interactive()) {
  corr
} else {
  perCol <- ceiling(length(corr) / 3)
  temp <- data.frame(matrix(ncol = 6, nrow = perCol))
  for (idx in 1:3) {
    off <- (idx - 1) * perCol
    temp[, idx * 2 - 1] <- names(corr)[(1 + off):(perCol + off)]
    temp[, idx * 2] <- corr[(1 + off):(perCol + off)]
  }
  colnames(temp) <- rep(c("Score", "Value"), 3)
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Correlation of the ground truth with all other scores for pattern II.",
    label = "p3-avg-no-corr")
}
```

The correlation matrix looks as in figure \ref{fig:p3-avg-no-corr-mat}.

```{r p3-avg-no-corr-mat, echo=FALSE, fig.cap="Correlation matrix for non-aligned scores using pattern III (average).", fig.align="top", fig.pos="ht!"}
temp <- cbind(data.frame(gt_consensus = ground_truth$consensus_score), p3_avg_no_scores[,])

corrplot::corrplot(corr = stats::cor(temp), type = "upper",# order = "hclust",
                   tl.col = "black", tl.srt = 90)
```

And here in figure \ref{fig:p3-avg-no-corr-mat} we got the result that was the most-expected. We get almost always positive correlations, and most of them range between medium and significant strength. If we look at the correlation for `sd_p`, it is almost perfect with `r round(corr["sd_p"], 3)`. The Jensen--Shannon divergence score (note: while called divergence, this is a score, the higher the better) of `r round(corr["jsd_m"], 3)` is at a high level now. I mention this because this measure is less primitive than the others and tends to capture more properties of the signals. If this is high, it means we get a robust measure that ought to be usable stand-alone. The other low-level scores probably would need to be combined instead.

### Linear combination of scores

So far we have tested whether the calculated scores correlate with the scores of the ground truth, and we find some good examples. However, these scores are not scaled or translated in any way, so it is probably best to A) take multiple scores into account and B) create a regression model that makes these adjustments. We will test some linear combination of the scores `corr_p`, `jsd_m`, `RMS_m` and `sd_p`.

```{r}
p3_avg_lm <- stats::glm(formula = gt_consensus ~ corr_p + jsd_m + RMS_m + sd_p, data = temp)
stats::coef(p3_avg_lm)
plot(p3_avg_lm, ask = FALSE, which = 1:2)
```

Of course, this model should not be used to predict on the training data, but what we wanted to learn here is simply how to linearly combine the scores in order to get scores that are in the same range as the ground truth, we learn a re-scale so to say.

```{r}
p3_avg_lm_scores <- stats::predict(p3_avg_lm, temp)
round(p3_avg_lm_scores * 10, 3)
stats::cor(p3_avg_lm_scores, ground_truth$consensus_score)
```

This also increased the correlation to `r round(stats::cor(p3_avg_lm_scores, ground_truth$consensus_score), 3)`.


### Finding the most important scores\label{sssec:var-imp}

Previously, we have combined some hand-picked scores into a linear model, in order to mainly scale and translate them, in order to be able to obtain predictions in the range of the actual ground truth. However, this approach is not suitable for a model that shall generalize.

We have shown in figure \ref{fig:p3-avg-no-corr-mat} that most scores capture different properties of the alignment. It therefore makes sense to choose scores for a regression that, once combined, have the most predictive power in terms of accuracy (precision), and generalizability. There are quite many techniques and approaches to _feature selection_. Here, we will attempt a __recursive feature elimination__ (RFE), that uses partial least squares as estimator, a measure of __variable importance__, and using bootstrapped data (for example, @darst_using_2018).

Before we start, a few things are of importance. First, a feature selection should be done whenever a new model with prediction and generalization capabilities is to be trained, i.e., it is specific to the case (here process model and observed processes). Therefore, the features we select here are only a valid selection for a model that is to make predictions on scores as obtained from the pattern type III (average, no align). Secondly, the amount of data we have only suffices for running the suggested RFE, but the data is too scarce to obtain a robust model. Therefore, the results we will obtain here are practically only usable for giving an indication as to the relative importance of features (here: scores), but we must not deduce a genuine truth from them, nor can we conclude an absolute ordering that will hold for future runs of this approach, given different data. Thirdly, and that is specific to our case, we have separate scores for each interval (previously aggregated once using the mean, and once using the product), but the score is an aggregation across all variables (activities). Having one score represent the deviation over only one contiguous, aggregated interval, should be avoided in a real-world setting, and scores should be localized to each interval, to become their own feature. However, that requires even more data. Also, one would probably not aggregate scores across variables. We will examine this scenario more closely for issue-tracking data.

Let's start, we'll use the scores from `p3_avg_no_scores`:

```{r}
rfe_data <- cbind(
  p3_avg_no_scores,
  data.frame(gt = ground_truth$consensus))
```

The RFE is done via caret, with 3-times repeated, 10-fold cross validation as outer resampling method[^2].

[^2]: https://web.archive.org/web/20211120164401/https://topepo.github.io/caret/recursive-feature-elimination.html


```{r warning=FALSE}
library(caret, quietly = TRUE)

set.seed(1337)

control <- caret::trainControl(method="repeatedcv", number=10, repeats=3)
modelFit <- caret::train(gt ~., data=rfe_data, method="pls", trControl=control)

imp <- caret::varImp(object = modelFit)
```

```{r p3-avg-no-var-imp-plot, warning=FALSE, echo=FALSE, fig.height=5, fig.cap="Plot of the relative variable importances of scores as computed against pattern III (average).", fig.align="top", fig.pos="ht!"}

plot_var_imp(imp)
```


The relative variable importances are shown in table \ref{tab:p3-avg-no-varimp}. Unsurprisingly, correlation is the most important feature (score), as it expresses the degree to which two curves resemble each other (regardless of their absolute difference). This is then captured by the next most important score, the RMSE, closely followed by the standard deviation and area between curves. Note that area, RMSE, sd and variance are all highly correlated, so this does not come as a surprise. The next two places is the Impulsefactor and Jenson--Shannon divergence. We observe a somewhat healthy mix between mean- and product-measures. For values that tend to be comparatively tiny, such as the JSD or KL divergence, the product reduces variance too extreme (should have used log-sums), such that the score forfeits too much of its importance.


```{r echo=FALSE}
temp <- imp$importance[order(-imp$importance$Overall), , drop=FALSE]

if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Relative variable importances of scores as computed from all projects against pattern type III (average, no align).",
    label = "p3-avg-no-varimp")
}
```

The final model as selected by the RFE approach has these properties:

```{r}
modelFit
```

Now if we were to make predictions with this model, the results would be these:

```{r}
temp <- predict(object = modelFit, newdata = p3_avg_no_scores)
round(x = temp, digits = 2)
ground_truth$consensus
```

The correlation with the ground truth would then be:

```{r}
cor.test(ground_truth$consensus, temp)
```


### Pattern as confidence surface

Exactly how it was done for issue-tracking data (refer for details to subsection \ref{sssec:inhomo-conf-interval}, as here we will only create and show the resulting pattern), we can produce a data-only pattern that features the weighted average for each variable, but also a confidence surface that takes the ground truth into account. Note that as the time of adding this section, we have access to a second batch of projects, including a ground truth. We will therefore include those here, as data-only patterns become better with each observation.

```{r}
ground_truth_2nd_batch <- read.csv(file = "../data/ground-truth_2nd-batch.csv", sep = ";")
ground_truth_2nd_batch$consensus_score <- ground_truth_2nd_batch$consensus / 10
rownames(ground_truth_2nd_batch) <- paste0((1+nrow(ground_truth)):(nrow(ground_truth)+nrow(ground_truth_2nd_batch)))

temp <- rbind(ground_truth, ground_truth_2nd_batch)
omega <- temp$consensus_score
names(omega) <- paste0("Project", 1:length(omega))
```

The weighted averaged signals are stored in `p3_avg_signals`, so we need to compute two things: first, we need functions to delineate the confidence surface ($\operatorname{CI}_{\text{upper}}(x)$ and $\operatorname{CI}_{\text{upper}}(x)$), and second, we need a function that produces the gradated surface within those bounds ($\operatorname{CI}(x,y)$). This is very similar to how it was done for issue-tracking, so most of the code is not shown here (check the source code for this notebook).

Let's load the new batch and do some required preprocessing:

```{r}
student_projects_2nd_batch <- read.csv(file = "../data/student-projects_2nd-batch.csv", sep = ";")
student_projects_2nd_batch$label <- as.factor(student_projects_2nd_batch$label)
student_projects_2nd_batch$project <- as.factor(student_projects_2nd_batch$project)
student_projects_2nd_batch$AuthorTimeNormalized <- NA_real_

for (pId in levels(student_projects_2nd_batch$project)) {
  student_projects_2nd_batch[student_projects_2nd_batch$project == pId, ]$AuthorTimeNormalized <-
    (student_projects_2nd_batch[student_projects_2nd_batch$project == pId, ]$AuthorTimeUnixEpochMilliSecs -
      min(student_projects_2nd_batch[student_projects_2nd_batch$project == pId, ]$AuthorTimeUnixEpochMilliSecs))
  student_projects_2nd_batch[student_projects_2nd_batch$project == pId, ]$AuthorTimeNormalized <-
    (student_projects_2nd_batch[student_projects_2nd_batch$project == pId, ]$AuthorTimeNormalized /
      max(student_projects_2nd_batch[student_projects_2nd_batch$project == pId, ]$AuthorTimeNormalized))
}
```

Below, we show the amount of commits in each project:

```{r}
table(student_projects_2nd_batch$project)
```


```{r echo=FALSE}
project_signals_2nd_batch <- list()

# passed to stats::density
use_kernel <- "gauss" # "rect"

for (pId in levels(student_projects_2nd_batch$project)) {
  temp <- student_projects_2nd_batch[student_projects_2nd_batch$project == pId, ]
  
  # We'll need these for the densities:
  acp_ratios <- table(temp$label) / sum(table(temp$label))
  
  dens_a <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp[temp$label == "a", ]$AuthorTimeNormalized,
    ratio = acp_ratios[["a"]], kernel = use_kernel)
  
  dens_cp <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp[temp$label == "c" | temp$label == "p", ]$AuthorTimeNormalized,
    ratio = acp_ratios[["c"]] + acp_ratios[["p"]], kernel = use_kernel)
  
  dens_freq <- densitySafe(
    from = 0, to = 1, safeVal = NA_real_,
    data = temp$AuthorTimeNormalized, ratio = 1, kernel = use_kernel)
  
  # All densities need to be scaled together once more, by dividing
  # for the maximum value of the FREQ-variable.
  ymax <- max(c(attr(dens_a, "ymax"), attr(dens_cp, "ymax"), attr(dens_freq, "ymax")))
  dens_a <- stats::approxfun(
    x = attr(dens_a, "x"), y = sapply(X = attr(dens_a, "x"), FUN = dens_a) / ymax)
  dens_cp <- stats::approxfun(
    x = attr(dens_cp, "x"), y = sapply(X = attr(dens_cp, "x"), FUN = dens_cp) / ymax)
  dens_freq <- stats::approxfun(
    x = attr(dens_freq, "x"), y = sapply(X = attr(dens_freq, "x"), FUN = dens_freq) / ymax)
  
  project_signals_2nd_batch[[pId]] <- list(
    A = Signal$new(name = paste(pId, "A", sep = "_"),
                   func = dens_a, support = c(0, 1), isWp = FALSE),
    CP = Signal$new(name = paste(pId, "CP", sep = "_"),
                    func = dens_cp, support = c(0, 1), isWp = FALSE),
    FREQ = Signal$new(name = paste(pId, "FREQ", sep = "_"),
                      func = dens_freq, support = c(0, 1), isWp = FALSE)
  )
}


# Now, for each project, we estimate the variable for the source code density as follows:
for (pId in levels(student_projects_2nd_batch$project)) {
  temp <- data.frame(
    x = student_projects_2nd_batch[student_projects_2nd_batch$project == pId, ]$AuthorTimeNormalized,
    y = student_projects_2nd_batch[student_projects_2nd_batch$project == pId, ]$Density)
  temp <- temp[with(temp, order(x)), ]
  
  # Using a polynomial with maximum possible degree, we smooth the
  # SCD-data, as it can be quite "peaky"
  temp_poly <- poly_autofit_max(x = temp$x, y = temp$y, startDeg = 13)
  
  dens_scd <- Vectorize((function() {
    rx <- range(temp$x)
    ry <- range(temp$y)
    poly_y <- stats::predict(temp_poly, x = temp$x)
    tempf <- stats::approxfun(x = temp$x, y = poly_y, ties = "ordered")
    function(x) {
      if (x < rx[1] || x > rx[2]) {
        return(NA_real_)
      }
      max(ry[1], min(ry[2], tempf(x)))
    }
  })())
  
  project_signals_2nd_batch[[pId]][["SCD"]] <- Signal$new(
    name = paste(pId, "SCD", sep = "_"), func = dens_scd,
    support = c(0, 1), isWp = FALSE)
}
```

The second batch of projects and their variables are shown in figure \ref{fig:project-vars-2nd-batch}.

```{r echo=FALSE}
tempdf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(tempdf) <- c("x", "y", "p", "v")

n <- 500
x <- seq(from = 0, to = 1, length.out = n)
for (pId in levels(student_projects_2nd_batch$project)) {
  for (v in c("A", "CP", "FREQ", "SCD")) {
    tempdf <- rbind(tempdf, data.frame(
      x = x,
      y = sapply(X = x, FUN = project_signals_2nd_batch[[pId]][[v]]$get0Function()),
      p = pId,
      v = v
    ))
  }
}
```


```{r project-vars-2nd-batch, echo=FALSE, fig.cap="All variables over each project's time span (second batch of projects).", fig.align="top", fig.pos="ht!"}
plot_all_acp <- ggplot(data = tempdf, aes(x = x, y = y, color = v)) +
  geom_line() +
  facet_wrap(p ~.) +
  theme_light() +
  labs(color = "Variable") + xlab("Relative Time") + ylab("Value") +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"))
plot_all_acp
```

We will need to produce the weighted average for each variable, as we have done it previously for creating pattern type III. It needs to be redone, because we want to include the new projects.

```{r}
p3_avg_signals_all <- list()

for (vartype in names(weight_vartype)) {
  p3_avg_signals_all[[vartype]] <- Signal$new(
    name = paste0("p3_avg_", vartype), support = c(0, 1), isWp = TRUE,
    func = gt_weighted_avg(
      vartype = vartype,
      use_signals = append(project_signals, project_signals_2nd_batch),
      use_ground_truth = rbind(ground_truth, ground_truth_2nd_batch)))
}
```

Next, we'll produce the lower and upper bounds vor each variable:

```{r echo=FALSE}
funclist_A <- list()
funclist_CP <- list()
funclist_FREQ <- list()
funclist_SCD <- list()

for (pId in names(project_signals)) {
  # transform "project_6" to "Project6"
  pId_fl <- paste0("Project", regmatches(x = pId, gregexpr(pattern = "\\d+", text = pId))[[1]])
  funclist_A[[pId_fl]] <- project_signals[[pId]]$A$get0Function()
  funclist_CP[[pId_fl]] <- project_signals[[pId]]$CP$get0Function()
  funclist_FREQ[[pId_fl]] <- project_signals[[pId]]$FREQ$get0Function()
  funclist_SCD[[pId_fl]] <- project_signals[[pId]]$SCD$get0Function()
}

for (pId in names(project_signals_2nd_batch)) {
  # transform "project_6" to "Project6"
  pId_fl <- paste0("Project", regmatches(x = pId, gregexpr(pattern = "\\d+", text = pId))[[1]])
  funclist_A[[pId_fl]] <- project_signals_2nd_batch[[pId]]$A$get0Function()
  funclist_CP[[pId_fl]] <- project_signals_2nd_batch[[pId]]$CP$get0Function()
  funclist_FREQ[[pId_fl]] <- project_signals_2nd_batch[[pId]]$FREQ$get0Function()
  funclist_SCD[[pId_fl]] <- project_signals_2nd_batch[[pId]]$SCD$get0Function()
}

CI_bound_p3avg <- function(x, funclist, omega, upper = TRUE) {
  sapply(X = x, FUN = function(x_) {
  val <- unlist(lapply(X = names(funclist), FUN = function(fname) {
    if (omega[fname] == 0)
      (if (upper)
        -Inf else Inf) else funclist[[fname]](x_)
  }))
  
  if (upper)
    max(val) else min(val)
  })
}
```


```{r}
a_ci_upper_p3avg <- function(x) CI_bound_p3avg(
  x = x, funclist = funclist_A, omega = omega, upper = TRUE)
a_ci_lower_p3avg <- function(x) CI_bound_p3avg(
  x = x, funclist = funclist_A, omega = omega, upper = FALSE)

cp_ci_upper_p3avg <- function(x) CI_bound_p3avg(
  x = x, funclist = funclist_CP, omega = omega, upper = TRUE)
cp_ci_lower_p3avg <- function(x) CI_bound_p3avg(
  x = x, funclist = funclist_CP, omega = omega, upper = FALSE)

freq_ci_upper_p3avg <- function(x) CI_bound_p3avg(
  x = x, funclist = funclist_FREQ, omega = omega, upper = TRUE)
freq_ci_lower_p3avg <- function(x) CI_bound_p3avg(
  x = x, funclist = funclist_FREQ, omega = omega, upper = FALSE)

scd_ci_upper_p3avg <- function(x) CI_bound_p3avg(
  x = x, funclist = funclist_SCD, omega = omega, upper = TRUE)
scd_ci_lower_p3avg <- function(x) CI_bound_p3avg(
  x = x, funclist = funclist_SCD, omega = omega, upper = FALSE)
```


```{r echo=FALSE}
h_p3avg <- function(funclist, x, y, upper = TRUE, f_ci) {
  unlist(lapply(X = funclist, FUN = function(f) {
    sapply(X = f(x), function(val) {
      if (upper && val >= y) {
        1
        #f_ci(x) - val <-- this could be an alternative using the boundaries
      } else if (!upper && val <= y) {
        1
        #val - f_ci(x)
      } else {
        0
      }
    })
  }))
}

h_upper_p3avg <- function(funclist, x, y, f_ci) h_p3avg(funclist = funclist, x = x, y = y, upper = TRUE, f_ci = f_ci)
h_lower_p3avg <- function(funclist, x, y, f_ci) h_p3avg(funclist = funclist, x = x, y = y, upper = FALSE, f_ci = f_ci)

CI_p3avg <- function(x, y, funclist, f_ci_upper, f_ci_lower, gbar, omega) {
  stopifnot(length(x) == length(y))
  
  sapply(X = seq_len(length.out = length(x)), FUN = function(idx) {
    xi <- x[idx]
    yi <- y[idx]
    
    if (yi > f_ci_upper(xi) || yi < f_ci_lower(xi)) {
      return(0)
    }
    
    gbarval <- gbar(xi)
    hval <- if (gbarval < yi) {
      h_upper_p3avg(funclist = funclist, x = xi, y = yi, f_ci = f_ci_upper)
    } else {
      h_lower_p3avg(funclist = funclist, x = xi, y = yi, f_ci = f_ci_lower)
    }
    
    omega %*% hval / sum(omega)
  })
}
```


```{r}
CI_a_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_A, f_ci_upper = a_ci_upper_p3avg, f_ci_lower = a_ci_lower_p3avg, gbar = p3_avg_signals_all$A$get0Function(), omega = omega)
CI_cp_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_CP, f_ci_upper = cp_ci_upper_p3avg, f_ci_lower = cp_ci_lower_p3avg, gbar = p3_avg_signals_all$CP$get0Function(), omega = omega)
CI_freq_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_FREQ, f_ci_upper = freq_ci_upper_p3avg, f_ci_lower = freq_ci_lower_p3avg, gbar = p3_avg_signals_all$FREQ$get0Function(), omega = omega)
CI_scd_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_SCD, f_ci_upper = scd_ci_upper_p3avg, f_ci_lower = scd_ci_lower_p3avg, gbar = p3_avg_signals_all$SCD$get0Function(), omega = omega)

saveRDS(object = list(
  CI_a_p3avg = CI_a_p3avg,
  CI_cp_p3avg = CI_cp_p3avg,
  CI_freq_p3avg = CI_freq_p3avg,
  CI_scd_p3avg = CI_scd_p3avg
), file = "../data/CI_p3avg_funcs_SC.rds")
```

```{r echo=FALSE}
x <- seq(0, 1, length.out = 200)
y <- seq(0, 1, length.out = 200)

compute_z_p3avg <- function(varname, x, y, interp = NA_real_) {
  # We cannot call outer because our functions are not properly vectorized.
  #z <- outer(X = x, Y = y, FUN = CI_req_p3avg)
  f <- if (varname == "A") {
    CI_a_p3avg
  } else if (varname == "CP") {
    CI_cp_p3avg
  } else if (varname == "FREQ") {
    CI_freq_p3avg
  } else {
    CI_scd_p3avg
  }
  
  z <- matrix(nrow = length(x), ncol = length(y))
  for (i in 1:length(x)) {
    for (j in 1:length(y)) {
      z[i, j] <- f(x = x[i], y = y[j])
    }
  }
  
  res <- list(x = x, y = y, z = z)
  
  if (!is.na(interp)) {
    res <- fields::interp.surface.grid(obj = res, grid.list = list(
      x = seq(from = min(x), to = max(x), length.out = interp),
      y = seq(from = min(y), to = max(y), length.out = interp)))
  }
  
  res
}
```


```{r}
z_a <- loadResultsOrCompute(file = "../results/ci_p3avg_z_a.rds", computeExpr = {
  compute_z_p3avg(varname = "A", x = x, y = y)
})
z_cp <- loadResultsOrCompute(file = "../results/ci_p3avg_z_cp.rds", computeExpr = {
  compute_z_p3avg(varname = "CP", x = x, y = y)
})
z_freq <- loadResultsOrCompute(file = "../results/ci_p3avg_z_freq.rds", computeExpr = {
  compute_z_p3avg(varname = "FREQ", x = x, y = y)
})
z_scd <- loadResultsOrCompute(file = "../results/ci_p3avg_z_scd.rds", computeExpr = {
  compute_z_p3avg(varname = "SCD", x = x, y = y)
})
```

In figure \ref{fig:p3-emp-cis-sc} we finally show the empirical confidence surfaces for all four variables, as computed over the first two batches of projects.

```{r p3-emp-cis-sc, echo=FALSE, fig.height=7, fig.cap="The empirical confidence intervals for the four variables as mined from the source code of all projects. Higher saturation of the color correlates with higher confidence. Projects with zero weight contribute to the CIs' boundaries, but not to the hyperplane.", fig.align="center", fig.pos="ht!"}
# Those are the colors from ggplot2
# cols <- scales::hue_pal()(4)
par(mfrow = c(2, 2),     # 2x2 layout
    oma = c(0, 0, 0, 0), # two rows of text at the outer left and bottom margin
    mar = c(4, 3, 3, 1), # space for one row of text at ticks and to separate plots
    mgp = c(2, 1, 0),    # axis label at 2 rows distance, tick labels at 1 row
    xpd = NA)  


## A:
image(x = z_a$x, y = z_a$y, z = z_a$z, main = "Variable: A",
      xlab = "Relative time", ylab = "Relative amount of activity",
      col = colorRampPalette(colors = c("#ffffff", "#f8766d"))(100), zlim = c(0,1))
grid()

curve2(p3_avg_signals_all$A$get0Function(), 0, 1, col = "red", add = TRUE)
curve2(a_ci_upper_p3avg, 0, 1, col = "#f8766d33", add = TRUE, lty = 2)
curve2(a_ci_lower_p3avg, 0, 1, col = "#f8766d33", add = TRUE, lty = 3)

temp <- append(project_signals, project_signals_2nd_batch)
for (i in 1:length(temp)) {
  pId <- names(temp)[i]
  tempf <- temp[[pId]]$A$get0Function()
  curve2(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

legend(0.01, .98, legend = c("Weight = 0", "Weight > 0"),
       col = "#00000033", lty = c(3, 2), lwd = 2)


## CP:
image(x = z_cp$x, y = z_cp$y, z = z_cp$z, main = "Variable: CP",
      xlab = "Relative time", ylab = "Relative amount of activity",
      col = colorRampPalette(colors = c("#ffffff", "#7cae00"))(100), zlim = c(0,1))
grid()

curve2(p3_avg_signals_all$CP$get0Function(), 0, 1, col = "darkgreen", add = TRUE)
curve2(cp_ci_upper_p3avg, 0, 1, col = "#7cae0033", add = TRUE, lty = 2)
curve2(cp_ci_lower_p3avg, 0, 1, col = "#7cae0033", add = TRUE, lty = 3)

temp <- append(project_signals, project_signals_2nd_batch)
for (i in 1:length(temp)) {
  pId <- names(temp)[i]
  tempf <- temp[[pId]]$CP$get0Function()
  curve2(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

legend(0.01, .98, legend = c("Weight = 0", "Weight > 0"),
       col = "#00000033", lty = c(3, 2), lwd = 2)


## FREQ:
image(x = z_freq$x, y = z_freq$y, z = z_freq$z, main = "Variable: FREQ",
      xlab = "Relative time", ylab = "Relative amount of activity",
      col = colorRampPalette(colors = c("#ffffff", "#00BFC4"))(100), zlim = c(0,1))
grid()

curve2(p3_avg_signals_all$FREQ$get0Function(), 0, 1, col = "blue", add = TRUE)
curve2(freq_ci_upper_p3avg, 0, 1, col = "#00BFC433", add = TRUE, lty = 2)
curve2(freq_ci_lower_p3avg, 0, 1, col = "#00BFC433", add = TRUE, lty = 3)

temp <- append(project_signals, project_signals_2nd_batch)
for (i in 1:length(temp)) {
  pId <- names(temp)[i]
  tempf <- temp[[pId]]$FREQ$get0Function()
  curve2(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

legend(0.01, .98, legend = c("Weight = 0", "Weight > 0"),
       col = "#00000033", lty = c(3, 2), lwd = 2)


## SCD:
image(x = z_scd$x, y = z_scd$y, z = z_scd$z, main = "Variable: SCD",
      xlab = "Relative time", ylab = "Source code density",
      col = colorRampPalette(colors = c("#ffffff", "#C77CFF"))(100), zlim = c(0,1))
grid()

curve2(p3_avg_signals_all$SCD$get0Function(), 0, 1, col = "purple", add = TRUE)
curve2(scd_ci_upper_p3avg, 0, 1, col = "#C77CFF33", add = TRUE, lty = 2)
curve2(scd_ci_lower_p3avg, 0, 1, col = "#C77CFF33", add = TRUE, lty = 3)

temp <- append(project_signals, project_signals_2nd_batch)
for (i in 1:length(temp)) {
  pId <- names(temp)[i]
  tempf <- temp[[pId]]$SCD$get0Function()
  curve2(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

legend(0.325, .22, legend = c("Weight = 0", "Weight > 0"),
       col = "#00000033", lty = c(3, 2), lwd = 2)
```



## Pattern III (b)\label{ssec:score-pattern3}

The third and last pattern is based on the ground truth only. Starting with straight lines and equally long intervals, a pattern was generated and selected using a best trade-off between number of parameters and highest _likelihood_, or by the lowest loss. Like pattern II, all of these patterns were produced with time warping applied, so that we do not need to align the projects to it. We have produced 16 patterns, so let's compute the scores for all of them.


```{r p3b-no-scores}
p3b_no_scores <- loadResultsOrCompute(file = "../results/p3b_no_scores.rds", computeExpr = {
  unlist(doWithParallelCluster(numCores = min(8, parallel::detectCores()), expr = {
    foreach::foreach(
      numIntervals = seq_len(length.out = 16),
      .inorder = FALSE
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
      
      temp_no_align <- list()
      
      for (project in ground_truth$project) {
        inst <- srBTAW$new(
          theta_b = seq(0, 1, length.out = 2),
          gamma_bed = c(0, 1, .Machine$double.eps),
          lambda = .Machine$double.eps,
          begin = 0, end = 1, openBegin = FALSE, openEnd = FALSE,
          useAmplitudeWarping = FALSE)
        
        for (vartype in names(weight_vartype)) {
          # Set WP and WC:
          wp <- p3b_all[[paste0("i_", numIntervals)]]$signals[[vartype]]
          wc <- project_signals[[project]][[vartype]]
            
          inst$setSignal(signal = wp)
          inst$setSignal(signal = wc)
          
          srbtwbaw <- inst$.__enclos_env__$private$createInstance(
            wp = wp$get0Function(), wc = wc$get0Function())
          inst$.__enclos_env__$private$addInstance(
            instance = srbtwbaw, wpName = wp$getName(), wcName = wc$getName())
        }
        
        inst$setParams(params = c(vtl_1 = 1))
        temp_no_align[[project]] <- inst
      }
      
      res <- `names<-`(list(tryCatch(expr = {
        # compute_all_scores is already parallel!
        temp <-as.data.frame(compute_all_scores(alignment = temp_no_align, patternName = "p3"))
        saveRDS(object = temp, file = paste0("../results/p3-compute/p3b_no_", numIntervals, ".rds"))
        temp
      }, error = function(cond) {
        list(cond = cond, tb = traceback())
      })), paste0("i_", numIntervals))
    }
  }), recursive = FALSE)
})
```

We will have a lot of results. In order to give an overview, we will show the correlation of the ground truth with the scores as obtained by scoring against each of the 16 patterns. The correlation plot in figure \ref{fig:p3b-no-scores-corr} should give us a good idea of how the scores changes with increasing number of parameters.

```{r p3b-no-scores-corr, echo=FALSE, fig.cap="Correlation-scores for all 16 patterns (type III, b).", fig.align="top", fig.pos="ht!"}
# dummy:
p3b_corr <- NULL

for (idx in seq_len(length.out = length(p3b_no_scores))) {
  temp <- p3b_no_scores[[paste0("i_", idx)]]
  if (!is.data.frame(temp)) {
    next
  }
  p3b_corr <- rbind(
    p3b_corr, `rownames<-`(stats::cor(ground_truth$consensus, temp), paste0("P III (b) [numInt=", idx, "]")))
}

corrplot::corrplot(corr = p3b_corr)
```

If `RMS` was to score to use, then the even the $1$-interval pattern will do, as the correlation for it is always strongly positive. In general, we can observe how some correlations get weaker, and some get stronger for patterns with higher number of parameters. There is no clear winner here, and the results are quite similar. What can say clearly is, that it is likely not worth to use highly parameterized models to detect the Fire Drill as it was present in our projects, as the manifestation is just not strong enough to warrant for patterns with high degrees of freedom. It is probably best, to use one of the other pattern types.

Let's also show an overview of the correlation with the ground truth for all of the other patterns:

```{r pall-corr, echo=FALSE, fig.cap="Overview of correlation-scores for all other types of patterns.", fig.align="top", fig.pos="ht!"}
pAll_corr <- rbind(
  `rownames<-`(stats::cor(ground_truth$consensus, p1_scores), "Pattern I"),
  `rownames<-`(stats::cor(ground_truth$consensus, p2_scores), "Pattern II"),
  `rownames<-`(stats::cor(ground_truth$consensus, p2_no_scores), "Pattern II (no align)"),
  `rownames<-`(stats::cor(ground_truth$consensus, p3_avg_scores), "Pattern III (avg)"),
  `rownames<-`(stats::cor(ground_truth$consensus, p3_avg_no_scores), "Pattern III (avg, no align)"),
  `rownames<-`(p3b_corr["P III (b) [numInt=4]",, drop = FALSE], "Pattern III (b, numInt=4)"),
  `rownames<-`(p3b_corr["P III (b) [numInt=11]",, drop = FALSE], "Pattern III (b, numInt=11)")
)

corrplot::corrplot(corr = pAll_corr)
```

The correlation overview in figure \ref{fig:pall-corr} suggests that the no-alignment patterns have most of the strong positive-correlated scores. However, it appears that it was still worth adapting our initial pattern using the ground truth, as it is the only pattern with very high positive correlations for Peak and Impulse-factor. These however disappear, if we do not do the double warping.



### Linear combination of scores

The pattern that incurred the lowest loss was number __`r p3_params[which.min(p3_params$loss), ]$numInt`__. We should however also test the pattern that had the highest correlations (positive _or_ negative) on average:

```{r}
p3b_corr_all <- apply(X = p3b_corr, MARGIN = 1,
                      FUN = function(row) mean(abs(row)))
```

```{r echo=FALSE}
temp <- `rownames<-`(data.frame(corr = p3b_corr_all), names(p3b_corr_all))

if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Mean absolute correlation for all patterns of type III (b).",
    label = "p3b-corr-all"
  )
}
```

In table \ref{tab:p3b-corr-all} we show the mean absolute correlation for the scores of all projects as computed against each pattern of type III (b).

```{r}
p3b_highest_corr <- names(which.max(p3b_corr_all))
p3b_highest_corr
```

The pattern using __`r gsub("[^0-9]", "", p3b_highest_corr)`__ has the highest correlation.

Taking the best pattern of type III (b), we can also do a linear combination again. "Best" may refer to the pattern with the highest correlations over all scores or the one with the lowest loss when fitting to all projects.

```{r}
# Take the project with the highest mean absolute correlation.
temp <- cbind(
  data.frame(gt_consensus = ground_truth$consensus_score),
  p3b_no_scores[[paste0("i_", gsub("[^0-9]", "", p3b_highest_corr))]])
p3b_lm <- stats::lm(
  formula = gt_consensus ~ area_p + corr_p + jsd_m + sd_p + rmse_p + RMS_p,
  data = temp)
stats::coef(p3b_lm)
plot(p3b_lm, ask = FALSE, which = 1:2)
```

```{r}
p3b_lm_scores <- stats::predict(p3b_lm, temp)
round(p3b_lm_scores * 10, 3)
```

```{r}
stats::cor(p3b_lm_scores, ground_truth$consensus_score)
```

With this linear model, we can report a high correlation with the consensus, too.



# Automatic calibration of the continuous process models\label{sec:auto-calib}

This is a new section in the seventh iteration of this report.
"Automatic calibration" refers to a new approach of gauging a continuous PM, so that it can compute true scores for any process, also scores that are __comparable__ across PMs, given the same configuration (segments, objectives, regularizers) was used (we say that PMs using the same configuration are part of a _family_).
This approach is based on the idea of __rectification of scores__[^3]. The goal is to learn for every objective a __uniform__ score, where uniform refers to both, a uniform distribution _and_ a codomain of $[0,1]$. With such scores, a normalizing linear scalarizer will produce a score with the __same__ properties, and that means we will be able to compare the continuous process models we formulated so far!
Even beyond that, we can use this score similar to a probabilistic measure of how well a process matches the PM, so in our case we can translate this information into the degree to which a Fire Drill is present in a project, according to the process model in question. We will also be able to compute the Brier- and Logarithmic scoring rules, and compare against the results that we got using the binary decision rule.

[^3]: https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/rectify-score.md

The plan for this new section is to demonstrate the following:

1. Computation of available metrics/scores for all types of PMs (I, II, III (average), and III (b, numInt=11)).
2. Computation for each variable (e.g., FREQ), using a large number of random processes ($\approx$ 1k--100k). Also, computing for each segment (we will be subdividing each PM into $10$ equally long segments).
3. Scalarization of the scores, with and without weights (we will be able to choose a weight for each segment or score).
4. Perhaps make an attempt at using only the most important scores, or some automatic approach (e.g., RFE) to find the most important scores.
5. Computing the Brier- and Log-scoring rules so that we can compare to the binary decision rule's results.


The reason for using segments is simple. First, the longer a segment is, the less explainability a score/metric has, as it captures too much of the characteristics. Second, we will be able to introduce weights for segments. Since it is difficult to choose weights a priori, we will only use two weighting schemes. In the first one, all weights are equal (calculate mean). In the second we will use a scheme to gradually (e.g., linearly or slightly exponentially) increase the weights towards the end of the project. This is simply based on the assumption that newer data is more important.

Example: In source code data, we have four variables. Using all $13$ scores and ten segments requires to compute $520$ single scores, __per__ random process. Times four (for each kind of PM) equals $2080$ computations. If we simulate $10$k random processes, we're looking at $20.8$m computations. This needs to be scaled reasonably so we get sufficiently well approximated results quickly.


## Prerequisites

Before we can start, we should prepare this expensive task well. First is a function that takes a PM+P, a segment specification, and the name of the score to compute.

```{r}
segment_unit_transform <- function(func, from, to) {
  ext <- to - from
  
  function(x) {
    # func has a support of [0, 1], and the returned function
    # has a support of [from, to].
    func(from + (x * ext))
  }
}

compute_segment_metric <- function(
  PM, P, segment, numSamples = 1e3,
  use = c("area", "corr", "jsd", "kl", "arclen", "sd", "var", "mae",
          "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")) {
  
  # We'll have to make this transform as all the scores we compute
  # assume a support of [0,1] and compute over that.
  f1 <- segment_unit_transform(PM, from = segment[1], to = segment[2])
  f2 <- segment_unit_transform(P, from = segment[1], to = segment[2])
  
  # Some metrics will require scaling after the segment-unit-transform
  ext <- segment[2] - segment[1]
  
  diff <- NA_real_
  if (use == "area") {
    # This is integration and susceptible to scaling!
    diff <- ext * area_diff_2_functions(f1 = f1, f2 = f2)$value
  } else if (use == "corr") {
    # Returns a correlation in range [-1,1], but we want a difference,
    diff <- stat_diff_2_functions_cor_score(
      allowReturnNA = TRUE, requiredSign = 0, numSamples = numSamples)(f1 = f1, f2 = f2)
    # so we'll subtract it from 1 -> [0,2]
    diff <- 1 - (if (is.na(diff)) 0 else diff) # NA means no correlation +/-
  } else if (use == "jsd") {
    diff <- stat_diff_2_functions_symmetric_JSD_sampled(f1 = f1, f2 = f2, numSamples = numSamples)$value
  } else if (use == "kl") {
    diff <- stat_diff_2_functions_symmetric_KL_sampled(f1 = f1, f2 = f2, numSamples = numSamples)$value
  } else if (use == "arclen") {
    # Returns a ration min/max -> (0,1], where 1 is ideal.
    # We should inverse that because we are returning a difference.
    diff <- 1 - stat_diff_2_functions_arclen_score(requiredSign = 0, numSamples = numSamples)(f1 = f1, f2 = f2)
  } else if (use %in% c("sd", "var", "mae", "rmse")) {
    temp <- switch (use,
      "sd"   = stat_diff_2_functions_sd(f1 = f1, f2 = f2, numSamples = numSamples)$value,
      "var"  = stat_diff_2_functions_var(f1 = f1, f2 = f2, numSamples = numSamples)$value,
      "mae"  = ext * stat_diff_2_functions_mae(f1 = f1, f2 = f2, numSamples = numSamples)$value,
      "rmse" = ext * stat_diff_2_functions_rmse(f1 = f1, f2 = f2, numSamples = numSamples)$value,
      {
        stop(paste0("Don't know ", use, "."))
      })
    diff <- temp
  } else if (use %in% c("RMS", "Kurtosis", "Peak", "ImpulseFactor")) {
    # Returns a ratio min/max -> (0,1], where 1 is ideal.
    diff <- 1 - stat_diff_2_functions_signals_score(use = use, requiredSign = 0, numSamples = numSamples)(f1 = f1, f2 = f2)
  }
  
  diff
}
```


We have previously used a function to get a smooth random process. We will be slightly adapting this function here to also include the process' derivative. We will not be working with derivative PMs in this notebook, but the very next when we use issue-tracking data.

It is important to note that we do not have any expectation as to how such a random process will look like, except for the fact that we expect it to run within the bounds of the process model, that is, we allow any random process to "behave" arbitrarily within the bounds of $[0,1]$.
In reality, however, we probably would have some expectation (or at least stricter limitations or some confidence intervals) as to how some process may look/behave.
In that case, it is perhaps better to construct a _generative model_ from these expectations, and then to uniformly sample from its quantile function (PPF) in order to generate random processes that match our expectations and are within some limitations.
However, that would require that we have such expectations. Those could come from another set of observations (not the observed projects), or we could perhaps craft some expert-designed expectations by reasoning about potential limits and confidence intervals, or by simply designing a set of plausible processes.
However here, we took the road where we do not have any of these, and thus allow any process to behave absolutely random within the bounds of our PMs. In other words, our expectation is that we do not have an expectation, and hence we randomly sample uniformly from all possibilities.


```{r}
get_smoothed_curve <- function(seed = NA, npoints = 15, include_deriv = FALSE) {
  if (!is.na(seed)) {
    set.seed(seed = seed)
  }
  
  x <- sort(c(0, 1, runif(npoints - 2)))
  y <- runif(length(x))
  temp <- loess.smooth(x = x, y = y, span = 0.35, family = "g", evaluation = 1000)
  appr <- stats::approxfun(
    x = ((temp$x - min(temp$x)) / (max(temp$x) - min(temp$x))),
    y = temp$y / (max(temp$x) - min(temp$x)),
    yleft = utils::head(temp$y, 1),
    yright = utils::tail(temp$y, 1))
  
  tempf <- Vectorize(function(x) {
    # Limit the resulting function to the bounding box of [0,1]
    min(1, max(0, appr(x)))
  })
  
  f1 <- NULL
  if (include_deriv) {
    tempf1 <- Vectorize(function(x) {
      ltol <- x < sqrt(.Machine$double.eps)
      rtol <- x > 1 - sqrt(.Machine$double.eps)
      pracma::fderiv(f = tempf, x = x, method = if (ltol) "forward" else if (rtol) "backward" else "central")
    })
    x <- seq(from = 0, to = 1, length.out = 2e3)
    y <- tempf1(x)
    
    f1 = stats::approxfun(
      x = x, y = y, yleft = utils::head(y, 1), yright = utils::tail(y, 1))
  }
  
  list(f0 = tempf, f1 = f1)
}
```



```{r echo=FALSE}
# We make a caching version of getting a smoothed random process, as obtaining
# a precise numeric derivative is expensive, and we do not want to repeat this.
get_smoothed_curve_buffered <- function(seed = NA, npoints = 15, include_deriv = FALSE) {
  if (!include_deriv) {
    return(get_smoothed_curve(seed = seed, npoints = npoints, include_deriv = include_deriv))
  }
  lock_file <- paste0(dirname(tempdir()), "/gsc", (seed %% 10), ".lock")
  lock <- filelock::lock(path = lock_file, exclusive = TRUE)
  
  tryCatch(expr = {
    gsc_file <- paste0(dirname(tempdir()), "/gsc_", seed, "_", npoints, "_", include_deriv, ".rds")
    
    if (file.exists(gsc_file)) {
      readRDS(file = gsc_file)
    } else {
      sc <- get_smoothed_curve(seed = seed, npoints = npoints, include_deriv = include_deriv)
      saveRDS(object = sc, file = gsc_file)
      sc
    }
  }, finally = {
    filelock::unlock(lock = lock)
  })
}
```


## Computing the metrics

We have a handful of dimensions, and the goal is to obtain a data frame with the following columns:

* Process model (ID or name)
* Variable name
* Seed of random process
* Segment index
* Metric 1, ..., Metric 13 (one column per metric)

Each worker shall compute all metrics for one specific segment. It will therefore be given the PM's variable, the seed/random process, and the segment's index. This way, we can chunk each worker's workload into comfortably sized items.

### Worker

We will be making a grid of all combinations, and the worker will compute the results of that grid. Then finally, we will just be concatenating the grid with the results horizontally and store the result.

```{r}
ac_worker <- function(PM, P, seg_idx, total_segments = 10) {
  use <- c("area", "corr", "jsd", "kl", "arclen", "sd", "var", "mae",
           "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")
  df <- `colnames<-`(matrix(nrow = 1, ncol = length(use)), use)
  
  seg_ext <- 1 / total_segments
  
  for (metric in use) {
    df[1, metric] <- compute_segment_metric(PM = PM, P = P, use = metric, segment = c(
      (seg_idx - 1) * seg_ext, seg_idx * seg_ext
    ))
  }
  df
}
```

### Grid for the random processes

Let's define the grid of parameters, which will eventually be concatenated with the results.

```{r}
ac_grid <- loadResultsOrCompute(file = "../results/ac_grid_sc.rds", computeExpr = {
  expand.grid(list(
    PM = c("I", "II", "III(avg)", "III(b,11)"),
    Var = names(weight_vartype),
    Seed = seq(from = 1, length.out = 1e4)))
})

nrow(ac_grid)
```

Let's compute the grid (attention: this is expensive).

```{r ac-grid-results}
ac_grid_results <- loadResultsOrCompute(file = "../results/ac_grid_results_sc.rds", computeExpr = {
  library(foreach)
  
  p3b11 <- p3b_all$i_11
  
  doWithParallelCluster(numCores = min(124, parallel::detectCores()), expr = {
    pb <- utils::txtProgressBar(min = 1, max = nrow(ac_grid), style = 3)
    progress <- function(n) {
      if (0 == (n %% 25)) {
        print(n)
      }
      utils::setTxtProgressBar(pb = pb, value = n)
    }
    
    foreach::foreach(
      grid_idx = rownames(ac_grid),
      .combine = rbind,
      .inorder = FALSE,
      .verbose = TRUE,
      .options.snow = list(progress = progress)
    ) %dopar% {
      options(warn=2)
      params <- ac_grid[grid_idx, ]
      params$PM <- as.character(params$PM)
      params$Var <- as.character(params$Var)
      
      pm_var <- switch (params$PM,
        "I" = p1_signals,
        "II" = p2_signals,
        "III(avg)" = p3_avg_signals,
        "III(b,11)" = p3b11$signals,
        {
          stop(paste0("Don't know ", params$PM, "."))
        }
      )[[params$Var]]$get0Function()
      
      p_var <- get_smoothed_curve(seed = params$Seed, include_deriv = FALSE)$f0
      
      df <- NULL
      for (seg_idx in 1:10) {
        # Important so we can correctly concatenate the results with the grid parameters!
        res <- as.data.frame(
          ac_worker(PM = pm_var, P = p_var, seg_idx = seg_idx, total_segments = 10))
        res$GridIdx <- grid_idx
        res$SegIdx <- seg_idx
        res <- cbind(res, params)
        df <- if (is.null(df)) res else rbind(df, res)
      }
      df
    }
  })
})
```



## Approximating marginal cumulative densities\label{ssec:ac-approx-marginal-ecdfs}

Now with the data at hand, the goal is to approximate the empirical cumulative distribution function (ECDF) for each objective, in each segment, for each variable, for each process model. For example, we will have one ECDF for the correlation of the A-variable in the first segment of the type-III (average) pattern.
All these "address" the specific ECDF. We will first write a helper function that can extract this.

```{r}
ac_extract_data <- function(pmName, varName, segIdx, metricName, ac_grid, ac_grid_results) {
  rows <- ac_grid[ac_grid$PM == pmName & ac_grid$Var == varName, ]
  ac_grid_results[ac_grid_results$SegIdx == segIdx & ac_grid_results$GridIdx %in% rownames(rows), metricName]
}
```

So let's plot a few randomly picked examples:

```{r ac-grid-example, echo=FALSE, fig.height=5, fig.cap="Four randomly picked ECCDFs as simulated using the random processes of the automatic calibration."}
par(mfrow = c(2,2), mgp = c(2.5, 1, 0))


set.seed(5)
temp.PM <- levels(ac_grid$PM)
temp.Var <- unique(ac_grid$Var)
temp.Seg <- 1:10
temp.Metric <- c("area", "corr", "jsd", "kl", "arclen", "sd", "var", "mae",
                 "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")

for (i in 1:4) {
  temp.pm <- sample(temp.PM, 1)
  temp.PM <- temp.PM[temp.PM != temp.pm]
  temp.var <- sample(temp.Var, 1)
  temp.Var <- temp.Var[temp.Var != temp.var]
  temp.seg <- sample(temp.Seg, 1)
  temp.Seg <- temp.Seg[temp.Seg != temp.seg]
  temp.metric <- sample(temp.Metric, 1)
  temp.Metric <- temp.Metric[temp.Metric != temp.metric]
  
  temp <- ac_extract_data(pmName = temp.pm, varName = temp.var, segIdx = temp.seg, metricName = temp.metric, ac_grid = ac_grid, ac_grid_results = ac_grid_results)
  tempf <- stats::ecdf(temp)
  curve2(function(x) 1 - tempf(x), min(temp), max(temp), ylim = c(0, 1), xlab = "", ylab = "Score", main = paste0(
    "PM=", temp.pm,
    ", Var=", temp.var
  ), sub = paste0(
    "SegIdx=", temp.seg,
    ", Metric=", temp.metric
  ))
}
```

So as we see in figure \ref{fig:ac-grid-example}, the ECCDFs can be quite diverse. Increasing distances are now non-linearly correlated with lower scores.
In almost any case, the ECDF is __non-linear__, which is exactly what we are after, as it will then be used to rectify the corresponding objective later.


## Calculating scores

In the previous sections we have simulated random continuous processes in order to approximate the marginal densities for each and every single objective.
Now we want to find out how each of our observed projects scores. So the first step is to compute these in the _same_ way as we did the random processes (we can reuse the function `ac_worker()`), and then the next step will be to _rectify_ each score using the previously approximated densities. Only after that, we can continue to evaluate the PMs and see which one is best, and where the strong and weak parts are.

In order to calculate each project's scores, we need to calculate one objective per:

* Project (15),
* Process model (4),
* Variable (4),
* Segment index (10), and
* Metric (13) [this is not part of the grid, the `ac_worker()` does this].

That will leave us with $15\times 4\times 4\times 10\times 13=31,200$ computations. It's probably best to generate a grid for that:

```{r}
ac_grid_projects <- expand.grid(list(
  Project = c(names(project_signals), names(project_signals_2nd_batch)),
  PM = c("I", "II", "III(avg)", "III(b,11)"),
  Var = names(weight_vartype),
  SegIdx = 1:10))

nrow(ac_grid_projects)
```

Now we can compute the projects' scores:

```{r ac-grid-projects-results}
ac_grid_projects_results <- loadResultsOrCompute(file = "../results/ac_grid_projects_results_sc.rds", computeExpr = {
  library(foreach)
  
  p3b11 <- p3b_all$i_11
  project_signals_all <- append(project_signals, project_signals_2nd_batch)
  
  doWithParallelCluster(numCores = 16, expr = {
    pb <- utils::txtProgressBar(min = 1, max = nrow(ac_grid_projects), style = 3)
    progress <- function(n) {
      if (0 == (n %% 25)) {
        print(n)
      }
      utils::setTxtProgressBar(pb = pb, value = n)
    }
    
    foreach::foreach(
      grid_idx = rownames(ac_grid_projects),
      .combine = rbind,
      .inorder = FALSE,
      .verbose = TRUE,
      .options.snow = list(progress = progress)
    ) %dopar% {
      options(warn=2)
      params <- ac_grid_projects[grid_idx, ]
      params$Project <- as.character(params$Project)
      params$PM <- as.character(params$PM)
      params$Var <- as.character(params$Var)
    
      pm_var <- switch (params$PM,
        "I" = p1_signals,
        "II" = p2_signals,
        "III(avg)" = p3_avg_signals,
        "III(b,11)" = p3b11$signals,
        {
          stop(paste0("Don't know ", params$PM, "."))
        }
      )[[params$Var]]$get0Function()
      
      p_var <- project_signals_all[[params$Project]][[params$Var]]$get0Function()
      
      # Important so we can correctly concatenate the results with the grid parameters!
      res <- as.data.frame(
        ac_worker(PM = pm_var, P = p_var, seg_idx = params$SegIdx, total_segments = 10))
      
      res$grid_idx <- grid_idx
      cbind(res, params)
    }
  })
})
```


### Rectification of raw scores

Now that we have the raw scores, it is time to transform them using the marginal densities. Each single objective (ECDF) is reused 15 times (once for each project).
We will make a new temporary grid, where the number of rows in this grid corresponds to the number of ECDFs:

```{r}
temp.Metric <- c("area", "corr", "jsd", "kl", "arclen", "sd", "var", "mae",
                 "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")
temp.grid <- expand.grid(list(
  PM = c("I", "II", "III(avg)", "III(b,11)"),
  Var = names(weight_vartype),
  SegIdx = 1:10,
  Metric = temp.Metric))

nrow(temp.grid)
```

Now it's time to rectify the raw scores.
Also, and this __is important__, we are reversing the the scores here into a notion where a score of $0$ is the worst possible score, and $1$ the best possible.
So far, we were using distances, and a lower distance was proportional with a low score, now we are inverting this.

```{r}
ac_grid_projects_results_uniform <- loadResultsOrCompute(file = "../results/ac_grid_projects_results_uniform_sc.rds", computeExpr = {
  doWithParallelCluster(numCores = min(32, parallel::detectCores()), expr = {
    library(foreach)
    
    project_signals_all <- append(project_signals, project_signals_2nd_batch)
    
    foreach::foreach(
      grid_idx = rownames(temp.grid),
      .combine = rbind,
      .inorder = FALSE,
      .verbose = TRUE
    ) %dopar% {
      options(warn = 2)
      params <- temp.grid[grid_idx, ]
      params$PM <- as.character(params$PM)
      params$Var <- as.character(params$Var)
      params$Metric <- as.character(params$Metric)
      
      temp.data <- ac_extract_data(
        pmName = params$PM, varName = params$Var, segIdx = params$SegIdx,
        metricName = params$Metric, ac_grid = ac_grid, ac_grid_results = ac_grid_results)
      tempf <- stats::ecdf(temp.data)
      
      res <- matrix(nrow = 1, ncol = length(project_signals_all))
      for (i in 1:length(project_signals_all)) {
        # Attention! It is only here that we also transform a score into
        # the notion of 1=best, 0=worst! Beware that the data for the ECDF
        # is the empirical distribution of distances. So for any x where
        # ECDF(x)=0 is the best possible score, so we need to subtract that
        # from 1 -- the more we subtract, the worse the score.
        res[1, i] <- 1 - tempf(ac_grid_projects_results[
          ac_grid_projects_results$Project == names(project_signals_all)[i] &
          ac_grid_projects_results$PM == params$PM &
          ac_grid_projects_results$Var == params$Var &
          ac_grid_projects_results$SegIdx == params$SegIdx, params$Metric])
      }
      cbind(`colnames<-`(res, names(project_signals_all)), params)
    }
  })
})
```


### Non-weighted

With the results from above, we can do many things, for example:

* Calculate a score between each pair of PM and P
* Compare scores between PMs (e.g., which model has the highest average score and thus resembles the projects best?)
  * Compare Brier-score of PMs to the decision rule.
* Fitting the normalized linear scalarizer:
  * Compare scores across (groups of) metrics, segments, and variables to find out what is important and where for each PM.
  * Prune overdetermined models (remove scores where the weight is (close to) zero in models that are specified using too many scores). This could perhaps also be understood as _regularization_ of an ill-posed problem, where _ill-posed_ refers to the fact of having too many coefficients in a regression model.
* Fit other kind of regression model to analyze what Brier-score or correlation is possible.
* Fit other kind of model that allows examining the variable importance (e.g., a Random forest).



Here, we will take a first look at which PM appears to be the best. Since all PMs are in the same family, we can now perfectly compare them after the automatic calibration.
_Same family_ refers to having done the automatic calibration using the same segments, variables, and objectives. For example, the objective for process model type II, variable CP, segment 4, correlation, is the same across all PMs. The PM that returns the highest average score (across all projects) would then be the best for that single objective.
We have $4*10*13=520$ objectives and the non-weighted approach is to simply add their scores together and divide by $520$.

We will need a function that takes PM/P and returns that score:

```{r}
ac_pmp_score <- function(pmName, projName, results_uniform, use_metrics) {
  temp <- results_uniform[
    results_uniform$PM == pmName &
    results_uniform$Metric %in% as.character(use_metrics), projName]
  
  mean(temp)
}
```


In order to compare PMs, we will find out the following: Which PM produces the highest/lowest score for the best/worst PM, and what is the Brier(MSE)-/Log-score and correlation for all projects. The result is shown in table \ref{tab:ac-compare-pms-sc}.


```{r}
temp <- append(project_signals, project_signals_2nd_batch)
temp.gt <- c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score)

temp.df <- NULL

for (pmName in c("I", "II", "III(avg)", "III(b,11)")) {
  temp.pred <- data.frame(
    pred = sapply(X = names(temp), function(pName) ac_pmp_score(
      use_metrics = temp.Metric,
      pmName = pmName, projName = pName, results_uniform = ac_grid_projects_results_uniform)),
    ground_truth = temp.gt)
  
  temp.df <- rbind(temp.df, data.frame(
    PM = pmName,
    Score_for_best = ac_pmp_score(
      use_metrics = temp.Metric,
      pmName = pmName, projName = paste0("project_", which.max(temp.gt)), results_uniform = ac_grid_projects_results_uniform),
    Score_for_worst = ac_pmp_score(
      use_metrics = temp.Metric,
      pmName = pmName, projName = paste0("project_", which.min(temp.gt)), results_uniform = ac_grid_projects_results_uniform),
    MSE = Metrics::mse(actual = temp.gt, predicted = temp.pred$pred),
    Log = mean(scoring::logscore(object = ground_truth ~ pred, data = temp.pred))
  ))
}
```

```{r echo=FALSE}
if (interactive()) {
  temp.df
} else {
  knitr::kable(
    x = temp.df,
    booktabs = TRUE,
    caption = "Comparison of continuous PMs using source code for best/worst projects, as well as MSE- and Log-scores as average deviation from the ground truth consensus.",
    label = "ac-compare-pms-sc"
  )
}
```

As we show in the technical report for issue-tracking, the lowest MSE for the binary decision rule is $\approx0.14$ across all 15 projects. All of the PMs using source code data are undercutting this threshold here (using all scores with equal weight).
Also, all of the PMs are slightly better than ZeroR across all projects, which was $\approx0.137$.
Other than that, it seems that PM type I appears best for detecting the presence of the Fire Drill, at least for the project with the strongest ground truth.
Process model type II appears best for detecting the absence of the anti-pattern. However, we have multiple projects with a ground truth of $0$, and ideally the champion PM would be picked by examining multiple projects where the confidence in the accuracy of the ground truth is high.
Nonetheless, all these scores are close together, so picking a champion needs to be done with care. Also, the score from each PM is the unweighted aggregation across hundreds of objectives, and at this point we have yet to examine which objectives are the most important (next two sections).
Since the score for the best project is consistently higher than the score for the worst project, it indicates that all process models capture the presence of the Fire Drill (and not its absence), which is a first good sign.


### Weighted by expert decision maker

The scores as of table \ref{tab:ac-compare-pms-sc} are close together, too close perhaps. This is due to the massive amount of objectives that were used. Also, all of the available objectives were used, across all segments and variables.
In reality however, an expert would almost certainly not configure a PM this way. Rather, they would choose specific objectives. We will simulate this scenario by choosing the metrics `corr`, `jsd`, and `ImpulseFactor`. As for the weights, they will be the same for each metric, and only vary between segments. The weights will simply increase exponentially towards the end of the project.
The expression for the weights is $\frac{1}{4}+\frac{3x^3}{4}$.

We have a large number of weights, so it is important to be able to "address" them correctly.
Therefore, we will use a grid as address register.
Let's define a weight grid:

```{r}
ac_weight_grid_expert <- expand.grid(list(
  Var = names(weight_vartype),
  Metric = c("area", "corr"),
  SegIdx = 1:10
))

ac_weight_vector_expert <- sapply(X = ac_weight_grid_expert$SegIdx, FUN = function(si) {
  0.25 + 3 * (si / 10)^3 / 4
})
```


Also, we will implement a variant of the function `ac_pmp_score()` that can handle weights:

```{r}
ac_pmp_score_weighted <- function(pmName, projName, weightGrid, weightVector, results_uniform) {
  if (length(weightVector) != nrow(weightGrid)) {
    stop("Address register grid does not match weight vector.")
  }
  
  res <- c()
  for (i in 1:length(weightVector)) {
    params <- weightGrid[i,]
    params$Var <- as.character(params$Var)
    params$Metric <- as.character(params$Metric)
    
    # The following selection will result in a single value:
    score <- results_uniform[
      results_uniform$PM == pmName &
      results_uniform$Var == params$Var &
      results_uniform$Metric == params$Metric &
      results_uniform$SegIdx == params$SegIdx, projName]
    
    if (length(score) != 1) {
      stop()
    }
    
    res <- c(res, weightVector[i] * score)
  }
  
  sum(res) / sum(weightVector)
}
```

The results are shown in table \ref{tab:ac-compare-pms-sc-expert}. Apparently, our expert did not make a good choice, compared to using all available objectives with equal weight.
Also, there is still no clear winner among the process models.


```{r echo=FALSE}
temp <- append(project_signals, project_signals_2nd_batch)
temp.gt <- c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score)

temp.df <- NULL

for (pmName in c("I", "II", "III(avg)", "III(b,11)")) {
  temp.pred <- data.frame(
    pred = sapply(X = names(temp), function(pName) ac_pmp_score_weighted(
      results_uniform = ac_grid_projects_results_uniform,
      pmName = pmName, projName = pName, weightGrid = ac_weight_grid_expert, weightVector = ac_weight_vector_expert)),
    ground_truth = temp.gt)
  
  temp.df <- rbind(temp.df, data.frame(
    PM = pmName,
    Score_for_best = ac_pmp_score_weighted(
      results_uniform = ac_grid_projects_results_uniform,
      pmName = pmName, projName = paste0("project_", which.max(temp.gt)),
      weightGrid = ac_weight_grid_expert, weightVector = ac_weight_vector_expert),
    Score_for_worst = ac_pmp_score_weighted(
      results_uniform = ac_grid_projects_results_uniform,
      pmName = pmName, projName = paste0("project_", which.min(temp.gt)),
      weightGrid = ac_weight_grid_expert, weightVector = ac_weight_vector_expert),
    MSE = Metrics::mse(actual = temp.gt, predicted = temp.pred$pred),
    Log = mean(scoring::logscore(object = ground_truth ~ pred, data = temp.pred))
  ))
}
```

```{r echo=FALSE}
if (interactive()) {
  temp.df
} else {
  knitr::kable(
    x = temp.df,
    booktabs = TRUE,
    caption = "Comparison of continuous PMs using source code for best/worst projects, as well as MSE- and Log-scores as average deviation from the ground truth consensus, using expert-picked objectives and weights.",
    label = "ac-compare-pms-sc-expert"
  )
}
```

It appears our hypothetical expert made a choice that is worse than using all available objectives with same weight, as the MSE and Log-score for all PMs worsened significantly.
The only thing that we still see is the consistency of score for best $>$ score for worst.


### Weighted by optimization

Here we will attempt to find the best weights (variable importances, actually) by optimization.
The interesting thing is, that the weights have a $1:1$ correlation to the principle of variable importance, since the objective is normalizing linear scalarizer, and and all objectives are scores with a linear co-domain of $[0,1]$.
A weight is specific to the variable (4), segment (10), and metric (13), so it is best to define an address grid again.

```{r}
ac_weight_grid <- expand.grid(list(
  Var = names(weight_vartype),
  Metric = temp.Metric,
  SegIdx = 1:10))

nrow(ac_weight_grid)
```


Now the objective for the optimization will search for weights that minimize the MSE between the weighted projects and the ground truth.
We will optimize the weights once for each process model, and then follow this up by a closer inspection of the champion model (the model with the lowest MSE).


```{r}
temp.names <- names(append(project_signals, project_signals_2nd_batch))

ac_sc_weights_optim <- function(pmName) {
  loadResultsOrCompute(file = paste0("../results/ac_sc_weights_pm_", pmName, ".rds"), computeExpr = {
    cl <- parallel::makePSOCKcluster(min(123, parallel::detectCores()))
    parallel::clusterExport(cl, varlist = list(
      "temp.gt", "temp.names", "ac_pmp_score_weighted",
      "ac_weight_grid", "ac_grid_projects_results_uniform"))

    doWithParallelClusterExplicit(cl = cl, expr = {
      optimParallel::optimParallel(
        par = rep(0.5, nrow(ac_weight_grid)),
        method = "L-BFGS-B",
        lower = rep(0, nrow(ac_weight_grid)),
        upper = rep(1, nrow(ac_weight_grid)),
        fn = function(x) {
          Metrics::mse(actual = temp.gt, predicted = sapply(X = temp.names, FUN = function(projName) {
            ac_pmp_score_weighted(
              results_uniform = ac_grid_projects_results_uniform,
              pmName = pmName, projName = projName, weightGrid = ac_weight_grid, weightVector = x)
          }))
        },
        parallel = list(cl = cl, forward = FALSE, loginfo = TRUE)
      )
    })
  })
}
```


#### Variable importance (traditional approach)

Assessing the variable importance can be done as we have done previously in section \ref{sssec:var-imp}.
So before we jump to the results of finding the weights (and thus the importance of each score), we attempt the traditional approach using a Random forest:

```{r}
ac_sc_weights_varimp <- loadResultsOrCompute(file = "../results/ac_sc_weights_varimp.rds", computeExpr = {
  temp.gt <- c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score)
  projNames <- names(append(project_signals, project_signals_2nd_batch))
  temp <- matrix(nrow = length(projNames), ncol = 1 + nrow(ac_weight_grid))
  temp[, ncol(temp)] <- temp.gt
  
  for (i in 1:length(projNames)) {
    for (j in 1:nrow(ac_weight_grid)) {
      params <- ac_weight_grid[j,]
      params$Var <- as.character(params$Var)
      params$Metric <- as.character(params$Metric)
      
      temp[i, j] <- ac_grid_projects_results_uniform[
        ac_grid_projects_results_uniform$PM == "I" &
        ac_grid_projects_results_uniform$Project == projNames[i] &
        ac_grid_projects_results_uniform$Var == params$Var &
        ac_grid_projects_results_uniform$SegIdx == params$SegIdx, params$Metric]
    }
  }
  
  temp <- as.data.frame(temp)
  colnames(temp) <- c(paste0("w", 1:nrow(ac_weight_grid)), "gt")
  
  doWithParallelCluster(numCores = 10, expr = {
    library(caret, quietly = TRUE)
    
    set.seed(1)
    control <- caret::trainControl(method = "repeatedcv", number = 10, repeats = 3)
    modelFit <- caret::train(gt ~., data = temp, method = "rf", trControl = control)
    imp <- caret::varImp(object = modelFit)
    
    list("fit" = modelFit, "imp" = imp)
  })
})
```

```{r echo=FALSE}
if (interactive()) {
  print(ac_sc_weights_varimp$fit$finalModel)
} else {
  # A little hack because otherwise it prints everything.
  # The below is an exact copy of what is printed interactively.
  cat("Call:
 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 2

          Mean of squared residuals: 0.1018337
                    % Var explained: -15.37")
}
```

The Random forest is overfit to the data, as the $R^2$ (explained variance) is negative, i.e., it is worse than a random guess.
Therefore, it is hard to assess the validity of the obtained results for the variable importance.
Let's take a look at the fitted model's performance (table \ref{tab:ac-sc-weights-varimp-mse}): It appears we are able to reach acceptable MSE's (last column), albeit not $\approx0$ for some reason. This model and the other fitted models cannot be used in prediction scenarios, as they do not generalize at all, the exercise was only to examine the variable importance.

```{r}
ac_sc_weights_varimp$fit$results$MSE <- ac_sc_weights_varimp$fit$results$RMSE**2
if (interactive()) {
  ac_sc_weights_varimp$fit$results
} else {
  knitr::kable(
    x = ac_sc_weights_varimp$fit$results,
    booktabs = TRUE,
    caption = "Best models of fitting of a Random forest to the calibrated data in order to assess the variable importance.",
    label = "ac-sc-weights-varimp-mse"
  )
}
```




#### Results PM vs. PM

Now for the actual optimization of each type of process model.
An overview of the optimization process on a per-project basis is shown in table \ref{tab:ac-sc-weights-optim-overview}.

```{r ac-calc-varimp-weights-sc}
ac_sc_weights_pm_I <- ac_sc_weights_optim(pmName = "I")
ac_sc_weights_pm_II <- ac_sc_weights_optim(pmName = "II")
ac_sc_weights_pm_IIIavg <- ac_sc_weights_optim(pmName = "III(avg)")
ac_sc_weights_pm_IIIb11 <- ac_sc_weights_optim(pmName = "III(b,11)")
```


```{r echo=FALSE}
templ <- list(ac_sc_weights_pm_I, ac_sc_weights_pm_II, ac_sc_weights_pm_IIIavg, ac_sc_weights_pm_IIIb11)
temp <- data.frame(
  PM = c("I", "II", "III(avg)", "III(b,11)"),
  MSE = round(unlist(lapply(templ, function(res) res$value)), 5),
  RMSE = round(unlist(lapply(templ, function(res) sqrt(res$value))), 5),
  Num_weights_gt0 = unlist(lapply(templ, function(res) sum(res$par > 0))),
  Weights_pruned = unlist(lapply(templ, function(res) paste0(round(100 * (1 - sum(res$par > 0) / length(res$par)), 2), "%"))),
  Num_iter = unlist(lapply(templ, function(res) res$counts["function"])),
  Num_grad = unlist(lapply(templ, function(res) res$counts["gradient"])))

if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Overview of the results of optimizing the weights using the normalizing linear scalarizer objective, on a per-project basis.",
    label = "ac-sc-weights-optim-overview"
  )
}
```

The results from table \ref{tab:ac-sc-weights-optim-overview} show that in general, about 90% or more of the weights are $0$, and hence the corresponding objective is not used, or not important at all for assessing the goodness of fit, i.e., how well the pair PM/P matches (mutual resemblance).
This affords us to considerably _prune_ the given models.
With relatively few iterations of the optimizer, we can obtain optimized weights for all models. All MSEs are quite low and close together (i.e., no significant variance). Also, all MSE are lower than the ones found using a Random forest. So, not only do we get pure variable importance information, we also get more precise models with this approach.

Model type I performs surprisingly well and is among the three best models. After pruning almost $97$% of its objectives, it achieves a respectable MSE. It required the 2nd-most optimization iterations.
The results are surprising because this models was designed without inspecting any of the data or ground truth. All other PM types are either enhanced by data, or solely based on it.

Model type II appears to have the lowest MSE, and requires the second fewest weights to be used (only $14$). This PM is an optimized version of type I, so the expectation is that fewer adjustments (fewer weights, less weight in total) are required for this model to perform well, when compared to type I.

Model type III(b, numInt=11) even only requires the usage of $10$ weights ($\approx1.92$% of all objectives), but is simultaneously the hardest to optimize, requiring two to three times the iterations.
Its MSE is the 2nd best (though only insignificantly worse than the best) and comparable to that of model type II, which is also expected, since this type of PM was crafted out of no expectation, and then simply aligned to all projects using a weighted average.
Recall that this type of model consists of piece-wise linear functions per variable, which means no perturbations. This could explain why it is among the best in terms of (R)MSE and number of weights as its nature means it is simple, yet effective.

Model type III(avg), while performing similar to model types II and III(b, numInt=11), requires adjustment and usage of comparatively more weights and has the largest MSE. This is expected, since this kind of model is the weighted average of all observed projects, and hence also includes all fine details and perturbations of each project. It is hence more detailed than the other two previously mentioned types and supposedly requires more weight adjustments to account for that.
Re-running the optimization for this model often uses less than $49$ weights ($\approx5$ to $10$ fewer), and achieves a little lower MSE ($\approx0.033$).

In any case, the residual MSE also indicates that a PM cannot probably be optimized beyond what the optimization found. Thus, a residual MSE$>0$ indicates an error in the model's formulation, and that even by using some best constellation of the weights, it is not possible to achieve perfect goodness of fit, which indicates a design flaw in either the model and/or the particular objectives used.
In other words, an MSE$=0$ would mean that a given PM can perfectly assess the goodness of fit of the given processes, and that it was configured in a perfect way (i.e., using the right segments, variables, metrics, etc.) to do so.


```{r echo=FALSE}
temp <- append(project_signals, project_signals_2nd_batch)
temp.gt <- c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score)

temp.df <- NULL

for (pmName in c("I", "II", "III(avg)", "III(b,11)")) {
  temp.weights <- switch (
    pmName,
    "I" = ac_sc_weights_pm_I$par,
    "II" = ac_sc_weights_pm_II$par,
    "III(avg)" = ac_sc_weights_pm_IIIavg$par,
    "III(b,11)" = ac_sc_weights_pm_IIIb11$par,
    {
      stop(paste0("Unknown process model: ", pmName))
    })
  temp.pred <- data.frame(
    pred = sapply(X = names(temp), function(pName) ac_pmp_score_weighted(
      results_uniform = ac_grid_projects_results_uniform,
      pmName = pmName, projName = pName, weightGrid = ac_weight_grid, weightVector = temp.weights)),
    ground_truth = temp.gt)
  
  temp.df <- rbind(temp.df, data.frame(
    PM = pmName,
    Score_for_best = ac_pmp_score_weighted(
      results_uniform = ac_grid_projects_results_uniform,
      pmName = pmName, projName = paste0("project_", which.max(temp.gt)),
      weightGrid = ac_weight_grid, weightVector = temp.weights),
    Score_for_worst = ac_pmp_score_weighted(
      results_uniform = ac_grid_projects_results_uniform,
      pmName = pmName, projName = paste0("project_", which.min(temp.gt)),
      weightGrid = ac_weight_grid, weightVector = temp.weights),
    MSE = Metrics::mse(actual = temp.gt, predicted = temp.pred$pred),
    Log = mean(scoring::logscore(object = ground_truth ~ pred, data = temp.pred))
  ))
}
```

```{r echo=FALSE}
if (interactive()) {
  temp.df
} else {
  knitr::kable(
    x = temp.df,
    booktabs = TRUE,
    caption = "Comparison of continuous PMs using source code for best/worst projects, as well as MSE- and Log-scores as average deviation from the ground truth consensus, using some best set of weights per PM as found by optimization.",
    label = "ac-compare-pms-sc-optim"
  )
}
```


Table \ref{tab:ac-compare-pms-sc-optim} shows the scores per optimized PM. We observe a significantly larger margin between the scores for the worst and best project, as well as significantly lower MSE.
Going by only these values, the champion model is perhaps type II, given the spread and MSE. Model type III (b, numInt=11) performs similar, with even a slightly lower Log-score, which indicates that it produces less-extreme outliers, as the Log-score is much more sensible to that than MSE.
Since the scores for the best/worst project are not $1$ resp. $0$, this indicates that either the models are flawed (that is, the design of the continuous expectation is off), the choice of objectives is inappropriate (to some degree), or that the ground truth is not accurate -- or any combination of these.
It could also be a hint at the fact that the observed projects do neither represent the worst nor best possible manifestation of the Fire Drill in source code which means that our ground truth is using too-extreme values (e.g., a $10$ in the ground truth might in reality just be a $6$, and likewise for a low value).
In summary, the conclusions that can be drawn from finding the optimal weights depend on the assumptions we are certain to make. So for example, if we are confident about our ground truth, it indicates a flaw in, e.g., the model (expectation) or the (choice of) objectives.
Or if we are confident in the expectation (that is, the design of the continuous model itself), then the flaw could be in the (choice of) objectives or the ground truth, and so on.
In any case, the fitted weights (actually, variable importances) allow us to reason about the shortcomings of the model (the expectation), its configuration (objectives), and the ground truth.


#### Results in detail

With each weight linked to a single objective (score), we have the ability to gain rich insights into which of these scores are most important.
Also, since each such score is tied to one variable, one segment, and one metric, we can find out more about any of these (for example, which is the most important variable in a PM, or which metrics are important and which may be discarded, and when/where).

The more scores that are used (i.e., weight $>0$), the more insights we can gain. As a compromise, we will first look into the weights of model type III(avg), as it performs similarly to the absolute best ones, and uses relatively many weights ($49$). Also during some runs, it was equally good.
Then we will merge the results of all PMs in order to get a better look at the variables, segments, and metrics.
We will look at the average importance of each segment, each variable, and each metric. Of course, this could be further drilled down. However, this shall just be a demonstration of what is possible in terms of analyzing a fitted PM.
It is also worth noting that in this scenario we are using all available metrics, variables, and segments. In a more realistic scenario, an expert would probably only pick a few types of metrics. Also, as long as there are sufficiently many segments that are measured, it is of less importance what actually is measured, because with enough degrees of freedom, we can find optimal weights that will lead to an MSE that has enough utility.

Before the analysis, let's show all the weights that are $>0$ for PM type III(avg) in table \ref{tab:ac-sc-weights-detailed}.
The first obvious fact that we observe is that the first $14$ weights are all maximally and equally important.

```{r echo=FALSE}
temp <- cbind(ac_weight_grid, data.frame(weight = ac_sc_weights_pm_IIIavg$par))
temp <- temp[temp$weight > 0,]
temp <- temp[order(-temp$weight),]

if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    row.names = FALSE,
    booktabs = TRUE,
    caption = "All non-zero weights as optimized for PM type III(avg), also showing which variable, metric, and segment they address.",
    label = "ac-sc-weights-detailed"
  )
}
```


```{r echo=FALSE}
temp <- cbind(ac_weight_grid, data.frame(weight = ac_sc_weights_pm_I$par + ac_sc_weights_pm_II$par + ac_sc_weights_pm_IIIavg$par + ac_sc_weights_pm_IIIb11$par))
temp$weight <- temp$weight / max(temp$weight) # let's normalize this again
temp <- temp[temp$weight > 0,]
temp <- temp[order(-temp$weight),]
```


Let's look at the average importance per segment, variable, and metric across __all__ projects (figure \ref{fig:ac-sc-weights-detailed-plot}).

```{r ac-sc-weights-detailed-plot, echo=FALSE, fig.height=8, fig.cap="Average and total importances of scores per segment, variable, and metric, normalized across all process models."}
layout(mat = matrix(data = c(1,3,2,4,5,5,6,6), ncol = 2, byrow = TRUE))

temp.seg <- `names<-`(sapply(X = 1:10, FUN = function(si) {
  weights <- temp[temp$SegIdx == si,]$weight
  if (length(weights) == 0) 0 else mean(weights)
}), paste0(1:10))
barplot(temp.seg, main = "Average importance per segment")
grid()
temp.seg <- `names<-`(sapply(X = 1:10, FUN = function(si) {
  weights <- temp[temp$SegIdx == si,]$weight
  if (length(weights) == 0) 0 else sum(weights)
}), paste0(1:10))
barplot(temp.seg, main = "Total importance per segment")
grid()


temp.var <- `names<-`(sapply(X = sort(unique(temp$Var)), function(var) {
  weights <- temp[temp$Var == var,]$weight
  if (length(weights) == 0) 0 else mean(weights)
}), sort(unique(temp$Var)))
barplot(temp.var, main = "Average importance per variable")
grid()
temp.var <- `names<-`(sapply(X = sort(unique(temp$Var)), function(var) {
  weights <- temp[temp$Var == var,]$weight
  if (length(weights) == 0) 0 else sum(weights)
}), sort(unique(temp$Var)))
barplot(temp.var, main = "Total importance per variable")
grid()


temp.metric <- `names<-`(sapply(X = sort(unique(temp$Metric)), function(m) {
  weights <- temp[temp$Metric == m,]$weight
  if (length(weights) == 0) 0 else mean(weights)
}), sort(unique(temp$Metric))) #%>% sort %>% rev
# We need to shorten some metrics' names:
names(temp.metric) <- sapply(names(temp.metric), function(m) {
  if (m == "Kurtosis") return("Kurt.")
  if (m == "ImpulseFactor") return("ImpFac")
  m
})
barplot(temp.metric, main = "Average importance per metric")
grid()


temp.metric <- `names<-`(sapply(X = sort(unique(temp$Metric)), function(m) {
  weights <- temp[temp$Metric == m,]$weight
  if (length(weights) == 0) 0 else sum(weights)
}), sort(unique(temp$Metric))) #%>% sort %>% rev
# We need to shorten some metrics' names:
names(temp.metric) <- sapply(names(temp.metric), function(m) {
  if (m == "Kurtosis") return("Kurt.")
  if (m == "ImpulseFactor") return("ImpFac")
  m
})
barplot(temp.metric, main = "Total importance per metric")
grid()
```


We may also show the most frequented segments, variables, and metrics:

```{r ac-sc-weights-detailed-plot-counts, echo=FALSE, fig.height=5.5, fig.cap="Number of score usages per segment, variable, and metric (across all process models)."}
layout(mat = matrix(data = c(1,2,3,3), ncol = 2, byrow = TRUE))

barplot(table(temp$SegIdx), main = "No. of scores used per segment")
grid()
barplot(table(temp$Var)[levels(temp$Var)], main = "No. of scores used per variable")
grid()

temp.metric <- table(temp$Metric)
names(temp.metric) <- sapply(names(temp.metric), function(m) {
  if (m == "Kurtosis") return("Kurt.")
  if (m == "ImpulseFactor") return("ImpFac")
  m
})
barplot(temp.metric, main = "No. of usages per type of score")
grid()
```

The differences as of figures \ref{fig:ac-sc-weights-detailed-plot} and \ref{fig:ac-sc-weights-detailed-plot-counts} are interesting and a bit unexpected.
While the average importance per segment suggests that the first segments are somewhat more important than the last ones, we see the most scores were used in the last segments.
This means that it is more important to sample from a process in these segments, and that fluctuations in these segments are more sensible to the outcome (assessing the goodness of fit). This finding is also interesting w.r.t. the binary decision rule as used for issue-tracking data. It measures at three characteristic points in time of a project, namely at $t_1=0.4$, $t_2=0.85$, and $t_{\mathrm{end}}=1$. If we look at the average importance of each segment, those choices are perhaps not ideal (under the assumptions that the source code variables quantify the same thing as the issue-tracking variables, which they do not).

The average importance per variable in figure \ref{fig:ac-sc-weights-detailed-plot} favored `CP` and `FREQ`, and then with some distance `A` and `SCD`, which are similarly important. In figure \ref{fig:ac-sc-weights-detailed-plot-counts} however, `CP`, and `SCD` have a lot more scores on them than `A` and `FREQ` ($\approx26$). Requiring more scores on a variable could indicate it having a large variance, for example. In other words, if relatively more scores are required in a segment or for a variable, then those are probably required to accommodate the larger variances.
We also see a significant difference between average importance and number of type of scores, with the `rmse`` being the score used most frequently.
The impulse factor divides the process' peak by its average and is therefore sensitive to perturbations of the entire signal and not just its extremes. A difference between Peak and ImpulseFactor could indicate that in fact differences between process and model exist.



# References {-}

<div id="refs"></div>

