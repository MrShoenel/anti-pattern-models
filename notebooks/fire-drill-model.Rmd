---
title: "Modeling the Fire Drill"
bibliography: ../inst/REFERENCES.bib
output:
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 6
    df_print: kable
  md_document:
    toc: true
    toc_depth: 6
    df_print: kable
  html_document:
    number_sections: true
    toc: true
    toc_depth: 6
    toc_float: true
    df_print: kable
  word_document: default
---

```{r}
source("../helpers.R")
source("./common-funcs.R", echo = FALSE)
```

# Importing the pattern from file

We have previously designed our initial best guess as a vector graphic, and then rasterized it and exported each path with x/y coordinates in CSV. There are a few caveats:

* Not all paths have the same length -- this is due to the amount of detail of each path. However, the x/y coordinates of each path cover the same co-domain.
* The SVG coordinate system starts with 0,0 in the top left corner, so y-values should be flipped.
* The pattern is hence not in the unit square, and neither do we want to present it there as of now. However, we will scale its y-values into `[0,1]` and preserve its aspect ratio, so that we can define the intervals in the next step more easily.
* The paths in the modeled SVG have color names: Gold is adaptive, green is corrective+perfective, indigo is the commit frequency, and coral is the source code density.

```{r}
fd_data <- read.csv("../data/Fire-Drill_first-best-guess.csv", sep = ";")
fd_data <- list(
  adaptive = stats::na.exclude(data.frame(
    x = fd_data$gold_x,
    y = fd_data$gold_y,
    t = "adaptive"
  )),
  
  corrPerf = stats::na.exclude(data.frame(
    x = fd_data$green_x,
    y = fd_data$green_y,
    t = "corrPerf"
  )),
  
  commFreq = stats::na.exclude(data.frame(
    x = fd_data$indigo_x,
    y = fd_data$indigo_y,
    t = "commFreq"
  )),
  
  codeDens = stats::na.exclude(data.frame(
    x = fd_data$coral_x,
    y = fd_data$coral_y,
    t = "codeDens"
  ))
)

fd_data_concat <- rbind(
  fd_data$adaptive,
  fd_data$corrPerf,
  fd_data$commFreq,
  fd_data$codeDens
)

# Also, let's already correct the x-axis a bit:
fd_data_concat$x <- fd_data_concat$x - min(fd_data_concat$x)
fd_data_concat$x <- fd_data_concat$x / max(fd_data_concat$x)

# .. and since we're at it, let's do y:
fd_data_concat$y <- -1 * fd_data_concat$y - min(-1 * fd_data_concat$y)
fd_data_concat$y <- fd_data_concat$y / max(fd_data_concat$y)
```

Let's plot the original data (note that we take the negative y-values to conform to our coordinate system where `[0,0]` is in the __lower__ right corner):

```{r}
library(ggplot2)

ggplot(data = fd_data_concat, aes(x = x, y = y, color = t)) +
  geom_line() +
  labs(color = "Data series") +
  scale_x_continuous(
    breaks = seq(0, 1, by = .05)
  ) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0))
```
Now the pattern is already in the unit-square (`[0,1]`). This is OK, as some variables in the pattern are defined to reach their absolute minimum or maximum. The other variables are designed relative to it, resulting in some space above or below.


# Defining Intervals and sub-models

In this section, we define the intervals' boundaries and decide for models to use for each variable in each interval. __The boundaries are our primary optimization goal__, as we want to minimize the loss of the models fit over the variables in between. We will defined initial boundaries, as well as valid ranges for them, according to the literature. As of our discussions and current definition, we define the four phases:


1. Begin -- Short project warm-up phase
2. Long Stretch -- The longest phase in the project, about which we do not know much about, except for that there should be a rather constant amount of activities over time.
3. Fire Drill -- Characteristic is a sudden and steep increase of adaptive activities. This phase is over once these activities reached their apex.
4. Aftermath -- Everything after the apex. We should see even steeper declines.


@brown1998refactoring describe a typical scenario where about six months are spent on non-developmental activities, and the actual software is then developed in less than four weeks. If we were to include some of the aftermath, the above first guess would describe a project of about eight weeks.

We define the boundaries and their leeway as follows (there are three boundaries to split the pattern into four intervals):

```{r echo=FALSE}
fd_data_boundaries <- c(
  "b1" = 0.085,
  "b2" = 0.625,
  "b3" = 0.875
)
```


* `b1` -- start at `r round(fd_data_boundaries["b1"], 3)` and allow ranges of `[0.025, 0.25]`
* `b2` -- start at `r round(fd_data_boundaries["b2"], 3)` and allow ranges of `[0.5, 0.9]`
* `b3` -- start at `r round(fd_data_boundaries["b3"], 3)` and allow ranges of `[0.5, 0.975]`


Also, we need to define additional inequality constraints between the boundaries, so that we guarantee a minimum distance between boundaries. In general, we only want to impose the rather loose constraint of having at least `0.05` between any two boundaries. While `b1` and `b2` cannot cross and can never undercut this threshold, `b2` and `b3` can. The inequality constraint hence is `b3 - b2 >= 0.05`.


The following plot includes the initial boundaries and their allowed ranges:

```{r}
ggplot(data = fd_data_concat, aes(x = x, y = y, color = t)) +
  geom_line() +
  labs(color = "Data series") +
  scale_x_continuous(
    breaks = seq(0, 1, by = .05)
  ) +
  theme_light() +
  theme(axis.text.x = element_text(angle = -45, vjust = 0)) +
  geom_vline(xintercept = fd_data_boundaries["b1"], color = "blue", size = .5) +
  geom_vline(xintercept = fd_data_boundaries["b2"], color = "blue", size = .5) +
  geom_vline(xintercept = fd_data_boundaries["b3"], color = "blue", size = .5) +
  geom_rect(data = data.frame(
    xmin = c(.025, .5, .5),
    xmax = c(.25, .9, .975),
    ymin = c(0, .05, 0),
    ymax = c(1, .95, 1),
    boundary = c("b1", "b2", "b3"),
    fill = c("green", "red", "blue")
  ), aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = fill), color = "black", alpha = .25, inherit.aes = FALSE, show.legend = FALSE)
```

## Extracting all sub-patterns

The last step is to update the concatenated data and attach a factor column with the interval:

```{r}
fd_data_concat$interval <- sapply(fd_data_concat$x, function(x) {
  if (x < fd_data_boundaries["b1"]) {
    return("Begin")
  } else if (x < fd_data_boundaries["b2"]) {
    return("LongStretch")
  } else if (x < fd_data_boundaries["b3"]) {
    return("FireDrill")
  }
  return("Aftermath")
})

fd_data_concat$interval <- factor(
  x = fd_data_concat$interval,
  levels = c("Begin", "LongStretch", "FireDrill", "Aftermath"), ordered = TRUE)
```

Now show a faceted plot:

```{r}
ggplot(data = fd_data_concat, aes(x = x, y = y, color = t)) +
  geom_line(size = 1) +
  labs(color = "Data series") +
  scale_x_continuous(
    breaks = seq(0, 1, by = .1)
  ) +
  facet_grid(t ~ interval, scales = "free_x")
```

Note that in the above plot, every column of plots has the same width. However, this becomes obvious when looking at the x-axis for each column. While we could adjust the widths to the actual interval lengths, we keep it like this for two reasons: first, it allows us better insight into shorter intervals. Second, models that do transformation into the unit-square will "see" the pattern very similar to how it looks above.

The split was made according to the initial boundaries. What is important to remember, is that each sub-plot in the above grid __is a reference pattern__, that is, each of these above represents the reference we want to fit the data (the query extracted according to the boundaries given during the optimization) against using some model later.


# Load real-world project

We load the same project as in the notebook `student-project-1.Rmd`.

```{r}
spFile <- "../data/student-project-1.csv"
sp <- read.csv(spFile)

dateFormat <- "%Y-%m-%d %H:%M:%S"

sp$CommitterTimeObj <- as.POSIXct(strptime(
  sp$CommitterTime, format = dateFormat))
sp$AuthorTimeObj <- as.POSIXct(strptime(
  sp$AuthorTime, format = dateFormat))

# Cut off data way after project end:
sp <- sp[sp$AuthorTimeObj <= as.POSIXct(strptime("2020-08-31", format = "%Y-%m-%d")), ]

# Create normalized timestamps:
sp$AuthorTimeNormalized <- sp$AuthorTimeUnixEpochSecs - min(sp$AuthorTimeUnixEpochSecs)
sp$AuthorTimeNormalized <- sp$AuthorTimeNormalized / max(sp$AuthorTimeNormalized)
```

Now we define each variable as a function:

```{r}
# passed to stats::density
use_kernel <- "gauss" # "rect"

# We'll need these for the densities:
acp_ratios <- table(sp$label) / sum(table(sp$label))

dens_a <- densitySafe(
  sp[sp$label == "a", ]$AuthorTimeNormalized, acp_ratios[["a"]], kernel = use_kernel)
dens_c <- densitySafe(
  sp[sp$label == "c", ]$AuthorTimeNormalized, acp_ratios[["c"]], kernel = use_kernel)
dens_p <- densitySafe(
  sp[sp$label == "p", ]$AuthorTimeNormalized, acp_ratios[["p"]], kernel = use_kernel)

# Also compute a combined density for corr+perf:
dens_cp <- densitySafe(
  sp[sp$label == "c" | sp$label == "p", ]$AuthorTimeNormalized,
  acp_ratios[["c"]] + acp_ratios[["p"]], kernel = use_kernel)
# .. and the overall frequency:
dens_acp <- densitySafe(
  sp$AuthorTimeNormalized, kernel = use_kernel)

use_acp_attr <- c("min", "max", "ratio", "ymax")
acp_attr <- rbind(
  data.frame(attributes(dens_a)[use_acp_attr]),
  data.frame(attributes(dens_c)[use_acp_attr]),
  data.frame(attributes(dens_p)[use_acp_attr]))
```

## Add Source Code density

The source code density is a different kind of variable. Like any other metric, its sampling frequency has no effect on the density, but rather its value. Sampling more frequently will only increase precision. So while we estimated a Kernel for the activities, for all other variables that are metrics, the density (or curve) depends on the observed value, not on the frequency of observations.

```{r}
# scd is source code density
dens_scd_data <- data.frame(
  x = sp$AuthorTimeNormalized,
  y = sp$Density
)

# We want to smooth the density data a little:
temp <- stats::loess.smooth(
  x = dens_scd_data$x, y = dens_scd_data$y, span = .07, family = "sym", degree = 1, evaluation = nrow(dens_scd_data))
dens_scd_data$x <- temp$x
dens_scd_data$y <- temp$y - min(temp$y)
dens_scd_data$y <- dens_scd_data$y / max(dens_scd_data$y)

dens_scd <- (function() {
  r <- range(dens_scd_data$x)
  temp <- stats::approxfun(x = dens_scd_data$x, y = dens_scd_data$y, ties = "ordered")
  f1 <- Vectorize(function(x) {
    if (x < r[1] || x > r[2]) {
      return(NaN)
    }
    return(temp(x))
  })
  
  attributes(f1) <- list(
    min = r[1], max = r[2], x = dens_scd_data$x, y = dens_scd_data$y
  )
  
  f1
})()
```



This is how the project looks. Please note that we have already cut off all activity after August 31 2020.

```{r}
ggplot(data.frame(x = range(acp_attr$min, acp_attr$max)), aes(x)) +
  stat_function(fun = dens_a, aes(color="A"), size = 1, n = 2^11) +
  #stat_function(fun = dens_c, aes(color="C"), size = 1, n = 2^11) +
  #stat_function(fun = dens_p, aes(color="P"), size = 1, n = 2^11) +
  stat_function(fun = dens_cp, aes(color="C+P"), size = 1, n = 2^11) +
  stat_function(fun = dens_acp, aes(color="Freq"), size = 1, n = 2^11) +
  stat_function(fun = dens_scd, aes(color="SCD"), size = 1, n = 2^11) +
  theme_light() +
  labs(color = "Activity") +
  scale_color_brewer(palette = "Set1")
```

In the above plot, it appears that the initial perfective activities (in the approximate range of `[~-0..5, 0.2]`) may or may not belong to the project's actual work range. For example, it seems plausible that the project manager made some initial commit(s) with directions for the students. In a real-world analysis, one would need to decide whether to cut this off or not, based on the knowledge of what was going on. We allow the remainder of this notebook to make the switch using a variable. This will allow us to examine model-fits under different configurations.

_Note_: As of our suspicion, the very first commit is not the project's start, and the right decision for this project would to cut it off.

```{r}
# Use this obvious variable to control cutting off initial perfective commits.
CUT_OFF_PERFECTIVE_COMMITS_IN_BEGIN <- TRUE

dens_acp_x_min <- min(acp_attr$min)
if (CUT_OFF_PERFECTIVE_COMMITS_IN_BEGIN) {
  dens_acp_x_min <- min(acp_attr[1:2,]$min) # 1:2 are a,c; see above
}
dens_acp_x_max <- max(acp_attr$max) # we're not cutting off at end

dens_a_x_idx <- attributes(dens_a)$x >= dens_acp_x_min
dens_c_x_idx <- attributes(dens_c)$x >= dens_acp_x_min
dens_p_x_idx <- attributes(dens_p)$x >= dens_acp_x_min
dens_cp_x_idx <- attributes(dens_cp)$x >= dens_acp_x_min
dens_scd_x_idx <- attributes(dens_scd)$x >= dens_acp_x_min
dens_acp_x_idx <- attributes(dens_acp)$x >= dens_acp_x_min

dens_acp_data <- rbind(
  data.frame(
    x = attributes(dens_a)$x[dens_a_x_idx],
    y = attributes(dens_a)$y[dens_a_x_idx] * attributes(dens_a)$ratio,
    t = rep("A", sum(dens_a_x_idx))
  ),
#  data.frame(
#    x = attributes(dens_c)$x[dens_c_x_idx],
#    y = attributes(dens_c)$y[dens_c_x_idx] * attributes(dens_c)$ratio,
#    t = rep("C", sum(dens_c_x_idx))
#  ),
#  data.frame(
#    x = attributes(dens_p)$x[dens_p_x_idx],
#    y = attributes(dens_p)$y[dens_p_x_idx] * attributes(dens_p)$ratio,
#    t = rep("P", sum(dens_p_x_idx))
#  ),
  data.frame(
    x = attributes(dens_cp)$x[dens_cp_x_idx],
    y = attributes(dens_cp)$y[dens_cp_x_idx] * attributes(dens_cp)$ratio,
    t = rep("CP", sum(dens_cp_x_idx))
  ),
  data.frame(
    x = attributes(dens_acp)$x[dens_acp_x_idx],
    y = attributes(dens_acp)$y[dens_acp_x_idx],
    t = rep("FREQ", sum(dens_acp_x_idx))
  ),
  data.frame(
    x = attributes(dens_scd)$x[dens_scd_x_idx],
    y = attributes(dens_scd)$y[dens_scd_x_idx],
    t = rep("SCD", sum(dens_scd_x_idx))
  )
)
dens_acp_data$t <- factor(x = dens_acp_data$t, levels = unique(dens_acp_data$t), ordered = TRUE)


# Now it is important to scale the densities for a,c,p on the x-axis together,
# so that we get all three within [0,1].
dens_acp_data$x <- dens_acp_data$x - min(dens_acp_data$x)
dens_acp_data$x <- dens_acp_data$x / max(dens_acp_data$x)
```

## The Final project

Let's plot how that looks:

```{r}
ggplot(dens_acp_data, aes(x = x, y = y)) +
  geom_line(aes(color = t), size = .75) +
  theme_light() +
  labs(color = "Activity") +
  scale_color_brewer(palette = "Set1")
```

The above plot represents now the final project that we will fit our entire pattern to:

* The domain is now `[0,1]` -- note that the source code density was not estimated using KDE, so that we do not have an estimation for the bandwidth, which leads to initial inclines and declines at the end for such variables.
* The integral of the frequency is roughly `1` -- the sum of the three integrals for each of the three activities is also `1`. It is important that each activity has a density that is proportional to the amount of the other activities, and that in sum everything should become `1` again.


# Defining the objective function

Now we can finally start to define an objective function. Before that, we need to decide how to score each variable (i.e., what kind of model) in each interval. For each step of the fitting process, we will get a vector of scores that needs to be aggregated.


## Scoring per variable and interval

We want to choose an adequate model for each variable in each interval, and then optimize the boundaries to maximize the fit of each model. We will only use two different models (if any), and these are described below:

Models:

__No Score__: If there is no or nearly no information available for a variable in an interval, we should choose not to score it.

__Linear Model__: A linear model (regression) can be used for when we do not know much about the course of a variable in an interval. We use this model when our best expectation is a linear course of the variable, and that is what we extract from the model. We may have an expectation for the intercept, slope or the residuals. When using this model, we usually choose a lower weight for it.

__Rectifier__: A model to match shapes of curves, that uses _DTW_ to __rectify__ a signal and then computes the goodness-of-fit using one or more of the previously developed scores. This is the most sophisticated method we currently have. The underlying DTW is computed with either closed or open begin/end. In the later case, the score of the rectifier model also reflects how much of the query was matched.

__No Model__ (approximate function): We can choose to just extract the data from the current interval, to approximate a function for its data, and then compute a score in the same way the Rectifier does, by comparing two functions. Skipping the rectification seems to work well in most cases; however, we assume that the data from the interval has linear time and no time dilations exist.

__Polynomial__: Similar to __No Model__, we fit a 2nd- or 3rd-order polynomial over the data (currently, 3rd order should suffice given the peculiarities of our defined Fire Drill) and then compare the resulting function against the reference. Again, this approach assumes no distortions in time.




| Interval     | Variable    | Model                            | Weight | Model notes    |
| :-           | :-          | :----                            | :-:    | :------------- |
| Begin        | A           | Rectifier, No Model, Poly(2)     | `0.8`  | -              |
| Begin        | C+P         | Rectifier, No Model, Poly(2)     | `0.8`  | -              |
| Begin        | FREQ        | Rectifier, No Model, Poly(2)     | `0.8`  | -              |
| Begin        | SCD        | Rectifier, No Model, Poly(2)     | `0.8`  | -              |
| ---------    | ----------- | -------------------------        | -------- | ---------------------------------------------------- |
| Long Stretch | A           | LM ($\mathop{{}\mathbb{E}}[0^{\circ}]$) | `0.5`  | -              |
| Long Stretch | C+P         | LM ($\mathop{{}\mathbb{E}}[0^{\circ}]$) | `0.5`  | -              |
| Long Stretch | FREQ        | LM ($\mathop{{}\mathbb{E}}[0^{\circ}]$) | `0.5`  | -              |
| Long Stretch | SCD         | LM ($\mathop{{}\mathbb{E}}[\text{slope}<0^{\circ}]$), Rectifier, No Model, Poly(2) | `0.5`  | Use of non-LM may be applicable, considering the somewhat smooth decline in the beginning of this phase. |
| ---------    | ----------- | -------------------------        | -------- | ---------------------------------------------------- |
| Fire Drill   | A           | LM ($\mathop{{}\mathbb{E}}[\text{slope}\geq 45^{\circ}]$), Rectifier, No Model, Poly(3) | `1.0`  | LM may be applicable as we know nothing about _how_ the variable rises, just that it will. It is however the least-preferred option. Usage of LM is probably a bit sketchy, as we need to make assumptions about its slope. |
| Fire Drill   | C+P         | LM ($\mathop{{}\mathbb{E}}[\text{slope} \approx -45^{\circ}]$), Rectifier, No Model, Poly(3) | `1.0`  | Like adaptive (A), an LM may be worth checking out, as this variable is almost linear in our model. Also, when using an LM for any variable in this interval, we should use a smaller weight. |
| Fire Drill   | FREQ        | LM ($\mathop{{}\mathbb{E}}[\text{slope}\geq 45^{\circ}]$), Rectifier, No Model, Poly(3) | `1.0`  | Like the previous two. |
| Fire Drill   | S           | Rectifier, No Model, Poly(3)     | `1.0`  | -              |
| ---------    | ----------- | -------------------------        | -------- | ---------------------------------------------------- |
| Aftermath    | A           | Rectifier, No Model, Poly(3)     | `1.0`  | -              |
| Aftermath    | C+P         | LM ($\mathop{{}\mathbb{E}}[\text{slope} \approx -45^{\circ}]$), Rectifier, No Model, Poly(3) | `1.0`  | This is basically a slightly less steep continuation of the previous interval |
| Aftermath    | FREQ        | Rectifier, No Model, Poly(3)     | `1.0`  | -              |
| Aftermath    | SCD         | Rectifier, No Model, Poly(3)     | `1.0`  | -              |


Notes:

* If not noted otherwise, when using an LM, we expect a slope of `0`, and any (positive or negative) slope will decrease the score.
* When using an LM, we currently do not score the residuals, as we cannot make assumptions about how the data is distributed in an interval. If we were to score lower residuals better, then the LM would fit towards data with less noise better, but we do not know anything about the noise. That's why we skip it.
* I do not see a reason for mixing non-linear models within the entire model, i.e., all variables that use a non-linear model should use the same kind of model, otherwise it will be also more difficult to compare approaches.
  * The only thing we can compare is whether all non-linear models converge to the same interval boundaries. Different models will result in different scores, so there is no way to say which model is best, at least not until we have all the ground truth. Until then the only conclusions we may draw are those from models that result in obviously erroneous fits.



## Score weights and aggregation

Given a weight-vector $\boldsymbol w$ and a score-vector $\boldsymbol s$ of same length, we want to aggregate all scores using this expression:

$$
\displaystyle \prod^{\Vert \boldsymbol s \Vert}_{i=1}1+ \boldsymbol w_i * f_{t,k}(\boldsymbol s_i),\;\; \forall \boldsymbol w_i \; 0 < \boldsymbol w_i \leq 1 \; \wedge \; \forall \boldsymbol s_i \; 0 \leq \boldsymbol s_i \leq 1,
$$

where $f_{t,k}:\mathbb R \to \mathbb R$ is a non-linear scaling function (see below). The advantages of the above expression are these:

* We can introduce weights to linearly scale each score and thus alter its importance.
* Instead of diminishing the product, low scores will rather not increase it.
* We know the lower and upper bound (remember, scores are in range `[0,1]`) for this expression.

Furthermore, we want to use a non-linear scaling function, $f_{t,k}$, that penalizes low scores and exponentially rewards higher scores. That function will be linear until some threshold $t$ (typically $0.1 \leq t \leq 0.5$), and exponential afterwards (with typical exponent $2 \leq k \leq 5$).

$$
f_{t,k}(x) = \begin{cases}
  x * t^{k-1}, & \text{for } 0 \leq x \leq t, \\
  x^k, & \text{for } x > t.
\end{cases}
$$

Penalize small scores:
```{r}
penalizeThreshold <- .3
penalizeExponent <- 3

penalizeScore <- (function(t, k) {
  return(Vectorize(function(x) {
    if (x <= t) x * t^(k - 1) else x^k
  }))
})(penalizeThreshold, penalizeExponent)

curve(penalizeScore)
```





# References {-}

<div id="refs"></div>
