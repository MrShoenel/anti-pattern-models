---
title: "Modeling the Fire Drill"
bibliography: ../inst/REFERENCES.bib
output:
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 6
    df_print: kable
  md_document:
    toc: true
    toc_depth: 6
    df_print: kable
  html_document:
    number_sections: true
    toc: true
    toc_depth: 6
    toc_float: true
    df_print: kable
  word_document: default
---

```{r}
source("../helpers.R")
source("./common-funcs.R", echo = FALSE)
```

# Importing the pattern from file

We have previously designed our initial best guess as a vector graphic, and then rasterized it and exported each path with x/y coordinates in CSV. There are a few caveats:

* Not all paths have the same length -- this is due to the amount of detail of each path. However, the x/y coordinates of each path cover the same co-domain.
* The SVG coordinate system starts with 0,0 in the top left corner, so y-values should be flipped.
* The pattern is hence not in the unit square, and neither do we want to present it there as of now. However, we will scale its y-values into `[0,1]` and preserve its aspect ratio, so that we can define the intervals in the next step more easily.
* The paths in the modeled SVG have color names: Gold is adaptive, green is corrective+perfective, indigo is the commit frequency, and coral is the source code density.

```{r}
fd_data <- read.csv("../data/Fire-Drill_first-best-guess.csv", sep = ";")
fd_data <- list(
  adaptive = stats::na.exclude(data.frame(
    x = fd_data$gold_x,
    y = fd_data$gold_y,
    t = "adaptive"
  )),
  
  corrPerf = stats::na.exclude(data.frame(
    x = fd_data$green_x,
    y = fd_data$green_y,
    t = "corrPerf"
  )),
  
  commFreq = stats::na.exclude(data.frame(
    x = fd_data$indigo_x,
    y = fd_data$indigo_y,
    t = "commFreq"
  )),
  
  codeDens = stats::na.exclude(data.frame(
    x = fd_data$coral_x,
    y = fd_data$coral_y,
    t = "codeDens"
  ))
)

fd_data_concat <- rbind(
  fd_data$adaptive,
  fd_data$corrPerf,
  fd_data$commFreq,
  fd_data$codeDens
)

# Also, let's already correct the x-axis a bit:
fd_data_concat$x <- fd_data_concat$x - min(fd_data_concat$x)
fd_data_concat$x <- fd_data_concat$x / max(fd_data_concat$x)

# .. and since we're at it, let's do y:
fd_data_concat$y <- -1 * fd_data_concat$y - min(-1 * fd_data_concat$y)
fd_data_concat$y <- fd_data_concat$y / max(fd_data_concat$y)
```

Let's plot the original data (note that we take the negative y-values to conform to our coordinate system where `[0,0]` is in the __lower__ right corner):

```{r}
library(ggplot2)

ggplot(data = fd_data_concat, aes(x = x, y = y, color = t)) +
  geom_line() +
  labs(color = "Data series") +
  scale_x_continuous(
    breaks = seq(0, 1, by = .05)
  ) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0))
```
Now the pattern is already in the unit-square (`[0,1]`). This is OK, as some variables in the pattern are defined to reach their absolute minimum or maximum. The other variables are designed relative to it, resulting in some space above or below.


# Defining Intervals and sub-models

In this section, we define the intervals' boundaries and decide for models to use for each variable in each interval. __The boundaries are our primary optimization goal__, as we want to minimize the loss of the models fit over the variables in between. We will defined initial boundaries, as well as valid ranges for them, according to the literature. As of our discussions and current definition, we define the four phases:


1. Begin -- Short project warm-up phase
2. Long Stretch -- The longest phase in the project, about which we do not know much about, except for that there should be a rather constant amount of activities over time.
3. Fire Drill -- Characteristic is a sudden and steep increase of adaptive activities. This phase is over once these activities reached their apex.
4. Aftermath -- Everything after the apex. We should see even steeper declines.


@brown1998refactoring describe a typical scenario where about six months are spent on non-developmental activities, and the actual software is then developed in less than four weeks. If we were to include some of the aftermath, the above first guess would describe a project of about eight weeks.

We define the boundaries and their leeway as follows (there are three boundaries to split the pattern into four intervals):

```{r echo=FALSE}
fd_data_boundaries <- c(
  "b1" = 0.085,
  "b2" = 0.625,
  "b3" = 0.875
)
```


* `b1` -- start at `r round(fd_data_boundaries["b1"], 3)` and allow ranges of `[0.025, 0.25]`
* `b2` -- start at `r round(fd_data_boundaries["b2"], 3)` and allow ranges of `[0.5, 0.9]`
* `b3` -- start at `r round(fd_data_boundaries["b3"], 3)` and allow ranges of `[0.5, 0.975]`


Also, we need to define additional inequality constraints between the boundaries, so that we guarantee a minimum distance between boundaries. In general, we only want to impose the rather loose constraint of having at least `0.05` between any two boundaries. While `b1` and `b2` cannot cross and can never undercut this threshold, `b2` and `b3` can. The inequality constraint hence is `b3 - b2 >= 0.05`.


The following plot includes the initial boundaries and their allowed ranges:

```{r}
ggplot(data = fd_data_concat, aes(x = x, y = y, color = t)) +
  geom_line() +
  labs(color = "Data series") +
  scale_x_continuous(
    breaks = seq(0, 1, by = .05)
  ) +
  theme_light() +
  theme(axis.text.x = element_text(angle = -45, vjust = 0)) +
  geom_vline(xintercept = fd_data_boundaries["b1"], color = "blue", size = .5) +
  geom_vline(xintercept = fd_data_boundaries["b2"], color = "blue", size = .5) +
  geom_vline(xintercept = fd_data_boundaries["b3"], color = "blue", size = .5) +
  geom_rect(data = data.frame(
    xmin = c(.025, .5, .5),
    xmax = c(.25, .9, .975),
    ymin = c(0, .05, 0),
    ymax = c(1, .95, 1),
    boundary = c("b1", "b2", "b3"),
    fill = c("green", "red", "blue")
  ), aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = fill), color = "black", alpha = .25, inherit.aes = FALSE, show.legend = FALSE)
```

## Extracting all sub-patterns

The last step is to update the concatenated data and attach a factor column with the interval:

```{r}
fd_data_concat$interval <- sapply(fd_data_concat$x, function(x) {
  if (x < fd_data_boundaries["b1"]) {
    return("Begin")
  } else if (x < fd_data_boundaries["b2"]) {
    return("LongStretch")
  } else if (x < fd_data_boundaries["b3"]) {
    return("FireDrill")
  }
  return("Aftermath")
})

fd_data_concat$interval <- factor(
  x = fd_data_concat$interval,
  levels = c("Begin", "LongStretch", "FireDrill", "Aftermath"), ordered = TRUE)
```

Now show a faceted plot:

```{r}
ggplot(data = fd_data_concat, aes(x = x, y = y, color = t)) +
  geom_line(size = 1) +
  labs(color = "Data series") +
  scale_x_continuous(
    breaks = seq(0, 1, by = .1)
  ) +
  facet_grid(t ~ interval, scales = "free_x")
```

Note that in the above plot, every column of plots has the same width. However, this becomes obvious when looking at the x-axis for each column. While we could adjust the widths to the actual interval lengths, we keep it like this for two reasons: first, it allows us better insight into shorter intervals. Second, models that do transformation into the unit-square will "see" the pattern very similar to how it looks above.

The split was made according to the initial boundaries. What is important to remember, is that each sub-plot in the above grid __is a reference pattern__, that is, each of these above represents the reference we want to fit the data (the query extracted according to the boundaries given during the optimization) against using some model later.


# Load real-world project

We load the same project as in the notebook `student-project-1.Rmd`.

```{r}
spFile <- "../data/student-project-1.csv"
sp <- read.csv(spFile)

dateFormat <- "%Y-%m-%d %H:%M:%S"

sp$CommitterTimeObj <- as.POSIXct(strptime(
  sp$CommitterTime, format = dateFormat))
sp$AuthorTimeObj <- as.POSIXct(strptime(
  sp$AuthorTime, format = dateFormat))

# Cut off data way after project end:
sp <- sp[sp$AuthorTimeObj <= as.POSIXct(strptime("2020-08-31", format = "%Y-%m-%d")), ]

# Create normalized timestamps:
sp$AuthorTimeNormalized <- sp$AuthorTimeUnixEpochSecs - min(sp$AuthorTimeUnixEpochSecs)
sp$AuthorTimeNormalized <- sp$AuthorTimeNormalized / max(sp$AuthorTimeNormalized)
```

Now we define each variable as a function:

```{r}
# passed to stats::density
use_kernel <- "gauss" # "rect"

# We'll need these for the densities:
acp_ratios <- table(sp$label) / sum(table(sp$label))

dens_a <- densitySafe(
  sp[sp$label == "a", ]$AuthorTimeNormalized, acp_ratios[["a"]], kernel = use_kernel)
dens_c <- densitySafe(
  sp[sp$label == "c", ]$AuthorTimeNormalized, acp_ratios[["c"]], kernel = use_kernel)
dens_p <- densitySafe(
  sp[sp$label == "p", ]$AuthorTimeNormalized, acp_ratios[["p"]], kernel = use_kernel)

# Also compute a combined density for corr+perf:
dens_cp <- densitySafe(
  sp[sp$label == "c" | sp$label == "p", ]$AuthorTimeNormalized,
  acp_ratios[["c"]] + acp_ratios[["p"]], kernel = use_kernel)
# .. and the overall frequency:
dens_acp <- densitySafe(
  sp$AuthorTimeNormalized, kernel = use_kernel)

use_acp_attr <- c("min", "max", "ratio", "ymax")
acp_attr <- rbind(
  data.frame(attributes(dens_a)[use_acp_attr]),
  data.frame(attributes(dens_c)[use_acp_attr]),
  data.frame(attributes(dens_p)[use_acp_attr]))
```

## Add Source Code density

The source code density is a different kind of variable. Like any other metric, its sampling frequency has no effect on the density, but rather its value. Sampling more frequently will only increase precision. So while we estimated a Kernel for the activities, for all other variables that are metrics, the density (or curve) depends on the observed value, not on the frequency of observations.

```{r}
# scd is source code density
dens_scd_data <- data.frame(
  x = sp$AuthorTimeNormalized,
  y = sp$Density
)

# We want to smooth the density data a little:
temp <- stats::loess.smooth(
  x = dens_scd_data$x, y = dens_scd_data$y, span = .07, family = "sym", degree = 1, evaluation = nrow(dens_scd_data))
dens_scd_data$x <- temp$x
dens_scd_data$y <- temp$y - min(temp$y)
dens_scd_data$y <- dens_scd_data$y / max(dens_scd_data$y)

dens_scd <- (function() {
  r <- range(dens_scd_data$x)
  temp <- stats::approxfun(x = dens_scd_data$x, y = dens_scd_data$y, ties = "ordered")
  f1 <- Vectorize(function(x) {
    if (x < r[1] || x > r[2]) {
      return(NaN)
    }
    return(temp(x))
  })
  
  attributes(f1) <- list(
    min = r[1], max = r[2], x = dens_scd_data$x, y = dens_scd_data$y
  )
  
  f1
})()
```



This is how the project looks. Please note that we have already cut off all activity after August 31 2020.

```{r}
ggplot(data.frame(x = range(acp_attr$min, acp_attr$max)), aes(x)) +
  stat_function(fun = dens_a, aes(color="A"), size = 1, n = 2^11) +
  #stat_function(fun = dens_c, aes(color="C"), size = 1, n = 2^11) +
  #stat_function(fun = dens_p, aes(color="P"), size = 1, n = 2^11) +
  stat_function(fun = dens_cp, aes(color="C+P"), size = 1, n = 2^11) +
  stat_function(fun = dens_acp, aes(color="Freq"), size = 1, n = 2^11) +
  stat_function(fun = dens_scd, aes(color="SCD"), size = 1, n = 2^11) +
  theme_light() +
  labs(color = "Activity") +
  scale_color_brewer(palette = "Set1")
```

In the above plot, it appears that the initial perfective activities (in the approximate range of `[~-0..5, 0.2]`) may or may not belong to the project's actual work range. For example, it seems plausible that the project manager made some initial commit(s) with directions for the students. In a real-world analysis, one would need to decide whether to cut this off or not, based on the knowledge of what was going on. We allow the remainder of this notebook to make the switch using a variable. This will allow us to examine model-fits under different configurations.

_Note_: As of our suspicion, the very first commit is not the project's start, and the right decision for this project would to cut it off.

```{r}
# Use this obvious variable to control cutting off initial perfective commits.
CUT_OFF_PERFECTIVE_COMMITS_IN_BEGIN <- TRUE

dens_acp_x_min <- min(acp_attr$min)
if (CUT_OFF_PERFECTIVE_COMMITS_IN_BEGIN) {
  dens_acp_x_min <- min(acp_attr[1:2,]$min) # 1:2 are a,c; see above
}
dens_acp_x_max <- max(acp_attr$max) # we're not cutting off at end

dens_a_x_idx <- attributes(dens_a)$x >= dens_acp_x_min
dens_c_x_idx <- attributes(dens_c)$x >= dens_acp_x_min
dens_p_x_idx <- attributes(dens_p)$x >= dens_acp_x_min
dens_cp_x_idx <- attributes(dens_cp)$x >= dens_acp_x_min
dens_scd_x_idx <- attributes(dens_scd)$x >= dens_acp_x_min
dens_acp_x_idx <- attributes(dens_acp)$x >= dens_acp_x_min

dens_acp_data <- rbind(
  data.frame(
    x = attributes(dens_a)$x[dens_a_x_idx],
    y = attributes(dens_a)$y[dens_a_x_idx] * attributes(dens_a)$ratio,
    t = rep("A", sum(dens_a_x_idx))
  ),
#  data.frame(
#    x = attributes(dens_c)$x[dens_c_x_idx],
#    y = attributes(dens_c)$y[dens_c_x_idx] * attributes(dens_c)$ratio,
#    t = rep("C", sum(dens_c_x_idx))
#  ),
#  data.frame(
#    x = attributes(dens_p)$x[dens_p_x_idx],
#    y = attributes(dens_p)$y[dens_p_x_idx] * attributes(dens_p)$ratio,
#    t = rep("P", sum(dens_p_x_idx))
#  ),
  data.frame(
    x = attributes(dens_cp)$x[dens_cp_x_idx],
    y = attributes(dens_cp)$y[dens_cp_x_idx] * attributes(dens_cp)$ratio,
    t = rep("CP", sum(dens_cp_x_idx))
  ),
  data.frame(
    x = attributes(dens_acp)$x[dens_acp_x_idx],
    y = attributes(dens_acp)$y[dens_acp_x_idx],
    t = rep("FREQ", sum(dens_acp_x_idx))
  ),
  data.frame(
    x = attributes(dens_scd)$x[dens_scd_x_idx],
    y = attributes(dens_scd)$y[dens_scd_x_idx],
    t = rep("SCD", sum(dens_scd_x_idx))
  )
)
dens_acp_data$t <- factor(x = dens_acp_data$t, levels = unique(dens_acp_data$t), ordered = TRUE)


# Now it is important to scale the densities for a,c,p on the x-axis together,
# so that we get all three within [0,1].
dens_acp_data$x <- dens_acp_data$x - min(dens_acp_data$x)
dens_acp_data$x <- dens_acp_data$x / max(dens_acp_data$x)
```

## The Final project

Let's plot how that looks:

```{r}
ggplot(dens_acp_data, aes(x = x, y = y)) +
  geom_line(aes(color = t), size = .75) +
  theme_light() +
  labs(color = "Activity") +
  scale_color_brewer(palette = "Set1")
```

The above plot represents now the final project that we will fit our entire pattern to:

* The domain is now `[0,1]` -- note that the source code density was not estimated using KDE, so that we do not have an estimation for the bandwidth, which leads to initial inclines and declines at the end for such variables.
* The integral of the frequency is roughly `1` -- the sum of the three integrals for each of the three activities is also `1`. It is important that each activity has a density that is proportional to the amount of the other activities, and that in sum everything should become `1` again.






# References {-}

<div id="refs"></div>
