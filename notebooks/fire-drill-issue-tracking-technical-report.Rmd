---
title: "Technical Report: Detecting the Fire Drill anti-pattern using issue-tracking data"
author: "Sebastian HÃ¶nel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ../inst/REFERENCES.bib
urlcolor: blue
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  md_document:
    toc: yes
    toc_depth: 6
    df_print: kable
    variant: gfm
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: kable
  word_document: default
header-includes:
- \usepackage{bm}
- \usepackage{mathtools}
- \usepackage{xurl}
---

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\norm}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}
\newcommand\argmax[1]{\underset{#1}{arg\,max}}
\newcommand\argmin[1]{\underset{#1}{arg\,min}}


```{r setoptions, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(tidy = TRUE, tidy.opts = list(indent=2))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

library(ggplot2)
library(ggpubr)
```

# Introduction\label{tr:fire-drill-issue-tracking-technical-report}




# Importing the data

## Ground truth

We have $9$ projects conducted by students, and two raters have __independently__, i.e., without prior communication, assessed to what degree the AP is present in each project. This was done using a scale from zero to ten, where zero means that the AP was not present, and ten would indicate a strong manifestation The entire ground truth is shown in table \ref{tab:groundtruth}.

```{r}
ground_truth <- read.csv(file = "../data/ground-truth.csv", sep = ";")
ground_truth$consensus_score <- ground_truth$consensus / 10
```

```{r echo=FALSE}
if (interactive()) {
  ground_truth
} else {
  knitr::kable(
    x = ground_truth,
    booktabs = TRUE,
    caption = "Entire ground truth as of both raters",
    label = "groundtruth"
  )
}
```


## Project data

In this section we import the projects' __issue-tracking__-data. All projects' data will be normalized w.r.t. the time, i.e., each project will have a support of $[0,1]$. The variables are modeled as cumulative time spent on issues. Each variable in each project will be loaded into an instance of `Signal`.

```{r}
library(readxl)

load_project_issue_data <- function(pId) {
  data <- read_excel(
    "../data/FD_issue-based_detection.xlsx", sheet = pId)
  data$desc[is.na(data$desc)] <- 0
  
  req_cs <- cumsum(data$req) / sum(data$req)
  dev_cs <- cumsum(data$dev) / sum(data$dev)
  desc_cs <- cumsum(data$desc) / max(cumsum(data$dev))
  X <- seq(from = 0, to = 1, length.out = length(req_cs))
  
  signal_rec <- Signal$new(
    func = stats::approxfun(x = X, y = req_cs, yleft = 0, yright = 1),
    name = "REQ", support = c(0,1), isWp = TRUE)
  signal_dev <- Signal$new(
    func = stats::approxfun(x = X, y = dev_cs, yleft = 0, yright = 1),
    name = "DEV", support = c(0,1), isWp = TRUE)
  signal_desc <- Signal$new(
    func = stats::approxfun(x = X, y = desc_cs, yleft = 0, yright = max(desc_cs)),
    name = "DESC", support = c(0,1), isWp = TRUE)
  
  list(
    REQ = signal_rec,
    DEV = signal_dev,
    DESC = signal_desc
  )
}
```

Let's attempt to replicate the graphs of the first project (cf. figure \ref{fig:p1-example}):

```{r}
p3_signals <- load_project_issue_data(pId = "Project3")
req_f <- p3_signals$REQ$get0Function()
dev_f <- p3_signals$DEV$get0Function()
desc_f <- p3_signals$DESC$get0Function()
```


```{r p1-example, echo=FALSE, fig.cap="The two variables of the first project.", fig.align="center", fig.pos="ht!"}
curve(req_f, 0, 1, col = "blue", xlab = "Relative time", ylab = "Cumulative time spent")
curve(dev_f, 0, 1, col = "red", add = TRUE)
curve(desc_f, 0, 1, col = "gold", add = TRUE)
legend(0, 1, legend = c("cum. req%", "cum. dev%", "cum. desc%"), col = c("blue", "red", "gold"), lty = 1, lwd = 2)
```

OK, that works well. It'll be the same for all projects, i.e., only two variables, time spent on requirements- and time spent on development-issues, is tracked. That means we will only be fitting two variables later.

Let's load, store and visualize all projects (cf. figure \ref{fig:project-it-vars}):

```{r load-data}
all_signals <- list()
for (pId in paste0("Project", 1:9)) {
  all_signals[[pId]] <- load_project_issue_data(pId = pId)
}
```

```{r echo=FALSE}
tempdf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(tempdf) <- c("x", "y", "p", "v")
n <- 500
x <- seq(from = 0, to = 1, length.out = n)
for (pId in names(all_signals)) {
  for (v in c("REQ", "DEV", "DESC")) {
    tempdf <- rbind(tempdf, data.frame(
      x = x,
      y = sapply(X = x, FUN = all_signals[[pId]][[v]]$get0Function()),
      p = pId,
      v = v
    ))
  }
}
```

```{r project-it-vars, echo=FALSE, fig.cap="All variables over each project's time span.", fig.align="top", fig.pos="ht!"}
plot_all_req_dev <- ggplot(data = tempdf, aes(x = x, y = y, color = v)) +
  geom_line() +
  scale_color_manual(values = c("gold", "blue", "red")) +
  facet_wrap(p ~.) +
  theme_light() +
  labs(color = "Variable") + xlab("Relative Time") + ylab("Cumulative Time spent") +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"))
plot_all_req_dev
```

```{r echo=FALSE, eval=FALSE}
# Let's save this as tikz:
tikzDevice::tikz(file = "../figures/all_it_acp.tex", width = 3.4, height = 1.8)
plot_all_req_dev +
  theme_light(base_size = 10) +
  theme(
    axis.title.x.bottom = element_text(margin = margin(b=5), size = 10),
    axis.title.y.left = element_text(margin = margin(r=3), size = 10),
    legend.position = "bottom",
    legend.margin = margin(-7.5, 0, 0, 0),
    strip.background = element_rect(fill="#dfdfdf"))
dev.off()
```

# Patterns for scoring the projects


## Pattern I: Consensus of two experts

The initial pattern as defined for the detection of the Fire Drill AP is imported/created/defined here, and its variables and confidence intervals are modeled as continuous functions over time.

There are some values (x/y coordinates) for which we want to guarantee that the confidence intervals or the variables themselves pass through. Also, the two points in time $t_1,t_2$ are defined to be at $0.4$ and $0.85$, respectively.

```{r p1-constants}
t_1 <- 0.4
t_2 <- 0.85

# req(t_1)
req_t_1 <- 0.7

# dev(t_1), dev(t_2)
dev_t_1 <- 0.075
dev_t_2 <- 0.4
```

This initial version of the pattern is not based on any data, observation or ground truth, but solely on two independent experts that reached a consensus for every value a priori any of the detection approaches.


### Variable: Requirements, analysis, planning

The variable itself is not given, only its upper- and lower confidence-intervals (CI), where the latter simply is $\operatorname{req}^{\text{CI}}_{\text{lower}}(x)=x$. The upper CI is given by the informal expression $\operatorname{req}^{\text{CI}}_{\text{upper}}(x)=1.02261-1.02261\times\exp{(-3.811733\times x)}$. All together is shown in figure \ref{fig:req-cis}.

The variable itself is not given, as it was not important for the binary decision rule, whether or not a project's variable is within the confidence interval. It is still not important, what matters is that it runs through the confidence interval, and we will design it by fitting a polynomial through some inferred points from the plot. In some practical case however, the variable's course may be important, and while we will later use the variable to compute some kind of loss between it, the confidence interval and some project's variable, we only do this for demonstration purposes.

Let's first define the variable using some supporting x/y coordinates. It needs to be constrained such that it runs through 0,0 and 1,1:

```{r}
req_poly <- cobs::cobs(
  x = seq(from = 0, to = 1, by = .1),
  y = c(0, .25, .425, .475, .7, .8, .85, .9, .95, .975, 1),
  pointwise = matrix(data = c(
    c(0, 0, 0),
    c(0, t_1, req_t_1),
    c(0, 1, 1)
  ), byrow = TRUE, ncol = 3)
)

# Now we can define the variable simply by predicting from
# the polynomial (btw. this is vectorized automatically):
req <- function(x) {
  stats::predict(object = req_poly, z = x)[, "fit"]
}
```

And now for the confidence intervals:

```{r}
req_ci_lower <- function(x) x
req_ci_upper <- function(x) 1.02261 - 1.02261 * exp(-3.811733 * x)
```


```{r req-cis, echo=FALSE, fig.cap="req\\% and its lower- and upper confidence interval.", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = seq(from = 0, to = 1, length.out = 50),
  y = req_ci_upper(seq(from = 0, to = 1, length.out = 50)),
  col = "#ff000033",
  border = NA)
curve(req_ci_lower, 0, 1, col = "red", lty = 2, add = TRUE)
curve(req_ci_upper, 0, 1, col = "red", lty = 3, add = TRUE)
curve(req, 0, 1, col = "red", lty = 1, lwd = 2, add = TRUE)
legend(.74, .3, legend = c("upper CI", "lower CI", "req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))
abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")


# t_1:
abline(v = t_1)
text(x = .47, y = .98, paste0("t_1=", t_1))

# other thresholds:
abline(h = req(t_1), col = "#888888")
text(x = .5, y = .65, paste0("req(t_1)=", round(req(t_1), 2)))
abline(h = req_ci_upper(t_1), col = "#888888")
text(x = .24, y = .86, paste0("req_ci_upper(t_1)=", round(req_ci_upper(t_1), 2)))
```


### Variables: Design, implementation, testing, bugfixing and Descoping

Again, the variable for design etc. is not given, but rather only its upper confidence interval. Its lower CI is simply always zero. The upper CI is given by the informal expression $\operatorname{dev}^{\text{CI}}_{\text{upper}}(x)=0.07815904\times x+0.6222767\times x^2+0.2995643\times x^3$. The variable for de-scoping comes without confidence interval, and is defined by $\operatorname{desc}(x)=0.01172386\times x + 0.0933415\times x^2 + 0.04493464\times x^3$.

First we will define/fit a polynomial that describes the variable for design etc., the same way we did for requirements etc. we do know that it should pass through the points $[t_1,\approx0.075]$, as well as $[t_2,\approx0.4]$.

```{r}
dev_poly <- cobs::cobs(
  x = seq(from = 0, to = 1, by = .1),
  y = c(0, .0175, .035, .055, dev_t_1, .014, .165, .2, .28, .475, 1),
  pointwise = matrix(data = c(
    c(0, t_1, dev_t_1),
    c(0, t_2, dev_t_2),
    c(0, 1, 1)
  ), byrow = TRUE, ncol = 3)
)

# Now we can define the variable simply by predicting from
# the polynomial (btw. this is vectorized automatically):
dev <- function(x) {
  temp <- stats::predict(object = dev_poly, z = x)[, "fit"]
  # I cannot constrain the polynomial at 0,0 and it returns
  # a very slight negative value there, so let's do it this way:
  temp[temp < 0] <- 0
  temp[temp > 1] <- 1
  temp
}
```

Next we define the upper confidence interval for the variable `DEV`, then the variable for de-scoping. All is shown in figure \ref{fig:dev-desc-cis}.

```{r}
dev_ci_upper <- function(x) 0.07815904 * x + 0.6222767 * x^2 + 0.2995643 * x^3
desc <- function(x) 0.01172386 * x + 0.0933415 * x^2 + 0.04493464 * x^3
```


```{r dev-desc-cis, echo=FALSE, fig.cap="The variable dev\\% and its upper confidence interval, as well as the variable desc\\%.", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff33",
  border = NA)

curve(dev_ci_upper, 0, 1, col = "blue", lty = 2, add = TRUE)
curve(desc, 0, 1, add = TRUE, col = "#00cc00", lty = 1, lwd = 3)
curve(dev, 0, 1, col = "blue", lty = 1, lwd = 3, add = TRUE)
legend(.02, 1.01, legend = c("dev upper CI", "dev%", "desc%"),
       col = c("blue", "blue", "#00cc00"), lty = c(3, 1, 1), lwd = c(1, 3, 3))

abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")

# t_1,t_2:
abline(v = t_1)
text(x = .47, y = .98, paste0("t_1=", t_1))
abline(v = t_2)
text(x = .775, y = .98, paste0("t_2=", t_2))

# other thresholds:
abline(h = dev_ci_upper(t_2), col = "#888888")
text(x = .565, y = .76, paste0("dev_ci_upper(t_2)=", round(dev_ci_upper(t_2), 2)))
abline(h = desc(1), col = "#888888")
text(x = .94, y = .24, paste0("desc(1)\n   =", round(desc(1), 2)))

# t_1(dev), t_2(dev):
abline(h = dev(t_1), col = "#888888")
text(x = .29, y = .2, paste0("dev(t_1)=", round(dev(t_1), 3)))
abline(h = dev(t_2), col = "#888888")
text(x = .5, y = .46, paste0("dev(t_2)=", round(dev(t_2), 2)))
```


## Pattern II: Partial adaptation of first pattern

We will be attempting three kinds of adaptations to the first pattern:

a)    Learn $t_1,t_2$ from the data: There is not much to learn, but we could attempt to define these two thresholds as weighted average over the ground truth. Alternatively, we could formulate an optimization problem. We then use _time warping_ to alter the first pattern, __including__ its confidence intervals.
b)    Additionally to a (after learning $t_1,t_2$), we will apply _amplitude warping_ using __`srBTAW`__.
c)    Take the first pattern and apply both, _boundary time warping_ __and__ _boundary amplitude warping_, to produce a pattern that is (hopefully) closest to all projects in the ground truth. This is the very same approach we attempted for adapting the pattern of type I that we defined for the Fire Drill in source code.


### Type II (a): Adapt type I using thresholds $t_1,t_2$

The two variables `REQ` and `DEV` in the first pattern describe the cumulative time spent on two distinct activities. It was designed with focus on the confidence intervals, and a binary decision rule, such that the actual variables' course was not of interest.

To find the optimal value for a threshold, we could look at when each project is closest to $\operatorname{req}(t_1)$ and $\operatorname{dev}(t_2)$ (in relative time), and then compute a weighted average over it. However, since we already modeled each project's variables as *continuous-time stochastic process*, I suggest we use an optimization-based approach.

```{r t1t2-example}
tempf <- all_signals$Project5$REQ$get0Function()
tempf1 <- function(x) abs(tempf(x) - req_t_1)
optR <- optimize(tempf1, interval = c(0, 1))
optR
```

```{r t1t2-example-fig, echo=FALSE, fig.cap="The non-optimal optimum found by gradient-based optimization in project 5.", fig.align="center", fig.pos="ht!"}
curve(tempf1, 0, 1, xlab = "Relative time", ylab = "abs(REQ_p5(x) - req_t_1)")
points(x = optR$minimum, y = optR$objective,
       col = "red", pch = 4, cex = 1.5)
```

We will find the optimum using `nlopt` and a global optimization, because we actually will have a global optimum by re-arranging each project's variables. Also, gradient-based methods do not work well because of the nature of the variables, having large horizontal plateaus. This can be seen in figure \ref{fig:t1t2-example-fig}. Approaches using the built-in `optim` do hence not work well, the problem is clearly demonstrated in the previous code chunk, resulting in an objective $\gg0$ (which should ideally be $0$).

We want to find out when each project is closest to the previously defined thresholds. Each variable is a cumulative aggregation of the underlying values, which means that we have monotonic behavior.

$$
\begin{aligned}
  \min_{\hat{t}_1,\hat{t}_2\in R}&\;{\operatorname{req}(\hat{t}_1), \operatorname{dev}(\hat{t}_2)}\;\text{,}
  \\[1ex]
  \text{subject to}&\;0\leq\hat{t}_1,\hat{t}_2\leq1\;\text{, using}
  \\[1ex]
  \mathcal{L}_{\operatorname{req}}(x)=&\;\norm{\operatorname{req}(x)-\operatorname{req}(t_1)}\;\text{, and}
  \\[1ex]
  \mathcal{L}_{\operatorname{dev}}(x)=&\;\norm{\operatorname{dev}(x)-\operatorname{dev}(t_2)}\;\text{(quasi-convex loss functions).}
\end{aligned}
$$

The objective functions hence will be to find the global optimum (minimum), which occurs at $y\approx0$. Since we have plateaus in our data, we will potentially have infinitely many global optima. However, we are satisfied with any that is $\approx0$.

$$
\begin{aligned}
  \mathcal{O}_{\operatorname{dev}}(x)=&\;\argmin{\hat{x}\in R}\;{L_{\operatorname{dev}}(x)}\;\text{, and}
  \\[1ex]
  \mathcal{O}_{\operatorname{req}}(x)=&\;\argmin{\hat{x}\in R}\;{L_{\operatorname{req}}(x)}\text{.}
\end{aligned}
$$


```{r t1t2-nlopt}
library(nloptr)

set.seed(1)

t1t2_opt <- matrix(ncol = 4, nrow = length(all_signals))
rownames(t1t2_opt) <- names(all_signals)
colnames(t1t2_opt) <- c("req_sol", "dev_sol", "req_obj", "dev_obj")

find_global_low <- function(f) {
  nloptr(
    x0 = 0.5,
    opts = list(
      maxeval = 1e3,
      algorithm = "NLOPT_GN_DIRECT_L_RAND"),
    eval_f = f,
    lb = 0,
    ub = 1
  )
}

```{r eval=FALSE}
for (i in 20:10000) {
  set.seed(i)
  d <- rnorm(5e3)
  bla1 <- shapiro.test(d)
  bla2 <- moments::agostino.test(d)
  bla3 <- nortest::ad.test(d)
  bla4 <- nortest::lillie.test(d)
  bla5 <- nortest::pearson.test(d)
  bla6 <- nortest::sf.test(d)
  
  if (all(unlist(lapply(list(bla1, bla2, bla3, bla4, bla5, bla6), function(b) b$p.value)) < 0.025)) break
}
i
```

```{r eval=FALSE}
data(akima)
plot(y ~ x, data = akima, main = "akima example data")
with(akima, text(x, y, formatC(z,dig=2), adj = -0.1))

## linear interpolation
akima.li <- interp(akima$x, akima$y, akima$z)
li.zmin <- min(akima.li$z,na.rm=TRUE)
li.zmax <- max(akima.li$z,na.rm=TRUE)
breaks <- pretty(c(li.zmin,li.zmax),10)
colors <- heat.colors(length(breaks)-1)
with(akima.li, image  (x,y,z, breaks=breaks, col=colors))
with(akima.li,contour(x,y,z, levels=breaks, add=TRUE))
points (akima, pch = 3)
```

```{r eval=FALSE}
akima.smooth <-
    with(akima, interp(x, y, z, nx=100, ny=100))
si.zmin <- min(akima.smooth$z,na.rm=TRUE)
si.zmax <- max(akima.smooth$z,na.rm=TRUE)
breaks <- pretty(c(si.zmin,si.zmax),10)
colors <- heat.colors(length(breaks)-1)

image  (akima.smooth, main = "interp(<akima data>, *) on finer grid",
        breaks=breaks, col=colors)
contour(akima.smooth, add = TRUE, levels=breaks, col = "thistle")
points(akima, pch = 3, cex = 2, col = "blue")
```















