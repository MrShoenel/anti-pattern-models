---
title: "Technical Report: Detecting the Fire Drill anti-pattern using issue-tracking data"
author: "Sebastian HÃ¶nel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ../inst/REFERENCES.bib
urlcolor: blue
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  md_document:
    toc: yes
    toc_depth: 6
    df_print: kable
    variant: gfm
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: kable
  word_document: default
header-includes:
- \usepackage{bm}
- \usepackage{mathtools}
- \usepackage{xurl}
---

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\norm}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}
\newcommand\argmax[1]{\underset{#1}{arg\,max}}
\newcommand\argmin[1]{\underset{#1}{arg\,min}}


```{r setoptions, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(tidy = TRUE, tidy.opts = list(indent=2))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

library(ggplot2)
library(ggpubr)
```

# Introduction\label{tr:fire-drill-issue-tracking-technical-report}




# Importing the data

## Ground truth

We have $9$ projects conducted by students, and two raters have __independently__, i.e., without prior communication, assessed to what degree the AP is present in each project. This was done using a scale from zero to ten, where zero means that the AP was not present, and ten would indicate a strong manifestation The entire ground truth is shown in table \ref{tab:groundtruth}.

```{r}
ground_truth <- read.csv(file = "../data/ground-truth.csv", sep = ";")
ground_truth$consensus_score <- ground_truth$consensus / 10
```

```{r echo=FALSE}
if (interactive()) {
  ground_truth
} else {
  knitr::kable(
    x = ground_truth,
    booktabs = TRUE,
    caption = "Entire ground truth as of both raters",
    label = "groundtruth"
  )
}
```


## Project data

In this section we import the projects' __issue-tracking__-data. All projects' data will be normalized w.r.t. the time, i.e., each project will have a support of $[0,1]$. The variables are modeled as cumulative time spent on issues. Each variable in each project will be loaded into an instance of `Signal`.

```{r}
library(readxl)

load_project_issue_data <- function(pId) {
  data <- read_excel(
    "../data/FD_issue-based_detection.xlsx", sheet = pId)
  data[is.na(data)] <- 0
  
  req_cs <- cumsum(data$req) / sum(data$req)
  dev_cs <- cumsum(data$dev) / sum(data$dev)
  desc_cs <- cumsum(data$desc) / max(cumsum(data$dev))
  X <- seq(from = 0, to = 1, length.out = length(req_cs))
  
  signal_rec <- Signal$new(
    func = stats::approxfun(x = X, y = req_cs, yleft = 0, yright = 1),
    name = "REQ", support = c(0,1), isWp = TRUE)
  signal_dev <- Signal$new(
    func = stats::approxfun(x = X, y = dev_cs, yleft = 0, yright = 1),
    name = "DEV", support = c(0,1), isWp = TRUE)
  signal_desc <- Signal$new(
    func = stats::approxfun(x = X, y = desc_cs, yleft = 0, yright = max(desc_cs)),
    name = "DESC", support = c(0,1), isWp = TRUE)
  
  list(
    data = data,
    REQ = signal_rec,
    DEV = signal_dev,
    DESC = signal_desc
  )
}
```

Let's attempt to replicate the graphs of the first project (cf. figure \ref{fig:p1-example}):

```{r}
p3_signals <- load_project_issue_data(pId = "Project3")
req_f <- p3_signals$REQ$get0Function()
dev_f <- p3_signals$DEV$get0Function()
desc_f <- p3_signals$DESC$get0Function()
```


```{r p1-example, echo=FALSE, fig.cap="The two variables of the first project.", fig.align="center", fig.pos="ht!"}
curve(req_f, 0, 1, col = "blue", xlab = "Relative time", ylab = "Cumulative time spent")
curve(dev_f, 0, 1, col = "red", add = TRUE)
curve(desc_f, 0, 1, col = "gold", add = TRUE)
legend(0, 1, legend = c("cum. req%", "cum. dev%", "cum. desc%"), col = c("blue", "red", "gold"), lty = 1, lwd = 2)
```

OK, that works well. It'll be the same for all projects, i.e., only two variables, time spent on requirements- and time spent on development-issues, is tracked. That means we will only be fitting two variables later.

Let's load, store and visualize all projects (cf. figure \ref{fig:project-it-vars}):

```{r load-data}
all_signals <- list()
for (pId in paste0("Project", 1:9)) {
  all_signals[[pId]] <- load_project_issue_data(pId = pId)
}
```

```{r echo=FALSE}
tempdf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(tempdf) <- c("x", "y", "p", "v")
n <- 500
x <- seq(from = 0, to = 1, length.out = n)
for (pId in names(all_signals)) {
  for (v in c("REQ", "DEV", "DESC")) {
    tempdf <- rbind(tempdf, data.frame(
      x = x,
      y = sapply(X = x, FUN = all_signals[[pId]][[v]]$get0Function()),
      p = pId,
      v = v
    ))
  }
}
```

```{r project-it-vars, echo=FALSE, fig.cap="All variables over each project's time span.", fig.align="top", fig.pos="ht!"}
plot_all_req_dev <- ggplot(data = tempdf, aes(x = x, y = y, color = v)) +
  geom_line() +
  scale_color_manual(values = c("forestgreen", "blue", "red")) +
  facet_wrap(p ~.) +
  theme_light() +
  labs(color = "Variable") + xlab("Relative Time") + ylab("Cumulative Time spent") +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"))
plot_all_req_dev
```

```{r echo=FALSE, eval=FALSE}
# Let's save this as tikz:
tikzDevice::tikz(file = "../figures/all_it_acp.tex", width = 3.4, height = 1.8)
plot_all_req_dev +
  theme_light(base_size = 10) +
  theme(
    axis.title.x.bottom = element_text(margin = margin(b=5), size = 10),
    axis.title.y.left = element_text(margin = margin(r=3), size = 10),
    legend.position = "bottom",
    legend.margin = margin(-7.5, 0, 0, 0),
    strip.background = element_rect(fill="#dfdfdf"))
dev.off()
```

# Patterns for scoring the projects


## Pattern I: Consensus of two experts

The initial pattern as defined for the detection of the Fire Drill AP is imported/created/defined here, and its variables and confidence intervals are modeled as continuous functions over time.

There are some values (x/y coordinates) for which we want to guarantee that the confidence intervals or the variables themselves pass through. Also, the two points in time $t_1,t_2$ are defined to be at $0.4$ and $0.85$, respectively.

```{r p1-constants}
t_1 <- 0.4
t_2 <- 0.85

# req(t_1)
req_t_1 <- 0.7

# dev(t_1), dev(t_2)
dev_t_1 <- 0.075
dev_t_2 <- 0.4
```

This initial version of the pattern is not based on any data, observation or ground truth, but solely on two independent experts that reached a consensus for every value a priori any of the detection approaches.


### Variable: Requirements, analysis, planning

The variable itself is not given, only its upper- and lower confidence-intervals (CI), where the latter simply is $\operatorname{req}^{\text{CI}}_{\text{lower}}(x)=x$. The upper CI is given by the informal expression $\operatorname{req}^{\text{CI}}_{\text{upper}}(x)=1.02261-1.02261\times\exp{(-3.811733\times x)}$. All together is shown in figure \ref{fig:req-cis}.

The variable itself is not given, as it was not important for the binary decision rule, whether or not a project's variable is within the confidence interval. It is still not important, what matters is that it runs through the confidence interval, and we will design it by fitting a polynomial through some inferred points from the plot. In some practical case however, the variable's course may be important, and while we will later use the variable to compute some kind of loss between it, the confidence interval and some project's variable, we only do this for demonstration purposes.

Let's first define the variable using some supporting x/y coordinates. It needs to be constrained such that it runs through 0,0 and 1,1:

```{r}
req_poly <- cobs::cobs(
  x = seq(from = 0, to = 1, by = .1),
  y = c(0, .25, .425, .475, .7, .8, .85, .9, .95, .975, 1),
  pointwise = matrix(data = c(
    c(0, 0, 0),
    c(0, t_1, req_t_1),
    c(0, 1, 1)
  ), byrow = TRUE, ncol = 3)
)

# Now we can define the variable simply by predicting from
# the polynomial (btw. this is vectorized automatically):
req <- function(x) {
  stats::predict(object = req_poly, z = x)[, "fit"]
}
```

And now for the confidence intervals:

```{r}
req_ci_lower <- function(x) x
req_ci_upper <- function(x) 1.02261 - 1.02261 * exp(-3.811733 * x)
```


```{r req-cis, echo=FALSE, fig.cap="req\\% and its lower- and upper confidence interval.", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = seq(from = 0, to = 1, length.out = 50),
  y = req_ci_upper(seq(from = 0, to = 1, length.out = 50)),
  col = "#ff000033",
  border = NA)
curve(req_ci_lower, 0, 1, col = "red", lty = 2, add = TRUE)
curve(req_ci_upper, 0, 1, col = "red", lty = 3, add = TRUE)
curve(req, 0, 1, col = "red", lty = 1, lwd = 2, add = TRUE)
legend(.74, .3, legend = c("upper CI", "lower CI", "req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))
abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")


# t_1:
abline(v = t_1)
text(x = .47, y = .98, paste0("t_1=", t_1))

# other thresholds:
abline(h = req(t_1), col = "#888888")
text(x = .5, y = .65, paste0("req(t_1)=", round(req(t_1), 2)))
abline(h = req_ci_upper(t_1), col = "#888888")
text(x = .24, y = .86, paste0("req_ci_upper(t_1)=", round(req_ci_upper(t_1), 2)))
```


### Variables: Design, implementation, testing, bugfixing and Descoping

Again, the variable for design etc. is not given, but rather only its upper confidence interval. Its lower CI is simply always zero. The upper CI is given by the informal expression $\operatorname{dev}^{\text{CI}}_{\text{upper}}(x)=0.07815904\times x+0.6222767\times x^2+0.2995643\times x^3$. The variable for de-scoping comes without confidence interval, and is defined by $\operatorname{desc}(x)=0.01172386\times x + 0.0933415\times x^2 + 0.04493464\times x^3$.

First we will define/fit a polynomial that describes the variable for design etc., the same way we did for requirements etc. we do know that it should pass through the points $[t_1,\approx0.075]$, as well as $[t_2,\approx0.4]$.

```{r}
dev_poly <- cobs::cobs(
  x = seq(from = 0, to = 1, by = .1),
  y = c(0, .0175, .035, .055, dev_t_1, .014, .165, .2, .28, .475, 1),
  print.warn = FALSE,
  print.mesg = FALSE,
  pointwise = matrix(data = c(
    c(0, t_1, dev_t_1),
    c(0, t_2, dev_t_2),
    c(0, 1, 1)
  ), byrow = TRUE, ncol = 3)
)

# Now we can define the variable simply by predicting from
# the polynomial (btw. this is vectorized automatically):
dev <- function(x) {
  temp <- stats::predict(object = dev_poly, z = x)[, "fit"]
  # I cannot constrain the polynomial at 0,0 and it returns
  # a very slight negative value there, so let's do it this way:
  temp[temp < 0] <- 0
  temp[temp > 1] <- 1
  temp
}
```

Next we define the upper confidence interval for the variable `DEV`, then the variable for de-scoping. All is shown in figure \ref{fig:dev-desc-cis}.

```{r}
dev_ci_upper <- function(x) 0.07815904 * x + 0.6222767 * x^2 + 0.2995643 * x^3
desc <- function(x) 0.01172386 * x + 0.0933415 * x^2 + 0.04493464 * x^3
```


```{r dev-desc-cis, echo=FALSE, fig.cap="The variable dev\\% and its upper confidence interval, as well as the variable desc\\%.", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff33",
  border = NA)

curve(dev_ci_upper, 0, 1, col = "blue", lty = 2, add = TRUE)
curve(desc, 0, 1, add = TRUE, col = "forestgreen", lty = 1, lwd = 3)
curve(dev, 0, 1, col = "blue", lty = 1, lwd = 3, add = TRUE)
legend(.02, 1.01, legend = c("dev upper CI", "dev%", "desc%"),
       col = c("blue", "blue", "forestgreen"), lty = c(3, 1, 1), lwd = c(1, 3, 3))

abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")

# t_1,t_2:
abline(v = t_1)
text(x = .47, y = .98, paste0("t_1=", t_1))
abline(v = t_2)
text(x = .775, y = .98, paste0("t_2=", t_2))

# other thresholds:
abline(h = dev_ci_upper(t_2), col = "#888888")
text(x = .565, y = .76, paste0("dev_ci_upper(t_2)=", round(dev_ci_upper(t_2), 2)))
abline(h = desc(1), col = "#888888")
text(x = .94, y = .24, paste0("desc(1)\n   =", round(desc(1), 2)))

# t_1(dev), t_2(dev):
abline(h = dev(t_1), col = "#888888")
text(x = .29, y = .2, paste0("dev(t_1)=", round(dev(t_1), 3)))
abline(h = dev(t_2), col = "#888888")
text(x = .5, y = .46, paste0("dev(t_2)=", round(dev(t_2), 2)))
```


## Pattern II: Partial adaptation of first pattern

We will be attempting three kinds of adaptations to the first pattern:

a)    Learn $t_1,t_2$ from the data: There is not much to learn, but we could attempt to define these two thresholds as weighted average over the ground truth. Alternatively, we could formulate an optimization problem. We then use _time warping_ to alter the first pattern, __including__ its confidence intervals.
b)    Additionally to a (after learning $t_1,t_2$), we will apply _amplitude warping_ using __`srBTAW`__.
c)    Take the first pattern and apply both, _boundary time warping_ __and__ _boundary amplitude warping_, to produce a pattern that is (hopefully) closest to all projects in the ground truth. This is the very same approach we attempted for adapting the pattern of type I that we defined for the Fire Drill in source code.


### Type II (a): Adapt type I using thresholds $t_1,t_2$

The two variables `REQ` and `DEV` in the first pattern describe the cumulative time spent on two distinct activities. It was designed with focus on the confidence intervals, and a binary decision rule, such that the actual variables' course was not of interest.

To find the optimal value for a threshold, we could look at when each project is closest to $\operatorname{req}(t_1)$ and $\operatorname{dev}(t_2)$ (in relative time), and then compute a weighted average over it. However, since we already modeled each project's variables as *continuous-time stochastic process*, I suggest we use an optimization-based approach.

```{r t1t2-example}
tempf <- all_signals$Project5$REQ$get0Function()
tempf1 <- function(x) abs(tempf(x) - req_t_1)
optR <- optimize(tempf1, interval = c(0, 1))
optR
```

```{r t1t2-example-fig, echo=FALSE, fig.cap="The non-optimal optimum found by gradient-based optimization in project 5.", fig.align="center", fig.pos="ht!"}
curve(tempf1, 0, 1, xlab = "Relative time", ylab = "abs(REQ_p5(x) - req_t_1)")
points(x = optR$minimum, y = optR$objective,
       col = "red", pch = 4, cex = 1.5)
```

We will find the optimum using `nlopt` and a global optimization, because we actually will have a global optimum by re-arranging each project's variables. Also, gradient-based methods do not work well because of the nature of the variables, having large horizontal plateaus. This can be seen in figure \ref{fig:t1t2-example-fig}. Approaches using the built-in `optim` do hence not work well, the problem is clearly demonstrated in the previous code chunk, resulting in an objective $\gg0$ (which should ideally be $0$).

We want to find out when each project is closest to the previously defined thresholds. Each variable is a cumulative aggregation of the underlying values, which means that we have monotonic behavior.

$$
\begin{aligned}
  \min_{\hat{t}_1,\hat{t}_2\in R}&\;{\operatorname{req}(\hat{t}_1), \operatorname{dev}(\hat{t}_2)}\;\text{,}
  \\[1ex]
  \text{subject to}&\;0\leq\hat{t}_1,\hat{t}_2\leq1\;\text{, using}
  \\[1ex]
  \mathcal{L}_{\operatorname{req}}(x)=&\;\norm{\operatorname{req}(x)-\operatorname{req}(t_1)}\;\text{, and}
  \\[1ex]
  \mathcal{L}_{\operatorname{dev}}(x)=&\;\norm{\operatorname{dev}(x)-\operatorname{dev}(t_2)}\;\text{(quasi-convex loss functions).}
\end{aligned}
$$

The objective functions hence will be to find the global optimum (minimum), which occurs at $y\approx0$. Since we have plateaus in our data, we will potentially have infinitely many global optima. However, we are satisfied with any that is $\approx0$.

$$
\begin{aligned}
  \mathcal{O}_{\operatorname{dev}}(x)=&\;\argmin{\hat{x}\in R}\;{L_{\operatorname{dev}}(x)}\;\text{, and}
  \\[1ex]
  \mathcal{O}_{\operatorname{req}}(x)=&\;\argmin{\hat{x}\in R}\;{L_{\operatorname{req}}(x)}\text{.}
\end{aligned}
$$


```{r t1t2-nlopt}
library(nloptr)

set.seed(1)

t1t2_opt <- matrix(ncol = 4, nrow = length(all_signals))
rownames(t1t2_opt) <- names(all_signals)
colnames(t1t2_opt) <- c("req_sol", "dev_sol", "req_obj", "dev_obj")

find_global_low <- function(f) {
  nloptr(
    x0 = 0.5,
    opts = list(
      maxeval = 1e3,
      algorithm = "NLOPT_GN_DIRECT_L_RAND"),
    eval_f = f,
    lb = 0,
    ub = 1
  )
}

for (pId in paste0("Project", 1:9)) {
  sig_REQ <- all_signals[[pId]]$REQ$get0Function()
  req_abs <- function(x) abs(sig_REQ(x) - req_t_1)
  
  sig_DEV <- all_signals[[pId]]$DEV$get0Function()
  dev_abs <- function(x) abs(sig_DEV(x) - dev_t_2)
  
  optRes_REQ <- find_global_low(f = req_abs)
  optRes_DEV <- find_global_low(f = dev_abs)
  
  t1t2_opt[pId, ] <- c(optRes_REQ$solution, optRes_DEV$solution,
                       optRes_REQ$objective, optRes_DEV$objective)
}
```


```{r echo=FALSE}
if (interactive()) {
  t1t2_opt
} else {
  knitr::kable(
    x = t1t2_opt,
    booktabs = TRUE,
    caption = "Optimum values for $t_1$ and $t_2$ for each project, together with the loss of each project's objective function at that offset (ideally 0).",
    label = "t1t2-optvals"
  )
}
```

From the table \ref{tab:t1t2-optvals}, we can take an example to demonstrate that the optimization indeed found the global optimum (or a value very close to it, usually with a deviation $<1e^{-15}$). For the picked example of project 5, we can clearly observe a value for $x$ that results in a solution that is $\approx0$.

```{r test-test, echo=FALSE, fig.cap="Example of finding the optimum for req($t_1$) in project 5.", fig.align="center", fig.pos="ht!"}
pId <- "Project5"
tempf <- function(x) {
  abs(all_signals[[pId]]$REQ$get0Function()(x) - req_t_1)
}
curve(tempf, 0, 1)
points(x = t1t2_opt[pId, 1], y = t1t2_opt[pId, 3],
       col = "red", pch = 4, cex = 1.5)
```

Figure \ref{fig:test-test} depicts the optimal solution as found by using global optimization. Now what is left, is to calculate the weighted average for the optimized $\bm{\hat{t}_1},\bm{\hat{t}_2}$. The weighted arithmetic mean is defined as:

$$
\begin{aligned}
  \text{weighted mean}=&\;\Big[\sum\bm{\omega}\Big]^{-1}\times\bm{\omega}^\top\cdot\bm{\hat{t}}\;\text{, where}
  \\[1ex]
  \bm{\omega}\dots&\;\text{weight vector that corresponds to the consensus-score, and}
  \\[1ex]
  \bm{\hat{t}}\dots&\;\text{vector with optimal values for either}\;t_1\;\text{or}\;t_2\;\text{(as learned earlier).}
\end{aligned}
$$

```{r}
omega <- ground_truth$consensus_score
names(omega) <- paste0("Project", 1:length(omega))

t1_wavg <- t1t2_opt[, 1] %*% omega / sum(omega)
print(c(t_1, t1_wavg))
t2_wavg <- t1t2_opt[, 2] %*% omega / sum(omega)
print(c(t_2, t2_wavg))
```

Originally, $t_1$ was guessed to be located at `r t_1`, with $\operatorname{req}(t_1)=0.7$. The weighted average over the optimized values (where $\operatorname{req}(\hat{t}_1)\approx0.7$) suggests defining $\hat{t}_1$=`r round(t1_wavg, 5)`.

$t_2$ on the other hand was originally located at `r t_2`, with $\operatorname{dev}(t_2)=0.4$. The weighted average over the optimized values (where $\operatorname{dev}(\hat{t}_2)\approx0.4$) suggests defining $\hat{t}_2$=`r round(t2_wavg, 5)`.

Having learned these values, we can now adapt the pattern using time warping. For that, we have to instantiate the pattern using `srBTAW`, add the original boundaries and set them according to what we learned. Below, we define a function that can warp a single variable.

```{r}
timewarp_variable <- function(f, t, t_new) {
  temp <- SRBTW$new(
    wp = f, wc = f,
    theta_b = c(0, t_new, 1),
    gamma_bed = c(0, 1, sqrt(.Machine$double.eps)),
    lambda = c(0, 0),
    begin = 0, end = 1,
    openBegin = FALSE, openEnd = FALSE)
  temp$setParams(`names<-`(c(t, 1 - t), c("vtl_1", "vtl_2")))
  function(x) sapply(X = x, FUN = temp$M)
}
```

```{r}
req_p2a <- timewarp_variable(f = req, t = t_1, t_new = t1_wavg)
req_ci_lower_p2a <- timewarp_variable(f = req_ci_lower, t = t_1, t_new = t1_wavg)
req_ci_upper_p2a <- timewarp_variable(f = req_ci_upper, t = t_1, t_new = t1_wavg)
```


```{r req-p1a-cis, echo=FALSE, fig.cap="req\\% and its lower- and upper confidence interval after time warping for pattern type I (a).", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), t1_wavg),
  y = c(req_ci_upper_p2a(seq(from = 0, to = 1, length.out = 50)), req_ci_lower_p2a(t1_wavg)),
  col = "#ff000033",
  border = NA)
curve(req_ci_lower_p2a, 0, 1, col = "red", lty = 2, add = TRUE)
curve(req_ci_upper_p2a, 0, 1, col = "red", lty = 3, add = TRUE)
curve(req_p2a, 0, 1, col = "red", lty = 1, lwd = 2, add = TRUE)
legend(.65, .3, legend = c("Pt. I(a) upper CI", "Pt. I(a) lower CI", "Pt. I(a) req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))
abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")


# t1_wavg:
abline(v = t1_wavg)
text(x = .42, y = .93, paste0("t1_wavg=", round(t1_wavg, 4)))
```

Moving the boundary $t_1$ farther behind, changes the variable `REQ` and its confidence interval slightly, as can be seen in figure \ref{fig:req-p1a-cis}. Next, we will adapt the remaining variables and their confidence intervals.

```{r}
dev_p2a <- timewarp_variable(f = dev, t = t_2, t_new = t2_wavg)
dev_ci_upper_p2a <- timewarp_variable(f = dev_ci_upper, t = t_2, t_new = t2_wavg)
desc_p2a <- timewarp_variable(f = desc, t = t_2, t_new = t2_wavg)
```

```{r dev-desc-p1a-cis, echo=FALSE, fig.cap="The variable dev\\% and its upper confidence interval, as well as the variable desc\\%, after time warping for pattern type I (a).", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper_p2a(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff33",
  border = NA)

curve(dev_ci_upper_p2a, 0, 1, col = "blue", lty = 2, add = TRUE)
curve(desc_p2a, 0, 1, add = TRUE, col = "forestgreen", lty = 1, lwd = 3)
curve(dev_p2a, 0, 1, col = "blue", lty = 1, lwd = 3, add = TRUE)
legend(.02, 1.01, legend = c("Pt. I(a) dev upper CI", "Pt. I(a) dev%", "Pt. I(a) desc%"),
       col = c("blue", "blue", "forestgreen"), lty = c(3, 1, 1), lwd = c(1, 3, 3))

abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")

# t2_wavg:
abline(v = t2_wavg)
text(x = .71, y = .94, paste0("t2_wavg=", round(t2_wavg, 4)))
```

Moving the boundary $t_2$ was a more significant change for variables `DEV` and `DESC` than it was for `REQ`, as of figure \ref{fig:dev-desc-p1a-cis}.



## Pattern III: Averaging the ground truth

This is the same approach we undertook for pattern type III (average) for the Fire Drill in source code. However, we will also learn an __empirical confidence interval__, which is later used for two additional detection methods. These methods have the advantage that they work over arbitrary (integration) intervals, making them also applicable for early detection of the process (i.e., not the entire process needs to be observed, and we can just attempt to detect what we have so far).


```{r}
p3_weighted_var <- function(name, omega) {
  funcs <- list()
  for (pId in names(all_signals)) {
    funcs[[pId]] <- all_signals[[pId]][[name]]$get0Function()
  }
  
  function(x) sapply(X = x, FUN = function(x_) {
    omega %*% unlist(lapply(funcs, function(f) f(x_))) / sum(omega)
  })
}
```

```{r}
req_p3 <- p3_weighted_var(name = "REQ", omega = omega)
dev_p3 <- p3_weighted_var(name = "DEV", omega = omega)
desc_p3 <- p3_weighted_var(name = "DESC", omega = omega)
```

The computed weighted average-variables for `REQ` and `DEV` are shown in figure \ref{fig:avg-req-dev}.

```{r avg-req-dev, echo=FALSE, fig.cap="The weighted average for the two variables req\\% and dev\\%.", fig.align="center", fig.pos="ht!"}
curve(req_p3, 0, 1, col = "red", xlab = "Relative time", ylab = "Cumulative time spent")
curve(dev_p3, 0, 1, col = "blue", add = TRUE)
curve(desc_p3, 0, 1, col = "forestgreen", add = TRUE)
legend(0, 1, legend = c("cum. emp. req%", "cum. emp. dev%", "cum. emp. desc%"), col = c("blue", "red", "forestgreen"), lty = 1, lwd = 2)
```

### Determining an empirical and inhomogeneous confidence interval

Also, we want to calculate an empirical confidence interval and -surface, based on the projects' data and the consensus of the ground truth. The boundary of the lower confidence interval is defined as the infimum of all signals (and the upper CI as the supremum of all signals):

$$
\begin{aligned}
  \bm{f}\dots&\;\text{vector of functions (here: project signals),}
  \\[1ex]
  \operatorname{CI}_{\text{upper}}(x)=&\;\sup{\Bigg(\forall\,f\in\bm{f}\;\bigg[\begin{cases}
    -\infty,&\text{if}\;\bm{\omega}_n=0,
    \\
    f_n(x),&\text{otherwise}
  \end{cases}\bigg]\;,\;\frown\;,\;\Big[\dots\Big]\Bigg)}\;\text{,}
  \\[1ex]
  \operatorname{CI}_{\text{upper}}(x)=&\;\inf{\Bigg(\forall\,f\in\bm{f}\;\bigg[\begin{cases}
    \infty,&\text{if}\;\bm{\omega}_n=0,
    \\
    f_n(x),&\text{otherwise}
  \end{cases}\bigg]\;,\;\frown\;,\;\Big[\dots\Big]\Bigg)}\;\text{.}
\end{aligned}
$$

```{r}
funclist_REQ <- list()
funclist_DEV <- list()
funclist_DESC <- list()
for (pId in names(all_signals)) {
  funclist_REQ[[pId]] <- all_signals[[pId]]$REQ$get0Function()
  funclist_DEV[[pId]] <- all_signals[[pId]]$DEV$get0Function()
  funclist_DESC[[pId]] <- all_signals[[pId]]$DESC$get0Function()
}

CI_bound_p3avg <- function(x, funclist, omega, upper = TRUE) {
  sapply(X = x, FUN = function(x_) {
    val <- unlist(lapply(X = names(funclist), FUN = function(fname) {
      if (omega[fname] == 0) (if (upper) -Inf else Inf) else funclist[[fname]](x_)
    }))
    
    if (upper) max(val) else min(val)
  })
}

req_ci_upper_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_REQ, omega = omega, upper = TRUE)
req_ci_lower_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_REQ, omega = omega, upper = FALSE)
dev_ci_upper_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DEV, omega = omega, upper = TRUE)
dev_ci_lower_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DEV, omega = omega, upper = FALSE)
desc_ci_upper_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DESC, omega = omega, upper = TRUE)
desc_ci_lower_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DESC, omega = omega, upper = FALSE)
```

While the above expressions define the _boundaries_ of the lower and upper confidence intervals, we also need a function that interpolates in between. Recall that the CI of the first pattern was __homogeneous__, i.e., it provided no gradation, and was used for a binary decision rule. If we define a function that bases the strength of the confidence on the values of the ground truth of each project's variable, then it also means that with that pattern, all projects are included in the binary decision rule. Having gradation in the CI will allow us to make more probabilistic statements by computing some kind of score.

Figures \ref{fig:req-dev-p3avg-cis} and \ref{fig:desc-p3avg-cis} show the average variables and the empirical confidence intervals.

```{r req-dev-p3avg-cis, echo=FALSE, fig.height=8, fig.cap="Empirical (average) req\\% and dev\\% and their lower- and upper empirical weighted confidence intervals (here without gradation).", fig.align="center", fig.pos="ht!"}

par(mfrow=c(2,1))

plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     main = "Variable: REQ",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = seq(from = 0, to = 1, length.out = 50),
  y = req_ci_upper(seq(from = 0, to = 1, length.out = 50)),
  col = "#ff000011",
  border = NA)

polygon_x <- c(seq(from = 0, to = 1, length.out = 500), seq(from = 1, to = 0, length.out = 500))
polygon(
  x = polygon_x,
  y = c(req_ci_upper_p3avg(head(polygon_x, 500)), req_ci_lower_p3avg(tail(polygon_x, 500))),
  col = "#ff000044",
  border = NA)


curve(req_p3, 0, 1, col = "red", add = TRUE)

curve(req_ci_upper, 0, 1, col = "#ff000033", add = TRUE, lty = 2)
curve(req_ci_lower, 0, 1, col = "#ff000033", add = TRUE, lty = 3)

curve(req_ci_upper_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 2)
curve(req_ci_lower_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 3)


legend(0.5, .37, legend = c("Pt. 3 (avg) req% upper CI", "Pt. III (avg) req% lower CI", "Pt. III (avg) req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))




plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     main = "Variable: DEV",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff11",
  border = NA)

polygon(
  x = polygon_x,
  y = c(dev_ci_upper_p3avg(head(polygon_x, 500)), dev_ci_lower_p3avg(tail(polygon_x, 500))),
  col = "#0000ff44",
  border = NA)

curve(dev_p3, 0, 1, col = "blue", add = TRUE)

curve(dev_ci_upper, 0, 1, col = "#0000ff33", add = TRUE, lty = 2)

curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 3)

legend(0.01, .98, legend = c("Pt. 3 (avg) upper CI", "Pt. III (avg) lower CI", "Pt. III (avg) dev%"),
       col = "blue", lty = c(3, 2, 1), lwd = c(1, 1, 2))
```

```{r desc-p3avg-cis, echo=FALSE, fig.height=4, fig.cap="Empirical (average) desc\\% and its lower- and upper empirical weighted confidence intervals (here without gradation).", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     main = "Variable: DESC",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = polygon_x,
  y = c(desc_ci_upper_p3avg(head(polygon_x, 500)), desc_ci_lower_p3avg(tail(polygon_x, 500))),
  col = "#228B2244",
  border = NA)

curve(desc_p3, 0, 1, col = "forestgreen", add = TRUE)
curve(desc_ci_upper_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 2)
curve(desc_ci_lower_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 3)

legend(0.01, .98, legend = c("Pt. 3 (avg) desc% upper CI", "Pt. III (avg) desc% lower CI", "Pt. III (avg) desc%"),
       col = "forestgreen", lty = c(3, 2, 1), lwd = c(1, 1, 2))
```


But first, we define a function $f:R^2\mapsto R$ to compute a CI with gradation. For each x/y coordinate, it shall output a confidence based on the weights as of the ground truth, and all projects' variables that output a $y$ that is smaller than (larger than) the given $y$ shall be excluded.

$$
\begin{aligned}
  h_{\text{upper}}(f,x,y)=&\;\begin{cases}
    \begin{cases}
      f(x),&\text{if}\;f(x)\neq0,
      \\
      \epsilon,&\text{otherwise,}
    \end{cases},&\text{if}\;f(x)\geq y,
    \\
    0,&\text{otherwise}
  \end{cases}\;\text{, where}\;\epsilon\;\text{is small non-zero constant,}
  \\
  \operatorname{CI}^+(x,y)=&\;\bm{\omega}^\top\cdot h_{\text{upper}}(\bm{f},x,y)\times\Big[\sum\bm{\omega}\Big]^{-1}\text{.}
\end{aligned}
$$

$\operatorname{CI}^+$ is to be used for the upper confidence region, and likewise, we define $\operatorname{CI}^-$ to be equivalent, but it uses $h_{\text{lower}}$, that switches the condition to $f(x)\leq y$. The decision on whether to use $\operatorname{CI}^+$ or $\operatorname{CI}^-$ depends on whether the given $y$ is above or below the _computed average variable_, i.e.,

$$
\begin{aligned}
  \bar{g}(x)\dots&\;\text{the computed average variable,}
  \\[1ex]
  \operatorname{CI}(x,y)=&\;\begin{cases}
    0,&\text{if}\;y>\operatorname{CI}_{\text{upper}}(x)\;\text{,}
    \\
    0,&\text{if}\;y<\operatorname{CI}_{\text{lower}}(x)\;\text{,}
    \\
    \bm{\omega}^\top\cdot\Bigg[\begin{cases}
      h_{\text{upper}}(\bm{f},x,y),&\text{if}\;\bar{g}(x)<y,
      \\
      h_{\text{lower}}(\bm{f},x,y),&\text{otherwise}
    \end{cases}\Bigg]\times\Big[\sum\bm{\omega}\Big]^{-1},&\text{otherwise}
  \end{cases}\;\text{.}
\end{aligned}
$$

With this definition, we can compute a loss that is then based on a path that goes through this hyperplane. That path is a project's variable.

```{r}
h_p3avg <- function(funclist, x, y, upper = TRUE) {
  unlist(lapply(X = funclist, FUN = function(f) {
    sapply(X = f(x), function(val) {
      if (val == 0) val <- sqrt(.Machine$double.eps)
      if ((upper && val >= y) || (!upper && val <= y)) val else 0
    })
  }))
}

h_upper_p3avg <- function(funclist, x, y) h_p3avg(funclist = funclist, x = x, y = y, upper = TRUE)
h_lower_p3avg <- function(funclist, x, y) h_p3avg(funclist = funclist, x = x, y = y, upper = FALSE)

CI_p3avg <- function(x, y, funclist, f_ci_upper, f_ci_lower, gbar, omega) {
  stopifnot(length(x) == length(y))
  
  sapply(X = seq_len(length.out = length(x)), FUN = function(idx) {
    xi <- x[idx]
    yi <- y[idx]
    
    if (yi > f_ci_upper(xi) || yi < f_ci_lower(xi)) {
      return(0)
    }
    
    gbarval <- gbar(xi)
    hval <- if (gbarval < yi) {
      h_upper_p3avg(funclist = funclist, x = xi, y = yi)
    } else {
      h_lower_p3avg(funclist = funclist, x = xi, y = yi)
    }
    
    omega %*% hval / sum(omega)
  })
}

CI_req_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_REQ, f_ci_upper = req_ci_upper_p3avg, f_ci_lower = req_ci_lower_p3avg, gbar = req_p3, omega = omega)
CI_dev_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_DEV, f_ci_upper = dev_ci_upper_p3avg, f_ci_lower = dev_ci_lower_p3avg, gbar = dev_p3, omega = omega)
CI_desc_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_DESC, f_ci_upper = desc_ci_upper_p3avg, f_ci_lower = desc_ci_lower_p3avg, gbar = desc_p3, omega = omega)
```


```{r}
x <- seq(0, 1, length.out = 200)
y <- seq(0, 1, length.out = 200)

compute_z_p3avg <- function(varname, x, y, interp = NA_real_) {
  # We cannot call outer because our functions are not properly vectorized.
  #z <- outer(X = x, Y = y, FUN = CI_req_p3avg)
  f <- if (varname == "REQ") {
    CI_req_p3avg
  } else if (varname == "DEV") {
    CI_dev_p3avg
  } else {
    CI_desc_p3avg
  }
  
  z <- matrix(nrow = length(x), ncol = length(y))
  for (i in 1:length(x)) {
    for (j in 1:length(y)) {
      z[i, j] <- f(x = x[i], y = y[j])
    }
  }
  
  res <- list(x = x, y = y, z = z)
  
  if (!is.na(interp)) {
    res <- fields::interp.surface.grid(obj = res, grid.list = list(
      x = seq(from = min(x), to = max(x), length.out = interp),
      y = seq(from = min(y), to = max(y), length.out = interp)))
  }
  
  res
}

z_req <- loadResultsOrCompute(file = "../results/ci_p3avg_z_req.rds", computeExpr = {
  compute_z_p3avg(varname = "REQ", x = x, y = y)
})
z_dev <- loadResultsOrCompute(file = "../results/ci_p3avg_z_dev.rds", computeExpr = {
  compute_z_p3avg(varname = "DEV", x = x, y = y)
})
z_desc <- loadResultsOrCompute(file = "../results/ci_p3avg_z_desc.rds", computeExpr = {
  compute_z_p3avg(varname = "DESC", x = x, y = y)
})
```

Finally, we show the empirical confidence intervals in figures \ref{fig:p3-emp-cis} and \ref{fig:p3-emp-desc-cis}. Note that the minimum non-zero confidence is `r round(min(z_req$z[z_req$z>0]),6)`, while the maximum is $1$. Therefore, while we gradate the colors from $0$ to $1$, we slightly scale and transform the grid's non-zero values using the expression $0.9\times z+0.1$, to improve the visibility.

```{r p3-emp-cis, echo=FALSE, fig.height=8, fig.cap="The empirical confidence intervals for the two variables req\\% and dev\\%. Higher saturation of the color correlates with higher confidence. Projects with zero weight contribute to the CIs' boundaries, but not to the hyperplane.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(2,1))

temp <- z_req$z * 0.9 + 0.1
temp[temp == .1] <- 0

image(x = z_req$x, y = z_req$y, z = temp, main = "Variable: REQ",
      xlab = "Relative time", ylab = "Relative cumulative time spent",
      col = colorRampPalette(colors = c("#ffffff", "#ff0000"))(100), zlim = c(0,1))
grid()

curve(req_p3, 0, 1, col = "red", add = TRUE)

curve(req_ci_upper_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 2)
curve(req_ci_lower_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 3)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$REQ$get0Function()
  curve(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

legend(0.01, .98, legend = c("Weight = 0", "Weight > 0"),
       col = "#00000033", lty = c(3, 2), lwd = 2)



temp <- z_dev$z * 0.9 + 0.1
temp[temp == .1] <- 0

image(x = z_dev$x, y = z_dev$y, z = temp, main = "Variable: DEV",
      xlab = "Relative time", ylab = "Relative cumulative time spent",
      col = colorRampPalette(colors = c("#ffffff", "#0000ff"))(100), zlim = c(0,1))
grid()
curve(dev_p3, 0, 1, col = "blue", add = TRUE)

curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 3)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$DEV$get0Function()
  curve(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

legend(0.01, .98, legend = c("Weight = 0", "Weight > 0"),
       col = "#00000033", lty = c(3, 2), lwd = 2)
```


```{r p3-emp-desc-cis, echo=FALSE, fig.height=4, fig.cap="The empirical confidence intervals for the variable desc\\%. Higher saturation of the color correlates with higher confidence.", fig.align="center", fig.pos="ht!"}

temp <- z_desc$z * 0.9 + 0.1
temp[temp == .1] <- 0

image(x = z_desc$x, y = z_desc$y, z = temp, main = "Variable: DESC",
      xlab = "Relative time", ylab = "Relative cumulative time spent",
      col = colorRampPalette(colors = c("#ffffff", "forestgreen"))(100), ylim = c(0, desc_ci_upper_p3avg(1) * 1.1), zlim = c(0, max(temp)))
grid()
curve(desc_p3, 0, 1, col = "forestgreen", add = TRUE)

curve(desc_ci_upper_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 2)
curve(desc_ci_lower_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 3)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$DESC$get0Function()
  curve(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

legend(0.01, desc_ci_upper_p3avg(1) * 1.1 - .02, legend = c("Weight = 0", "Weight > 0"),
       col = "#00000033", lty = c(3, 2), lwd = 2)
```

## Pattern IV

We had previously discussed the possibility of partial (early) detection. Some of the scores and losses use integration and custom intervals, and while these might be applicable in some cases without further changes, we need to be careful when dealing with variables that are normalized at project end. This is currently the case for the variables of pattern I.

Pattern IV explores the possibility of not using some pattern directly, but its __derivative__ for all of the variables and the confidence intervals. Here we exploit the fact that a cumulative variable has monotonic behavior, depending on how it was designed even _strictly monotonic_ behavior. That means the slope at any point in time is $>0$ (or at least $\geq0$) - resp. $<0$ (or at least $\leq0$). The confidence intervals model the minimum and maximum extents we would expect, and their derivatives give us lower and upper bounds for the expected rate of change. Regardless of the actual value of the variable, we can thus transform our expectation of its value into an expectation of how it changes over time. Furthermore, the rate of change can be (numerically or analytically) computed for any other time-series variable, the procedure introduced here is not limited to cumulative and/or normalized variables.

Pattern type IV thus is a __Meta-Process model__, and can be applied to any of the previously introduced patterns (which are process models). Generically speaking, it supports the transformation of variables and confidence interval boundaries (but not inhomogeneous confidence interval surfaces). By using any other pattern (process model), pattern type IV gets __instantiated__. For the remainder of this notebook, we will be instantiating the pattern type IV using pattern I. While this helps our demonstration purposes, pattern I is partially far off the real-world data, which means that we must expect mediocre results. In practice, one should use this meta pattern only with well-designed and/or data-enhanced or data-only patterns.


For creating the first derivatives, we can use either, analytical expressions (if available) or numeric methods, such finite difference approaches. In the following, we use both, as we also actually have analytical expressions for some of the curves. The first pattern, represented by its derivatives, is shown in figure \ref{fig:p4-req-dev}.


```{r}
func_d1 <- function(f, x, supp = c(0, 1)) {
  sapply(X = x, FUN = function(x_) {
    t <- 1e-5
    m <- if (x_ < (supp[1] + t)) "forward" else if (x_ > (supp[2] - t)) "backward" else "central"
    pracma::fderiv(f = f, x = x_, method = m)
  })
}

req_d1_p4 <- function(x) {
  func_d1(f = req, x = x)
}
req_ci_lower_d1_p4 <- function(x) rep(1, length(x))
req_ci_upper_d1_p4 <- function(x) 3.89791628313 * exp(-(3.811733 * x))

dev_d1_p4 <- function(x) {
  func_d1(f = dev, x = x)
}
dev_ci_lower_d1_p4 <- function(x) rep(0, length(x))
dev_ci_upper_d1_p4 <- function(x) 0.07815904 + x * (0.8986929 * x + 1.2445534)

# For DESC, there are no confidence intervals
desc_d1_p4 <- function(x) 0.01172386 + x * (0.13480392 * x + 0.186683)
```


While we need the derivatives of any function that describes a variable or confidence interval over time, the derivatives of the confidence intervals now represent upper and lower bounds for the expected range of change. Furthermore, at any point the rate of change of the variable or either of the confidence intervals may exceed any of the other, and the curves can also cross. So, in order to define the upper and lower boundaries, we require the definition of a helper function that returns the minimum and/or maximum of either of these three functions for every $x$:

$$
\begin{aligned}
  \operatorname{CI}_{\nabla \text{upper}}(\nabla f,\nabla f_{\text{lower}},\nabla f_{\text{upper}},x)=&\;\sup{\big(\nabla f(x),\nabla f_{\text{lower}}(x),\nabla f_{\text{upper}}(x)\big)}\;\text{.}
\end{aligned}
$$

Likewise, we define $\operatorname{CI}_{\nabla\text{lower}}(\dots)$ using the infimum.


```{r p4-req-dev, echo=FALSE, fig.height=8, fig.cap="Derivatives of all variables and confidence intervals as of pattern I.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(2,1))

d1_vals_req <- function(x) {
  sapply(X = x, FUN = function(x_) c(req_d1_p4(x_), req_ci_lower_d1_p4(x_), req_ci_upper_d1_p4(x_)))
}
d1_vals_dev <- function(x) {
  sapply(X = x, FUN = function(x_) c(dev_d1_p4(x_), dev_ci_lower_d1_p4(x_), dev_ci_upper_d1_p4(x_)))
}

plot(x=0, y=0, xlim = c(0,1), ylim = c(0, max(d1_vals_req(0))), col = "#00000000",
     main = "Derivative of variable: REQ",
     xlab = "Relative time", ylab = "Rate of change")
grid()

polygon(
  x = polygon_x,
  y = c(sapply(X = polygon_x[1:500], FUN = function(x) max(d1_vals_req(x=x))),
        sapply(X = polygon_x[501:1000], FUN = function(x) min(d1_vals_req(x=x)))),
  col = "#ff000033",
  border = NA)
curve(req_ci_lower_d1_p4, 0, 1, col = "red", lty = 2, add = TRUE)
curve(req_ci_upper_d1_p4, 0, 1, col = "red", lty = 3, add = TRUE)
curve(req_d1_p4, 0, 1, col = "red", lty = 1, lwd = 2, add = TRUE)
legend(.74, max(d1_vals_req(0)) - 0.01, legend = c("upper CI", "lower CI", "req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))



plot(x=0, y=0, xlim = c(0,1), ylim = c(0, max(d1_vals_dev(1))), col = "#00000000",
     main = "Derivatives of variables: REQ and DEV",
     xlab = "Relative time", ylab = "Rate of change")
grid()

polygon(
  x = polygon_x,
  y = c(sapply(X = polygon_x[1:500], FUN = function(x) max(d1_vals_dev(x=x))),
        sapply(X = polygon_x[501:1000], FUN = function(x) min(d1_vals_dev(x=x)))),
  col = "#0000ff33",
  border = NA)

curve(dev_ci_lower_d1_p4, 0, 1, col = "blue", lty = 2, add = TRUE)
curve(dev_ci_upper_d1_p4, 0, 1, col = "blue", lty = 3, add = TRUE)
curve(dev_d1_p4, 0, 1, col = "blue", lty = 1, lwd = 2, add = TRUE)
curve(desc_d1_p4, 0, 1, col = "forestgreen", lty = 1, lwd = 2, add = TRUE)
legend(.02, max(d1_vals_dev(1)) - 0.01, legend = c("upper CI", "lower CI", "dev%", "desc%"),
       col = c(rep("blue", 3), "forestgreen"), lty = c(3, 2, 1, 1), lwd = c(1, 1, 2, 2))
```

The next challenge lies in representing the data which, in this case, is cumulative time spent on issues. If we approximate functions in a _zero-hold_-fashion as we did so far, then the gradients of these have extreme steps, and are likely unusable. In the following we attempt a few techniques to approximate a cumulative variable using LOESS-smoothing, constrained B-splines non-parametric regression quantiles, and fitting of orthogonal polynomials. The latter two result in smooth gradients if the number of knots or the degree of the polynomials are kept low. In the following subsections we use the signals of the 3rd project as an example.


### Using averaged bins

In this approach we eliminate plateaus by aggregating all $x$ that have the same $y$, and moving $x$ by the amount of values that have the same $y$. Looking at the gradient in figure \ref{fig:p4-averaged-bins-d1}, it is still too extreme.

```{r}
temp <- table(p3_signals$data$`cum req`)
prev_x <- 0
x <- c()
y <- c()
for (i in 1:length(temp)) {
  x <- c(x, prev_x + temp[[i]] / max(temp))
  prev_x <- tail(x, 1)
  y <- c(y, as.numeric(names(temp[i])))
}
```


```{r p4-averaged-bins-d1, echo=FALSE, fig.height=3, fig.cap="The average-bin signal and its gradient.", fig.align="center", fig.pos="ht!"}

par(mfrow=c(1,2))
plot(x, y, type="l")
tempf <- stats::approxfun(x = x, y = y)
tempf1 <- function(x) func_d1(f = tempf, x = x)
curve(tempf1, min(x), max(x))
```


### Eliminate plateaus

The cumulative variables have large plateaus, and the following is a test to eliminate them. However, this additional manipulation step should be skipped, if possible.

In this test, we iterate over all values of the cumulative variable and only keep x/y pairs, when y increases. In figure \ref{fig:p4-no-plateaus} we compare the original cumulative variable against the one without plateaus, i.e., every $x_t>x_{t-1}$ (note that the unequal spread of $x$ in the first non-plateau plot is deliberately ignored in the figure).

```{r p4-no-plateaus, echo=FALSE, fig.height=3, fig.cap="Transforming a signal into a non-plateau signal, using unequal widths.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,3))
x <- c(0)
y <- c(0)
for (i in 1:nrow(p3_signals$data)) {
  if (p3_signals$data$`cum req`[i] > tail(y, 1)) {
    x <- c(x, i)
    y <- c(y, p3_signals$data$`cum req`[i])
  }
}
barplot(p3_signals$data$`cum req`)
barplot(height = y)
barplot(height = y, width = diff(x))
```


### LOESS smoothing

First we define a helper function for smoothing signals using LOESS. It will also help us to scale and translate the resulting function into a specific support.


```{r}
smooth_signal_loess <- function(x, y, support = c(0, 1), span = .35, family = c("s", "g"), neval = 1e4) {
  temp <- if (span <= 0) {
    list(x = x, y = y)
  } else {
    loess.smooth(x = x, y = y, span = span, family = match.arg(family), evaluation = neval)
  }
  
  stats::approxfun(
    # translate and scale to [0,1], then scale and translate to desired support.
    x = ((temp$x - min(temp$x)) / (max(temp$x) - min(temp$x))) * (support[2] - support[1]) + support[1],
    # scale y together with x, this is important
    y = temp$y / (max(temp$x) - min(temp$x)) * (support[2] - support[1]),
    yleft = utils::head(temp$y, 1),
    yright = utils::tail(temp$y, 1))
}
```


```{r p4-no-plateaus-smooth, echo=FALSE, fig.height=7, fig.cap="Increasing LOESS-smoothing of the non-plateau signal and its resulting gradient (span=0 means no smoothing).", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,2))

templ <- list()
for (idx in 1:9) {
  span <- c(0, seq(from = 0.2, by = 0.05, length.out = 8))[idx]
  templ[[idx]] <- Signal$new(func = smooth_signal_loess(
    x = 1:nrow(p3_signals$data),
    y = cumsum(p3_signals$data$req),# / sum(p3_signals$data$req),
    span = span), name = "P3 Req", support = c(0,1), isWp = TRUE)$plot(show1stDeriv = span > 0) + ylim(0, 1.4) + labs(subtitle = paste0("span=", span))
}

ggpubr::ggarrange(
  plotlist = templ,
  ncol = 3, nrow = 3)
```


In figure \ref{fig:p4-no-plateaus-smooth} we smooth raw project data, i.e., we do __not__ use the non-plateau data or data from averaged bins, but rather the cumulative variable as-is, even __without normalization__, to demonstrate the advantages of this fourth pattern. In the top-left plot we focus on showing the raw signal. The extent of its gradient is well beyond the other (cf. figure \ref{fig:p4-averaged-bins-d1}), smoothed versions. With increasing smoothing-span, both the signal and even more so its gradient become smooth. For our concern, which is the identification of a trend regarding the rate of change, I would say that a span in the range $[0.2,0.45]$ is probably most useful, so the default was selected to be $0.35$, as it smoothes out many of the local peaks, while still preserving the important characteristics of the gradient (rate of change). However, everything $\geq0.2$ appears to be applicable. Note that spans below that often result in errors (typically around $0.15$ and below), so I do not recommend going below $0.2$.

However, the smoothing-span should be chosen according to the intended application. For example, we can compute a score based on the area between the gradient and its expected value. Here, a less smooth gradient should be preferred as it conserves more details. If the application however were, e.g., to compute a correlation, and the curve used for comparison is smooth, then the span should be chosen accordingly higher ($\geq0.4$) to get usable results.


### Constrained B-splines non-parametric regression quantiles

.. or COBS, estimates (fits) a smooth function using B-splines through some "knots". In figure \ref{fig:p4-cobs-splines} we are attempting fitting with $[3,11]$ knots. The number of knots should probably not exceed $4$ or $5$ for the gradient to be useful, as otherwise it gets too many modes. However, we can clearly see that there is no good compromise. Too few knots result in too smooth a signal and gradient, and everything with $5$ knots or more is too rough an approximation, too.

```{r p4-cobs-splines, fig.height=7, fig.cap="Fitted splines with different number of knots and their derivative.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(4,4))

X <- 1:nrow(p3_signals$data)
Y <- p3_signals$data$`cum req` / (max(X) - min(X))
X <- X / (max(X) - min(X))
templ <- list()

for (i in 1:9) {
  templ[[i]] <- (function() {
    temp <- cobs::cobs(x = X, y = Y, nknots = i + 2, print.mesg = FALSE, print.warn = FALSE)
    Signal$new(
      func = function(x) stats::predict(object = temp, z = x)[, "fit"],
      name = "P3 Req", support = range(X), isWp = TRUE)$plot(
        show1stDeriv = TRUE) + labs(subtitle = paste0("nknots=", i + 2))
  })()
}

ggpubr::ggarrange(
  plotlist = templ,
  ncol = 3, nrow = 3)
```

### Orthogonal polynomials

Using a varying degree, we fit polynomials to the data in order to obtain a truly smooth curve. This technique always results in smooth approximations both for the signal and its gradient (unlike COBS). In figure \ref{fig:p4-polynomials-d1} we observe how the derivative captures more and more nuances of the signal, starting from a degree of approx. $4$. For our tests with this pattern later, polynomials might be an alternative to LOESS-smoothed polynomials. I do recommend using these here polynomials of degree $\geq3$, because the true purpose of all this is to estimate the non-linear trend for the variable and its rate of change.


```{r p4-polynomials-d1, fig.height=7, fig.cap="Fitted polynomials and their first derivative using varying degrees of freedom.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(4,4))

use_degs <- c(2, 3, 4, 5, 7, 10, 14, 19, length(stats::coef(poly_autofit_max(x = X, y = Y))) - 1)

templ <- list()
for (idx in 1:length(use_degs)) {
  
  templ[[idx]] <- (function() {
    ud <- use_degs[idx]
    temp <- stats::lm(Y ~ poly(x = X, degree = ud))
    Signal$new(
      func = function(x) stats::predict(temp, newdata = data.frame(X=x)),
      name = "P3 Req", support = range(X), isWp = TRUE)$plot(
        show1stDeriv = TRUE) + labs(subtitle = paste0("degree=", ud))
  })()
}

ggpubr::ggarrange(
  plotlist = templ,
  ncol = 3, nrow = 3)
```

\clearpage

# Assessing the Goodness of Fit\label{sec:assess-gof}

In the technical report for detecting the Fire Drill in source code, we had previously introduced a plethora of methods to assess the goodness of fit. However, here we introduce additional methods that can exploit __confidence intervals__, both of homogeneous and inhomogeneous nature.


## Score based on CI hyperplane

Simply put, this loss is based on the confidence interval's hyperplane, and calculates an absolute average confidence based on it. Each signal evaluated against the hyperplane is a slice of it. Since we are computing an average confidence, strictly speaking this is not a loss, but a score (higher is better).

$$
\begin{aligned}
  \mathit{L}^{\text{avgconf}}(x_1,x_2,f)=&\;\Bigg[\int_{x_1}^{x_2}\,\operatorname{CI}(x, f(x))\,dx\Bigg]\times(x_2-x_1)^{-1}\;\text{,}
  \\
  &\;\text{where}\;f\;\text{is the signal/variable and}\;x_2>x_1\text{.}
\end{aligned}
$$

We will do a full evaluation later, including creating a decision rule or learning how to scale the average weight to the consensus score, but let's take one project and test this.


```{r}
L_avgconf_p3_avg <- function(x1, x2, f, CI) {
  cubature::cubintegrate(
    f = function(x) {
      CI(x = x, y = f(x))
    },
    lower = x1, upper = x2
  )$integral / (x2 - x1)
}

loadResultsOrCompute(file = "../results/ci_p3avg_Lavg-test.rds", computeExpr = {
  c(
    "P2_REQ" = L_avgconf_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project2$REQ$get0Function(), CI = CI_req_p3avg),
    "P4_REQ" = L_avgconf_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project4$REQ$get0Function(), CI = CI_req_p3avg)
  )
})
```

Plotting `L_avgconf_p3_avg` for the `REQ`-variable of both projects 2 and 4 gives us the function in figure \ref{fig:p2p4-l3avgconf}. We can clearly observe that the area under the latter is larger, and so will be the average.

```{r p2p4-l3avgconf, echo=FALSE, fig.height=3.5, fig.cap="The three variables of the first project.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,2))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    CI_req_p3avg(x = x_, y = all_signals$Project2$REQ$get0Function()(x_))
  })
}

curve(tempf, 0, 1, xlab = "Relative time", ylab = "Empirical confidence", main = "Project: 2 (REQ)")


tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    CI_req_p3avg(x = x_, y = all_signals$Project4$REQ$get0Function()(x_))
  })
}

curve(tempf, 0, 1, xlab = "Relative time", ylab = "Empirical confidence", main = "Project: 4 (REQ)")
```

From this example we can clearly see a difference in average confidence, which seems to be somewhat reconciling the projects' weights (consensus score). Let's try the next method, too.


## Loss based on distance to reference-variable

We may choose to ignore the previously computed confidence hyperplane and compute a cost by, e.g., quantifying the distance between the previously computed average variable (or any other reference-variable) and another signal/variable. More precisely, we quantify the area between both variables, and compare it to the largest possible area, thereby obtaining an upper bound (with the lower bound being $0$ obviously). Unlike the previous attempt, this function is a loss, that maps to the range $[0,1]$, where $1$ means the largest possible distance (i.e., the variable compared is entirely outside (or collinear with) the confidence intervals). Hence this function is an attempt of measuring of dissimilarity.

This method requires three features: A reference-variable, and an upper- and a lower confidence interval. However, none of these are required to be of empirical nature. For example, we can even apply this method to the first of our patterns. A reference-variable may be generated as the average between the confidence intervals, etc. All this makes this method versatile. Its robustness comes from the fact any variable it scores, is confined to the boundaries of the confidence intervals.


$$
\begin{aligned}
  \mathit{L}^{\text{areadist}}(x_1,x_2,f)=&\;\int_{x_1}^{x_2}\,\norm{\bar{g}(x)-\overbrace{\min{\Big(\operatorname{CI}_{\text{upper}}(x),\;\max{\big(\operatorname{CI}_{\text{lower}}(x), f(x)\big)}\Big)}}^{\text{Confining of}\;f\;\text{to the confidence intervals.}}}\,dx
  \\[1ex]
  &\;\times\Bigg[\;\overbrace{\int_{x_1}^{x_2}\,\sup{\Big(\bar{g}(x)-\operatorname{CI}_{\text{lower}}(x)\;,\;\operatorname{CI}_{\text{upper}}(x)-\bar{g}(x)\Big)\,dx}}^{\text{maximum possible area between}\;\bar{g}(x)\;\text{and either CI.}}\;\Bigg]^{-1}\;\text{.}
\end{aligned}
$$

This loss may also be alternatively defined using the following denominator:

$$
\begin{aligned}
  \mathit{L}^{\text{areadist2}}(x_1,x_2,f)=&\;\int_{x_1}^{x_2}\,\norm{\bar{g}(x)-\min{\Big(\operatorname{CI}_{\text{upper}}(x),\;\max{\big(\operatorname{CI}_{\text{lower}}(x), f(x)\big)}\Big)}}\,dx
  \\[1ex]
  &\;\times\Bigg[\;\int_{x_1}^{x_2}\,\begin{cases}
    \operatorname{CI}_{\text{upper}}(x)-\bar{g}(x),&\text{if}\;f(x)\geq\bar{g}(x),
    \\
    \bar{g}(x)-\operatorname{CI}_{\text{lower}}(x),&\text{otherwise.}
  \end{cases}\,dx\;\Bigg]^{-1}\;\text{.}
\end{aligned}
$$

The difference is subtle but important, and corrects better for asymmetric confidence intervals. It now captures the maximum possible area, based on the currently valid confidence interval (at $x$). For the remainder, we will always be using the __second variant of this loss__ because of this.

Again, we will do a full evaluation later, but let's just attempt computing this loss once.

```{r}
L_areadist_p3_avg <- function(x1, x2, f, gbar, CI_upper, CI_lower, use2ndVariant = FALSE) {
  int1 <- cubature::cubintegrate(
    f = function(x) {
      abs(gbar(x) - min(CI_upper(x), max(CI_lower(x), f(x))))
    },
    lower = x1, upper = x2
  )$integral
  
  int2 <- cubature::cubintegrate(
    f = function(x) {
      gbarval <- gbar(x)
      if (use2ndVariant) {
        if (f(x) >= gbarval) {
          CI_upper(x) - gbarval
        } else {
          gbarval - CI_lower(x)
        }
      } else {
        max(gbarval - CI_lower(x), CI_upper(x) - gbarval)
      }
    },
    lower = x1, upper = x2
  )$integral
  
  c("area" = int1, "maxarea" = int2, "dist" = int1 / int2)
}

loadResultsOrCompute(file = "../results/ci_p3avg_Larea-test.rds", computeExpr = {
  list(
    "P2_REQ" = L_areadist_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project2$REQ$get0Function(), use2ndVariant = TRUE,
      gbar = req_p3, CI_upper = req_ci_upper_p3avg, CI_lower = req_ci_lower_p3avg),
    "P4_REQ" = L_areadist_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project4$REQ$get0Function(), use2ndVariant = TRUE,
      gbar = req_p3, CI_upper = req_ci_upper_p3avg, CI_lower = req_ci_lower_p3avg)
  )
})
```

Let's show the maximum possible distance vs. the distance of a project's variable in a plot (cf. figure \ref{fig:p2p4-lareadist}). These figures clearly show the smaller distance of project 4 to the average. This is expected, as this project has the highest weight, so the average `REQ%`-variable resembles this project most.

```{r p2p4-lareadist, echo=FALSE, fig.height=4, fig.cap="The two variables of the first project.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,2))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    abs(req_p3(x_) - min(req_ci_upper_p3avg(x_), max(req_ci_lower_p3avg(x_), all_signals$Project2$REQ$get0Function()(x_))))
  })
}
curve(tempf, 0, 1, col = "red", xlab = "Relative time", ylab = "Distance to avg. req\\%", main = "Project: 2", ylim = c(0, .5))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    max(req_p3(x_) - req_ci_lower_p3avg(x_), req_ci_upper_p3avg(x_) - req_p3(x_))
  })
}
curve(tempf, 0, 1, col = "blue", add = TRUE)
legend(0.01, 0.49, legend = c("Pr. 2 req% dist", "max. dist."), col = c("red", "blue"), lty = 1, lwd = 2)



tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    abs(req_p3(x_) - min(req_ci_upper_p3avg(x_), max(req_ci_lower_p3avg(x_), all_signals$Project4$REQ$get0Function()(x_))))
  })
}
curve(tempf, 0, 1, col = "red", xlab = "Relative time", ylab = "Distance to avg. req\\%", main = "Project: 4", ylim = c(0,0.5))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    max(req_p3(x_) - req_ci_lower_p3avg(x_), req_ci_upper_p3avg(x_) - req_p3(x_))
  })
}
curve(tempf, 0, 1, col = "blue", add = TRUE)
legend(0.01, 0.49, legend = c("Pr. 4 req% dist", "max. dist."), col = c("red", "blue"), lty = 1, lwd = 2)
```

## Loss based on the two previous approaches

This is an early-stadium idea. The essence is that for every $x$, we have a vertical "confidence-slice" that we can integrate over and get an average confidence. Then, we obtain the confidence for the variable in question at the same $x$. Both of these values can then be put into a relation. If we were to integrate this function, we would get the ratio between the variable's confidence and the average confidence, on average.


```{r}
use_x <- .8
dev_p3(use_x)

cubature::cubintegrate(
  f = function(x) {
    CI_dev_p3avg(x = use_x, y = x)
  },
  lower = dev_ci_lower_p3avg(use_x),
  upper = dev_ci_upper_p3avg(use_x)
)$integral / (dev_ci_upper_p3avg(use_x) - dev_ci_lower_p3avg(use_x))
```

```{r}
CI_dev_p3avg(x = use_x, all_signals$Project4$DEV$get0Function()(use_x))
```

At `x=0.8`, the average variable is at $\approx0.71$, the average confidence for the slice is $\approx0.19$, and the confidence of the evaluated variable (project 4, `DEV`) is at $\approx0.22$. This means that the ratio is in the interval $(0,\infty)$. A "perfect" ratio of $1.0$ would express that the tested variable is, on average, equal to the average confidence.


# Early detection

Many of the methods presented in the previous section \ref{sec:assess-gof} can also be used for early detection, by limiting (and re-scaling) the scores to the available interval. That means, that we would make a partial assessment of how well the process observed so far aligns with the process model in question. It only requires knowledge about at what point in time we are, i.e., $t_{\text{now}}$. Scores would then be computed over the interval $[0,t_{\text{now}}]$.

The early detection can be split into two regions of interest:

1.    What happened since project begin until now? Scores may also be computed over an arbitrary interval $[t_{\text{begin}},t_{\text{end}}]$. More generally, the limits can be chosen arbitrarily for most scores, as long as $t_{\text{begin}}<t_{\text{end}}$ and $t_{\text{end}}\leq t_{\text{now}}$.
2.    What will be the probable future, starting from now until a chosen point in the future, $t_{\text{future}}$? Here, $t_{\text{end}}<t_{\text{future}}$ and the interval may be chosen arbitrarily.

Early detection is widely applicable and can be, e.g., determined for each single variable separately. It may also be computed for derived variables, to make probabilistic statements about the expected _rate of change_.


## Forecasting within Vector Fields

If we were to derive for an _inhomogeneous_ confidence interval, the result would be a vector field that, for each pair of coordinates $x,y$, points into the direction of the largest change, which here means the direction into which the confidence would increase the most, on a cartesian coordinate system. This is exemplary shown in figure \ref{fig:example-vectorfield}. We suggest an operationalization in these ways:

*   Using methods usually applied in time series forecasting, we have the means to determine the trends and probable path of a variable. Also, some of these methods will give as a confidence interval, which, over the vector field, is an area that may (partially) overlap.
*   We can determine the total and average confidence of the vector field that is affected by the overlap. This supports assumptions about whether we are currently in or headed into regions with lower or higher confidence (where the confidence represents the degree to which a process model is present).
*   We can determine the direction and strength of steepest increase of the confidence. The direction can be compared to the direction of the forecast. This may be exploited for making decisions that lead to heading away or into the confidence surface, depending on which choice is desirable. It also gives insights into the current projected trend.


```{r example-vectorfield, echo=FALSE, fig.height=6, fig.cap="The inhomogeneous confidence interval of the dev\\% variable and its vector field, pointing towards the largest increase in confidence for each pair of x/y coordinates. Here we use a non-smooth surface.", fig.align="center", fig.pos="ht!"}

pracma::vectorfield(fun = Vectorize(FUN = function(x,y) {
    v <- CI_dev_p3avg(x, y)
    if (v == 0) -Inf else -log(2 * v)
}), xlim = c(0.4, 0.8), ylim = c(0.2,.8), n = 20, col = "#0000dd99", scale = .015)
curve(dev_p3, 0, 1, col = "blue", lwd = 2, add = TRUE)
curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 3)
```


In figure \ref{fig:example-vectorfield} we show an example of a vector field that is non-smooth. In the following examples however, we use smoothed version of x/y-slices [@green1993nonparametric].


```{r}
CI_dev_smooth_slice_x <- function(y, nsamp = 100, deriv = 0) {
  stopifnot(length(y) == 1)
  
  data_x <- seq(from = 0, to = 1, length.out = nsamp)
  data_y <- sapply(X = data_x, FUN = function(xslice) {
    CI_dev_p3avg(x = xslice, y = y)
  })
  pred_smooth <- suppressWarnings(expr = {
    stats::smooth.spline(x = data_x, y = data_y)
  })
  
  function(x) {
    vals <- stats::predict(object = pred_smooth, deriv = deriv, x = x)$y
    if (deriv == 0) {
      vals[vals < 0] <- 0
      vals[vals > 1] <- 1
    }
    vals
  }
}
```

```{r}
CI_dev_smooth_slice_y <- function(x, nsamp = 100, deriv = 0) {
  stopifnot(length(x) == 1)
  
  data_x <- seq(from = 0, to = 1, length.out = nsamp)
  data_y <- sapply(X = data_x, FUN = function(yslice) {
    CI_dev_p3avg(x = x, y = yslice)
  })
  pred_smooth <- suppressWarnings(expr = {
    stats::smooth.spline(x = data_x, y = data_y)
  })
  
  function(y) {
    vals <- stats::predict(object = pred_smooth, deriv = deriv, x = y)$y
    if (deriv == 0) {
      vals[vals < 0] <- 0
      vals[vals > 1] <- 1
    }
    vals
  }
}
```

Ideally, the x-slice of $y$ at $x$ returns the same value as the y-slice of $x$ at $y$. This is however only approximately true for the smoothed slices, so we return the mean of these two values and the actual value, to obtain a final smoothed $z$ value that is closest to the ground truth, i.e.,

$$
\begin{aligned}
  \operatorname{Slice}^X(y),\;\operatorname{Slice}^Y(x)\dots&\;\text{horizontal/vertical}\;x\text{/}y\text{-slice,}
  \\[1ex]
  \operatorname{Slice}_{\text{smooth}}^X(y),\;\operatorname{Slice}_{\text{smooth}}^Y(x)\dots&\;\text{smoothed versions, such that}
  \\[1ex]
  \operatorname{Slice}^X(y)\approx\operatorname{Slice}_{\text{smooth}}^X(y)\;\land&\;\operatorname{Slice}^Y(x)\approx\operatorname{Slice}_{\text{smooth}}^Y(x)\;\text{,}
  \\[1em]
  \operatorname{CI}_{\text{smooth}}(x,y)=&\;\Big(\operatorname{CI}(x,y)\;+\;\operatorname{Slice}_{\text{smooth}}^X(y)\;+\;\operatorname{Slice}_{\text{smooth}}^Y(x)\Big)\div3\;\text{.}
\end{aligned}
$$

```{r}
CI_dev_smooth_p3avg <- Vectorize(function(x, y, nsamp = 100) {
  stopifnot(length(x) == 1 && length(y) == 1)
  
  xsl <- CI_dev_smooth_slice_x(y = y, nsamp = nsamp)
  ysl <- CI_dev_smooth_slice_y(x = x, nsamp = nsamp)
  
  mean(c(xsl(x = x), ysl(y = y), CI_dev_p3avg(x = x, y = y)))
  
},  vectorize.args = c("x", "y"))
```


The functions `CI_dev_smooth_slice_x()` and `CI_dev_smooth_slice_y()` can also return the derivative (gradient) of the slice, and we will use this when computing the direction and magnitude in each dimension. At every point $x,y$, we can obtain now two vectors pointing into the steepest increase for either dimension, i.e., $\overrightarrow{x},\overrightarrow{y}$.

```{r}
arrow_dir_smooth <- function(x, y) {
  xsl <- CI_dev_smooth_slice_x(y = y, deriv = 1)
  ysl <- CI_dev_smooth_slice_y(x = x, deriv = 1)
  
  c(x=x, y=y, x1 = xsl(x = x), y1 = ysl(y = y))
}
```

```{r echo=FALSE}
temp <- doWithParallelCluster(expr = {
  g <- expand.grid(data.frame(
    x = seq(0.4, 0.9, length.out=20),
    y = seq(0.4, 0.8, length.out=20)
  ))
  
  foreach::foreach(
    idx = rownames(g),
    .inorder = FALSE,
    .combine = rbind
  ) %dopar% {
    p <- g[idx,]
    
    matrix(data = arrow_dir_smooth(x = p$x, y = p$y), nrow = 1)
  }
})
```


Finally, we compute a vector field based on a smoothed confidence surface, shown in figure \ref{fig:example-vectorfield-smooth}. Note that all arrows have the same length in this figure, such that it does not depend on the magnitude of the steepest increase they are pointing towards.


```{r example-vectorfield-smooth, echo=FALSE, fig.height=6, fig.cap="Example vector field computed using a smoothed surface. We can clearly observe how the arrows point now properly towards the direction with steepest increase.", fig.align="center", fig.pos="ht!"}
temp1 <- z_dev$z * 0.9 + 0.1
temp1[temp1 == .1] <- 0
temp1[temp1 > 0] <- -log(1 - temp1[temp1 > 0])

image(x = z_dev$x, y = z_dev$y, z = temp1, main = "Variable: DEV",
      xlab = "Relative time", ylab = "Relative cumulative time spent",
      col = colorRampPalette(colors = c("#ffffff", "#0000ff"))(100),
      zlim = c(0,1), xlim = c(0.4,0.9), ylim = c(0.4,0.6))
grid()


curve(dev_ci_upper_p3avg, 0, 1, add = TRUE, col = "blue", lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, add = TRUE, col = "blue", lty = 2)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$DEV$get0Function()
  curve(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

for (i in 1:nrow(temp)) {
  r <- temp[i, ]
  to <- r[3:4]
  if (r[2] < dev_ci_lower_p3avg(r[1]) || r[2] > dev_ci_upper_p3avg(r[1])) {
    next
  }
  to <- to / max(abs(to)) * 0.01
  arrows(x0 = r[1] + (if (to[1] < 0) abs(to[1]) else 0), y0 = r[2] + (if (to[2] < 0) abs(to[2]) else 0),
         x1 = r[1] + (if (to[1] < 0) abs(to[1]) else 0) + to[1], y1 = r[2] + (if (to[2] < 0) abs(to[2]) else 0) + to[2], length = 0.05)
}
```

We want to demonstrate the previously suggested methods using a section of the confidence surfaces in figures \ref{fig:example-vectorfield} and \ref{fig:example-vectorfield-smooth} and an example variable. We will be using the `DEV`-variable of project 5, and make a forecast from $0.65$ to $0.75$ in relative time. This also demonstrates another advantage of having modeled discrete variables in continuous time, as we are not required to make forecasts in discrete time either.

### Average confidence in overlapped surface


```{r}
n_periods <- 10
p5_signals <- all_signals$Project5
pr5_dev <- p5_signals$DEV$get0Function()
forecast_y <- sapply(X = seq(from = 0, to = 0.65, length.out = 65), FUN = pr5_dev)

# Fit an EMA to the data and produce 80% and 95% confidence intervals:
fch <- forecast::holt(y = forecast_y, h = 10)
fch_x <- c(65, seq(from = 66, to = 65 + n_periods, by = 1)) / 100

fch_mean <- stats::approxfun(x = fch_x, y = c(pr5_dev(0.65), as.vector(fch$mean)))
fch_80_upper <- stats::approxfun(x = fch_x, y = c(pr5_dev(0.65), as.vector(fch$upper[,1])), yleft = 0, yright = 0)
fch_80_lower <- stats::approxfun(x = fch_x, y = c(pr5_dev(0.65), as.vector(fch$lower[,1])), yleft = 0, yright = 0)
fch_95_upper <- stats::approxfun(x = fch_x, y = c(pr5_dev(0.65), as.vector(fch$upper[,2])), yleft = 0, yright = 0)
fch_95_lower <- stats::approxfun(x = fch_x, y = c(pr5_dev(0.65), as.vector(fch$lower[,2])), yleft = 0, yright = 0)
```


In figure \ref{fig:ed-avg-conf} we use the smoothed confidence surface again. We show a section of the variable `DEV` and its vector field, pointing towards the steepest increase of confidence at every $x,y$. Then, the course of project 5's `DEV` variable is shown until $0.65$, and forecast until $0.75$ (red line). The prediction confidence intervals are shown in lightgray (80%) and darkgray (95%).


```{r ed-avg-conf, echo=FALSE, fig.cap="Overlap of empirical confidence surface and project 5 (variable: dev\\%). The 80\\% and 95\\% prediction confidence intervals are shown in light- and darkgray.", fig.align="center", fig.pos="ht!"}
pracma::vectorfield(fun = Vectorize(FUN = function(x,y) {
  # CI_dev_p3avg(x, y)
  v <- if (y < dev_ci_lower_p3avg(x) || y > dev_ci_upper_p3avg(x)) 0 else CI_dev_smooth_p3avg(x, y)
  if (v == 0) -Inf else -log(2 * v)
}), xlim = c(0.55, 0.8), ylim = c(0.4, 0.9), n = 12, col = "#0000dd99", scale = .01)
curve(dev_p3, 0, 1, col = "blue", lwd = 2, add = TRUE)
curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 3)

curve(pr5_dev, 0, 0.65, col = "blue", lty = 2, lwd = 2, add = TRUE)
curve(pr5_dev, 0.65, 0.75, col = "blue", lty = 1, lwd = 1, add = TRUE)
curve(fch_mean, 0.65, 0.75, col = "red", lty = 1, lwd = 1, add = TRUE)
curve(fch_80_upper, 0.65, 0.75, col="lightgrey", add = TRUE)
curve(fch_80_lower, 0.65, 0.75, col="lightgrey", add = TRUE)
curve(fch_95_upper, 0.65, 0.75, col="darkgrey", add = TRUE)
curve(fch_95_lower, 0.65, 0.75, col="darkgrey", add = TRUE)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_upper(fch_x), fch_95_upper(rev(fch_x))),
  border = NA,
  col = "#00000011"
)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_lower(fch_x), fch_95_lower(rev(fch_x))),
  border = NA,
  col = "#00000011"
)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_upper(fch_x), fch_80_lower(rev(fch_x))),
  border = NA,
  col = "#00000033"
)

legend(0.555, .89, legend = c("dev%", "dev% CI", "Pr. 5 dev%", "Pr. 5 actual", "Pr. 5 prediction"),
       bg="white", col = c(rep("blue", 4), "red"), lty = c(1, 2, 2, 1, 1), lwd = c(3, 1.5, 2, 1.5, 1.5))
```

In order to calculate the average confidence in the area that overlaps with the vector field, we need a double integral. The upper 95% confidence interval is partially outside the vector field. Generally, we must not integrate areas that do not overlap. Therefore, we define a function $\operatorname{CI}_{\text{overlap}}(x,y)$ to help with that:

$$
\begin{aligned}
  f_{\text{lower}}(x),f_{\text{upper}}(x)\dots&\;\text{functions for the lower/upper prediction confidence intervals,}
  \\[1ex]
  \operatorname{CI}_{\text{lower}}(x),\operatorname{CI}_{\text{upper}}(x)\dots&\;\text{lower/upper confidence boundaries for confidence surface,}
  \\[1ex]
  \operatorname{CI}_{\text{overlap}}(x,y)=&\;\begin{cases}
    0,&\text{if}\;f_{\text{lower}}(x)<\operatorname{CI}_{\text{lower}}(x),
    \\
    0,&\text{if}\;f_{\text{upper}}(x)>\operatorname{CI}_{\text{upper}}(x),
    \\
    \operatorname{CI}(x,y),&\text{otherwise}
  \end{cases}\;\text{,}
  \\[1em]
  \text{average confidence}\;=&\;\int_a^b\bigg[\int_{f_{\text{lower}}(x)}^{f_{\text{upper}}(x)}\,\operatorname{CI}_{\text{overlap}}(x,y)\times\big(f_{\text{upper}}(x)-f_{\text{lower}}(x)\big)^{-1}\,dy\bigg]\times(b-a)^{-1}\,dx\;\text{.}
\end{aligned}
$$

The approximate value for 80% / 95% prediction confidence intervals for the concrete example is calculated as:

```{r}
average_confidence <- function(f_low, f_upp, CI_low, CI_upp, CI, lower, upper, maxEval = 15) {
  cubature::cubintegrate(f = Vectorize(function(x) {
    l <- f_low(x)
    u <- f_upp(x)
    cubature::cubintegrate(f = Vectorize(function(y) {
      if (l < CI_low(x) || u > CI_upp(x)) 0 else CI(x = x, y = y)
    }), lower = l, upper = u, maxEval = maxEval)$integral / (u - l)
  }), lower = lower, upper = upper, maxEval = maxEval)$integral / (upper - lower)
}
```


```{r}
loadResultsOrCompute(file = "../results/ed_avgconf_80.rds", computeExpr = {
  average_confidence(
    f_low = fch_80_lower, f_upp = fch_80_upper,
    CI_low = dev_ci_lower_p3avg, CI_upp = dev_ci_upper_p3avg,
    CI = CI_dev_smooth_p3avg, # CI_dev_p3avg,
    lower = 0.65, upper = 0.75)
})
```

```{r}
loadResultsOrCompute(file = "../results/ed_avgconf_95.rds", computeExpr = {
  average_confidence(
    f_low = fch_95_lower, f_upp = fch_95_upper,
    CI_low = dev_ci_lower_p3avg, CI_upp = dev_ci_upper_p3avg,
    CI = CI_dev_smooth_p3avg, # CI_dev_p3avg,
    lower = 0.65, upper = 0.75)
})
```

As expected, the average confidence in the overlap of the 95% prediction interval is a little lower.

### Average direction of steepest confidence increase

Similar to finding the average confidence in the overlapped surface, we may also determine the direction and strength of the steepest increase in confidence. For every point that is part of the overlapped area, we need to sum up the gradient in x- and y-direction. The resulting combined vector then gives the direction of the steepest increase.

This is quite similar to how we computed the average confidence, and requires the following double integral:

$$
\begin{aligned}
  x\text{/}y\;\text{total change}\;=&\;\int_a^b\bigg[\int_{f_{\text{lower}}(x)}^{f_{\text{upper}}(x)}\,\bigg\{\frac{\partial}{\partial\,x}\,\operatorname{CI}_{\text{overlap}}(x,y)\;,\;\frac{\partial}{\partial\,y}\,\operatorname{CI}_{\text{overlap}}(x,y)\bigg\}^\top\,dy\bigg]\,dx\;\text{,}
  \\[1ex]
  &\;\text{or using individual slices and an overlap-helper:}
  \\[1em]
  \operatorname{overlap}(x,y)=&\;\begin{cases}
    0,&\text{if}\;f_{\text{lower}}(x)<\operatorname{CI}_{\text{lower}}(x),
    \\
    0,&\text{if}\;f_{\text{upper}}(x)>\operatorname{CI}_{\text{upper}}(x),
    \\
    1,&\text{otherwise,}
  \end{cases}
  \\[1ex]
  x\;\text{total change}\;=&\;\int_a^b\bigg[\int_{f_{\text{lower}}(x)}^{f_{\text{upper}}(x)}\,\operatorname{overlap}(x,y)\times\frac{\partial}{\partial\,x}\,\operatorname{Slice}_{\text{smooth}}^X(y)\,dy\bigg]\,dx\;\text{, also similarly for}\;y\text{.}
\end{aligned}
$$

```{r}
total_change <- function(axis = c("x", "y"), f_low, f_upp, CI_low, CI_upp, lower, upper, maxEval = 15) {
  use_x <- match.arg(axis) == "x"
  
  cubature::cubintegrate(f = Vectorize(function(x) {
    l <- f_low(x)
    u <- f_upp(x)
    
    cubature::cubintegrate(f = function(y) {
      sapply(X = y, FUN = function(y_) {
        if (l < CI_low(y_) || u > CI_upp(y_)) {
          return(0)
        }
        val <- arrow_dir_smooth(x = x, y = y_)
        if (use_x) val["x1"] else val["y1"]
      })
    }, lower = l, upper = u, maxEval = maxEval)$integral / (u - l)
  }), lower = lower, upper = upper, maxEval = maxEval)$integral / (upper - lower)
}
```

```{r}
ed_tc_x <- loadResultsOrCompute(file = "../results/ed_total_change_x.rds", computeExpr = {
  total_change(
    f_low = fch_95_lower, f_upp = fch_95_upper,
    CI_low = dev_ci_lower_p3avg, CI_upp = dev_ci_upper_p3avg,
    lower = 0.65, upper = 0.75, axis = "x")
})
ed_tc_x
```

```{r}
ed_tc_y <- loadResultsOrCompute(file = "../results/ed_total_change_y.rds", computeExpr = {
  total_change(
    f_low = fch_95_lower, f_upp = fch_95_upper,
    CI_low = dev_ci_lower_p3avg, CI_upp = dev_ci_upper_p3avg,
    lower = 0.65, upper = 0.75, axis = "y")
})
ed_tc_y
```

These results mean that the slope of the average change is $\approx$`r round(ed_tc_y / ed_tc_x, 2)` in the 95% confidence interval of the prediction that overlaps with the vector field of the `DEV`-variable's confidence surface. This is shown in figure \ref{fig:ed-avg-dir}.


```{r ed-avg-dir, echo=FALSE, fig.cap="The average direction towards the steepest increase in confidence for the dev\\% variable in its confidence surface that is overlapped by the 95\\% confidence interval of the prediction. Note that the direction may change if computed over the 80\\% confidence interval.", fig.align="center", fig.pos="ht!"}
pracma::vectorfield(fun = Vectorize(FUN = function(x,y) {
  # CI_dev_p3avg(x, y)
  v <- if (y < dev_ci_lower_p3avg(x) || y > dev_ci_upper_p3avg(x)) 0 else CI_dev_smooth_p3avg(x, y)
  if (v == 0) -Inf else -log(2 * v)
}), xlim = c(0.55, 0.8), ylim = c(0.4, 0.9), n = 12, col = "#0000dd99", scale = .01)
curve(dev_p3, 0, 1, col = "blue", lwd = 2, add = TRUE)
curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 3)

curve(pr5_dev, 0, 0.65, col = "blue", lty = 2, lwd = 2, add = TRUE)
curve(pr5_dev, 0.65, 0.75, col = "blue", lty = 1, lwd = 1, add = TRUE)
curve(fch_mean, 0.65, 0.75, col = "red", lty = 1, lwd = 1, add = TRUE)
curve(fch_80_upper, 0.65, 0.75, col="lightgrey", add = TRUE)
curve(fch_80_lower, 0.65, 0.75, col="lightgrey", add = TRUE)
curve(fch_95_upper, 0.65, 0.75, col="darkgrey", add = TRUE)
curve(fch_95_lower, 0.65, 0.75, col="darkgrey", add = TRUE)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_upper(fch_x), fch_95_upper(rev(fch_x))),
  border = NA,
  col = "#00000011"
)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_lower(fch_x), fch_95_lower(rev(fch_x))),
  border = NA,
  col = "#00000011"
)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_upper(fch_x), fch_80_lower(rev(fch_x))),
  border = NA,
  col = "#00000033"
)

legend(0.555, .89, legend = c("dev%", "dev% CI", "Pr. 5 dev%", "Pr. 5 actual", "Pr. 5 prediction", "Avg. direction"),
       bg="white", col = c(rep("blue", 4), "red", "purple"), lty = c(1, 2, 2, 1, 1, 1), lwd = c(3, 1.5, 2, 1.5, 1.5, 2))

arrows(x0 = 0.65, x1 = 0.75, y0 = 0.65, y1 = 0.65 + 0.1 / ed_tc_x * ed_tc_y, lwd = 2, col = "purple")
```

Now we can use this information to calculate, e.g., an angle between the predicted variable and the average direction in the field. Depending on the case, one usually wants to either follow into the average direction of steepest increase or diverge from it. In our case of predicting the presence of the Fire Drill anti-pattern, we would want to move (stay) away from it.

```{r}
angle <- function(A, B, deg = TRUE) {
  (A %*% B) / (sqrt(sum(A^2)) * sqrt(sum(B^2))) * (if (deg) 180 / pi else 1)
}

angle(B = c(.1, fch_mean(0.75) - fch_mean(0.65)), A = c(ed_tc_x, ed_tc_y))
```

The angle between the projected (predicted) `DEV`-variable of project 5 and the average direction of steepest increase in confidence is $\approx17.19^{\circ}$.





# Scoring of projects

In the technical report for detecting the Fire Drill using source code data, we already explored a wide range of possible patterns and scoring mechanisms. All of them are based on comparing/scoring the variables (process model vs. process). Some of these we will apply here, too, but our focus is first on detection mechanisms that can facilitate the __confidence intervals__.


## Pattern I

This is the expert-guess of how the Fire Drill would manifest in issue-tracking data. The pattern was conceived with precise values for the confidence intervals at certain points ($t_1,t_2$), and the variables were not of importance. It was only used thus far using a binary decision rule.


### Binary detection decision rule

In this section, we will only replicate earlier results by applying the existing rule. It is further formulated using indicators and thresholds as:

$$
\begin{aligned}
  I_1 =&\;\operatorname{req}(t_1) < y_1 \land \operatorname{req}(t_1) > y_2,
  \\[1ex]
  I_2 =&\;\operatorname{dev}(t_1) < y_3 \land \operatorname{dev}(t_2) < y_4,
  \\[1ex]
  I_3 =&\;\operatorname{desc}(1) > y_5,
  \\[1em]
  \operatorname{detect}^{\text{binary}}(I_1,I_2,I_3) =&\;\begin{cases}
    1,&\text{if}\;I_1 \land (I_2 \lor I_3),
    \\
    0,&\text{otherwise}
  \end{cases}\;\text{, using the threshold values}
  \\[1ex]
  \bm{y}=&\;\{0.8,0.4,0.15,0.7,0.15\}^\top\;\text{.}
\end{aligned}
$$

This can be encapsulated in a single function:

```{r}
p1_dr <- function(projName, y = c(0.8, 0.4, 0.15, 0.7, 0.15)) {
  req <- all_signals[[projName]]$REQ$get0Function()
  dev <- all_signals[[projName]]$DEV$get0Function()
  desc <- all_signals[[projName]]$DESC$get0Function()
  
  I1 <- req(t_1) < y[1] && req(t_1) > y[2]
  I2 <- dev(t_1) < y[3] && dev(t_2) < y[4]
  I3 <- desc(1) > y[5]
  
  I1 && (I2 || I3)
}
```

```{r}
temp <- sapply(X = names(all_signals), FUN = p1_dr)
p1_detect <- data.frame(
  detect = temp,
  ground_truth = ground_truth$consensus,
  correct = (temp & ground_truth$consensus >= 5) | (!temp & ground_truth$consensus < 5)
)
```

```{r echo=FALSE}
if (interactive()) {
  p1_detect
} else {
  knitr::kable(
    x = p1_detect,
    booktabs = TRUE,
    caption = "Binary detection using a decision rule based on homogeneous confidence intervals of pattern I.",
    label = "p1-bin-detect"
  )
}
```

In table \ref{tab:p1-bin-detect} we show the results of the binary detection, which is based on the manually defined homogeneous confidence intervals.


### Average distance to reference

As a bonus, to demonstrate the versatility and robustness of this method, we will score the projects against the first pattern, and the second pattern type II (a), which is supposed to be a slight improvement over type I. We will use the variables and confidence intervals as they were defined there, i.e., those are neither data-based nor data-enhanced. For the following tests, the variables `REQ` and `DEV` will be considered, as `DESC` does not have non-empirical confidence intervals.

Pattern I's CIs for the `REQ` variable align quite well with the boundaries of the empirical CIs, which means that most projects lie, by a large degree, within the expert-guessed CIs, so this method should work well. For the `DEV` variable's CIs in pattern I however, only a fraction of the projects are within the guessed CI, the averaged-variable appears even to be completely outside of the empirical CI. Therefore, we cannot expect the method to work well in any way in this scenario (cf. figure \ref{fig:req-dev-p3avg-cis}).

```{r echo=FALSE}
p1_avg_area_scores <- loadResultsOrCompute(file = "../results/p1_avg_area_scores.rds", computeExpr =  {
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    # There is no 'dev_ci_lower', because it would just be f(x)=0
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind,
      .packages = c("cobs")
    ) %dopar% {
      req_p <- all_signals[[pId]]$REQ$get0Function()
      dev_p <- all_signals[[pId]]$DEV$get0Function()
      
      `rownames<-`(data.frame(
        REQ = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = req_p, gbar = req,
          CI_upper = req_ci_upper, CI_lower = req_ci_lower, use2ndVariant = TRUE)["dist"],
        
        DEV = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = dev_p, gbar = dev,
          CI_upper = dev_ci_upper, CI_lower = function(x) 0, use2ndVariant = TRUE)["dist"]
      ), pId)
    }
  })
})
```

```{r p1-avg-area-scores, echo=FALSE, fig.height=8, fig.cap="All projects plotted against the two variables req\\% and dev\\% of the first pattern.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(2,1))

set.seed(1)
myCol = sample(c("pink1", "violet", "mediumpurple1", "slateblue1", "purple", "purple3",
          "turquoise2", "skyblue", "steelblue", "blue2", "navyblue",
          "orange", "tomato", "coral2", "palevioletred", "violetred", "red2",
          "springgreen2", "yellowgreen", "palegreen4",
          "wheat2", "tan", "tan2", "tan3", "brown",
          "grey70", "grey50", "grey30"))


plot(x=0, y=0, xlim = c(0,1), ylim = c(0, 1), col = "#00000000",
     xlab = "Relative time", ylab = "Rate of change")
grid()
curve(req, 0, 1, col="red", lwd=2, add = TRUE)
curve(req_ci_upper, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)
curve(req_ci_lower, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)
polygon(
  x = seq(from = 0, to = 1, length.out = 50),
  y = req_ci_upper(seq(from = 0, to = 1, length.out = 50)),
  col = "#ff00000d",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$REQ$get0Function()
  curve(tempf, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.01, .98, legend = c("req%", "req% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))



plot(x=0, y=0, xlim = c(0,1), ylim = c(0, 1), col = "#00000000",
     xlab = "Relative time", ylab = "Rate of change")
grid()
curve(dev, 0, 1, col="blue", lwd=2, add = TRUE)
tempf <- function(x) rep(0, length(x))
curve(tempf, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)
curve(dev_ci_upper, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)
polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff0d",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$DEV$get0Function()
  curve(tempf, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.01, .98, legend = c("dev%", "dev% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))
```

In figure \ref{fig:p1-avg-area-scores} we show the first pattern and its confidence intervals, plotted against all projects' variables. The method used here to compute a score calculates the area between the pattern's variable (here: `REQ` or `DEV`) and a project's variable, where the upper bound would be the confidence intervals. While the first variant of this loss uses as max distance the CI that is farther away, the 2nd variant uses the CI based on whether the signal is above or below the compared-to variable. If we look at figure \ref{fig:p1-avg-area-scores}, it becomes apparent why that is better: Consider, e.g., project 9 and the `DEV` variable. It is completely above the upper CI. The are between the upper CI and `DEV` and the lower CI and `DEV` are differently large, with the latter being definitely larger. In variant 1, due to the confinement, the area-distance, the are between the variable and the signal would be approximately equivalent to the area between `DEV` and the upper CI. That is then put into relation to the maximum area, which is the one between `DEV` and the lower CI. This results in a distance $\ll1$, even though it could not be worse. Variant 2 however considers, for all realizations of $X$, which CI should be used, and correctly determines the distance as $\approx1$ (note: it might be slightly less or more, due to numerical error).


```{r echo=FALSE}
if (interactive()) {
  p1_avg_area_scores
} else {
  knitr::kable(
    x = p1_avg_area_scores,
    booktabs = TRUE,
    caption = "The average distance of the variables REQ and DEV of each project to the reference-variables REQ/DEV as defined by pattern I.",
    label = "p1-avg-area-scores"
  )
}
```

```{r}
cor(x = ground_truth$consensus_score, y = p1_avg_area_scores[, "REQ"])
cor(x = ground_truth$consensus_score, y = p1_avg_area_scores[, "DEV"])
```

I think it is fair to say that the first pattern and its confidence intervals do not work well with this detection approach. While we do get a moderate correlation for `REQ`, it is positive, when it should have been negative.

As expected, `DEV` is quite unusable, as the correlations are low, albeit negative.



## Pattern II

_TODO_.

### Average distance to reference


Let's check the next pattern: In type II (a), we have adapted the thresholds $t_1,t_2$ according to the ground truth. We can see, that the CIs of this align already much better with the projects, esp. for the `DEV` variable.

```{r echo=FALSE}
p2a_avg_area_scores <- loadResultsOrCompute(file = "../results/p2a_avg_area_scores.rds", computeExpr =  {
  temp_x <- seq(0, 1, length.out = 1e4)
  req_p2a_temp <- stats::approxfun(x = temp_x, y = req_p2a(temp_x), yleft = 0, yright = 1)
  dev_p2a_temp <- stats::approxfun(x = temp_x, y = dev_p2a(temp_x), yleft = 0, yright = 1)
  
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    # There is no 'dev_ci_lower', because it would just be f(x)=0
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind,
      .packages = c("cobs")
    ) %dopar% {
      req_p <- all_signals[[pId]]$REQ$get0Function()
      dev_p <- all_signals[[pId]]$DEV$get0Function()

      `rownames<-`(data.frame(
        REQ = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = req_p, gbar = req_p2a_temp,
          CI_upper = req_ci_upper_p2a, CI_lower = req_ci_lower_p2a, use2ndVariant = TRUE)["dist"],

        DEV = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = dev_p, gbar = dev_p2a_temp,
          CI_upper = dev_ci_upper_p2a, CI_lower = function(x) 0, use2ndVariant = TRUE)["dist"]
      ), pId)
    }
  })
})
```

```{r p2a-avg-area-scores, echo=FALSE, fig.height=8, fig.cap="All projects plotted against the two variables req\\% and dev\\% of pattern type II (a).", fig.align="center", fig.pos="ht!"}
par(mfrow=c(2,1))


plot(x=0, y=0, xlim = c(0,1), ylim = c(0, 1), col = "#00000000",
     xlab = "Relative time", ylab = "Rate of change")
grid()
curve(req_p2a, 0, 1, col="red", lwd=2, add = TRUE)
curve(req_ci_upper_p2a, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)
curve(req_ci_lower_p2a, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)
polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), t1_wavg),
  y = c(req_ci_upper_p2a(seq(from = 0, to = 1, length.out = 50)), req_ci_lower_p2a(t1_wavg)),
  col = "#ff00000d",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$REQ$get0Function()
  curve(tempf, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.01, .98, legend = c("req%", "req% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))



plot(x=0, y=0, xlim = c(0,1), ylim = c(0, 1), col = "#00000000",
     xlab = "Relative time", ylab = "Rate of change")
grid()
curve(dev_p2a, 0, 1, col="blue", lwd=2, add = TRUE)
tempf <- function(x) rep(0, length(x))
curve(tempf, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)
curve(dev_ci_upper_p2a, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)
polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper_p2a(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff0d",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$DEV$get0Function()
  curve(tempf, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.01, .98, legend = c("dev%", "dev% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))
```

```{r echo=FALSE}
if (interactive()) {
  p2a_avg_area_scores
} else {
  knitr::kable(
    x = p2a_avg_area_scores,
    booktabs = TRUE,
    caption = "The average distance of the variables REQ and DEV of each project to the reference-variables $\\operatorname{REQ},\\operatorname{DEV}$ as taken from pattern I and adjusted by the optimized $t_1,t_2$ thresholds and timewarping.",
    label = "p2a-avg-area-scores"
  )
}
```

```{r}
cor(x = ground_truth$consensus_score, y = p2a_avg_area_scores[, "REQ"])
cor(x = ground_truth$consensus_score, y = p2a_avg_area_scores[, "DEV"])
```

Now we can observe quite an improvement for both variables. The correlation for `REQ` has increased by more than $1$, so it is moderate now and has the right sign. As for `DEV`, the correlation is almost four times as strong.


## Pattern III (average)

The third kind of pattern is based on data only, all the variables, confidence intervals and the strength thereof are based on the nine projects and the weight, which is the same as their consensus score.

_TODO_.


### Scoring based on the confidence intervals

We have calculated gradated confidence intervals, which means two things. First, we cannot apply a binary detection rule any longer, as the boundaries of the intervals include each project, only the weight is different. Second, when calculating a score, we will obtain a continuous measure, of which we can calculate a correlation to the consensus score of the ground truth, or, e.g., fit a linear model for scaling these scores.


```{r}
p3_avg_ci_scores <- loadResultsOrCompute(file = "../results/p3_avg_ci_scores.rds", computeExpr =  {
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind
    ) %dopar% {
      req <- all_signals[[pId]]$REQ$get0Function()
      dev <- all_signals[[pId]]$DEV$get0Function()
      desc <- all_signals[[pId]]$DESC$get0Function()
      
      `rownames<-`(data.frame(
        REQ = L_avgconf_p3_avg(x1 = 0, x2 = 1, f = req, CI = CI_req_p3avg),
        DEV = L_avgconf_p3_avg(x1 = 0, x2 = 1, f = dev, CI = CI_dev_p3avg),
        DESC = L_avgconf_p3_avg(x1 = 0, x2 = 1, f = desc, CI = CI_desc_p3avg)
      ), pId)
    }
  })
})
```

Table \ref{tab:p3-avg-ci-scores} shows the computed scores.

```{r echo=FALSE}
if (interactive()) {
  p3_avg_ci_scores
} else {
  knitr::kable(
    x = p3_avg_ci_scores,
    booktabs = TRUE,
    caption = "The average confidence of the variables REQ, DEV and DESC of each project as integrated over the confidence intervals' hyperplane",
    label = "p3-avg-ci-scores"
  )
}
```

Let's test the correlation between either kind of score and the ground truth consensus score. The null hypothesis of this test states that both samples have no correlation.

```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_ci_scores[, "REQ"])
```

For the variable `REQ` we get a significant correlation of $\approx0.83$, and there is no significant evidence for the null hypothesis, so must reject it.


```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_ci_scores[, "DEV"])
```

For the variable `DEV` however, the correlation is quite low, $\approx0.17$. Also, we have significant evidence for accepting the null hypothesis (no correlation).

```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_ci_scores[, "DESC"])
```

Looks like we are getting some strong positive correlation for the variable `DESC` of $\approx0.85$. There is almost no evidence at all for accepting the null hypothesis.



### Scoring based on the distance to average

Here we compute the distance of each project's variables to the previously averaged variables. This approach does not rely on inhomogeneous confidence intervals, and only considers the intervals' boundaries to integrate some distance. Ideally, the distance is $0$, and in the worst case it is $1$. If this ought to be used as score, we probably would want to compute it as $\mathit{S}^{\text{areadist}}=1-\mathit{L}^{\text{areadist}}$.

```{r}
p3_avg_area_scores <- loadResultsOrCompute(file = "../results/p3_avg_area_scores.rds", computeExpr =  {
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind
    ) %dopar% {
      req <- all_signals[[pId]]$REQ$get0Function()
      dev <- all_signals[[pId]]$DEV$get0Function()
      desc <- all_signals[[pId]]$DESC$get0Function()
      
      `rownames<-`(data.frame(
        REQ = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = req, gbar = req_p3, use2ndVariant = TRUE,
          CI_upper = req_ci_upper_p3avg, CI_lower = req_ci_lower_p3avg)["dist"],
        
        DEV = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = dev, gbar = dev_p3, use2ndVariant = TRUE,
          CI_upper = dev_ci_upper_p3avg, CI_lower = dev_ci_lower_p3avg)["dist"],
        
        DESC = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = desc, gbar = desc_p3, use2ndVariant = TRUE,
          CI_upper = desc_ci_upper_p3avg, CI_lower = desc_ci_lower_p3avg)["dist"]
      ), pId)
    }
  })
})
```

Table \ref{tab:p3-avg-area-scores} shows the computed scores.

```{r echo=FALSE}
if (interactive()) {
  p3_avg_area_scores
} else {
  knitr::kable(
    x = p3_avg_area_scores,
    booktabs = TRUE,
    caption = "The average distance of the variables REQ, DEV and DESC of each project to the previously averaged reference-variables $\\overline{\\operatorname{REQ}},\\overline{\\operatorname{DEV}},\\overline{\\operatorname{DESC}}$.",
    label = "p3-avg-area-scores"
  )
}
```

As for the correlation tests, ideally, we get negative correlations, as the computed score is __lower__ the less distance we find between the area of the average variable and a project's variable, hence the relation must be antiproportional.


```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_area_scores[, "REQ"])
```

For the variable `REQ` we get a weaker, yet moderate correlation of $\approx-0.42$. However, there is evidence for the null hypothesis, which suggests that there is no statistical significant correlation. This means, we will have to use this with care, if at all.


```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_area_scores[, "DEV"])
```

The correlation for the variable `DEV` is poor again. Also, it is positive, which means that the scores calculated using this method are proportional, when they should not be. The p-value is significant, so there is most likely no significant correlation for this variable and the ground truth.

```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_area_scores[, "DESC"])
```

The correlation for `DESC` is substantial, and also it is negative like it should be. The p-value is below the significance level $0.05$, suggesting correlation. We might be able to use this score.



### Linear combination of the two methods

```{r}
temp <- data.frame(
  gt_consensus = ground_truth$consensus_score,
  ci_req = p3_avg_ci_scores$REQ,
  area_req = p3_avg_area_scores$REQ,
  ci_dev = p3_avg_ci_scores$DEV,
  area_dev = p3_avg_area_scores$DEV,
  ci_desc = p3_avg_ci_scores$DESC,
  area_desc = p3_avg_area_scores$DESC)

# ci_req + area_desc gives us ~0.951 already!
# ci_req + ci_dev + area_desc gives ~0.962
p3_avg_lm <- stats::lm(formula = gt_consensus ~ ci_req + ci_dev + area_desc, data = temp)
stats::coef(p3_avg_lm)
par(mfrow=c(1,2))
plot(p3_avg_lm, ask = FALSE, which = 1:2)
```

Using the approximate coefficients of the linear model, we can define the detector as follows:

$$
\begin{aligned}
  x_1,x_2,\operatorname{req},\operatorname{dev},\operatorname{desc}\dots&\;\text{lower/upper integration interval and project signals,}
  \\[1ex]
  \bm{\tau}=&\;\Big\{\mathit{L}^{\text{avgconf}}(x_1,x_2,\operatorname{req}),\;\mathit{L}^{\text{avgconf}}(x_1,x_2,\operatorname{dev}),\;\mathit{L}^{\text{areadist2}}(x_1,x_2,\operatorname{desc})\Big\}\;\text{,}
  \\[1ex]
  \operatorname{detect}^{\text{ci+area}}(\bm{\tau})=&\;-0.029 + 3.011\times\bm{\tau}_1 + 0.839\times\bm{\tau}_2 - 0.478\times\bm{\tau}_3\;\text{.}
\end{aligned}
$$


```{r}
p3_avg_lm_scores <- stats::predict(p3_avg_lm, temp)
# Since we are attempting a regression to positive scores,
# we set any negative predictions to 0. Same goes for >1.
p3_avg_lm_scores[p3_avg_lm_scores < 0] <- 0
p3_avg_lm_scores[p3_avg_lm_scores > 1] <- 1

round(p3_avg_lm_scores * 10, 3)
stats::cor(p3_avg_lm_scores, ground_truth$consensus_score)
```

With this linear combination of only three scores (out of six), we were able to significantly boost the detection to $\approx0.96$, which implies that combining both methods is of worth for a detection using (inhomogeneous) confidence intervals only. If we only combine the scores of the variable `REQ` into a model, we still achieve a correlation of $\approx0.88$. This should probably be preferred to keep the degrees of freedom low, countering overfitting. Using four or more scores goes beyond $0.97$.


## Pattern IV

This pattern emerged only recently, but we have some options of scoring the projects against it. Let me start by emphasizing again how the concrete tests here are performed using an instantiation of type IV using pattern I, and how improper type I actually fits the data. The tests here, in the best case, should be run for all of the previously introduced patterns, and our expectation is that the data-only pattern would perform best, with the data-enhanced one coming in at second place. We mostly chose type I as we have analytical expressions for some of the variables.

Again, we have (derived) confidence intervals for the variables `REQ` and `DEV`. Then we have derivatives for all variables. Scoring based on CIs are hence applicable, but only for the first two variables. Other than that, any method based on comparing two variables is applicable. For our evaluations, we will use LOESS-smoothing with different smoothing-spans for either score, that are applicable.


In figure \ref{fig:p4-deriv-signals} we show all projects against the fourth pattern, as smoothed using LOESS and derived. It is interesting to see that at $\approx0.37$, where the confidence surface is the smallest for `REQ` and the upper confidence interval's rate of change becomes $0$, most projects also have a turning point.


```{r p4-deriv-signals, echo=FALSE, fig.height=8, fig.cap="All projects' derivative variables plotted against the derivatives of the two variables req\\% and dev\\% of pattern type I. The selected smoothing-span was 0.3.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(2,1))
use_span <- 0.3


plot(x=0, y=0, xlim = c(0,1), ylim = c(0, max(d1_vals_req(0))), col = "#00000000",
     xlab = "Relative time", ylab = "Rate of change")
grid()
curve(req_d1_p4, 0, 1, col="red", lwd=2, add = TRUE)
curve(req_ci_lower_d1_p4, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)
curve(req_ci_upper_d1_p4, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)

polygon(
  x = polygon_x,
  y = c(sapply(X = polygon_x[1:500], FUN = function(x) max(d1_vals_req(x=x))),
        sapply(X = polygon_x[501:1000], FUN = function(x) min(d1_vals_req(x=x)))),
  col = "#ff000033",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  
  X <- 1:nrow(all_signals[[pId]]$data)
  Y_req <- cumsum(all_signals[[pId]]$data$req)
  
  loess_req <- smooth_signal_loess(x = X, y = Y_req, span = use_span)
  req_tempf_d1 <- function(x) func_d1(f = loess_req, x = x)
  
  curve(req_tempf_d1, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.75, max(d1_vals_req(0)) - 0.02, legend = c("req%", "req% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))



plot(x=0, y=0, xlim = c(0,1), ylim = c(0, max(d1_vals_dev(1))), col = "#00000000",
     xlab = "Relative time", ylab = "Rate of change")
grid()
curve(dev_d1_p4, 0, 1, col="blue", lwd=2, add = TRUE)
curve(dev_ci_lower_d1_p4, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)
curve(dev_ci_upper_d1_p4, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)

polygon(
  x = polygon_x,
  y = c(sapply(X = polygon_x[1:500], FUN = function(x) max(d1_vals_dev(x=x))),
        sapply(X = polygon_x[501:1000], FUN = function(x) min(d1_vals_dev(x=x)))),
  col = "#0000FF33",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  
  X <- 1:nrow(all_signals[[pId]]$data)
  Y_dev <- cumsum(all_signals[[pId]]$data$dev)
  
  loess_dev <- smooth_signal_loess(x = X, y = Y_dev, span = use_span)
  dev_tempf_d1 <- function(x) func_d1(f = loess_dev, x = x)
  
  curve(dev_tempf_d1, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.01, max(d1_vals_dev(1)) - 0.02, legend = c("dev%", "dev% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))
```


### Scoring based on the distance to reference

This method is applicable because we only do have the confidence intervals' boundaries, and a homogeneous surface. The derivative of the respective variable is the expectation for the rate of change, and the confidence intervals demarcate a minimum and maximum expectation. This applies only to the variables `REQ` and `DEV`. Since we compute the distance in terms of the area between curves, the gradients should use a lower smoothing-span to be detail-preserving. However, we will try a few to see what works best.


```{r}
p4_compute_areadist <- function(span, intFrom = 0, intTo = 1) {
  d1_vals <- function(f, f_low, f_upp, x, useMax = TRUE) {
    sapply(X = x, FUN = function(x_) {
      temp <- c(f(x_), f_low(x_), f_upp(x_))
      if (useMax) max(temp) else min(temp)
    })
  }
  
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind,
      .packages = c("cobs"),
      .export = c("all_signals", "L_areadist_p3_avg",
                  "func_d1", "req", "dev", "req_d1_p4", "dev_d1_p4",
                  "req_ci_upper_d1_p4", "dev_ci_upper_d1_p4",
                  "req_ci_lower_d1_p4", "dev_ci_lower_d1_p4",
                  "smooth_signal_loess", "req_poly", "dev_poly")
    ) %dopar% {
      X <- 1:nrow(all_signals[[pId]]$data)
      Y_req <- cumsum(all_signals[[pId]]$data$req)
      Y_dev <- cumsum(all_signals[[pId]]$data$dev)
      
      req_tempf <- smooth_signal_loess(x = X, y = Y_req, span = span, neval = 1e3)
      dev_tempf <- smooth_signal_loess(x = X, y = Y_dev, span = span, neval = 1e3)
      
      req_d1 <- function(x) func_d1(f = req_tempf, x = x)
      dev_d1 <- function(x) func_d1(f = dev_tempf, x = x)
      
      req_upper_tempf <- function(x) d1_vals(
        f = req_d1_p4, f_low = req_ci_lower_d1_p4, f_upp = req_ci_upper_d1_p4, x = x, useMax = TRUE)
      req_lower_tempf <- function(x) d1_vals(
        f = req_d1_p4, f_low = req_ci_lower_d1_p4, f_upp = req_ci_upper_d1_p4, x = x, useMax = FALSE)
      
      dev_upper_tempf <- function(x) d1_vals(
        f = dev_d1_p4, f_low = dev_ci_lower_d1_p4, f_upp = dev_ci_upper_d1_p4, x = x, useMax = TRUE)
      dev_lower_tempf <- function(x) d1_vals(
        f = dev_d1_p4, f_low = dev_ci_lower_d1_p4, f_upp = dev_ci_upper_d1_p4, x = x, useMax = FALSE)
      
      `rownames<-`(data.frame(
        REQ = L_areadist_p3_avg(
          x1 = intFrom, x2 = intTo, f = req_d1, gbar = req_d1_p4, use2ndVariant = TRUE,
          CI_upper = req_upper_tempf, CI_lower = req_lower_tempf)["dist"],
        
        DEV = L_areadist_p3_avg(
          x1 = intFrom, x2 = intTo, f = dev_d1, gbar = dev_d1_p4, use2ndVariant = TRUE,
          CI_upper = dev_upper_tempf, CI_lower = dev_lower_tempf)["dist"],
        
        span = span,
        begin = intFrom,
        end = intTo
      ), pId)
    }
  })
}
```

```{r echo=FALSE}
p4_area_scores <- loadResultsOrCompute(file = "../results/p4_area_scores.rds", computeExpr =  {
  temp <- NULL
  for (span in seq(from = 0.2, to = 1, by = 0.1)) {
    begin <- as.numeric(Sys.time())
    temp <- rbind(temp, p4_compute_areadist(span = span))
    print(paste0("Finished computing for span=", span, " in ", round(as.numeric(Sys.time()) - begin), "s"))
  }
  temp
})
```

```{r echo=FALSE}
p4_area_scores_corr <- NULL

for (span in sort(unique(p4_area_scores$span))) {
  temp <- p4_area_scores[p4_area_scores$span == span, ]
  temp <- temp[order(rownames(temp)), ]
  
  p4_area_scores_corr <- rbind(p4_area_scores_corr, data.frame(
    span = min(temp$span),
    corr_REQ = cor(x = ground_truth$consensus_score, y = temp[, "REQ"]),
    corr_DEV = cor(x = ground_truth$consensus_score, y = temp[, "DEV"]),
    corr_REQ_pval = cor.test(x = ground_truth$consensus_score, y = temp[, "REQ"])$p.value,
    corr_DEV_pval = cor.test(x = ground_truth$consensus_score, y = temp[, "DEV"])$p.value
  ))
}
```

```{r echo=FALSE}
if (interactive()) {
  p4_area_scores_corr
} else {
  knitr::kable(
    x = p4_area_scores_corr,
    booktabs = TRUE,
    caption = "",
    label = "p4-area-scores-corr"
  )
}
```

In table \ref{tab:p4-area-scores-corr} we can observe a clear impact of the smoothing-span on the correlation of the computed distance vs. the ground truth. Note that ideally, the correlation is negative, as a higher distance corresponds to a lower score.

The correlations for the `REQ` variable get slightly stronger with increasing smoothing-spans. However, the correlations are positive when they should not be. This increase is perhaps explained by the gradually increasing area overlaps. However, since it affects all projects and all of them have a similar overlap with the confidence interval, the distance gets lower, hence resulting in higher correlations.

As for the `DEV` variable, with increasing span the correlations get lower. That is because most of the projects run outside the confidence intervals of the pattern, and with increasing span, those parts that were inside, are getting more and more outside, as the peaks are smoothed out, hence resulting in a lower correlation. The correlation for the smallest span is close to being acceptable, if we consider the p-value. Also, all the correlations are negative, like they should be.


```{r echo=FALSE}
p4_area_scores_partial <- loadResultsOrCompute(file = "../results/p4_area_scores_partial.rds", computeExpr =  {
  temp <- NULL
  for (span in c(.2, .3, .4, .6, .8, 1)) {
    for (end in c(1/3, 1/2, 2/3)) {
      temp <- rbind(temp, p4_compute_areadist(span = span, intTo = end))
    }
    print(paste0("Finished computing for span=", span))
  }
  temp
})
```

```{r echo=FALSE}
p4_area_scores_partial_corr <- NULL

for (span in sort(unique(p4_area_scores_partial$span))) {
  for (end in c(1/3, 1/2, 2/3)) {
    temp <- p4_area_scores_partial[p4_area_scores_partial$span == span & p4_area_scores_partial$end == end, ]
    temp <- temp[order(rownames(temp)), ]
    
    p4_area_scores_partial_corr <- rbind(p4_area_scores_partial_corr, data.frame(
      span = min(temp$span),
      end = round(min(temp$end), 2),
      corr_REQ = cor(x = ground_truth$consensus_score, y = temp[, "REQ"]),
      corr_DEV = cor(x = ground_truth$consensus_score, y = temp[, "DEV"]),
      corr_REQ_pval = cor.test(x = ground_truth$consensus_score, y = temp[, "REQ"])$p.value,
      corr_DEV_pval = cor.test(x = ground_truth$consensus_score, y = temp[, "DEV"])$p.value
    ))
  }
}
```

```{r echo=FALSE}
p4_area_scores_partial_corr[is.na(p4_area_scores_partial_corr)] <- 0

if (interactive()) {
  p4_area_scores_partial_corr
} else {
  knitr::kable(
    x = p4_area_scores_partial_corr,
    booktabs = TRUE,
    caption = "",
    label = "p4-area-scores-partial-corr"
  )
}
```

In table \ref{tab:p4-area-scores-partial-corr} we can clearly observe how the correlation of the computed score declines in almost every group of spans (esp. for the `REQ` variable), the more time we consider, i.e., in a consistent manner the scores decline, and it is the same phenomenon for every smoothing-span. This is expected since pattern IV is the derivative of pattern I, which itself is only a poor reconciliation of how the Fire Drill apparently manifests in the data, which means that the more discrepancy we consider, the less good the results get. Again, we should consider computing this correlation-based score when using a data-enhanced or data-only pattern as actually, a typical moderate correlation with the scores of $\approx-0.4$ (or better) as in table \ref{tab:p4-area-scores-partial-corr} for the `DEV`-variable can be expected to substantially increase with a more suitable pattern.


### Correlation between curves

Here we will compare each project's variable with the corresponding variable from the fourth pattern. We will take samples from either, pattern and variable, at the same $x$, and then compute the sample correlation. This is a very simple but efficient test. Also, it is subject to user-defined intervals, i.e., we do not have to sample from project to project end. Note that this would probably not work for the first pattern, as it normalizes the variables at project end. Pattern IV however uses the rate of change, which is not affected by that.

We will be using LOESS-smoothed curved with a somewhat higher smoothing-span of $0.6$. Also, since this test does not consider confidence intervals, we can also compare the `DESC` variable.

```{r}
N <- 200
use_span <- 0.6
X_samp <- seq(from = 0, to = 1, length.out = N)

p4_samples <- list(
  REQ = req_d1_p4(X_samp),
  DEV = dev_d1_p4(X_samp),
  DESC = desc_d1_p4(X_samp))

p4_corr <- NULL
for (pId in names(all_signals)) {
  
  X <- 1:nrow(all_signals[[pId]]$data)
  Y_req <- cumsum(all_signals[[pId]]$data$req)
  Y_dev <- cumsum(all_signals[[pId]]$data$dev)
  Y_desc <- cumsum(all_signals[[pId]]$data$desc)
  
  loess_req <- smooth_signal_loess(x = X, y = Y_req, span = use_span)
  req_tempf_d1 <- function(x) func_d1(f = loess_req, x = x)
  loess_dev <- smooth_signal_loess(x = X, y = Y_dev, span = use_span)
  dev_tempf_d1 <- function(x) func_d1(f = loess_dev, x = x)
  loess_desc <- tryCatch({
    smooth_signal_loess(x = X, y = Y_desc, span = use_span)
  }, error = function(cond) function(x) rep(0, length(x)))
  desc_tempf_d1 <- function(x) func_d1(f = loess_desc, x = x)
        
  p4_corr <- rbind(p4_corr, `rownames<-`(data.frame(
    REQ = stats::cor(p4_samples$REQ, req_tempf_d1(X_samp)),
    DEV = stats::cor(p4_samples$DEV, dev_tempf_d1(X_samp)),
    DESC = stats::cor(p4_samples$DESC, desc_tempf_d1(X_samp))
  ), pId))
}
```


```{r echo=FALSE}
p4_corr[is.na(p4_corr)] <- 0
if (interactive()) {
  p4_corr
} else {
  knitr::kable(
    x = p4_corr,
    booktabs = TRUE,
    caption = "Correlation scores of the derivatives with the derived project signals.",
    label = "p4-corr"
  )
}
```

```{r}
c(
  REQ = cor(ground_truth$consensus_score, p4_corr$REQ),
  DEV = cor(ground_truth$consensus_score, p4_corr$DEV),
  DESC = cor(ground_truth$consensus_score, p4_corr$DESC, use = "pa"),
  
  REQ_pval = cor.test(ground_truth$consensus_score, p4_corr$REQ)$p.value,
  DEV_pval = cor.test(ground_truth$consensus_score, p4_corr$DEV)$p.value,
  DESC_pval = cor.test(ground_truth$consensus_score, p4_corr$DESC, use = "pa")$p.value
)
```

The correlations in table \ref{tab:p4-corr} are varying, but their correlation with the ground truth is poor, and in the case of `REQ` and `DESC` even negative. That means that computing the correlation of the derived data with the derived project signals is not a suitable detector using pattern IV, which is the derivative of the expert-designed pattern I. However, this might work with a data-enhanced or data-only pattern. Our previous attempt using the distance in areas was much better.






















