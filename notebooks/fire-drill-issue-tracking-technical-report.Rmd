---
title: "Technical Report: Detecting the Fire Drill anti-pattern using issue-tracking data"
author: "Sebastian HÃ¶nel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ../inst/REFERENCES.bib
urlcolor: blue
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  md_document:
    toc: yes
    toc_depth: 6
    df_print: kable
    variant: gfm
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: kable
  word_document: default
header-includes:
- \usepackage{bm}
- \usepackage{mathtools}
- \usepackage{xurl}
---

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\norm}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}
\newcommand\argmax[1]{\underset{#1}{arg\,max}}
\newcommand\argmin[1]{\underset{#1}{arg\,min}}


```{r setoptions, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(tidy = TRUE, tidy.opts = list(indent=2))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

library(ggplot2)
library(ggpubr)
```

# Introduction\label{tr:fire-drill-issue-tracking-technical-report}




# Importing the data

## Ground truth

We have $9$ projects conducted by students, and two raters have __independently__, i.e., without prior communication, assessed to what degree the AP is present in each project. This was done using a scale from zero to ten, where zero means that the AP was not present, and ten would indicate a strong manifestation The entire ground truth is shown in table \ref{tab:groundtruth}.

```{r}
ground_truth <- read.csv(file = "../data/ground-truth.csv", sep = ";")
ground_truth$consensus_score <- ground_truth$consensus / 10
```

```{r echo=FALSE}
if (interactive()) {
  ground_truth
} else {
  knitr::kable(
    x = ground_truth,
    booktabs = TRUE,
    caption = "Entire ground truth as of both raters",
    label = "groundtruth"
  )
}
```


## Project data

In this section we import the projects' __issue-tracking__-data. All projects' data will be normalized w.r.t. the time, i.e., each project will have a support of $[0,1]$. The variables are modeled as cumulative time spent on issues. Each variable in each project will be loaded into an instance of `Signal`.

```{r}
library(readxl)

load_project_issue_data <- function(pId) {
  data <- read_excel(
    "../data/FD_issue-based_detection.xlsx", sheet = pId)
  data$desc[is.na(data$desc)] <- 0
  
  req_cs <- cumsum(data$req) / sum(data$req)
  dev_cs <- cumsum(data$dev) / sum(data$dev)
  desc_cs <- cumsum(data$desc) / max(cumsum(data$dev))
  X <- seq(from = 0, to = 1, length.out = length(req_cs))
  
  signal_rec <- Signal$new(
    func = stats::approxfun(x = X, y = req_cs, yleft = 0, yright = 1),
    name = "REQ", support = c(0,1), isWp = TRUE)
  signal_dev <- Signal$new(
    func = stats::approxfun(x = X, y = dev_cs, yleft = 0, yright = 1),
    name = "DEV", support = c(0,1), isWp = TRUE)
  signal_desc <- Signal$new(
    func = stats::approxfun(x = X, y = desc_cs, yleft = 0, yright = max(desc_cs)),
    name = "DESC", support = c(0,1), isWp = TRUE)
  
  list(
    REQ = signal_rec,
    DEV = signal_dev,
    DESC = signal_desc
  )
}
```

Let's attempt to replicate the graphs of the first project (cf. figure \ref{fig:p1-example}):

```{r}
p3_signals <- load_project_issue_data(pId = "Project3")
req_f <- p3_signals$REQ$get0Function()
dev_f <- p3_signals$DEV$get0Function()
desc_f <- p3_signals$DESC$get0Function()
```


```{r p1-example, echo=FALSE, fig.cap="The two variables of the first project.", fig.align="center", fig.pos="ht!"}
curve(req_f, 0, 1, col = "blue", xlab = "Relative time", ylab = "Cumulative time spent")
curve(dev_f, 0, 1, col = "red", add = TRUE)
curve(desc_f, 0, 1, col = "gold", add = TRUE)
legend(0, 1, legend = c("cum. req%", "cum. dev%", "cum. desc%"), col = c("blue", "red", "gold"), lty = 1, lwd = 2)
```

OK, that works well. It'll be the same for all projects, i.e., only two variables, time spent on requirements- and time spent on development-issues, is tracked. That means we will only be fitting two variables later.

Let's load, store and visualize all projects (cf. figure \ref{fig:project-it-vars}):

```{r load-data}
all_signals <- list()
for (pId in paste0("Project", 1:9)) {
  all_signals[[pId]] <- load_project_issue_data(pId = pId)
}
```

```{r echo=FALSE}
tempdf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(tempdf) <- c("x", "y", "p", "v")
n <- 500
x <- seq(from = 0, to = 1, length.out = n)
for (pId in names(all_signals)) {
  for (v in c("REQ", "DEV", "DESC")) {
    tempdf <- rbind(tempdf, data.frame(
      x = x,
      y = sapply(X = x, FUN = all_signals[[pId]][[v]]$get0Function()),
      p = pId,
      v = v
    ))
  }
}
```

```{r project-it-vars, echo=FALSE, fig.cap="All variables over each project's time span.", fig.align="top", fig.pos="ht!"}
plot_all_req_dev <- ggplot(data = tempdf, aes(x = x, y = y, color = v)) +
  geom_line() +
  scale_color_manual(values = c("forestgreen", "blue", "red")) +
  facet_wrap(p ~.) +
  theme_light() +
  labs(color = "Variable") + xlab("Relative Time") + ylab("Cumulative Time spent") +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"))
plot_all_req_dev
```

```{r echo=FALSE, eval=FALSE}
# Let's save this as tikz:
tikzDevice::tikz(file = "../figures/all_it_acp.tex", width = 3.4, height = 1.8)
plot_all_req_dev +
  theme_light(base_size = 10) +
  theme(
    axis.title.x.bottom = element_text(margin = margin(b=5), size = 10),
    axis.title.y.left = element_text(margin = margin(r=3), size = 10),
    legend.position = "bottom",
    legend.margin = margin(-7.5, 0, 0, 0),
    strip.background = element_rect(fill="#dfdfdf"))
dev.off()
```

# Patterns for scoring the projects


## Pattern I: Consensus of two experts

The initial pattern as defined for the detection of the Fire Drill AP is imported/created/defined here, and its variables and confidence intervals are modeled as continuous functions over time.

There are some values (x/y coordinates) for which we want to guarantee that the confidence intervals or the variables themselves pass through. Also, the two points in time $t_1,t_2$ are defined to be at $0.4$ and $0.85$, respectively.

```{r p1-constants}
t_1 <- 0.4
t_2 <- 0.85

# req(t_1)
req_t_1 <- 0.7

# dev(t_1), dev(t_2)
dev_t_1 <- 0.075
dev_t_2 <- 0.4
```

This initial version of the pattern is not based on any data, observation or ground truth, but solely on two independent experts that reached a consensus for every value a priori any of the detection approaches.


### Variable: Requirements, analysis, planning

The variable itself is not given, only its upper- and lower confidence-intervals (CI), where the latter simply is $\operatorname{req}^{\text{CI}}_{\text{lower}}(x)=x$. The upper CI is given by the informal expression $\operatorname{req}^{\text{CI}}_{\text{upper}}(x)=1.02261-1.02261\times\exp{(-3.811733\times x)}$. All together is shown in figure \ref{fig:req-cis}.

The variable itself is not given, as it was not important for the binary decision rule, whether or not a project's variable is within the confidence interval. It is still not important, what matters is that it runs through the confidence interval, and we will design it by fitting a polynomial through some inferred points from the plot. In some practical case however, the variable's course may be important, and while we will later use the variable to compute some kind of loss between it, the confidence interval and some project's variable, we only do this for demonstration purposes.

Let's first define the variable using some supporting x/y coordinates. It needs to be constrained such that it runs through 0,0 and 1,1:

```{r}
req_poly <- cobs::cobs(
  x = seq(from = 0, to = 1, by = .1),
  y = c(0, .25, .425, .475, .7, .8, .85, .9, .95, .975, 1),
  pointwise = matrix(data = c(
    c(0, 0, 0),
    c(0, t_1, req_t_1),
    c(0, 1, 1)
  ), byrow = TRUE, ncol = 3)
)

# Now we can define the variable simply by predicting from
# the polynomial (btw. this is vectorized automatically):
req <- function(x) {
  stats::predict(object = req_poly, z = x)[, "fit"]
}
```

And now for the confidence intervals:

```{r}
req_ci_lower <- function(x) x
req_ci_upper <- function(x) 1.02261 - 1.02261 * exp(-3.811733 * x)
```


```{r req-cis, echo=FALSE, fig.cap="req\\% and its lower- and upper confidence interval.", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = seq(from = 0, to = 1, length.out = 50),
  y = req_ci_upper(seq(from = 0, to = 1, length.out = 50)),
  col = "#ff000033",
  border = NA)
curve(req_ci_lower, 0, 1, col = "red", lty = 2, add = TRUE)
curve(req_ci_upper, 0, 1, col = "red", lty = 3, add = TRUE)
curve(req, 0, 1, col = "red", lty = 1, lwd = 2, add = TRUE)
legend(.74, .3, legend = c("upper CI", "lower CI", "req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))
abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")


# t_1:
abline(v = t_1)
text(x = .47, y = .98, paste0("t_1=", t_1))

# other thresholds:
abline(h = req(t_1), col = "#888888")
text(x = .5, y = .65, paste0("req(t_1)=", round(req(t_1), 2)))
abline(h = req_ci_upper(t_1), col = "#888888")
text(x = .24, y = .86, paste0("req_ci_upper(t_1)=", round(req_ci_upper(t_1), 2)))
```


### Variables: Design, implementation, testing, bugfixing and Descoping

Again, the variable for design etc. is not given, but rather only its upper confidence interval. Its lower CI is simply always zero. The upper CI is given by the informal expression $\operatorname{dev}^{\text{CI}}_{\text{upper}}(x)=0.07815904\times x+0.6222767\times x^2+0.2995643\times x^3$. The variable for de-scoping comes without confidence interval, and is defined by $\operatorname{desc}(x)=0.01172386\times x + 0.0933415\times x^2 + 0.04493464\times x^3$.

First we will define/fit a polynomial that describes the variable for design etc., the same way we did for requirements etc. we do know that it should pass through the points $[t_1,\approx0.075]$, as well as $[t_2,\approx0.4]$.

```{r}
dev_poly <- cobs::cobs(
  x = seq(from = 0, to = 1, by = .1),
  y = c(0, .0175, .035, .055, dev_t_1, .014, .165, .2, .28, .475, 1),
  pointwise = matrix(data = c(
    c(0, t_1, dev_t_1),
    c(0, t_2, dev_t_2),
    c(0, 1, 1)
  ), byrow = TRUE, ncol = 3)
)

# Now we can define the variable simply by predicting from
# the polynomial (btw. this is vectorized automatically):
dev <- function(x) {
  temp <- stats::predict(object = dev_poly, z = x)[, "fit"]
  # I cannot constrain the polynomial at 0,0 and it returns
  # a very slight negative value there, so let's do it this way:
  temp[temp < 0] <- 0
  temp[temp > 1] <- 1
  temp
}
```

Next we define the upper confidence interval for the variable `DEV`, then the variable for de-scoping. All is shown in figure \ref{fig:dev-desc-cis}.

```{r}
dev_ci_upper <- function(x) 0.07815904 * x + 0.6222767 * x^2 + 0.2995643 * x^3
desc <- function(x) 0.01172386 * x + 0.0933415 * x^2 + 0.04493464 * x^3
```


```{r dev-desc-cis, echo=FALSE, fig.cap="The variable dev\\% and its upper confidence interval, as well as the variable desc\\%.", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff33",
  border = NA)

curve(dev_ci_upper, 0, 1, col = "blue", lty = 2, add = TRUE)
curve(desc, 0, 1, add = TRUE, col = "forestgreen", lty = 1, lwd = 3)
curve(dev, 0, 1, col = "blue", lty = 1, lwd = 3, add = TRUE)
legend(.02, 1.01, legend = c("dev upper CI", "dev%", "desc%"),
       col = c("blue", "blue", "forestgreen"), lty = c(3, 1, 1), lwd = c(1, 3, 3))

abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")

# t_1,t_2:
abline(v = t_1)
text(x = .47, y = .98, paste0("t_1=", t_1))
abline(v = t_2)
text(x = .775, y = .98, paste0("t_2=", t_2))

# other thresholds:
abline(h = dev_ci_upper(t_2), col = "#888888")
text(x = .565, y = .76, paste0("dev_ci_upper(t_2)=", round(dev_ci_upper(t_2), 2)))
abline(h = desc(1), col = "#888888")
text(x = .94, y = .24, paste0("desc(1)\n   =", round(desc(1), 2)))

# t_1(dev), t_2(dev):
abline(h = dev(t_1), col = "#888888")
text(x = .29, y = .2, paste0("dev(t_1)=", round(dev(t_1), 3)))
abline(h = dev(t_2), col = "#888888")
text(x = .5, y = .46, paste0("dev(t_2)=", round(dev(t_2), 2)))
```


## Pattern II: Partial adaptation of first pattern

We will be attempting three kinds of adaptations to the first pattern:

a)    Learn $t_1,t_2$ from the data: There is not much to learn, but we could attempt to define these two thresholds as weighted average over the ground truth. Alternatively, we could formulate an optimization problem. We then use _time warping_ to alter the first pattern, __including__ its confidence intervals.
b)    Additionally to a (after learning $t_1,t_2$), we will apply _amplitude warping_ using __`srBTAW`__.
c)    Take the first pattern and apply both, _boundary time warping_ __and__ _boundary amplitude warping_, to produce a pattern that is (hopefully) closest to all projects in the ground truth. This is the very same approach we attempted for adapting the pattern of type I that we defined for the Fire Drill in source code.


### Type II (a): Adapt type I using thresholds $t_1,t_2$

The two variables `REQ` and `DEV` in the first pattern describe the cumulative time spent on two distinct activities. It was designed with focus on the confidence intervals, and a binary decision rule, such that the actual variables' course was not of interest.

To find the optimal value for a threshold, we could look at when each project is closest to $\operatorname{req}(t_1)$ and $\operatorname{dev}(t_2)$ (in relative time), and then compute a weighted average over it. However, since we already modeled each project's variables as *continuous-time stochastic process*, I suggest we use an optimization-based approach.

```{r t1t2-example}
tempf <- all_signals$Project5$REQ$get0Function()
tempf1 <- function(x) abs(tempf(x) - req_t_1)
optR <- optimize(tempf1, interval = c(0, 1))
optR
```

```{r t1t2-example-fig, echo=FALSE, fig.cap="The non-optimal optimum found by gradient-based optimization in project 5.", fig.align="center", fig.pos="ht!"}
curve(tempf1, 0, 1, xlab = "Relative time", ylab = "abs(REQ_p5(x) - req_t_1)")
points(x = optR$minimum, y = optR$objective,
       col = "red", pch = 4, cex = 1.5)
```

We will find the optimum using `nlopt` and a global optimization, because we actually will have a global optimum by re-arranging each project's variables. Also, gradient-based methods do not work well because of the nature of the variables, having large horizontal plateaus. This can be seen in figure \ref{fig:t1t2-example-fig}. Approaches using the built-in `optim` do hence not work well, the problem is clearly demonstrated in the previous code chunk, resulting in an objective $\gg0$ (which should ideally be $0$).

We want to find out when each project is closest to the previously defined thresholds. Each variable is a cumulative aggregation of the underlying values, which means that we have monotonic behavior.

$$
\begin{aligned}
  \min_{\hat{t}_1,\hat{t}_2\in R}&\;{\operatorname{req}(\hat{t}_1), \operatorname{dev}(\hat{t}_2)}\;\text{,}
  \\[1ex]
  \text{subject to}&\;0\leq\hat{t}_1,\hat{t}_2\leq1\;\text{, using}
  \\[1ex]
  \mathcal{L}_{\operatorname{req}}(x)=&\;\norm{\operatorname{req}(x)-\operatorname{req}(t_1)}\;\text{, and}
  \\[1ex]
  \mathcal{L}_{\operatorname{dev}}(x)=&\;\norm{\operatorname{dev}(x)-\operatorname{dev}(t_2)}\;\text{(quasi-convex loss functions).}
\end{aligned}
$$

The objective functions hence will be to find the global optimum (minimum), which occurs at $y\approx0$. Since we have plateaus in our data, we will potentially have infinitely many global optima. However, we are satisfied with any that is $\approx0$.

$$
\begin{aligned}
  \mathcal{O}_{\operatorname{dev}}(x)=&\;\argmin{\hat{x}\in R}\;{L_{\operatorname{dev}}(x)}\;\text{, and}
  \\[1ex]
  \mathcal{O}_{\operatorname{req}}(x)=&\;\argmin{\hat{x}\in R}\;{L_{\operatorname{req}}(x)}\text{.}
\end{aligned}
$$


```{r t1t2-nlopt}
library(nloptr)

set.seed(1)

t1t2_opt <- matrix(ncol = 4, nrow = length(all_signals))
rownames(t1t2_opt) <- names(all_signals)
colnames(t1t2_opt) <- c("req_sol", "dev_sol", "req_obj", "dev_obj")

find_global_low <- function(f) {
  nloptr(
    x0 = 0.5,
    opts = list(
      maxeval = 1e3,
      algorithm = "NLOPT_GN_DIRECT_L_RAND"),
    eval_f = f,
    lb = 0,
    ub = 1
  )
}

for (pId in paste0("Project", 1:9)) {
  sig_REQ <- all_signals[[pId]]$REQ$get0Function()
  req_abs <- function(x) abs(sig_REQ(x) - req_t_1)
  
  sig_DEV <- all_signals[[pId]]$DEV$get0Function()
  dev_abs <- function(x) abs(sig_DEV(x) - dev_t_2)
  
  optRes_REQ <- find_global_low(f = req_abs)
  optRes_DEV <- find_global_low(f = dev_abs)
  
  t1t2_opt[pId, ] <- c(optRes_REQ$solution, optRes_DEV$solution,
                       optRes_REQ$objective, optRes_DEV$objective)
}
```


```{r echo=FALSE}
if (interactive()) {
  t1t2_opt
} else {
  knitr::kable(
    x = t1t2_opt,
    booktabs = TRUE,
    caption = "Optimum values for $t_1$ and $t_2$ for each project, together with the loss of each project's objective function at that offset (ideally 0).",
    label = "t1t2-optvals"
  )
}
```

From the table \ref{tab:t1t2-optvals}, we can take an example to demonstrate that the optimization indeed found the global optimum (or a value very close to it, usually with a deviation $<1e^{-15}$). For the picked example of project 5, we can clearly observe a value for $x$ that results in a solution that is $\approx0$.

```{r test-test, echo=FALSE, fig.cap="Example of finding the optimum for req($t_1$) in project 5.", fig.align="center", fig.pos="ht!"}
pId <- "Project5"
tempf <- function(x) {
  abs(all_signals[[pId]]$REQ$get0Function()(x) - req_t_1)
}
curve(tempf, 0, 1)
points(x = t1t2_opt[pId, 1], y = t1t2_opt[pId, 3],
       col = "red", pch = 4, cex = 1.5)
```

Figure \ref{fig:test-test} depicts the optimal solution as found by using global optimization. Now what is left, is to calculate the weighted average for the optimized $\bm{\hat{t}_1},\bm{\hat{t}_2}$. The weighted arithmetic mean is defined as:

$$
\begin{aligned}
  \text{weighted mean}=&\;\Big[\sum\bm{\omega}\Big]^{-1}\times\bm{\omega}^\top\cdot\bm{\hat{t}}\;\text{, where}
  \\[1ex]
  \bm{\omega}\dots&\;\text{weight vector that corresponds to the consensus-score, and}
  \\[1ex]
  \bm{\hat{t}}\dots&\;\text{vector with optimal values for either}\;t_1\;\text{or}\;t_2\;\text{(as learned earlier).}
\end{aligned}
$$

```{r}
omega <- ground_truth$consensus_score
names(omega) <- paste0("Project", 1:length(omega))

t1_wavg <- t1t2_opt[, 1] %*% omega / sum(omega)
print(c(t_1, t1_wavg))
t2_wavg <- t1t2_opt[, 2] %*% omega / sum(omega)
print(c(t_2, t2_wavg))
```

Originally, $t_1$ was guessed to be located at `r t_1`, with $\operatorname{req}(t_1)=0.7$. The weighted average over the optimized values (where $\operatorname{req}(\hat{t}_1)\approx0.7$) suggests defining $\hat{t}_1$=`r round(t1_wavg, 5)`.

$t_2$ on the other hand was originally located at `r t_2`, with $\operatorname{dev}(t_2)=0.4$. The weighted average over the optimized values (where $\operatorname{dev}(\hat{t}_2)\approx0.4$) suggests defining $\hat{t}_2$=`r round(t2_wavg, 5)`.

Having learned these values, we can now adapt the pattern using time warping. For that, we have to instantiate the pattern using `srBTAW`, add the original boundaries and set them according to what we learned. Below, we define a function that can warp a single variable.

```{r}
timewarp_variable <- function(f, t, t_new) {
  temp <- SRBTW$new(
    wp = f, wc = f,
    theta_b = c(0, t_new, 1),
    gamma_bed = c(0, 1, sqrt(.Machine$double.eps)),
    lambda = c(0, 0),
    begin = 0, end = 1,
    openBegin = FALSE, openEnd = FALSE)
  temp$setParams(`names<-`(c(t, 1 - t), c("vtl_1", "vtl_2")))
  function(x) sapply(X = x, FUN = temp$M)
}
```

```{r}
req_p1a <- timewarp_variable(f = req, t = t_1, t_new = t1_wavg)
req_ci_lower_p1a <- timewarp_variable(f = req_ci_lower, t = t_1, t_new = t1_wavg)
req_ci_upper_p1a <- timewarp_variable(f = req_ci_upper, t = t_1, t_new = t1_wavg)
```


```{r req-p1a-cis, echo=FALSE, fig.cap="req\\% and its lower- and upper confidence interval after time warping for pattern type I (a).", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), t1_wavg),
  y = c(req_ci_upper_p1a(seq(from = 0, to = 1, length.out = 50)), req_ci_lower_p1a(t1_wavg)),
  col = "#ff000033",
  border = NA)
curve(req_ci_lower_p1a, 0, 1, col = "red", lty = 2, add = TRUE)
curve(req_ci_upper_p1a, 0, 1, col = "red", lty = 3, add = TRUE)
curve(req_p1a, 0, 1, col = "red", lty = 1, lwd = 2, add = TRUE)
legend(.65, .3, legend = c("Pt. I(a) upper CI", "Pt. I(a) lower CI", "Pt. I(a) req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))
abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")


# t1_wavg:
abline(v = t1_wavg)
text(x = .42, y = .93, paste0("t1_wavg=", round(t1_wavg, 4)))
```

Moving the boundary $t_1$ farther behind, changes the variable `REQ` and its confidence interval slightly, as can be seen in figure \ref{fig:req-p1a-cis}. Next, we will adapt the remaining variables and their confidence intervals.

```{r}
dev_p1a <- timewarp_variable(f = dev, t = t_2, t_new = t2_wavg)
dev_ci_upper_p1a <- timewarp_variable(f = dev_ci_upper, t = t_2, t_new = t2_wavg)
desc_p1a <- timewarp_variable(f = desc, t = t_2, t_new = t2_wavg)
```

```{r dev-desc-p1a-cis, echo=FALSE, fig.cap="The variable dev\\% and its upper confidence interval, as well as the variable desc\\%, after time warping for pattern type I (a).", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper_p1a(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff33",
  border = NA)

curve(dev_ci_upper_p1a, 0, 1, col = "blue", lty = 2, add = TRUE)
curve(desc_p1a, 0, 1, add = TRUE, col = "forestgreen", lty = 1, lwd = 3)
curve(dev_p1a, 0, 1, col = "blue", lty = 1, lwd = 3, add = TRUE)
legend(.02, 1.01, legend = c("Pt. I(a) dev upper CI", "Pt. I(a) dev%", "Pt. I(a) desc%"),
       col = c("blue", "blue", "forestgreen"), lty = c(3, 1, 1), lwd = c(1, 3, 3))

abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")

# t2_wavg:
abline(v = t2_wavg)
text(x = .71, y = .94, paste0("t2_wavg=", round(t2_wavg, 4)))
```

Moving the boundary $t_2$ was a more significant change for variables `DEV` and `DESC` than it was for `REQ`, as of figure \ref{fig:dev-desc-p1a-cis}.



## Pattern III: Averaging the ground truth

This is the same approach we undertook for pattern type III (average) for the Fire Drill in source code. However, we will also learn an __empirical confidence interval__, which is later used for two additional detection methods. These methods have the advantage that they work over arbitrary (integration) intervals, making them also applicable for early detection of the process (i.e., not the entire process needs to be observed, and we can just attempt to detect what we have so far).


```{r}
p3_weighted_var <- function(name, omega) {
  funcs <- list()
  for (pId in names(all_signals)) {
    funcs[[pId]] <- all_signals[[pId]][[name]]$get0Function()
  }
  
  function(x) sapply(X = x, FUN = function(x_) {
    omega %*% unlist(lapply(funcs, function(f) f(x_))) / sum(omega)
  })
}
```

```{r}
req_p3 <- p3_weighted_var(name = "REQ", omega = omega)
dev_p3 <- p3_weighted_var(name = "DEV", omega = omega)
desc_p3 <- p3_weighted_var(name = "DESC", omega = omega)
```

The computed weighted average-variables for `REQ` and `DEV` are shown in figure \ref{fig:avg-req-dev}.

```{r avg-req-dev, echo=FALSE, fig.cap="The weighted average for the two variables req\\% and dev\\%.", fig.align="center", fig.pos="ht!"}
curve(req_p3, 0, 1, col = "red", xlab = "Relative time", ylab = "Cumulative time spent")
curve(dev_p3, 0, 1, col = "blue", add = TRUE)
curve(desc_p3, 0, 1, col = "forestgreen", add = TRUE)
legend(0, 1, legend = c("cum. emp. req%", "cum. emp. dev%", "cum. emp. desc%"), col = c("blue", "red", "forestgreen"), lty = 1, lwd = 2)
```

### Determining an empirical and inhomogeneous confidence interval

Also, we want to calculate an empirical confidence interval and -surface, based on the projects' data and the consensus of the ground truth. The boundary of the lower confidence interval is defined as the infimum of all signals (and the upper CI as the supremum of all signals):

$$
\begin{aligned}
  \bm{f}\dots&\;\text{vector of functions (here: project signals),}
  \\[1ex]
  \operatorname{CI}_{\text{upper}}(x)=&\;\sup{\Bigg(\forall\,f\in\bm{f}\;\bigg[\begin{cases}
    -\infty,&\text{if}\;\bm{\omega}_n=0,
    \\
    f_n(x),&\text{otherwise}
  \end{cases}\bigg]\;,\;\frown\;,\;\Big[\dots\Big]\Bigg)}\;\text{,}
  \\[1ex]
  \operatorname{CI}_{\text{upper}}(x)=&\;\inf{\Bigg(\forall\,f\in\bm{f}\;\bigg[\begin{cases}
    \infty,&\text{if}\;\bm{\omega}_n=0,
    \\
    f_n(x),&\text{otherwise}
  \end{cases}\bigg]\;,\;\frown\;,\;\Big[\dots\Big]\Bigg)}\;\text{.}
\end{aligned}
$$

```{r}
funclist_REQ <- list()
funclist_DEV <- list()
funclist_DESC <- list()
for (pId in names(all_signals)) {
  funclist_REQ[[pId]] <- all_signals[[pId]]$REQ$get0Function()
  funclist_DEV[[pId]] <- all_signals[[pId]]$DEV$get0Function()
  funclist_DESC[[pId]] <- all_signals[[pId]]$DESC$get0Function()
}

CI_bound_p3avg <- function(x, funclist, omega, upper = TRUE) {
  sapply(X = x, FUN = function(x_) {
    val <- unlist(lapply(X = names(funclist), FUN = function(fname) {
      if (omega[fname] == 0) (if (upper) -Inf else Inf) else funclist[[fname]](x_)
    }))
    
    if (upper) max(val) else min(val)
  })
}

req_ci_upper_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_REQ, omega = omega, upper = TRUE)
req_ci_lower_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_REQ, omega = omega, upper = FALSE)
dev_ci_upper_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DEV, omega = omega, upper = TRUE)
dev_ci_lower_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DEV, omega = omega, upper = FALSE)
desc_ci_upper_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DESC, omega = omega, upper = TRUE)
desc_ci_lower_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DESC, omega = omega, upper = FALSE)
```

While the above expressions define the _boundaries_ of the lower and upper confidence intervals, we also need a function that interpolates in between. Recall that the CI of the first pattern was __homogeneous__, i.e., it provided no gradation, and was used for a binary decision rule. If we define a function that bases the strength of the confidence on the values of the ground truth of each project's variable, then it also means that with that pattern, all projects are included in the binary decision rule. Having gradation in the CI will allow us to make more probabilistic statements by computing some kind of score.

Figures \ref{fig:req-dev-p3avg-cis} and \ref{fig:desc-p3avg-cis} show the average variables and the empirical confidence intervals.

```{r req-dev-p3avg-cis, echo=FALSE, fig.height=8, fig.cap="Empirical (average) req\\% and dev\\% and their lower- and upper empirical weighted confidence intervals (here without gradation).", fig.align="center", fig.pos="ht!"}

par(mfrow=c(2,1))

plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     main = "Variable: REQ",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = seq(from = 0, to = 1, length.out = 50),
  y = req_ci_upper(seq(from = 0, to = 1, length.out = 50)),
  col = "#ff000011",
  border = NA)

polygon_x <- c(seq(from = 0, to = 1, length.out = 500), seq(from = 1, to = 0, length.out = 500))
polygon(
  x = polygon_x,
  y = c(req_ci_upper_p3avg(head(polygon_x, 500)), req_ci_lower_p3avg(tail(polygon_x, 500))),
  col = "#ff000044",
  border = NA)


curve(req_p3, 0, 1, col = "red", add = TRUE)

curve(req_ci_upper, 0, 1, col = "#ff000033", add = TRUE, lty = 2)
curve(req_ci_lower, 0, 1, col = "#ff000033", add = TRUE, lty = 3)

curve(req_ci_upper_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 2)
curve(req_ci_lower_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 3)


legend(0.5, .37, legend = c("Pt. 3 (avg) req% upper CI", "Pt. III (avg) req% lower CI", "Pt. III (avg) req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))




plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     main = "Variable: DEV",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff11",
  border = NA)

polygon(
  x = polygon_x,
  y = c(dev_ci_upper_p3avg(head(polygon_x, 500)), dev_ci_lower_p3avg(tail(polygon_x, 500))),
  col = "#0000ff44",
  border = NA)

curve(dev_p3, 0, 1, col = "blue", add = TRUE)

curve(dev_ci_upper, 0, 1, col = "#0000ff33", add = TRUE, lty = 2)

curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 3)

legend(0.01, .98, legend = c("Pt. 3 (avg) upper CI", "Pt. III (avg) lower CI", "Pt. III (avg) dev%"),
       col = "blue", lty = c(3, 2, 1), lwd = c(1, 1, 2))
```

```{r desc-p3avg-cis, echo=FALSE, fig.height=4, fig.cap="Empirical (average) desc\\% and its lower- and upper empirical weighted confidence intervals (here without gradation).", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     main = "Variable: DESC",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = polygon_x,
  y = c(desc_ci_upper_p3avg(head(polygon_x, 500)), desc_ci_lower_p3avg(tail(polygon_x, 500))),
  col = "#228B2244",
  border = NA)

curve(desc_p3, 0, 1, col = "forestgreen", add = TRUE)
curve(desc_ci_upper_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 2)
curve(desc_ci_lower_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 3)

legend(0.01, .98, legend = c("Pt. 3 (avg) desc% upper CI", "Pt. III (avg) desc% lower CI", "Pt. III (avg) desc%"),
       col = "forestgreen", lty = c(3, 2, 1), lwd = c(1, 1, 2))
```


But first, we define a function $f:R^2\mapsto R$ to compute a CI with gradation. For each x/y coordinate, it shall output a confidence based on the weights as of the ground truth, and all projects' variables that output a $y$ that is smaller than (larger than) the given $y$ shall be excluded.

$$
\begin{aligned}
  h_{\text{upper}}(f,x,y)=&\;\begin{cases}
    \begin{cases}
      f(x),&\text{if}\;f(x)\neq0,
      \\
      \epsilon,&\text{otherwise,}
    \end{cases},&\text{if}\;f(x)\geq y,
    \\
    0,&\text{otherwise}
  \end{cases}\;\text{, where}\;\epsilon\;\text{is small non-zero constant,}
  \\
  \operatorname{CI}^+(x,y)=&\;\bm{\omega}^\top\cdot h_{\text{upper}}(\bm{f},x,y)\times\Big[\sum\bm{\omega}\Big]^{-1}\text{.}
\end{aligned}
$$

$\operatorname{CI}^+$ is to be used for the upper confidence region, and likewise, we define $\operatorname{CI}^-$ to be equivalent, but it uses $h_{\text{lower}}$, that switches the condition to $f(x)\leq y$. The decision on whether to use $\operatorname{CI}^+$ or $\operatorname{CI}^-$ depends on whether the given $y$ is above or below the _computed average variable_, i.e.,

$$
\begin{aligned}
  \bar{g}(x)\dots&\;\text{the computed average variable,}
  \\[1ex]
  \operatorname{CI}(x,y)=&\;\begin{cases}
    0,&\text{if}\;y>\operatorname{CI}_{\text{upper}}(x)\;\text{,}
    \\
    0,&\text{if}\;y<\operatorname{CI}_{\text{lower}}(x)\;\text{,}
    \\
    \bm{\omega}^\top\cdot\Bigg[\begin{cases}
      h_{\text{upper}}(\bm{f},x,y),&\text{if}\;\bar{g}(x)<y,
      \\
      h_{\text{lower}}(\bm{f},x,y),&\text{otherwise}
    \end{cases}\Bigg]\times\Big[\sum\bm{\omega}\Big]^{-1},&\text{otherwise}
  \end{cases}\;\text{.}
\end{aligned}
$$

With this definition, we can compute a loss that is then based on a path that goes through this hyperplane. That path is a project's variable.

```{r}
h_p3avg <- function(funclist, x, y, upper = TRUE) {
  unlist(lapply(X = funclist, FUN = function(f) {
    val <- f(x)
    if (val == 0) val <- sqrt(.Machine$double.eps)
    if ((upper && val >= y) || (!upper && val <= y)) val else 0
  }))
}

h_upper_p3avg <- function(funclist, x, y) h_p3avg(funclist = funclist, x = x, y = y, upper = TRUE)
h_lower_p3avg <- function(funclist, x, y) h_p3avg(funclist = funclist, x = x, y = y, upper = FALSE)

CI_p3avg <- function(x, y, funclist, f_ci_upper, f_ci_lower, gbar, omega) {
  if (y > f_ci_upper(x) || y < f_ci_lower(x)) {
    return(0)
  }
  
  gbarval <- gbar(x)
  hval <- if (gbarval < y) {
    h_upper_p3avg(funclist = funclist, x = x, y = y)
  } else {
    h_lower_p3avg(funclist = funclist, x = x, y = y)
  }
  
  omega %*% hval / sum(omega)
}

CI_req_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_REQ, f_ci_upper = req_ci_upper_p3avg, f_ci_lower = req_ci_lower_p3avg, gbar = req_p3, omega = omega)
CI_dev_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_DEV, f_ci_upper = dev_ci_upper_p3avg, f_ci_lower = dev_ci_lower_p3avg, gbar = dev_p3, omega = omega)
CI_desc_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_DESC, f_ci_upper = desc_ci_upper_p3avg, f_ci_lower = desc_ci_lower_p3avg, gbar = desc_p3, omega = omega)
```


```{r}
x <- seq(0, 1, length.out = 200)
y <- seq(0, 1, length.out = 200)

compute_z_p3avg <- function(varname) {
  # We cannot call outer because our functions are not properly vectorized.
  #z <- outer(X = x, Y = y, FUN = CI_req_p3avg)
  f <- if (varname == "REQ") {
    CI_req_p3avg
  } else if (varname == "DEV") {
    CI_dev_p3avg
  } else {
    CI_desc_p3avg
  }
  
  z <- matrix(nrow = length(x), ncol = length(y))
  for (i in 1:length(x)) {
    for (j in 1:length(y)) {
      z[i, j] <- f(x = x[i], y = y[j])
    }
  }
  z
}

z_req <- loadResultsOrCompute(file = "../results/ci_p3avg_z_req.rds", computeExpr = {
  compute_z_p3avg(varname = "REQ")
})
z_dev <- loadResultsOrCompute(file = "../results/ci_p3avg_z_dev.rds", computeExpr = {
  compute_z_p3avg(varname = "DEV")
})
z_desc <- loadResultsOrCompute(file = "../results/ci_p3avg_z_desc.rds", computeExpr = {
  compute_z_p3avg(varname = "DESC")
})
```

Finally, we show the empirical confidence intervals in figures \ref{fig:p3-emp-cis} and \ref{fig:p3-emp-desc-cis}. Note that the minimum non-zero confidence is `r round(min(z_req[z_req>0]),6)`, while the maximum is $1$. Therefore, while we gradate the colors from $0$ to $1$, we slightly scale and transform the grid's non-zero values using the expression $0.9\times z+0.1$, to improve the visibility.

```{r p3-emp-cis, echo=FALSE, fig.height=8, fig.cap="The empirical confidence intervals for the two variables req\\% and dev\\%. Higher saturation of the color correlates with higher confidence.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(2,1))

z_req <- z_req * 0.9 + 0.1
z_req[z_req == .1] <- 0

image(x = x, y = y, z = z_req, main = "Variable: REQ",
      xlab = "Relative time", ylab = "Relative cumulative time spent",
      col = colorRampPalette(colors = c("#ffffff", "#ff0000"))(100), zlim = c(0,1))
grid()

curve(req_p3, 0, 1, col = "red", add = TRUE)

curve(req_ci_upper_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 2)
curve(req_ci_lower_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 3)



z_dev <- z_dev * 0.9 + 0.1
z_dev[z_dev == .1] <- 0

image(x = x, y = y, z = z_dev, main = "Variable: DEV",
      xlab = "Relative time", ylab = "Relative cumulative time spent",
      col = colorRampPalette(colors = c("#ffffff", "#0000ff"))(100), zlim = c(0,1))
grid()
curve(dev_p3, 0, 1, col = "blue", add = TRUE)

curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 3)
```


```{r p3-emp-desc-cis, echo=FALSE, fig.height=4, fig.cap="The empirical confidence intervals for the variable desc\\%. Higher saturation of the color correlates with higher confidence.", fig.align="center", fig.pos="ht!"}

z_desc <- z_desc * 0.9 + 0.1
z_desc[z_desc == .1] <- 0

image(x = x, y = y, z = z_desc, main = "Variable: DESC",
      xlab = "Relative time", ylab = "Relative cumulative time spent",
      col = colorRampPalette(colors = c("#ffffff", "forestgreen"))(100), ylim = c(0, desc_ci_upper_p3avg(1) * 1.1), zlim = c(0, max(z_desc)))
grid()
curve(desc_p3, 0, 1, col = "forestgreen", add = TRUE)

curve(desc_ci_upper_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 2)
curve(desc_ci_lower_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 3)
```


### Loss based on CI hyperplane

Simply put, this loss is based on the confidence interval's hyperplane, and calculates an absolute average confidence based on it. Each signal evaluated against the hyperplane is a slice of it.

$$
\begin{aligned}
  \mathit{L}^{\text{avgconf}}(x_1,x_2,f)=&\;\Bigg[\int_{x_1}^{x_2}\,\operatorname{CI}(x, f(x))\,dx\Bigg]\times(x_2-x_1)^{-1}\;\text{,}
  \\
  &\;\text{where}\;f\;\text{is the signal/variable and}\;x_2>x_1\text{.}
\end{aligned}
$$

We will do a full evaluation later, including creating a decision rule or learning how to scale the average weight to the consensus score, but let's take one project and test this.


```{r}
L_avgconf_p3_avg <- function(x1, x2, f, CI) {
  cubature::cubintegrate(
    f = function(x) {
      CI(x = x, y = f(x))
    },
    lower = x1, upper = x2
  )$integral / (x2 - x1)
}

loadResultsOrCompute(file = "../results/ci_p3avg_Lavg-test.rds", computeExpr = {
  c(
    "P2_REQ" = L_avgconf_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project2$REQ$get0Function(), CI = CI_req_p3avg),
    "P4_REQ" = L_avgconf_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project4$REQ$get0Function(), CI = CI_req_p3avg)
  )
})
```

Plotting `L_avgconf_p3_avg` for the `REQ`-variable of both projects 2 and 4 gives us the function in figure \ref{fig:p2p4-l3avgconf}. We can clearly observe that the area under the latter is larger, and so will be the average.

```{r p2p4-l3avgconf, echo=FALSE, fig.height=3.5, fig.cap="The three variables of the first project.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,2))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    CI_req_p3avg(x = x_, y = all_signals$Project2$REQ$get0Function()(x_))
  })
}

curve(tempf, 0, 1, xlab = "Relative time", ylab = "Empirical confidence", main = "Project: 2 (REQ)")


tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    CI_req_p3avg(x = x_, y = all_signals$Project4$REQ$get0Function()(x_))
  })
}

curve(tempf, 0, 1, xlab = "Relative time", ylab = "Empirical confidence", main = "Project: 4 (REQ)")
```

From this example we can clearly see a difference in average confidence, which seems to be somewhat reconciling the projects' weights (consensus score). Let's try the next method, too.


### Loss based on distance to average

We may choose to ignore the previously computed confidence hyperplane and compute a cost by, e.g., quantifying the distance between the previously computed average variable and another signal/variable. More precisely, we quantify the area between both variables, and compare it to the largest possible area, thereby obtaining an upper bound (with the lower bound being $0$ obviously). Unlike the previous attempt, this function is a loss, that maps to the range $[0,1]$, where $1$ means the largest possible distance (i.e., the variable compared is entirely outside (or collinear with) the confidence intervals). Hence this function is an attempt of measuring of dissimilarity.

$$
\begin{aligned}
  \mathit{L}^{\text{areadist}}(x_1,x_2,f)=&\;\int_{x_1}^{x_2}\,\norm{\bar{g}(x)-\overbrace{\min{\Big(\operatorname{CI}_{\text{upper}}(x),\;\max{\big(\operatorname{CI}_{\text{lower}}(x), f(x)\big)}\Big)}}^{\text{Confining of}\;f\;\text{to the confidence intervals.}}}\,dx
  \\[1ex]
  &\;\times\Bigg[\;\overbrace{\int_{x_1}^{x_2}\,\sup{\Big(\bar{g}(x)-\operatorname{CI}_{\text{lower}}(x)\;,\;\operatorname{CI}_{\text{upper}}(x)-\bar{g}(x)\Big)\,dx}}^{\text{maximum possible area between}\;\bar{g}(x)\;\text{and either CI.}}\;\Bigg]^{-1}\;\text{.}
\end{aligned}
$$

Again, we will do a full evaluation later, but let's just attempt computing this loss once.

```{r}
L_areadist_p3_avg <- function(x1, x2, f, gbar, CI_upper, CI_lower) {
  int1 <- cubature::cubintegrate(
    f = function(x) {
      abs(gbar(x) - min(CI_upper(x), max(CI_lower(x), f(x))))
    },
    lower = x1, upper = x2
  )$integral
  
  int2 <- cubature::cubintegrate(
    f = function(x) {
      max(gbar(x) - CI_lower(x), CI_upper(x) - gbar(x))
    },
    lower = x1, upper = x2
  )$integral
  
  c("area" = int1, "maxarea" = int2, "dist" = int1 / int2)
}

loadResultsOrCompute(file = "../results/ci_p3avg_Larea-test.rds", computeExpr = {
  list(
    "P2_REQ" = L_areadist_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project2$REQ$get0Function(),
      gbar = req_p3, CI_upper = req_ci_upper_p3avg, CI_lower = req_ci_lower_p3avg),
    "P4_REQ" = L_areadist_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project4$REQ$get0Function(),
      gbar = req_p3, CI_upper = req_ci_upper_p3avg, CI_lower = req_ci_lower_p3avg)
  )
})
```

Let's show the maximum possible distance vs. the distance of a project's variable in a plot (cf. figure \ref{fig:p2p4-lareadist}). These figures clearly show the smaller distance of project 4 to the average. This is expected, as this project has the highest weight, so the average `REQ%`-variable resembles this project most.

```{r p2p4-lareadist, echo=FALSE, fig.height=4, fig.cap="The two variables of the first project.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,2))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    abs(req_p3(x_) - min(req_ci_upper_p3avg(x_), max(req_ci_lower_p3avg(x_), all_signals$Project2$REQ$get0Function()(x_))))
  })
}
curve(tempf, 0, 1, col = "red", xlab = "Relative time", ylab = "Distance to avg. req\\%", main = "Project: 2", ylim = c(0, .5))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    max(req_p3(x_) - req_ci_lower_p3avg(x_), req_ci_upper_p3avg(x_) - req_p3(x_))
  })
}
curve(tempf, 0, 1, col = "blue", add = TRUE)
legend(0.01, 0.49, legend = c("Pr. 2 req% dist", "max. dist."), col = c("red", "blue"), lty = 1, lwd = 2)



tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    abs(req_p3(x_) - min(req_ci_upper_p3avg(x_), max(req_ci_lower_p3avg(x_), all_signals$Project4$REQ$get0Function()(x_))))
  })
}
curve(tempf, 0, 1, col = "red", xlab = "Relative time", ylab = "Distance to avg. req\\%", main = "Project: 4", ylim = c(0,0.5))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    max(req_p3(x_) - req_ci_lower_p3avg(x_), req_ci_upper_p3avg(x_) - req_p3(x_))
  })
}
curve(tempf, 0, 1, col = "blue", add = TRUE)
legend(0.01, 0.49, legend = c("Pr. 4 req% dist", "max. dist."), col = c("red", "blue"), lty = 1, lwd = 2)
```

### Loss based on the two previous approaches

This is an early-stadium idea. The essence is that for every $x$, we have a vertical "confidence-slice" that we can integrate over and get an average confidence. Then, we obtain the confidence for the variable in question at the same $x$. Both of these values can then be put into a relation. If we were to integrate this function, we would get the ratio between the variable's confidence and the average confidence, on average.


```{r}
use_x <- .8
dev_p3(use_x)

cubature::cubintegrate(
  f = function(x) {
    CI_dev_p3avg(x = use_x, y = x)
  },
  lower = dev_ci_lower_p3avg(use_x),
  upper = dev_ci_upper_p3avg(use_x)
)$integral / (dev_ci_upper_p3avg(use_x) - dev_ci_lower_p3avg(use_x))
```

```{r}
CI_dev_p3avg(x = use_x, all_signals$Project4$DEV$get0Function()(use_x))
```

At `x=0.8`, the average variable is at $\approx0.71$, the average confidence for the slice is $\approx0.19$, and the confidence of the evaluated variable (project 4, `DEV`) is at $\approx0.22$. This means that the ratio is in the interval $(0,\infty)$. A "perfect" ratio of $1.0$ would express that the tested variable is, on average, equal to the average confidence.



# Scoring of projects

In the technical report for detecting the Fire Drill using source code data, we already explored a wide range of possible patterns and scoring mechanisms. All of them are based on comparing/scoring the variables (process model vs. process). Some of these we will apply here, too, but our focus is first on detection mechanisms that can facilitate the __confidence intervals__.


## Pattern I

This is the expert-guess of how the Fire Drill would manifest in issue-tracking data. The pattern was conceived with precise values for the confidence intervals at certain points ($t_1,t_2$), and the variables were not of importance. It was only used thus far using a binary decision rule.


### Binary detection decision rule

In this section, we will only replicate earlier results by applying the existing rule. It is further formulated using indicators and thresholds as:

$$
\begin{aligned}
  I_1 =&\;\operatorname{req}(t_1) < y_1 \land \operatorname{req}(t_1) > y_2,
  \\[1ex]
  I_2 =&\;\operatorname{dev}(t_1) < y_3 \land \operatorname{dev}(t_2) < y_4,
  \\[1ex]
  I_3 =&\;\operatorname{desc}(1) > y_5,
  \\[1em]
  \operatorname{detect}^{\text{binary}}(I_1,I_2,I_3) =&\;\begin{cases}
    1,&\text{if}\;I_1 \land (I_2 \lor I_3),
    \\
    0,&\text{otherwise}
  \end{cases}\;\text{, using the threshold values}
  \\[1ex]
  \bm{y}=&\;\{0.8,0.4,0.15,0.7,0.15\}^\top\;\text{.}
\end{aligned}
$$

This can be encapsulated in a single function:

```{r}
p1_dr <- function(projName, y = c(0.8, 0.4, 0.15, 0.7, 0.15)) {
  req <- all_signals[[projName]]$REQ$get0Function()
  dev <- all_signals[[projName]]$DEV$get0Function()
  desc <- all_signals[[projName]]$DESC$get0Function()
  
  I1 <- req(t_1) < y[1] && req(t_1) > y[2]
  I2 <- dev(t_1) < y[3] && dev(t_2) < y[4]
  I3 <- desc(1) > y[5]
  
  I1 && (I2 || I3)
}
```

```{r}
temp <- sapply(X = names(all_signals), FUN = p1_dr)
p1_detect <- data.frame(
  detect = temp,
  ground_truth = ground_truth$consensus,
  correct = (temp & ground_truth$consensus >= 5) | (!temp & ground_truth$consensus < 5)
)
```

```{r echo=FALSE}
if (interactive()) {
  p1_detect
} else {
  knitr::kable(
    x = p1_detect,
    booktabs = TRUE,
    caption = "Binary detection using a decision rule based on homogeneous confidence intervals of pattern I.",
    label = "p1-bin-detect"
  )
}
```

In table \ref{tab:p1-bin-detect} we show the results of the binary detection, which is based on the manually defined homogeneous confidence intervals.


## Pattern II

_TODO_.


## Pattern III (average)

The third kind of pattern is based on data only, all the variables, confidence intervals and the strength thereof are based on the nine projects and the weight, which is the same as their consensus score.

_TODO_.


### Scoring based on the confidence intervals

We have calculated gradated confidence intervals, which means two things. First, we cannot apply a binary detection rule any longer, as the boundaries of the intervals include each project, only the weight is different. Second, when calculating a score, we will obtain a continuous measure, of which we can calculate a correlation to the consensus score of the ground truth, or, e.g., fit a linear model for scaling these scores.


```{r}
p3_avg_ci_scores <- loadResultsOrCompute(file = "../results/p3_avg_ci_scores.rds", computeExpr =  {
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind
    ) %dopar% {
      req <- all_signals[[pId]]$REQ$get0Function()
      dev <- all_signals[[pId]]$DEV$get0Function()
      desc <- all_signals[[pId]]$DESC$get0Function()
      
      `rownames<-`(data.frame(
        REQ = L_avgconf_p3_avg(x1 = 0, x2 = 1, f = req, CI = CI_req_p3avg),
        DEV = L_avgconf_p3_avg(x1 = 0, x2 = 1, f = dev, CI = CI_dev_p3avg),
        DESC = L_avgconf_p3_avg(x1 = 0, x2 = 1, f = desc, CI = CI_desc_p3avg)
      ), pId)
    }
  })
})
```

Table \ref{tab:p3-avg-ci-scores} shows the computed scores.

```{r echo=FALSE}
if (interactive()) {
  p3_avg_ci_scores
} else {
  knitr::kable(
    x = p3_avg_ci_scores,
    booktabs = TRUE,
    caption = "Correlation of the scores as computed over the inhomogeneous confidence interval with the ground truth.",
    label = "p3-avg-ci-scores"
  )
}
```

Let's test the correlation between either kind of score and the ground truth consensus score. The null hypothesis of this test states that both samples have no correlation.

```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_ci_scores[, "REQ"])
```

For the variable `REQ` we get a significant correlation of $\approx0.83$, and there is no significant evidence for the null hypothesis, so must reject it.


```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_ci_scores[, "DEV"])
```

For the variable `DEV` however, the correlation is quite low, $\approx0.17$. Also, we have significant evidence for accepting the null hypothesis (no correlation).

```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_ci_scores[, "DESC"])
```

Looks like we are getting some strong positive correlation for the variable `DESC` of $\approx0.85$. There is almost no evidence at all for accepting the null hypothesis.



### Scoring based on the distance to average

Here we compute the distance of each project's variables to the previously averaged variables. This approach does not rely on inhomogeneous confidence intervals, and only considers the intervals' boundaries to integrate some distance. Ideally, the distance is $0$, and in the worst case it is $1$. If this ought to be used as score, we probably would want to compute it as $\mathit{S}^{\text{areadist}}=1-\mathit{L}^{\text{areadist}}$.

```{r}
p3_avg_area_scores <- loadResultsOrCompute(file = "../results/p3_avg_area_scores", computeExpr =  {
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind
    ) %dopar% {
      req <- all_signals[[pId]]$REQ$get0Function()
      dev <- all_signals[[pId]]$DEV$get0Function()
      desc <- all_signals[[pId]]$DESC$get0Function()
      
      `rownames<-`(data.frame(
        REQ = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = req, gbar = req_p3,
          CI_upper = req_ci_upper_p3avg, CI_lower = req_ci_lower_p3avg)["dist"],
        
        DEV = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = dev, gbar = dev_p3,
          CI_upper = dev_ci_upper_p3avg, CI_lower = dev_ci_lower_p3avg)["dist"],
        
        DESC = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = desc, gbar = desc_p3,
          CI_upper = desc_ci_upper_p3avg, CI_lower = desc_ci_lower_p3avg)["dist"]
      ), pId)
    }
  })
})
```

Table \ref{tab:p3-avg-area-scores} shows the computed scores.

```{r echo=FALSE}
if (interactive()) {
  p3_avg_area_scores
} else {
  knitr::kable(
    x = p3_avg_area_scores,
    booktabs = TRUE,
    caption = "Correlation of the scores as computed per the average distance to the averaged variable.",
    label = "p3-avg-area-scores"
  )
}
```

As for the correlation tests, ideally, we get negative correlations, as the computed score is __lower__, the less distance we find between the area of the average variable and a project's variable, hence the relation must be antiproportional.


```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_area_scores[, "REQ"])
```

For the variable `REQ` we get a weaker, yet moderate correlation of $\approx-0.50$. However, there is evidence for the null hypothesis, which suggests that there is no statistical significant correlation. This means, we will have to use this with care, if at all.


```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_area_scores[, "DEV"])
```

The correlation for the variable `DEV` is poor again. Also, it is positive, which means that the scores calculated using this method are proportional, when they should not be. The p-value is significant, so there is most likely no significant correlation for this variable and the ground truth.

```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_area_scores[, "DESC"])
```

The correlation for `DESC` is still respectable, and also it is negative like it should be. The p-value is quite high, suggesting no correlation. Again, if at all, this must be used with care.


### Linear combination of the two methods

```{r}
temp <- data.frame(
  gt_consensus = ground_truth$consensus_score,
  ci_req = p3_avg_ci_scores$REQ,
  area_req = p3_avg_area_scores$REQ,
  ci_dev = p3_avg_ci_scores$DEV,
  area_dev = p3_avg_area_scores$DEV,
  ci_desc = p3_avg_ci_scores$DESC,
  area_desc = p3_avg_area_scores$DESC)

# ci_req + area_desc gives us ~0.951 already!
# ci_req + ci_dev + area_desc gives ~0.962
p3_avg_lm <- stats::lm(formula = gt_consensus ~ ci_req + ci_dev + area_desc, data = temp)
stats::coef(p3_avg_lm)
par(mfrow=c(1,2))
plot(p3_avg_lm, ask = FALSE, which = 1:2)
```

Using the approximate coefficients of the linear model, we can define the detector as follows:

$$
\begin{aligned}
  x_1,x_2,\operatorname{req},\operatorname{dev},\operatorname{desc}\dots&\;\text{lower/upper integration interval and project signals,}
  \\[1ex]
  \bm{\tau}=&\;\Big\{\mathit{L}^{\text{avgconf}}(x_1,x_2,\operatorname{req}),\;\mathit{L}^{\text{avgconf}}(x_1,x_2,\operatorname{dev}),\;\mathit{L}^{\text{areadist}}(x_1,x_2,\operatorname{desc})\Big\}\;\text{,}
  \\[1ex]
  \operatorname{detect}^{\text{ci+area}}(\bm{\tau})=&\;-0.254 + 3.636\times\bm{\tau}_1 + 1.206\times\bm{\tau}_2 - 0.555\times\bm{\tau}_3\;\text{.}
\end{aligned}
$$


```{r}
p3_avg_lm_scores <- stats::predict(p3_avg_lm, temp)
# Since we are attempting a regression to positive scores,
# we set any negative predictions to 0. Same goes for >1.
p3_avg_lm_scores[p3_avg_lm_scores < 0] <- 0
p3_avg_lm_scores[p3_avg_lm_scores > 1] <- 1

round(p3_avg_lm_scores * 10, 3)
stats::cor(p3_avg_lm_scores, ground_truth$consensus_score)
```

With this linear combination of only three scores (out of six), we were able to significantly boost the detection to $\approx0.962$, which implies that combining both methods is of worth for a detection using (inhomogeneous) confidence intervals only. If we only combine the scores of the variable `REQ` into a model, we still achieve a correlation of $\approx0.88$. This should probably be preferred to keep the degrees of freedom low, countering overfitting. Using four or more scores goes beyond $0.97$.













