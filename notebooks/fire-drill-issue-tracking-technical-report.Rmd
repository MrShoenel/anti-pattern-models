---
title: "Technical Report: Detecting the Fire Drill anti-pattern using issue-tracking data"
author: "Sebastian HÃ¶nel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ../inst/REFERENCES.bib
urlcolor: blue
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  md_document:
    toc: yes
    toc_depth: 6
    df_print: kable
    variant: gfm
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: kable
  word_document: default
header-includes:
- \usepackage{bm}
- \usepackage{mathtools}
- \usepackage{xurl}
---

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\abs}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand{\norm}[1]{\left\lVert\,#1\,\right\rVert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}
\newcommand\argmax[1]{\underset{#1}{arg\,max}}
\newcommand\argmin[1]{\underset{#1}{arg\,min}}


```{r setoptions, echo=FALSE, warning=FALSE, message=FALSE}
Sys.setenv(LANG = "en_US.UTF-8")
Sys.setenv(LC_ALL = "en_US.UTF-8")
Sys.setenv(LC_CTYPE = "en_US.UTF-8")
library(knitr)
opts_chunk$set(tidy = TRUE, tidy.opts = list(indent=2))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

library(ggplot2)
library(ggpubr)
library(fields)
```

# Introduction\label{tr:fire-drill-issue-tracking-technical-report}

This is the complementary technical report for the paper/article tentatively entitled "Multivariate Continuous Processes: Modeling, Instantiation, Goodness-of-fit, Forecasting". Similar to the technical report for detecting the Fire Drill using source code, we import all projects' data and the ground truth. This notebook however is concerned with different and additional approaches, i.e., it is not just a repetition of the other technical report.

All complementary data and results can be found at Zenodo [@honel_picha_2021]. This notebook was written in a way that it can be run without any additional efforts to reproduce the outputs (using the pre-computed results). This notebook has a canonical URL^[[Link]](https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/fire-drill-issue-tracking-technical-report.Rmd)^ and can be read online as a rendered markdown^[[Link]](https://github.com/sse-lnu/anti-pattern-models/blob/master/notebooks/fire-drill-issue-tracking-technical-report.md)^ version. All code can be found in this repository, too.


# Importing the data

Here, we import the ground truth and the projects.

## Ground truth

We have $9$ projects conducted by students, and two raters have __independently__, i.e., without prior communication, assessed to what degree the AP is present in each project. This was done using a scale from zero to ten, where zero means that the AP was not present, and ten would indicate a strong manifestation The entire ground truth is shown in table \ref{tab:groundtruth}.

```{r}
ground_truth <- read.csv(file = "../data/ground-truth.csv", sep = ";")
ground_truth$consensus_score <- ground_truth$consensus / 10
```

```{r echo=FALSE}
if (interactive()) {
  ground_truth
} else {
  knitr::kable(
    x = ground_truth,
    booktabs = TRUE,
    caption = "Entire ground truth as of both raters",
    label = "groundtruth"
  )
}
```


## Project data

In this section we import the projects' __issue-tracking__-data. All projects' data will be normalized w.r.t. the time, i.e., each project will have a support of $[0,1]$. The variables are modeled as cumulative time spent on issues. Each variable in each project will be loaded into an instance of `Signal`.

```{r}
library(readxl)

load_project_issue_data <- function(pId) {
  data <- read_excel(
    "../data/FD_issue-based_detection.xlsx", sheet = pId)
  data[is.na(data)] <- 0
  
  data$req <- as.numeric(data$req)
  data$dev <- as.numeric(data$dev)
  data$desc <- as.numeric(data$desc)
  
  req_cs <- cumsum(data$req) / sum(data$req)
  dev_cs <- cumsum(data$dev) / sum(data$dev)
  desc_cs <- cumsum(data$desc) / max(cumsum(data$dev))
  X <- seq(from = 0, to = 1, length.out = length(req_cs))
  
  signal_req <- Signal$new(
    func = stats::approxfun(x = X, y = req_cs, yleft = 0, yright = 1),
    name = "REQ", support = c(0,1), isWp = FALSE)
  signal_dev <- Signal$new(
    func = stats::approxfun(x = X, y = dev_cs, yleft = 0, yright = 1),
    name = "DEV", support = c(0,1), isWp = FALSE)
  signal_desc <- Signal$new(
    func = stats::approxfun(x = X, y = desc_cs, yleft = 0, yright = max(desc_cs)),
    name = "DESC", support = c(0,1), isWp = FALSE)
  
  list(
    data = data,
    REQ = signal_req,
    DEV = signal_dev,
    DESC = signal_desc
  )
}
```

Let's attempt to replicate the graphs of the first project (cf. figure \ref{fig:p1-example}):

```{r}
p3_signals <- load_project_issue_data(pId = "Project3")
req_f <- p3_signals$REQ$get0Function()
dev_f <- p3_signals$DEV$get0Function()
desc_f <- p3_signals$DESC$get0Function()
```


```{r p1-example, echo=FALSE, fig.cap="The three variables of the first project.", fig.align="center", fig.pos="ht!"}
curve(req_f, 0, 1, col = "blue", xlab = "Relative time", ylab = "Cumulative time spent")
curve(dev_f, 0, 1, col = "red", add = TRUE)
curve(desc_f, 0, 1, col = "gold", add = TRUE)
legend(0, 1, legend = c("cum. req%", "cum. dev%", "cum. desc%"), col = c("blue", "red", "gold"), lty = 1, lwd = 2)
```

OK, that works well. It'll be the same for all projects, i.e., only two variables, time spent on requirements- and time spent on development-issues, is tracked. That means we will only be fitting two variables later.

Let's load, store and visualize all projects (cf. figure \ref{fig:project-it-vars}):

```{r load-data}
all_signals <- list()
for (pId in paste0("Project", 1:9)) {
  all_signals[[pId]] <- load_project_issue_data(pId = pId)
}
```

```{r echo=FALSE}
tempdf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(tempdf) <- c("x", "y", "p", "v")
n <- 500
x <- seq(from = 0, to = 1, length.out = n)
for (pId in names(all_signals)) {
  for (v in c("REQ", "DEV", "DESC")) {
    tempdf <- rbind(tempdf, data.frame(
      x = x,
      y = sapply(X = x, FUN = all_signals[[pId]][[v]]$get0Function()),
      p = pId,
      v = v
    ))
  }
}
```

```{r project-it-vars, echo=FALSE, fig.cap="All variables over each project's time span.", fig.align="top", fig.pos="ht!"}
plot_all_req_dev <- ggplot(data = tempdf, aes(x = x, y = y, color = v)) +
  geom_line() +
  scale_color_manual(values = c("forestgreen", "blue", "red")) +
  facet_wrap(p ~.) +
  theme_light() +
  labs(color = "Variable") + xlab("Relative Time") + ylab("Cumulative Time spent") +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"))
plot_all_req_dev
```

```{r echo=FALSE, eval=FALSE}
# Let's save this as tikz:
tikzDevice::tikz(file = "../figures/all_it_acp.tex", width = 3.4, height = 1.8)
plot_all_req_dev +
  theme_light(base_size = 10) +
  theme(
    axis.title.x.bottom = element_text(margin = margin(b=5), size = 10),
    axis.title.y.left = element_text(margin = margin(r=3), size = 10),
    legend.position = "bottom",
    legend.margin = margin(-7.5, 0, 0, 0),
    strip.background = element_rect(fill="#dfdfdf"))
dev.off()
```

# Patterns for scoring the projects

Here, we develop various patterns (process models) suitable for the detection of the Fire Drill anti-pattern using issue-tracking data.

## Pattern I: Consensus of two experts

The initial pattern as defined for the detection of the Fire Drill AP is imported/created/defined here, and its variables and confidence intervals are modeled as continuous functions over time.

There are some values (x/y coordinates) for which we want to guarantee that the confidence intervals or the variables themselves pass through. Also, the two points in time $t_1,t_2$ are defined to be at $0.4$ and $0.85$, respectively.

```{r p1-constants}
t_1 <- 0.4
t_2 <- 0.85

# req(t_1)
req_t_1 <- 0.7

# dev(t_1), dev(t_2)
dev_t_1 <- 0.075
dev_t_2 <- 0.4
```

This initial version of the pattern is not based on any data, observation or ground truth, but solely on two independent experts that reached a consensus for every value a priori any of the detection approaches.


### Variable: Requirements, analysis, planning

The variable itself is not given, only its upper- and lower confidence-intervals (CI), where the latter simply is $\operatorname{req}^{\text{CI}}_{\text{lower}}(x)=x$. The upper CI is given by the informal expression $\operatorname{req}^{\text{CI}}_{\text{upper}}(x)=1.02261-1.02261\times\exp{(-3.811733\times x)}$. All together is shown in figure \ref{fig:req-cis}.

The variable itself is not given, as it was not important for the binary decision rule, whether or not a project's variable is within the confidence interval. It is still not important, what matters is that it runs through the confidence interval, and we will design it by fitting a polynomial through some inferred points from the plot. In some practical case however, the variable's course may be important, and while we will later use the variable to compute some kind of loss between it, the confidence interval and some project's variable, we only do this for demonstration purposes.

Let's first define the variable using some supporting x/y coordinates. It needs to be constrained such that it runs through 0,0 and 1,1:

```{r}
req_poly <- cobs::cobs(
  x = seq(from = 0, to = 1, by = .1),
  y = c(0, .25, .425, .475, .7, .8, .85, .9, .95, .975, 1),
  pointwise = matrix(data = c(
    c(0, 0, 0),
    c(0, t_1, req_t_1),
    c(0, 1, 1)
  ), byrow = TRUE, ncol = 3)
)

# Now we can define the variable simply by predicting from
# the polynomial (btw. this is vectorized automatically):
req <- function(x) {
  stats::predict(object = req_poly, z = x)[, "fit"]
}
```

And now for the confidence intervals:

```{r}
req_ci_lower <- function(x) x
req_ci_upper <- function(x) 1.02261 - 1.02261 * exp(-3.811733 * x)
```


```{r req-cis, echo=FALSE, fig.cap="req\\% and its lower- and upper confidence interval.", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = seq(from = 0, to = 1, length.out = 50),
  y = req_ci_upper(seq(from = 0, to = 1, length.out = 50)),
  col = "#ff000033",
  border = NA)
curve(req_ci_lower, 0, 1, col = "red", lty = 2, add = TRUE)
curve(req_ci_upper, 0, 1, col = "red", lty = 3, add = TRUE)
curve(req, 0, 1, col = "red", lty = 1, lwd = 2, add = TRUE)
legend(.74, .3, legend = c("upper CI", "lower CI", "req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))
abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")


# t_1:
abline(v = t_1)
text(x = .47, y = .98, paste0("t_1=", t_1))

# other thresholds:
abline(h = req(t_1), col = "#888888")
text(x = .5, y = .65, paste0("req(t_1)=", round(req(t_1), 2)))
abline(h = req_ci_upper(t_1), col = "#888888")
text(x = .24, y = .86, paste0("req_ci_upper(t_1)=", round(req_ci_upper(t_1), 2)))
```


### Variables: Design, implementation, testing, bugfixing and Descoping

Again, the variable for design etc. is not given, but rather only its upper confidence interval. Its lower CI is simply always zero. The upper CI is given by the informal expression $\operatorname{dev}^{\text{CI}}_{\text{upper}}(x)=0.07815904\times x+0.6222767\times x^2+0.2995643\times x^3$. The variable for de-scoping comes without confidence interval, and is defined by $\operatorname{desc}(x)=0.01172386\times x + 0.0933415\times x^2 + 0.04493464\times x^3$.

First we will define/fit a polynomial that describes the variable for design etc., the same way we did for requirements etc. we do know that it should pass through the points $[t_1,\approx0.075]$, as well as $[t_2,\approx0.4]$.

```{r}
dev_poly <- cobs::cobs(
  x = seq(from = 0, to = 1, by = .1),
  y = c(0, .0175, .035, .055, dev_t_1, .014, .165, .2, .28, .475, 1),
  print.warn = FALSE,
  print.mesg = FALSE,
  pointwise = matrix(data = c(
    c(0, t_1, dev_t_1),
    c(0, t_2, dev_t_2),
    c(0, 1, 1)
  ), byrow = TRUE, ncol = 3)
)

# Now we can define the variable simply by predicting from
# the polynomial (btw. this is vectorized automatically):
dev <- function(x) {
  temp <- stats::predict(object = dev_poly, z = x)[, "fit"]
  # I cannot constrain the polynomial at 0,0 and it returns
  # a very slight negative value there, so let's do it this way:
  temp[temp < 0] <- 0
  temp[temp > 1] <- 1
  temp
}
```

Next we define the upper confidence interval for the variable `DEV`, then the variable for de-scoping. All is shown in figure \ref{fig:dev-desc-cis}.

```{r}
dev_ci_upper <- function(x) 0.07815904 * x + 0.6222767 * x^2 + 0.2995643 * x^3
desc <- function(x) 0.01172386 * x + 0.0933415 * x^2 + 0.04493464 * x^3
```


```{r dev-desc-cis, echo=FALSE, fig.cap="The variable dev\\% and its upper confidence interval, as well as the variable desc\\%.", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff33",
  border = NA)

curve(dev_ci_upper, 0, 1, col = "blue", lty = 2, add = TRUE)
curve(desc, 0, 1, add = TRUE, col = "forestgreen", lty = 1, lwd = 3)
curve(dev, 0, 1, col = "blue", lty = 1, lwd = 3, add = TRUE)
legend(.02, 1.01, legend = c("dev upper CI", "dev%", "desc%"),
       col = c("blue", "blue", "forestgreen"), lty = c(3, 1, 1), lwd = c(1, 3, 3))

abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")

# t_1,t_2:
abline(v = t_1)
text(x = .47, y = .98, paste0("t_1=", t_1))
abline(v = t_2)
text(x = .775, y = .98, paste0("t_2=", t_2))

# other thresholds:
abline(h = dev_ci_upper(t_2), col = "#888888")
text(x = .565, y = .76, paste0("dev_ci_upper(t_2)=", round(dev_ci_upper(t_2), 2)))
abline(h = desc(1), col = "#888888")
text(x = .94, y = .24, paste0("desc(1)\n   =", round(desc(1), 2)))

# t_1(dev), t_2(dev):
abline(h = dev(t_1), col = "#888888")
text(x = .29, y = .2, paste0("dev(t_1)=", round(dev(t_1), 3)))
abline(h = dev(t_2), col = "#888888")
text(x = .5, y = .46, paste0("dev(t_2)=", round(dev(t_2), 2)))
```


## Pattern II: Partial adaptation of first pattern

We will be attempting three kinds of adaptations to the first pattern:

a)    Learn $t_1,t_2$ from the data: There is not much to learn, but we could attempt to define these two thresholds as weighted average over the ground truth. Alternatively, we could formulate an optimization problem. We then use _time warping_ to alter the first pattern, __including__ its confidence intervals.
b)    Additionally to a (after learning $t_1,t_2$), we will apply _amplitude warping_ using __`srBTAW`__.
c)    Take the first pattern and apply both, _boundary time warping_ __and__ _boundary amplitude warping_, to produce a pattern that is (hopefully) closest to all projects in the ground truth. This is the very same approach we attempted for adapting the pattern of type I that we defined for the Fire Drill in source code.


### Type II (a): Adapt type I using thresholds $t_1,t_2$\label{ssec:optim-t1t2}

The two variables `REQ` and `DEV` in the first pattern describe the cumulative time spent on two distinct activities. It was designed with focus on the confidence intervals, and a binary decision rule, such that the actual variables' course was not of interest.

To find the optimal value for a threshold, we could look at when each project is closest to $\operatorname{req}(t_1)$ and $\operatorname{dev}(t_2)$ (in relative time), and then compute a weighted average over it. However, since we already modeled each project's variables as *continuous-time stochastic process*, I suggest we use an optimization-based approach.

```{r t1t2-example}
tempf <- all_signals$Project5$REQ$get0Function()
tempf1 <- function(x) abs(tempf(x) - req_t_1)
optR <- optimize(tempf1, interval = c(0, 1))
optR
```

```{r t1t2-example-fig, echo=FALSE, fig.cap="The non-optimal optimum found by gradient-based optimization in project 5.", fig.align="center", fig.pos="ht!"}
curve(tempf1, 0, 1, xlab = "Relative time", ylab = "abs(REQ_p5(x) - req_t_1)")
points(x = optR$minimum, y = optR$objective,
       col = "red", pch = 4, cex = 1.5)
```

We will find the optimum using `nlopt` and a global optimization, because we actually will have a global optimum by re-arranging each project's variables. Also, gradient-based methods do not work well because of the nature of the variables, having large horizontal plateaus. This can be seen in figure \ref{fig:t1t2-example-fig}. Approaches using the built-in `optim` do hence not work well, the problem is clearly demonstrated in the previous code chunk, resulting in an objective $\gg0$ (which should ideally be $0$).

We want to find out when each project is closest to the previously defined thresholds. Each variable is a cumulative aggregation of the underlying values, which means that we have monotonic behavior.

$$
\begin{aligned}
  \min_{\hat{t}_1,\hat{t}_2\in R}&\;{\operatorname{req}(\hat{t}_1), \operatorname{dev}(\hat{t}_2)}\;\text{,}
  \\[1ex]
  \text{subject to}&\;0\leq\hat{t}_1,\hat{t}_2\leq1\;\text{, using}
  \\[1ex]
  \mathcal{L}_{\operatorname{req}}(x)=&\;\norm{\operatorname{req}(x)-\operatorname{req}(t_1)}\;\text{, and}
  \\[1ex]
  \mathcal{L}_{\operatorname{dev}}(x)=&\;\norm{\operatorname{dev}(x)-\operatorname{dev}(t_2)}\;\text{(quasi-convex loss functions).}
\end{aligned}
$$

The objective functions hence will be to find the global optimum (minimum), which occurs at $y\approx0$. Since we have plateaus in our data, we will potentially have infinitely many global optima. However, we are satisfied with any that is $\approx0$.

$$
\begin{aligned}
  \mathcal{O}_{\operatorname{dev}}(x)=&\;\argmin{\hat{x}\in R}\;{L_{\operatorname{dev}}(x)}\;\text{, and}
  \\[1ex]
  \mathcal{O}_{\operatorname{req}}(x)=&\;\argmin{\hat{x}\in R}\;{L_{\operatorname{req}}(x)}\text{.}
\end{aligned}
$$


```{r t1t2-nlopt}
library(nloptr)

set.seed(1)

t1t2_opt <- matrix(ncol = 4, nrow = length(all_signals))
rownames(t1t2_opt) <- names(all_signals)
colnames(t1t2_opt) <- c("req_sol", "dev_sol", "req_obj", "dev_obj")

find_global_low <- function(f) {
  nloptr(
    x0 = 0.5,
    opts = list(
      maxeval = 1e3,
      algorithm = "NLOPT_GN_DIRECT_L_RAND"),
    eval_f = f,
    lb = 0,
    ub = 1
  )
}

for (pId in paste0("Project", 1:9)) {
  sig_REQ <- all_signals[[pId]]$REQ$get0Function()
  req_abs <- function(x) abs(sig_REQ(x) - req_t_1)
  
  sig_DEV <- all_signals[[pId]]$DEV$get0Function()
  dev_abs <- function(x) abs(sig_DEV(x) - dev_t_2)
  
  optRes_REQ <- find_global_low(f = req_abs)
  optRes_DEV <- find_global_low(f = dev_abs)
  
  t1t2_opt[pId, ] <- c(optRes_REQ$solution, optRes_DEV$solution,
                       optRes_REQ$objective, optRes_DEV$objective)
}
```


```{r echo=FALSE}
if (interactive()) {
  t1t2_opt
} else {
  knitr::kable(
    x = t1t2_opt,
    booktabs = TRUE,
    caption = "Optimum values for $t_1$ and $t_2$ for each project, together with the loss of each project's objective function at that offset (ideally 0).",
    label = "t1t2-optvals"
  )
}
```

From the table \ref{tab:t1t2-optvals}, we can take an example to demonstrate that the optimization indeed found the global optimum (or a value very close to it, usually with a deviation $<1e^{-15}$). For the picked example of project 5, we can clearly observe a value for $x$ that results in a solution that is $\approx0$.

```{r test-test, echo=FALSE, fig.cap="Example of finding the optimum for req($t_1$) in project 5.", fig.align="center", fig.pos="ht!"}
pId <- "Project5"
tempf <- function(x) {
  abs(all_signals[[pId]]$REQ$get0Function()(x) - req_t_1)
}
curve(tempf, 0, 1)
points(x = t1t2_opt[pId, 1], y = t1t2_opt[pId, 3],
       col = "red", pch = 4, cex = 1.5)
```

Figure \ref{fig:test-test} depicts the optimal solution as found by using global optimization. Now what is left, is to calculate the weighted average for the optimized $\bm{\hat{t}_1},\bm{\hat{t}_2}$. The weighted arithmetic mean is defined as:

$$
\begin{aligned}
  \text{weighted mean}=&\;\Big[\sum\bm{\omega}\Big]^{-1}\times\bm{\omega}^\top\cdot\bm{\hat{t}}\;\text{, where}
  \\[1ex]
  \bm{\omega}\dots&\;\text{weight vector that corresponds to the consensus-score, and}
  \\[1ex]
  \bm{\hat{t}}\dots&\;\text{vector with optimal values for either}\;t_1\;\text{or}\;t_2\;\text{(as learned earlier).}
\end{aligned}
$$

```{r}
omega <- ground_truth$consensus_score
names(omega) <- paste0("Project", 1:length(omega))

t1_wavg <- t1t2_opt[, 1] %*% omega / sum(omega)
print(c(t_1, t1_wavg))
t2_wavg <- t1t2_opt[, 2] %*% omega / sum(omega)
print(c(t_2, t2_wavg))
```

Originally, $t_1$ was guessed to be located at `r t_1`, with $\operatorname{req}(t_1)=0.7$. The weighted average over the optimized values (where $\operatorname{req}(\hat{t}_1)\approx0.7$) suggests defining $\hat{t}_1$=`r round(t1_wavg, 5)`.

$t_2$ on the other hand was originally located at `r t_2`, with $\operatorname{dev}(t_2)=0.4$. The weighted average over the optimized values (where $\operatorname{dev}(\hat{t}_2)\approx0.4$) suggests defining $\hat{t}_2$=`r round(t2_wavg, 5)`.

Having learned these values, we can now adapt the pattern using time warping. For that, we have to instantiate the pattern using `srBTAW`, add the original boundaries and set them according to what we learned. Below, we define a function that can warp a single variable.

```{r}
timewarp_variable <- function(f, t, t_new) {
  temp <- SRBTW$new(
    wp = f, wc = f,
    theta_b = c(0, t_new, 1),
    gamma_bed = c(0, 1, sqrt(.Machine$double.eps)),
    lambda = c(0, 0),
    begin = 0, end = 1,
    openBegin = FALSE, openEnd = FALSE)
  temp$setParams(`names<-`(c(t, 1 - t), c("vtl_1", "vtl_2")))
  function(x) sapply(X = x, FUN = temp$M)
}
```

```{r}
req_p2a <- timewarp_variable(f = req, t = t_1, t_new = t1_wavg)
req_ci_lower_p2a <- timewarp_variable(f = req_ci_lower, t = t_1, t_new = t1_wavg)
req_ci_upper_p2a <- timewarp_variable(f = req_ci_upper, t = t_1, t_new = t1_wavg)
```


```{r req-p1a-cis, echo=FALSE, fig.cap="req\\% and its lower- and upper confidence interval after time warping for pattern type I (a).", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), t1_wavg),
  y = c(req_ci_upper_p2a(seq(from = 0, to = 1, length.out = 50)), req_ci_lower_p2a(t1_wavg)),
  col = "#ff000033",
  border = NA)
curve(req_ci_lower_p2a, 0, 1, col = "red", lty = 2, add = TRUE)
curve(req_ci_upper_p2a, 0, 1, col = "red", lty = 3, add = TRUE)
curve(req_p2a, 0, 1, col = "red", lty = 1, lwd = 2, add = TRUE)
legend(.65, .3, legend = c("Pt. I(a) upper CI", "Pt. I(a) lower CI", "Pt. I(a) req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))
abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")


# t1_wavg:
abline(v = t1_wavg)
text(x = .42, y = .93, paste0("t1_wavg=", round(t1_wavg, 4)))
```

Moving the boundary $t_1$ farther behind, changes the variable `REQ` and its confidence interval slightly, as can be seen in figure \ref{fig:req-p1a-cis}. Next, we will adapt the remaining variables and their confidence intervals.

```{r}
dev_p2a <- timewarp_variable(f = dev, t = t_2, t_new = t2_wavg)
dev_ci_upper_p2a <- timewarp_variable(f = dev_ci_upper, t = t_2, t_new = t2_wavg)
desc_p2a <- timewarp_variable(f = desc, t = t_2, t_new = t2_wavg)
```

```{r dev-desc-p1a-cis, echo=FALSE, fig.cap="The variable dev\\% and its upper confidence interval, as well as the variable desc\\%, after time warping for pattern type I (a).", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper_p2a(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff33",
  border = NA)

curve(dev_ci_upper_p2a, 0, 1, col = "blue", lty = 2, add = TRUE)
curve(desc_p2a, 0, 1, add = TRUE, col = "forestgreen", lty = 1, lwd = 3)
curve(dev_p2a, 0, 1, col = "blue", lty = 1, lwd = 3, add = TRUE)
legend(.02, 1.01, legend = c("Pt. I(a) dev upper CI", "Pt. I(a) dev%", "Pt. I(a) desc%"),
       col = c("blue", "blue", "forestgreen"), lty = c(3, 1, 1), lwd = c(1, 3, 3))

abline(v = 0, col = "#888888")
abline(v = 1, col = "#888888")

# t2_wavg:
abline(v = t2_wavg)
text(x = .71, y = .94, paste0("t2_wavg=", round(t2_wavg, 4)))
```

Moving the boundary $t_2$ was a more significant change for variables `DEV` and `DESC` than it was for `REQ`, as of figure \ref{fig:dev-desc-p1a-cis}.



## Pattern III: Averaging the ground truth

This is the same approach we undertook for pattern type III (average) for the Fire Drill in source code. However, we will also learn an __empirical confidence interval__, which is later used for two additional detection methods. These methods have the advantage that they work over arbitrary (integration) intervals, making them also applicable for early detection of the process (i.e., not the entire process needs to be observed, and we can just attempt to detect what we have so far).


```{r}
p3_weighted_var <- function(name, omega) {
  funcs <- list()
  for (pId in names(all_signals)) {
    funcs[[pId]] <- all_signals[[pId]][[name]]$get0Function()
  }
  
  function(x) sapply(X = x, FUN = function(x_) {
    omega %*% unlist(lapply(funcs, function(f) f(x_))) / sum(omega)
  })
}
```

```{r}
req_p3 <- p3_weighted_var(name = "REQ", omega = omega)
dev_p3 <- p3_weighted_var(name = "DEV", omega = omega)
desc_p3 <- p3_weighted_var(name = "DESC", omega = omega)
```

The computed weighted average-variables for `REQ` and `DEV` are shown in figure \ref{fig:avg-req-dev}.

```{r avg-req-dev, echo=FALSE, fig.cap="The weighted average for the three variables req\\%, dev\\% and desc\\%.", fig.align="center", fig.pos="ht!"}
curve(req_p3, 0, 1, col = "red", xlab = "Relative time", ylab = "Cumulative time spent")
curve(dev_p3, 0, 1, col = "blue", add = TRUE)
curve(desc_p3, 0, 1, col = "forestgreen", add = TRUE)
legend(0, 1, legend = c("cum. emp. req%", "cum. emp. dev%", "cum. emp. desc%"), col = c("blue", "red", "forestgreen"), lty = 1, lwd = 2)
```

### Determining an empirical and inhomogeneous confidence interval\label{sssec:inhomo-conf-interval}

Also, we want to calculate an empirical confidence interval and -surface, based on the projects' data and the consensus of the ground truth. The boundary of the lower confidence interval is defined as the infimum of all signals (and the upper CI as the supremum of all signals):

$$
\begin{aligned}
  \bm{f}\dots&\;\text{vector of functions (here: project signals),}
  \\[1ex]
  \operatorname{CI}_{\text{upper}}(x)=&\;\sup{\Bigg(\forall\,f\in\bm{f}\;\bigg[\begin{cases}
    -\infty,&\text{if}\;\bm{\omega}_n=0,
    \\
    f_n(x),&\text{otherwise}
  \end{cases}\bigg]\;,\;\frown\;,\;\Big[\dots\Big]\Bigg)}\;\text{,}
  \\[1ex]
  \operatorname{CI}_{\text{upper}}(x)=&\;\inf{\Bigg(\forall\,f\in\bm{f}\;\bigg[\begin{cases}
    \infty,&\text{if}\;\bm{\omega}_n=0,
    \\
    f_n(x),&\text{otherwise}
  \end{cases}\bigg]\;,\;\frown\;,\;\Big[\dots\Big]\Bigg)}\;\text{.}
\end{aligned}
$$

```{r}
funclist_REQ <- list()
funclist_DEV <- list()
funclist_DESC <- list()
for (pId in names(all_signals)) {
  funclist_REQ[[pId]] <- all_signals[[pId]]$REQ$get0Function()
  funclist_DEV[[pId]] <- all_signals[[pId]]$DEV$get0Function()
  funclist_DESC[[pId]] <- all_signals[[pId]]$DESC$get0Function()
}

CI_bound_p3avg <- function(x, funclist, omega, upper = TRUE) {
  sapply(X = x, FUN = function(x_) {
    val <- unlist(lapply(X = names(funclist), FUN = function(fname) {
      if (omega[fname] == 0) (if (upper) -Inf else Inf) else funclist[[fname]](x_)
    }))
    
    if (upper) max(val) else min(val)
  })
}

req_ci_upper_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_REQ, omega = omega, upper = TRUE)
req_ci_lower_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_REQ, omega = omega, upper = FALSE)
dev_ci_upper_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DEV, omega = omega, upper = TRUE)
dev_ci_lower_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DEV, omega = omega, upper = FALSE)
desc_ci_upper_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DESC, omega = omega, upper = TRUE)
desc_ci_lower_p3avg <- function(x) CI_bound_p3avg(x = x, funclist = funclist_DESC, omega = omega, upper = FALSE)
```

While the above expressions define the _boundaries_ of the lower and upper confidence intervals, we also need a function that interpolates in between. Recall that the CI of the first pattern was __homogeneous__, i.e., it provided no gradation, and was used for a binary decision rule. If we define a function that bases the strength of the confidence on the values of the ground truth of each project's variable, then it also means that with that pattern, all projects are included in the binary decision rule. Having gradation in the CI will allow us to make more probabilistic statements by computing some kind of score.

Figures \ref{fig:req-dev-p3avg-cis} and \ref{fig:desc-p3avg-cis} show the average variables and the empirical confidence intervals.

```{r req-dev-p3avg-cis, echo=FALSE, fig.height=8, fig.cap="Empirical (average) req\\% and dev\\% and their lower- and upper empirical weighted confidence intervals (here without gradation).", fig.align="center", fig.pos="ht!"}

par(mfrow=c(2,1))

plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     main = "Variable: REQ",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = seq(from = 0, to = 1, length.out = 50),
  y = req_ci_upper(seq(from = 0, to = 1, length.out = 50)),
  col = "#ff000011",
  border = NA)

polygon_x <- c(seq(from = 0, to = 1, length.out = 500), seq(from = 1, to = 0, length.out = 500))
polygon(
  x = polygon_x,
  y = c(req_ci_upper_p3avg(head(polygon_x, 500)), req_ci_lower_p3avg(tail(polygon_x, 500))),
  col = "#ff000044",
  border = NA)


curve(req_p3, 0, 1, col = "red", add = TRUE)

curve(req_ci_upper, 0, 1, col = "#ff000033", add = TRUE, lty = 2)
curve(req_ci_lower, 0, 1, col = "#ff000033", add = TRUE, lty = 3)

curve(req_ci_upper_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 2)
curve(req_ci_lower_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 3)


legend(0.5, .37, legend = c("Pt. 3 (avg) req% upper CI", "Pt. III (avg) req% lower CI", "Pt. III (avg) req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))




plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     main = "Variable: DEV",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff11",
  border = NA)

polygon(
  x = polygon_x,
  y = c(dev_ci_upper_p3avg(head(polygon_x, 500)), dev_ci_lower_p3avg(tail(polygon_x, 500))),
  col = "#0000ff44",
  border = NA)

curve(dev_p3, 0, 1, col = "blue", add = TRUE)

curve(dev_ci_upper, 0, 1, col = "#0000ff33", add = TRUE, lty = 2)

curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 3)

legend(0.01, .98, legend = c("Pt. 3 (avg) upper CI", "Pt. III (avg) lower CI", "Pt. III (avg) dev%"),
       col = "blue", lty = c(3, 2, 1), lwd = c(1, 1, 2))
```

```{r desc-p3avg-cis, echo=FALSE, fig.height=4, fig.cap="Empirical (average) desc\\% and its lower- and upper empirical weighted confidence intervals (here without gradation).", fig.align="center", fig.pos="ht!"}
plot(x=0, y=0, xlim = c(0,1), ylim = c(0,1), col = "#00000000",
     main = "Variable: DESC",
     xlab = "Relative time", ylab = "Relative cumulative time spent")
grid()

polygon(
  x = polygon_x,
  y = c(desc_ci_upper_p3avg(head(polygon_x, 500)), desc_ci_lower_p3avg(tail(polygon_x, 500))),
  col = "#228B2244",
  border = NA)

curve(desc_p3, 0, 1, col = "forestgreen", add = TRUE)
curve(desc_ci_upper_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 2)
curve(desc_ci_lower_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 3)

legend(0.01, .98, legend = c("Pt. 3 (avg) desc% upper CI", "Pt. III (avg) desc% lower CI", "Pt. III (avg) desc%"),
       col = "forestgreen", lty = c(3, 2, 1), lwd = c(1, 1, 2))
```


But first, we define a function $f:R^2\mapsto R$ to compute a CI with gradation. For each x/y coordinate, it shall output a confidence based on the weights as of the ground truth, and all projects' variables that output a $y$ that is smaller than (larger than) the given $y$ shall be excluded.

$$
\begin{aligned}
  h_{\text{upper}}(f,x,y)=&\;\begin{cases}
        1,&\text{if}\;f(x)\geq y,
        \\
        0,&\text{otherwise,}
    \end{cases}
  \\
  \operatorname{CI}^+(x,y)=&\;\bm{\omega}^\top\cdot h_{\text{upper}}(\bm{f},x,y)\times\Big[\sum\bm{\omega}\Big]^{-1}\text{.}
\end{aligned}
$$

$\operatorname{CI}^+$ is to be used for the upper confidence region, and likewise, we define $\operatorname{CI}^-$ to be equivalent, but it uses $h_{\text{lower}}$, that switches the condition to $f(x)\leq y$. The decision on whether to use $\operatorname{CI}^+$ or $\operatorname{CI}^-$ depends on whether the given $y$ is above or below the _computed average variable_, i.e.,

$$
\begin{aligned}
  \bar{g}(x)\dots&\;\text{the computed average variable,}
  \\[1ex]
  \operatorname{CI}(x,y)=&\;\begin{cases}
    0,&\text{if}\;y>\operatorname{CI}_{\text{upper}}(x)\;\text{,}
    \\
    0,&\text{if}\;y<\operatorname{CI}_{\text{lower}}(x)\;\text{,}
    \\
    \bm{\omega}^\top\cdot\Bigg[\begin{cases}
      h_{\text{upper}}(\bm{f},x,y),&\text{if}\;\bar{g}(x)<y,
      \\
      h_{\text{lower}}(\bm{f},x,y),&\text{otherwise}
    \end{cases}\Bigg]\times\Big[\sum\bm{\omega}\Big]^{-1},&\text{otherwise}
  \end{cases}\;\text{.}
  \\
  =&\;\begin{cases}
    0,&\text{if}\;y>\operatorname{CI}_{\text{upper}}(x)\,\text{,}
    \\
    0,&\text{if}\;y<\operatorname{CI}_{\text{lower}}(x)\,\text{,}
    \\
    \bm{\omega}^\top\cdot\begin{cases}
      \operatorname{CI}^+(x,y),&\text{if}\;\bar{g}(x)<y,
      \\
      \operatorname{CI}^-(x,y),&\text{otherwise.}
    \end{cases}
  \end{cases}
\end{aligned}
$$

With this definition, we can compute a loss that is then based on a path that goes through this hyperplane. That path is a project's variable.

```{r}
# h_p3avg <- function(funclist, x, y, upper = TRUE) {
#   unlist(lapply(X = funclist, FUN = function(f) {
#     sapply(X = f(x), function(val) {
#       if (val == 0) val <- sqrt(.Machine$double.eps)
#       if ((upper && val >= y) || (!upper && val <= y)) val else 0
#     })
#   }))
# }

# We re-define this function to just indicate.
h_p3avg <- function(funclist, x, y, upper = TRUE, f_ci) {
  unlist(lapply(X = funclist, FUN = function(f) {
    sapply(X = f(x), function(val) {
      if (upper && val >= y) {
        1
        #f_ci(x) - val <-- this could be an alternative using the boundaries
      } else if (!upper && val <= y) {
        1
        #val - f_ci(x)
      } else {
        0
      }
    })
  }))
}

h_upper_p3avg <- function(funclist, x, y, f_ci) h_p3avg(funclist = funclist, x = x, y = y, upper = TRUE, f_ci = f_ci)
h_lower_p3avg <- function(funclist, x, y, f_ci) h_p3avg(funclist = funclist, x = x, y = y, upper = FALSE, f_ci = f_ci)

CI_p3avg <- function(x, y, funclist, f_ci_upper, f_ci_lower, gbar, omega) {
  stopifnot(length(x) == length(y))
  
  sapply(X = seq_len(length.out = length(x)), FUN = function(idx) {
    xi <- x[idx]
    yi <- y[idx]
    
    if (yi > f_ci_upper(xi) || yi < f_ci_lower(xi)) {
      return(0)
    }
    
    gbarval <- gbar(xi)
    hval <- if (gbarval < yi) {
      h_upper_p3avg(funclist = funclist, x = xi, y = yi, f_ci = f_ci_upper)
    } else {
      h_lower_p3avg(funclist = funclist, x = xi, y = yi, f_ci = f_ci_lower)
    }
    
    omega %*% hval / sum(omega)
  })
}

CI_req_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_REQ, f_ci_upper = req_ci_upper_p3avg, f_ci_lower = req_ci_lower_p3avg, gbar = req_p3, omega = omega)
CI_dev_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_DEV, f_ci_upper = dev_ci_upper_p3avg, f_ci_lower = dev_ci_lower_p3avg, gbar = dev_p3, omega = omega)
CI_desc_p3avg <- function(x, y) CI_p3avg(x = x, y = y, funclist = funclist_DESC, f_ci_upper = desc_ci_upper_p3avg, f_ci_lower = desc_ci_lower_p3avg, gbar = desc_p3, omega = omega)

saveRDS(object = list(
  CI_req_p3avg = CI_req_p3avg,
  CI_dev_p3avg = CI_dev_p3avg,
  CI_desc_p3avg = CI_desc_p3avg
), file = "../data/CI_p3avg_funcs.rds")
```


```{r}
x <- seq(0, 1, length.out = 200)
y <- seq(0, 1, length.out = 200)

compute_z_p3avg <- function(varname, x, y, interp = NA_real_) {
  # We cannot call outer because our functions are not properly vectorized.
  #z <- outer(X = x, Y = y, FUN = CI_req_p3avg)
  f <- if (varname == "REQ") {
    CI_req_p3avg
  } else if (varname == "DEV") {
    CI_dev_p3avg
  } else {
    CI_desc_p3avg
  }
  
  z <- matrix(nrow = length(x), ncol = length(y))
  for (i in 1:length(x)) {
    for (j in 1:length(y)) {
      z[i, j] <- f(x = x[i], y = y[j])
    }
  }
  
  res <- list(x = x, y = y, z = z)
  
  if (!is.na(interp)) {
    res <- fields::interp.surface.grid(obj = res, grid.list = list(
      x = seq(from = min(x), to = max(x), length.out = interp),
      y = seq(from = min(y), to = max(y), length.out = interp)))
  }
  
  res
}

z_req <- loadResultsOrCompute(file = "../results/ci_p3avg_z_req.rds", computeExpr = {
  compute_z_p3avg(varname = "REQ", x = x, y = y)
})
z_dev <- loadResultsOrCompute(file = "../results/ci_p3avg_z_dev.rds", computeExpr = {
  compute_z_p3avg(varname = "DEV", x = x, y = y)
})
z_desc <- loadResultsOrCompute(file = "../results/ci_p3avg_z_desc.rds", computeExpr = {
  compute_z_p3avg(varname = "DESC", x = x, y = y)
})
```

Finally, we show the empirical confidence intervals in figures \ref{fig:p3-emp-cis} and \ref{fig:p3-emp-desc-cis}. Note that the minimum non-zero confidence is `r round(min(z_req$z[z_req$z>0]),6)`, while the maximum is $1$. Therefore, while we gradate the colors from $0$ to $1$, we slightly scale and transform the grid's non-zero values using the expression $0.9\times z+0.1$, to improve the visibility.

```{r p3-emp-cis, echo=FALSE, fig.height=8, fig.cap="The empirical confidence intervals for the two variables req\\% and dev\\%. Higher saturation of the color correlates with higher confidence. Projects with zero weight contribute to the CIs' boundaries, but not to the hyperplane.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(2,1))

image(x = z_req$x, y = z_req$y, z = z_req$z, main = "Variable: REQ",
      xlab = "Relative time", ylab = "Relative cumulative time spent",
      col = colorRampPalette(colors = c("#ffffff", "#ff0000"))(100), zlim = c(0,1))
grid()

curve(req_p3, 0, 1, col = "red", add = TRUE)

curve(req_ci_upper_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 2)
curve(req_ci_lower_p3avg, 0, 1, col = "#ff000033", add = TRUE, lty = 3)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$REQ$get0Function()
  curve(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

legend(0.01, .98, legend = c("Weight = 0", "Weight > 0"),
       col = "#00000033", lty = c(3, 2), lwd = 2)



image(x = z_dev$x, y = z_dev$y, z = z_dev$z, main = "Variable: DEV",
      xlab = "Relative time", ylab = "Relative cumulative time spent",
      col = colorRampPalette(colors = c("#ffffff", "#0000ff"))(100), zlim = c(0,1))
grid()
curve(dev_p3, 0, 1, col = "blue", add = TRUE)

curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff33", add = TRUE, lty = 3)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$DEV$get0Function()
  curve(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

legend(0.01, .98, legend = c("Weight = 0", "Weight > 0"),
       col = "#00000033", lty = c(3, 2), lwd = 2)
```


```{r p3-emp-desc-cis, echo=FALSE, fig.height=4, fig.cap="The empirical confidence intervals for the variable desc\\%. Higher saturation of the color correlates with higher confidence.", fig.align="center", fig.pos="ht!"}


image(x = z_desc$x, y = z_desc$y, z = z_desc$z, main = "Variable: DESC",
      xlab = "Relative time", ylab = "Relative cumulative time spent",
      col = colorRampPalette(colors = c("#ffffff", "forestgreen"))(100), ylim = c(0, desc_ci_upper_p3avg(1) * 1.1), zlim = c(0, 1))
grid()
curve(desc_p3, 0, 1, col = "forestgreen", add = TRUE)

curve(desc_ci_upper_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 2)
curve(desc_ci_lower_p3avg, 0, 1, col = "#228B2233", add = TRUE, lty = 3)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$DESC$get0Function()
  curve(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

legend(0.01, desc_ci_upper_p3avg(1) * 1.1 - .02, legend = c("Weight = 0", "Weight > 0"),
       col = "#00000033", lty = c(3, 2), lwd = 2)
```



## Pattern IV\label{ssec:pattern-iv}

We had previously discussed the possibility of partial (early) detection. Some of the scores and losses use integration and custom intervals, and while these might be applicable in some cases without further changes, we need to be careful when dealing with variables that are normalized at project end. This is currently the case for the variables of pattern I.

Pattern IV explores the possibility of not using some pattern directly, but its __derivative__ for all of the variables and the confidence intervals. Here we exploit the fact that a cumulative variable has monotonic behavior, depending on how it was designed even _strictly monotonic_ behavior. That means the slope at any point in time is $>0$ (or at least $\geq0$) - resp. $<0$ (or at least $\leq0$). The confidence intervals model the minimum and maximum extents we would expect, and their derivatives give us lower and upper bounds for the expected rate of change. Regardless of the actual value of the variable, we can thus transform our expectation of its value into an expectation of how it changes over time. Furthermore, the rate of change can be (numerically or analytically) computed for any other time-series variable, the procedure introduced here is not limited to cumulative and/or normalized variables.

Pattern type IV thus is a __Meta-Process model__, and can be applied to any of the previously introduced patterns (which are process models). Generically speaking, it supports the transformation of variables and confidence interval boundaries (but not inhomogeneous confidence interval surfaces). By using any other pattern (process model), pattern type IV gets __instantiated__. For the remainder of this notebook, we will be instantiating the pattern type IV using pattern I. While this helps our demonstration purposes, pattern I is partially far off the real-world data, which means that we must expect mediocre results. In practice, one should use this meta pattern only with well-designed and/or data-enhanced or data-only patterns.


For creating the first derivatives, we can use either, analytical expressions (if available) or numeric methods, such finite difference approaches. In the following, we use both, as we also actually have analytical expressions for some of the curves. The first pattern, represented by its derivatives, is shown in figure \ref{fig:p4-req-dev}.


```{r}
func_d1 <- function(f, x, supp = c(0, 1)) {
  sapply(X = x, FUN = function(x_) {
    t <- 1e-5
    m <- if (x_ < (supp[1] + t)) "forward" else if (x_ > (supp[2] - t)) "backward" else "central"
    pracma::fderiv(f = f, x = x_, method = m)
  })
}

req_d1_p4 <- function(x) {
  func_d1(f = req, x = x)
}
req_ci_lower_d1_p4 <- function(x) rep(1, length(x))
req_ci_upper_d1_p4 <- function(x) 3.89791628313 * exp(-(3.811733 * x))

dev_d1_p4 <- function(x) {
  func_d1(f = dev, x = x)
}
dev_ci_lower_d1_p4 <- function(x) rep(0, length(x))
dev_ci_upper_d1_p4 <- function(x) 0.07815904 + x * (0.8986929 * x + 1.2445534)

# For DESC, there are no confidence intervals
desc_d1_p4 <- function(x) 0.01172386 + x * (0.13480392 * x + 0.186683)
```


While we need the derivatives of any function that describes a variable or confidence interval over time, the derivatives of the confidence intervals now represent upper and lower bounds for the expected range of change. Furthermore, at any point the rate of change of the variable or either of the confidence intervals may exceed any of the other, and the curves can also cross. So, in order to define the upper and lower boundaries, we require the definition of a helper function that returns the minimum and/or maximum of either of these three functions for every $x$:

$$
\begin{aligned}
  \operatorname{CI}_{\nabla \text{upper}}(\nabla f,\nabla f_{\text{lower}},\nabla f_{\text{upper}},x)=&\;\sup{\big(\nabla f(x),\nabla f_{\text{lower}}(x),\nabla f_{\text{upper}}(x)\big)}\;\text{.}
\end{aligned}
$$

Likewise, we define $\operatorname{CI}_{\nabla\text{lower}}(\dots)$ using the infimum.


```{r p4-req-dev, echo=FALSE, fig.height=8, fig.cap="Derivatives of all variables and confidence intervals as of pattern I.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(2,1))

d1_vals_req <- function(x) {
  sapply(X = x, FUN = function(x_) c(req_d1_p4(x_), req_ci_lower_d1_p4(x_), req_ci_upper_d1_p4(x_)))
}
d1_vals_dev <- function(x) {
  sapply(X = x, FUN = function(x_) c(dev_d1_p4(x_), dev_ci_lower_d1_p4(x_), dev_ci_upper_d1_p4(x_)))
}

plot(x=0, y=0, xlim = c(0,1), ylim = c(0, max(d1_vals_req(0))), col = "#00000000",
     main = "Derivative of variable: REQ",
     xlab = "Relative time", ylab = "Rate of change")
grid()

polygon(
  x = polygon_x,
  y = c(sapply(X = polygon_x[1:500], FUN = function(x) max(d1_vals_req(x=x))),
        sapply(X = polygon_x[501:1000], FUN = function(x) min(d1_vals_req(x=x)))),
  col = "#ff000033",
  border = NA)
curve(req_ci_lower_d1_p4, 0, 1, col = "red", lty = 2, add = TRUE)
curve(req_ci_upper_d1_p4, 0, 1, col = "red", lty = 3, add = TRUE)
curve(req_d1_p4, 0, 1, col = "red", lty = 1, lwd = 2, add = TRUE)
legend(.74, max(d1_vals_req(0)) - 0.01, legend = c("upper CI", "lower CI", "req%"),
       col = "red", lty = c(3, 2, 1), lwd = c(1, 1, 2))



plot(x=0, y=0, xlim = c(0,1), ylim = c(0, max(d1_vals_dev(1))), col = "#00000000",
     main = "Derivatives of variables: REQ and DEV",
     xlab = "Relative time", ylab = "Rate of change")
grid()

polygon(
  x = polygon_x,
  y = c(sapply(X = polygon_x[1:500], FUN = function(x) max(d1_vals_dev(x=x))),
        sapply(X = polygon_x[501:1000], FUN = function(x) min(d1_vals_dev(x=x)))),
  col = "#0000ff33",
  border = NA)

curve(dev_ci_lower_d1_p4, 0, 1, col = "blue", lty = 2, add = TRUE)
curve(dev_ci_upper_d1_p4, 0, 1, col = "blue", lty = 3, add = TRUE)
curve(dev_d1_p4, 0, 1, col = "blue", lty = 1, lwd = 2, add = TRUE)
curve(desc_d1_p4, 0, 1, col = "forestgreen", lty = 1, lwd = 2, add = TRUE)
legend(.02, max(d1_vals_dev(1)) - 0.01, legend = c("upper CI", "lower CI", "dev%", "desc%"),
       col = c(rep("blue", 3), "forestgreen"), lty = c(3, 2, 1, 1), lwd = c(1, 1, 2, 2))
```

The next challenge lies in representing the data which, in this case, is cumulative time spent on issues. If we approximate functions in a _zero-hold_-fashion as we did so far, then the gradients of these have extreme steps, and are likely unusable. In the following we attempt a few techniques to approximate a cumulative variable using LOESS-smoothing, constrained B-splines non-parametric regression quantiles, and fitting of orthogonal polynomials. The latter two result in smooth gradients if the number of knots or the degree of the polynomials are kept low. In the following subsections we use the signals of the 3rd project as an example.


### Using averaged bins

In this approach we eliminate plateaus by aggregating all $x$ that have the same $y$, and moving $x$ by the amount of values that have the same $y$. Looking at the gradient in figure \ref{fig:p4-averaged-bins-d1}, it is still too extreme.

```{r}
temp <- table(p3_signals$data$`cum req`)
prev_x <- 0
x <- c()
y <- c()
for (i in 1:length(temp)) {
  x <- c(x, prev_x + temp[[i]] / max(temp))
  prev_x <- tail(x, 1)
  y <- c(y, as.numeric(names(temp[i])))
}
```


```{r p4-averaged-bins-d1, echo=FALSE, fig.height=3, fig.cap="The average-bin signal and its gradient.", fig.align="center", fig.pos="ht!"}

par(mfrow=c(1,2))
plot(x, y, type="l")
tempf <- stats::approxfun(x = x, y = y)
tempf1 <- function(x) func_d1(f = tempf, x = x)
curve(tempf1, min(x), max(x))
```


### Eliminate plateaus

The cumulative variables have large plateaus, and the following is a test to eliminate them. However, this additional manipulation step should be skipped, if possible.

In this test, we iterate over all values of the cumulative variable and only keep x/y pairs, when y increases. In figure \ref{fig:p4-no-plateaus} we compare the original cumulative variable against the one without plateaus, i.e., every $x_t>x_{t-1}$ (note that the unequal spread of $x$ in the first non-plateau plot is deliberately ignored in the figure).

```{r p4-no-plateaus, echo=FALSE, fig.height=3, fig.cap="Transforming a signal into a non-plateau signal, using unequal widths.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,3))
x <- c(0)
y <- c(0)
for (i in 1:nrow(p3_signals$data)) {
  if (p3_signals$data$`cum req`[i] > tail(y, 1)) {
    x <- c(x, i)
    y <- c(y, p3_signals$data$`cum req`[i])
  }
}
barplot(p3_signals$data$`cum req`)
barplot(height = y)
barplot(height = y, width = diff(x))
```


### LOESS smoothing

First we define a helper function for smoothing signals using LOESS. It will also help us to scale and translate the resulting function into a specific support.


```{r}
smooth_signal_loess <- function(x, y, support = c(0, 1), span = .35, family = c("s", "g"), neval = 1e4) {
  temp <- if (span <= 0) {
    list(x = x, y = y)
  } else {
    loess.smooth(x = x, y = y, span = span, family = match.arg(family), evaluation = neval)
  }
  
  stats::approxfun(
    # translate and scale to [0,1], then scale and translate to desired support.
    x = ((temp$x - min(temp$x)) / (max(temp$x) - min(temp$x))) * (support[2] - support[1]) + support[1],
    # scale y together with x, this is important
    y = temp$y / (max(temp$x) - min(temp$x)) * (support[2] - support[1]),
    yleft = utils::head(temp$y, 1),
    yright = utils::tail(temp$y, 1))
}
```


```{r p4-no-plateaus-smooth, echo=FALSE, fig.height=7, fig.cap="Increasing LOESS-smoothing of the non-plateau signal and its resulting gradient (span=0 means no smoothing).", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,2))

templ <- list()
for (idx in 1:9) {
  span <- c(0, seq(from = 0.2, by = 0.05, length.out = 8))[idx]
  templ[[idx]] <- Signal$new(func = smooth_signal_loess(
    x = 1:nrow(p3_signals$data),
    y = cumsum(p3_signals$data$req),# / sum(p3_signals$data$req),
    span = span, family = "g"), name = "P3 Req", support = c(0,1), isWp = TRUE)$plot(show1stDeriv = span > 0) + ylim(0, 1.4) + labs(subtitle = paste0("span=", span))
}

ggpubr::ggarrange(
  plotlist = templ,
  ncol = 3, nrow = 3)
```


In figure \ref{fig:p4-no-plateaus-smooth} we smooth raw project data, i.e., we do __not__ use the non-plateau data or data from averaged bins, but rather the cumulative variable as-is, even __without normalization__, to demonstrate the advantages of this fourth pattern. In the top-left plot we focus on showing the raw signal. The extent of its gradient is well beyond the other (cf. figure \ref{fig:p4-averaged-bins-d1}), smoothed versions. With increasing smoothing-span, both the signal and even more so its gradient become smooth. For our concern, which is the identification of a trend regarding the rate of change, I would say that a span in the range $[0.2,0.45]$ is probably most useful, so the default was selected to be $0.35$, as it smoothes out many of the local peaks, while still preserving the important characteristics of the gradient (rate of change). However, everything $\geq0.2$ appears to be applicable. Note that spans below that often result in errors (typically around $0.15$ and below), so I do not recommend going below $0.2$.

However, the smoothing-span should be chosen according to the intended application. For example, we can compute a score based on the area between the gradient and its expected value. Here, a less smooth gradient should be preferred as it conserves more details. If the application however were, e.g., to compute a correlation, and the curve used for comparison is smooth, then the span should be chosen accordingly higher ($\geq0.4$) to get usable results.


### Constrained B-splines non-parametric regression quantiles

.. or COBS, estimates (fits) a smooth function using B-splines through some "knots". In figure \ref{fig:p4-cobs-splines} we are attempting fitting with $[3,11]$ knots. The number of knots should probably not exceed $4$ or $5$ for the gradient to be useful, as otherwise it gets too many modes. However, we can clearly see that there is no good compromise. Too few knots result in too smooth a signal and gradient, and everything with $5$ knots or more is too rough an approximation, too.

```{r p4-cobs-splines, fig.height=7, fig.cap="Fitted splines with different number of knots and their derivative.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(4,4))

X <- 1:nrow(p3_signals$data)
Y <- p3_signals$data$`cum req` / (max(X) - min(X))
X <- X / (max(X) - min(X))
templ <- list()

for (i in 1:9) {
  templ[[i]] <- (function() {
    temp <- cobs::cobs(x = X, y = Y, nknots = i + 2, print.mesg = FALSE, print.warn = FALSE)
    Signal$new(
      func = function(x) stats::predict(object = temp, z = x)[, "fit"],
      name = "P3 Req", support = range(X), isWp = TRUE)$plot(
        show1stDeriv = TRUE) + labs(subtitle = paste0("nknots=", i + 2))
  })()
}

ggpubr::ggarrange(
  plotlist = templ,
  ncol = 3, nrow = 3)
```

### Orthogonal polynomials

Using a varying degree, we fit polynomials to the data in order to obtain a truly smooth curve. This technique always results in smooth approximations both for the signal and its gradient (unlike COBS). In figure \ref{fig:p4-polynomials-d1} we observe how the derivative captures more and more nuances of the signal, starting from a degree of approx. $4$. For our tests with this pattern later, polynomials might be an alternative to LOESS-smoothed polynomials. I do recommend using these here polynomials of degree $\geq3$, because the true purpose of all this is to estimate the non-linear trend for the variable and its rate of change.


```{r p4-polynomials-d1, fig.height=7, fig.cap="Fitted polynomials and their first derivative using varying degrees of freedom.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(4,4))

use_degs <- c(2, 3, 4, 5, 7, 10, 14, 19, length(stats::coef(poly_autofit_max(x = X, y = Y))) - 1)

templ <- list()
for (idx in 1:length(use_degs)) {
  
  templ[[idx]] <- (function() {
    ud <- use_degs[idx]
    temp <- stats::lm(Y ~ poly(x = X, degree = ud))
    Signal$new(
      func = function(x) stats::predict(temp, newdata = data.frame(X=x)),
      name = "P3 Req", support = range(X), isWp = TRUE)$plot(
        show1stDeriv = TRUE) + labs(subtitle = paste0("degree=", ud))
  })()
}

ggpubr::ggarrange(
  plotlist = templ,
  ncol = 3, nrow = 3)
```

\clearpage

# Assessing the Goodness of Fit\label{sec:assess-gof}

In the technical report for detecting the Fire Drill in source code, we had previously introduced a plethora of methods to assess the goodness of fit. However, here we introduce additional methods that can exploit __confidence intervals__, both of homogeneous and inhomogeneous nature.


## Score based on CI hyperplane

Simply put, this loss is based on the confidence interval's hyperplane, and calculates an absolute average confidence based on it. Each signal evaluated against the hyperplane is a slice of it. Since we are computing an average confidence, strictly speaking this is not a loss, but a score (higher is better).

$$
\begin{aligned}
  \mathit{L}^{\text{avgconf}}(x_1,x_2,f)=&\;\Bigg[\int_{x_1}^{x_2}\,\operatorname{CI}(x, f(x))\,dx\Bigg]\times(x_2-x_1)^{-1}\;\text{,}
  \\
  &\;\text{where}\;f\;\text{is the signal/variable and}\;x_2>x_1\text{.}
\end{aligned}
$$

We will do a full evaluation later, including creating a decision rule or learning how to scale the average weight to the consensus score, but let's take one project and test this.


```{r}
L_avgconf_p3_avg <- function(x1, x2, f, CI) {
  cubature::cubintegrate(
    f = function(x) {
      CI(x = x, y = f(x))
    },
    lower = x1, upper = x2
  )$integral / (x2 - x1)
}

loadResultsOrCompute(file = "../results/ci_p3avg_Lavg-test.rds", computeExpr = {
  c(
    "P2_REQ" = L_avgconf_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project2$REQ$get0Function(), CI = CI_req_p3avg),
    "P4_REQ" = L_avgconf_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project4$REQ$get0Function(), CI = CI_req_p3avg)
  )
})
```

Plotting `L_avgconf_p3_avg` for the `REQ`-variable of both projects 2 and 4 gives us the function in figure \ref{fig:p2p4-l3avgconf}. We can clearly observe that the area under the latter is larger, and so will be the average.

```{r p2p4-l3avgconf, echo=FALSE, fig.height=3.5, fig.cap="The confidence of req\\% variable of projects 2 and 4 over relative project time.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,2))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    CI_req_p3avg(x = x_, y = all_signals$Project2$REQ$get0Function()(x_))
  })
}

curve(tempf, 0, 1, ylim = c(0,1), xlab = "Relative time", ylab = "Empirical confidence", main = "Project: 2 (REQ)")


tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    CI_req_p3avg(x = x_, y = all_signals$Project4$REQ$get0Function()(x_))
  })
}

curve(tempf, 0, 1, ylim = c(0,1), xlab = "Relative time", ylab = "Empirical confidence", main = "Project: 4 (REQ)")
```

From this example we can clearly see a difference in average confidence, which seems to be somewhat reconciling the projects' weights (consensus score). Let's try the next method, too.


## Loss based on distance to reference-variable

We may choose to ignore the previously computed confidence hyperplane and compute a cost by, e.g., quantifying the distance between the previously computed average variable (or any other reference-variable) and another signal/variable. More precisely, we quantify the area between both variables, and compare it to the largest possible area, thereby obtaining an upper bound (with the lower bound being $0$ obviously). Unlike the previous attempt, this function is a loss, that maps to the range $[0,1]$, where $1$ means the largest possible distance (i.e., the variable compared is entirely outside (or collinear with) the confidence intervals). Hence this function is an attempt of measuring of dissimilarity.

This method requires three features: A reference-variable, and an upper- and a lower confidence interval. However, none of these are required to be of empirical nature. For example, we can even apply this method to the first of our patterns. A reference-variable may be generated as the average between the confidence intervals, etc. All this makes this method versatile. Its robustness comes from the fact any variable it scores, is confined to the boundaries of the confidence intervals.


$$
\begin{aligned}
  \mathit{L}^{\text{areadist}}(x_1,x_2,f)=&\;\int_{x_1}^{x_2}\,\norm{\bar{g}(x)-\overbrace{\min{\Big(\operatorname{CI}_{\text{upper}}(x),\;\max{\big(\operatorname{CI}_{\text{lower}}(x), f(x)\big)}\Big)}}^{\text{Confining of}\;f\;\text{to the confidence intervals.}}}\,dx
  \\[1ex]
  &\;\times\Bigg[\;\overbrace{\int_{x_1}^{x_2}\,\sup{\Big(\bar{g}(x)-\operatorname{CI}_{\text{lower}}(x)\;,\;\operatorname{CI}_{\text{upper}}(x)-\bar{g}(x)\Big)\,dx}}^{\text{maximum possible area between}\;\bar{g}(x)\;\text{and either CI.}}\;\Bigg]^{-1}\;\text{.}
\end{aligned}
$$

This loss may also be alternatively defined using the following denominator:

$$
\begin{aligned}
  \mathit{L}^{\text{areadist2}}(x_1,x_2,f)=&\;\int_{x_1}^{x_2}\,\norm{\bar{g}(x)-\min{\Big(\operatorname{CI}_{\text{upper}}(x),\;\max{\big(\operatorname{CI}_{\text{lower}}(x), f(x)\big)}\Big)}}\,dx
  \\[1ex]
  &\;\times\Bigg[\;\int_{x_1}^{x_2}\,\begin{cases}
    \operatorname{CI}_{\text{upper}}(x)-\bar{g}(x),&\text{if}\;f(x)\geq\bar{g}(x),
    \\
    \bar{g}(x)-\operatorname{CI}_{\text{lower}}(x),&\text{otherwise.}
  \end{cases}\,dx\;\Bigg]^{-1}\;\text{.}
\end{aligned}
$$

The difference is subtle but important, and corrects better for asymmetric confidence intervals. It now captures the maximum possible area, based on the currently valid confidence interval (at $x$). For the remainder, we will always be using the __second variant of this loss__ because of this.

Again, we will do a full evaluation later, but let's just attempt computing this loss once.

```{r}
L_areadist_p3_avg <- function(x1, x2, f, gbar, CI_upper, CI_lower, use2ndVariant = FALSE) {
  int1 <- cubature::cubintegrate(
    f = function(x) {
      abs(gbar(x) - min(CI_upper(x), max(CI_lower(x), f(x))))
    },
    lower = x1, upper = x2
  )$integral
  
  int2 <- cubature::cubintegrate(
    f = function(x) {
      gbarval <- gbar(x)
      if (use2ndVariant) {
        if (f(x) >= gbarval) {
          CI_upper(x) - gbarval
        } else {
          gbarval - CI_lower(x)
        }
      } else {
        max(gbarval - CI_lower(x), CI_upper(x) - gbarval)
      }
    },
    lower = x1, upper = x2
  )$integral
  
  c("area" = int1, "maxarea" = int2, "dist" = int1 / int2)
}

loadResultsOrCompute(file = "../results/ci_p3avg_Larea-test.rds", computeExpr = {
  list(
    "P2_REQ" = L_areadist_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project2$REQ$get0Function(), use2ndVariant = TRUE,
      gbar = req_p3, CI_upper = req_ci_upper_p3avg, CI_lower = req_ci_lower_p3avg),
    "P4_REQ" = L_areadist_p3_avg(
      x1 = 0, x2 = 1, f = all_signals$Project4$REQ$get0Function(), use2ndVariant = TRUE,
      gbar = req_p3, CI_upper = req_ci_upper_p3avg, CI_lower = req_ci_lower_p3avg)
  )
})
```

Let's show the maximum possible distance vs. the distance of a project's variable in a plot (cf. figure \ref{fig:p2p4-lareadist}). These figures clearly show the smaller distance of project 4 to the average. This is expected, as this project has the highest weight, so the average `REQ%`-variable resembles this project most.

```{r p2p4-lareadist, echo=FALSE, fig.height=4, fig.cap="The req\\% variable of projects 2 and 4, together with the maximum possible distance.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,2))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    abs(req_p3(x_) - min(req_ci_upper_p3avg(x_), max(req_ci_lower_p3avg(x_), all_signals$Project2$REQ$get0Function()(x_))))
  })
}
curve(tempf, 0, 1, col = "red", xlab = "Relative time", ylab = "Distance to avg. req\\%", main = "Project: 2", ylim = c(0, .5))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    max(req_p3(x_) - req_ci_lower_p3avg(x_), req_ci_upper_p3avg(x_) - req_p3(x_))
  })
}
curve(tempf, 0, 1, col = "blue", add = TRUE)
legend(0.01, 0.49, legend = c("Pr. 2 req% dist", "max. dist."), col = c("red", "blue"), lty = 1, lwd = 2)



tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    abs(req_p3(x_) - min(req_ci_upper_p3avg(x_), max(req_ci_lower_p3avg(x_), all_signals$Project4$REQ$get0Function()(x_))))
  })
}
curve(tempf, 0, 1, col = "red", xlab = "Relative time", ylab = "Distance to avg. req\\%", main = "Project: 4", ylim = c(0,0.5))

tempf <- function(x) {
  sapply(X = x, FUN = function(x_) {
    max(req_p3(x_) - req_ci_lower_p3avg(x_), req_ci_upper_p3avg(x_) - req_p3(x_))
  })
}
curve(tempf, 0, 1, col = "blue", add = TRUE)
legend(0.01, 0.49, legend = c("Pr. 4 req% dist", "max. dist."), col = c("red", "blue"), lty = 1, lwd = 2)
```

## Loss based on the two previous approaches

This is an early-stadium idea. The essence is that for every $x$, we have a vertical "confidence-slice" that we can integrate over and get an average confidence. Then, we obtain the confidence for the variable in question at the same $x$. Both of these values can then be put into a relation. If we were to integrate this function, we would get the ratio between the variable's confidence and the average confidence, on average.


```{r}
use_x <- .8
dev_p3(use_x)

cubature::cubintegrate(
  f = function(x) {
    CI_dev_p3avg(x = use_x, y = x)
  },
  lower = dev_ci_lower_p3avg(use_x),
  upper = dev_ci_upper_p3avg(use_x)
)$integral / (dev_ci_upper_p3avg(use_x) - dev_ci_lower_p3avg(use_x))
```

```{r}
CI_dev_p3avg(x = use_x, all_signals$Project4$DEV$get0Function()(use_x))
```

At `x=0.8`, the average variable is at $\approx0.71$, the average confidence for the slice is $\approx0.19$, and the confidence of the evaluated variable (project 4, `DEV`) is at $\approx0.22$. This means that the ratio is in the interval $(0,\infty)$. A "perfect" ratio of $1.0$ would express that the tested variable is, on average, equal to the average confidence.


## $m$-dimensional relative continuous Pearson sample correlation coefficient

First, here is the formula:

$$
\begin{aligned}
  f,g\;\dots&\,m\text{-dimensional continuous variables},
  \\[1ex]
  \bm{\mathit{S}}_f,\bm{\mathit{S}}_g=&\;a_{i,j},b_{i,j}\in \mathbb{R}^{m\times2}\,\text{, supports for}\;f,g\;\text{along each dimension,}
  \\[1ex]
  \overline{f}=\mathrm{E}[f]=&\;\left[\int_{a_{1,1}}^{a_{1,2}}\dots\int_{a_{m,1}}^{a_{m,2}}\,f(x_1,\dots,x_m)\,dx_1\dots dx_m\right]
  \\[0ex]
  &\;\;\times\prod_{i=1}^{m}\,(a_{i,1}-a_{i,2})^{-1}\,\text{, (equivalently for}\;\overline{g}\;\text{using}\;\bm{\mathit{S}}_g\text{),}
  \\[1ex]
  =&\;\int_0^1\dots\int_0^1\,f(\mathsf{T}(a_{1,1},a_{1,2},x_1),\,\dots\,,\mathsf{T}(a_{m,1},a_{m,2},x_m))\,dx_1\,\dots\,dx_m\,\text{,}
  \\[1em]
  \operatorname{corr}(f,g)=&\frac{
    \splitfrac{
        \Big(f\big(\mathsf{T}(a_{1,1},a_{1,2},x_1),\,\dots,\,\mathsf{T}(a_{m,1},a_{m,2},x_m)\big)-\mathrm{E}[f]\Big)
    }{
        \times\Big(g\big(\mathsf{T}(b_{1,1},b_{1,2},x_1),\,\dots,\,\mathsf{T}(b_{m,1},b_{m,2},x_m)\big)-\mathrm{E}[g]\Big)
    }
  }{
    \splitfrac{
      \left[\int_0^1\dots\int_0^1\,\Big(f\big(\mathsf{T}(a_{1,1},a_{1,2},x_1),\,\dots\big)-\mathrm{E}[f]\Big)^2\,dx_1\,\dots\,dx_m\right]^{\frac{1}{2}}
  }{
      \times\left[\int_0^1\dots\int_0^1\,\Big(g\big(\mathsf{T}(b_{1,1},b_{1,2},x_1),\,\dots\big)-\mathrm{E}[g]\Big)^2\,dx_1\,\dots\,dx_m\right]^{\frac{1}{2}}
    }
  }\,\text{,}\label{eq:mdim-corr}
  \\[1em]
  c_{fg}=&\;\int_0^1\dots\int_0^1\,corr(f,g)\,dx_1\,\dots\,dx_m\,\text{.}
\end{aligned}
$$


However first, we will implement a continuous relative 1D version to test our approach. Let's generate some sample data that should be highly correlated, shown in figure \ref{fig:dens-example-data}. See how we deliberately have different means in order to dislocate both variables.


```{r}
set.seed(1)

a <- rnorm(500, mean = 5)
b <- rnorm(500, mean = 10)

dens_a <- density(a)
dens_b <- density(b)
dens_b$y <- -1 * dens_b$y

f_a <- approxfun(x = dens_a$x, y = dens_a$y, yleft = 0, yright = 0)
f_b <- approxfun(x = dens_b$x, y = dens_b$y, yleft = 0, yright = 0)
```

```{r dens-example-data, echo=FALSE, fig.height=4, fig.cap="The densities of both normally-distributed variables.", fig.align="center", fig.pos="ht!"}
curve(f_a, min(c(dens_a$x, dens_b$x)), max(c(dens_a$x, dens_b$x)), ylim = range(c(dens_a$y, dens_b$y)))
curve(f_b, min(c(dens_a$x, dens_b$x)), max(c(dens_a$x, dens_b$x)), add = TRUE)
```

As expected, the spatial difference in location does not influence the correlation:

```{r}
cor(dens_a$y, dens_b$y)
```

Before we go further, we want to manually implement the coefficient and visualize the correlation vector. It is shown in figure \ref{fig:oned-corrvec}.

```{r}
corrvec_ab <- (dens_a$y - mean(dens_a$y)) * (dens_b$y - mean(dens_b$y)) /
              (sqrt(sum((dens_a$y - mean(dens_a$y))^2)) * sqrt(sum((dens_b$y - mean(dens_b$y))^2)))

sum(corrvec_ab)
```


```{r oned-corrvec, echo=FALSE, fig.height=3, fig.cap="The one-dimensional correlation vector of the two discrete variables.", fig.align="center", fig.pos="ht!"}
plot(corrvec_ab)
```


### 1D continuous relative correlation

Now we define a continuous relative version of the Pearson sample correlation coefficient:

```{r}
coef_rel_pearson_1d <- function(f, g, supp_f = c(0,1), supp_g = c(0,1)) {
  # sum[ (x_i - bar_x) x (y_i - bar_y) ]
  # ------------------------------------
  # sqrt(sum[ (x_i - bar_x)^2 ]) x sqrt(...)
  
  transform_op <- function(a, b, x) a + x*b - x*a
  
  # Those work, too:
  # bar_f <- cubature::cubintegrate(
  #   f = f, lower = supp_f[1], upper = supp_f[2])$integral / (supp_f[2] - supp_f[1])
  # bar_g <- cubature::cubintegrate(
  #   f = g, lower = supp_g[1], upper = supp_g[2])$integral / (supp_g[2] - supp_g[1])
  
  bar_f <- cubature::cubintegrate(
    f = function(x) f(transform_op(supp_f[1], supp_f[2], x)), lower = 0, upper = 1)$integral
  bar_g <- cubature::cubintegrate(
    f = function(x) g(transform_op(supp_g[1], supp_g[2], x)), lower = 0, upper = 1)$integral
  
  denom_f <- sqrt(cubature::cubintegrate(f = function(x) {
    (f(transform_op(supp_f[1], supp_f[2], x)) - bar_f)^2
  }, lower = 0, upper = 1)$integral)
  denom_g <- sqrt(cubature::cubintegrate(f = function(x) {
    (g(transform_op(supp_g[1], supp_g[2], x)) - bar_g)^2
  }, lower = 0, upper = 1)$integral)
  
  fnum <- function(x) {
    (f(transform_op(a = supp_f[1], b = supp_f[2], x = x)) - bar_f) *
    (g(transform_op(a = supp_g[1], b = supp_g[2], x = x)) - bar_g)
  }
  
  numerator <- cubature::cubintegrate(f = fnum, lower = 0, upper = 1)$integral
  
  list(
    fnum = fnum,
    bar_f = bar_f,
    bar_g = bar_g,
    denom_f = denom_f,
    denom_g = denom_g,
    numerator = numerator,
    corr_func = function(x) {
      fnum(x) / (denom_f * denom_g)
    },
    corr_fg = numerator / (denom_f * denom_g)
  )
}
```

In figure \ref{fig:oned-corrfunc} we show the correlation of both continuous functions. Let's test using the previously generated data, density, and functions thereof:

```{r oned-corrfunc, echo=FALSE, fig.height=3, fig.cap="The one-dimensional correlation function of the two continuous variables.", fig.align="center", fig.pos="ht!"}
temp <- coef_rel_pearson_1d(f = f_a, g = f_b, supp_f = range(dens_a$x), supp_g = range(dens_b$x))
temp[!(names(temp) %in% c("fnum", "corr_func"))]

tempfff <- temp$corr_func
curve(tempfff, 0, 1, ylab = "Continuous correlation")
```

We can see that the results are very similar, except for some decimals. Next, we compare some individual intermittent results, that should be very similar (discrete vs. continuous). Starting with the mean/expectation of either variable:

```{r}
c(mean(dens_a$y), mean(dens_b$y))
c(temp$bar_f, temp$bar_g)
```

Check, very similar. Next, we compute and plot the result of the numerator (cf. fig. \ref{fig:disc-num}):

```{r}
sum((dens_a$y - mean(dens_a$y)) * (dens_b$y - mean(dens_b$y)))
```

```{r disc-num, echo=FALSE, fig.height=3, fig.cap="The discrete values that make up the numerator, plotted over all indexes.", fig.align="center", fig.pos="ht!"}
plot((dens_a$y - mean(dens_a$y)) * (dens_b$y - mean(dens_b$y)), ylab = "Discrete numerator")
```

The continuous version of the numerator is in the previously computed result. If we integrate it, we get the __mean__ of the function (since the area under the curve over the interval $[0,1]$ is always a mean), not the sum. The function for the continuous numerator is shown in figure \ref{fig:cont-num}.

```{r cont-num, echo=FALSE, fig.height=3, fig.cap="The relative function of the continuos numerator plotted over its support.", fig.align="center", fig.pos="ht!"}
tempfff <- temp$fnum
curve(tempfff, 0, 1, ylab = "Continuous numerator")
```

In order to get the same result as we got from the previous summation, we need to multiply by the number of elements in the discrete variable. The following result is very close to what we got from the summation:

```{r}
cubature::cubintegrate(tempfff, 0, 1)$integral * length(a)
```

Let's check some values as computed in the denominator for the two variables (cf. fig. \ref{fig:disc-denoms}):

```{r}
c(
  sqrt(sum((dens_a$y - mean(dens_a$y))^2)),
  sqrt(sum((dens_b$y - mean(dens_b$y))^2)))
```


```{r disc-denoms, echo=FALSE, fig.height=3, fig.cap="Plots of the discrete denominators for both data series.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,2))
plot((dens_a$y - mean(dens_a$y))^2, ylab = "Discrete denominator")
plot((dens_b$y - mean(dens_b$y))^2, ylab = "Discrete denominator")
```

The continuous version of the denominators is the following (cf. fig. \ref{fig:cont-denoms}):

```{r}
tempf_a <- Vectorize(function(x) {
  transform_op <- function(a, b, x) a + x*b - x*a
  
  (f_a(transform_op(min(dens_a$x), max(dens_a$x), x = x)) - temp$bar_f)^2
})
tempf_b <- Vectorize(function(x) {
  transform_op <- function(a, b, x) a + x*b - x*a
  (f_b(transform_op(min(dens_b$x), max(dens_b$x), x = x)) - temp$bar_g)^2
})

c(
  sqrt(cubature::cubintegrate(tempf_a, 0, 1)$integral * length(a)),
  sqrt(cubature::cubintegrate(tempf_b, 0, 1)$integral * length(b)))
```


```{r cont-denoms, echo=FALSE, fig.height=3, fig.cap="Plots of the continuous denominators for both data series.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(1,2))
curve(tempf_a, 0, 1, ylab = "Continuous denominator")
curve(tempf_b, 0, 1, ylab = "Continuous denominator")
```

### 2D continuous relative correlation

Finally, we'll implement the 2D version. Note that we even allow the support of $y$ (the 2nd dimension) to depend on $x$. Therefore, we pass in the support for the 2nd dimension as function. The following function works well, but the number of evaluations should be limited to get results in time (e.g., $50$). I tried $1,000$ evaluations and got very precise results, but it ran for $30$ minutes. With $50$ evaluations, results are similarly close. Remember that we calculate correlations, and there it is often sufficient to have precision up to 2-3 decimals.

```{r}
coef_rel_pearson_2d <- function(f, g, supp_f_d1 = c(0,1), supp_g_d1 = c(0,1), supp_f_d2 = function(x) c(0,1), supp_g_d2 = function(x) c(0,1), maxEval = 50) {
  # sum[ (x_i - bar_x) x (y_i - bar_y) ]
  # ------------------------------------
  # sqrt(sum[ (x_i - bar_x)^2 ]) x sqrt(...)
  
  transform_op <- function(a, b, x) a + x*b - x*a
  
  double_int_mean <- function(func, supp_d1, supp_d2, maxEval = 50) {
    cubature::cubintegrate(f = function(x) {
      x1 <- transform_op(a = supp_d1[1], b = supp_d1[2], x = x)
      d2_a <- supp_d2(x1)[1]
      d2_b <- supp_d2(x1)[2]
      
      cubature::cubintegrate(f = function(y) {
        y1 <- transform_op(a = d2_a, b = d2_b, x = y)
        func(x1, y1)
      }, lower = 0, upper = 1, maxEval = maxEval)$integral
    }, lower = 0, upper = 1, maxEval = maxEval)$integral
  }
  
  bar_f <- double_int_mean(func = f, supp_d1 = supp_f_d1, supp_d2 = supp_f_d2, maxEval = maxEval)
  bar_g <- double_int_mean(func = g, supp_d1 = supp_g_d1, supp_d2 = supp_g_d2, maxEval = maxEval)
  
  
  denom_f <- sqrt(double_int_mean(func = function(x, y) {
    (f(x, y) - bar_f)^2
  }, supp_d1 = supp_f_d1, supp_d2 = supp_f_d2))
  denom_g <- sqrt(double_int_mean(func = function(x, y) {
    (g(x, y) - bar_g)^2
  }, supp_d1 = supp_g_d1, supp_d2 = supp_g_d2))
  
  fnum <- function(x, y) {
    x1_f <- transform_op(a = supp_f_d1[1], b = supp_f_d1[2], x = x)
    d2_f_a <- supp_f_d2(x1_f)[1]
    d2_f_b <- supp_f_d2(x1_f)[2]
    y1_f <- transform_op(a = d2_f_a, b = d2_f_b, x = y)
    
    x1_g <- transform_op(a = supp_g_d1[1], b = supp_g_d1[2], x = x)
    d2_g_a <- supp_g_d2(x1_g)[1]
    d2_g_b <- supp_g_d2(x1_g)[2]
    y1_g <- transform_op(a = d2_g_a, b = d2_g_b, x = y)
    
    (f(x1_f, y1_f) - bar_f) * (g(x1_g, y1_g) - bar_g)
  }
  
  numerator <- cubature::cubintegrate(f = function(x) {
    cubature::cubintegrate(f = function(y) {
      fnum(x, y)
    }, lower = 0, upper = 1, maxEval = maxEval)$integral
  }, lower = 0, upper = 1, maxEval = maxEval)$integral
  
  list(
    fnum = fnum,
    bar_f = bar_f,
    bar_g = bar_g,
    denom_f = denom_f,
    denom_g = denom_g,
    numerator = numerator,
    corr_func = function(x, y) {
      fnum(x, y) / (denom_f * denom_g)
    },
    corr_fg = numerator / (denom_f * denom_g)
  )
}
```

The correlation of these two two-dimensional variables is $\approx0.258$:

```{r}
tempcorr <- loadResultsOrCompute(file = "../results/2dcorr.rds", computeExpr = {
  coef_rel_pearson_2db(f = CI_req_p3avg, g = CI_dev_p3avg, maxEval = 250)
})
tempcorr[!(names(tempcorr) %in% c("fnum", "corr_func"))]
```

In order to show the correlation in three dimensions, we'll compute a grid, cf. fig. \ref{fig:twod-correlation-req-dev-persp}:

```{r}
tempgrid <- loadResultsOrCompute(file = "../results/2dcorr_grid.rds", computeExpr = {
  outer(X = seq(0, 1, length.out = 75), Y = seq(0, 1, length.out = 75), FUN = tempcorr$fnum)
})
```

```{r twod-correlation-req-dev-persp, echo=FALSE, fig.cap="Correlation between the variables req\\% and dev\\%, plotted perspectively. On the left, we look at the correlation from above, while from the right, it is seen from underneath.", fig.align="center", fig.pos="ht!"}
suppressWarnings({
  par(mfrow=c(1,2))
  persp(tempgrid, zlim = c(-.1,.1), phi =  15, theta = -10, border = "#444444", ylab = "Y", xlab = "X", zlab = "Relative correlation")
  persp(tempgrid, zlim = c(-.1,.1), phi = -15, theta = -10, border = "#444444", ylab = "Y", xlab = "X", zlab = "Relative correlation")
})
```

From this example we see there is clearly both, positive and negative correlation. Here is the same as a colorful contour plot (fig. \ref{fig:twod-correlation-req-dev}) (correlation goes from blue/negative to red/positive, and no correlation is white):

```{r twod-correlation-req-dev, echo=FALSE, fig.cap="Correlation between the variables req\\% and dev\\%, plotted spatially.", fig.align="center", fig.pos="ht!"}

plotexpr <- expression({
  par(mar=c(4,4,.85,.85))
  
  fields::image.plot(
    tempgrid,
    zlim = .75*c(-max(tempgrid), max(tempgrid)),
    legend.shrink = .5, legend.lab = "Relative correlation strength", legend.line = 3,
    col = colorRampPalette(colors = c("#0000ff", "#ffffff", "#ff0000"))(101)) # uneven number, so we have a true white for 0 correlation..
  title(xlab = "Relative project time", ylab = "Average confidence", line = 2.5)
  grid()
  contour(tempgrid, zlim = .75*c(-max(tempgrid), max(tempgrid)), add = TRUE, col = "#888888", drawlabels = FALSE)
})
eval(plotexpr)
```


# Early detection

Many of the methods presented in the previous section \ref{sec:assess-gof} can also be used for early detection, by limiting (and re-scaling) the scores to the available interval. That means, that we would make a partial assessment of how well the process observed so far aligns with the process model in question. It only requires knowledge about at what point in time we are, i.e., $t_{\text{now}}$. Scores would then be computed over the interval $[0,t_{\text{now}}]$.

The early detection can be split into two scenarios of interest:

1.    What happened since project begin until now? Scores may also be computed over an arbitrary interval $[t_{\text{begin}},t_{\text{end}}]$. More generally, the limits can be chosen arbitrarily for most scores, as long as $t_{\text{begin}}<t_{\text{end}}$ and $t_{\text{end}}\leq t_{\text{now}}$.
2.    What will be the probable future, starting from now until a chosen point in the future, $t_{\text{future}}$? Here, $t_{\text{end}}<t_{\text{future}}$ and the interval may be chosen arbitrarily. This scenario can be generically transformed into an instance of the first, by using any preferred prediction method to forecast the course of any variable, and then to compute the scores over, e.g., only the forecast interval, within $[t_0,t_{\text{forecast}}]$ etc.

Early detection is widely applicable and can be, e.g., determined for each single variable separately. It may also be computed for derived variables, to make probabilistic statements about the expected _rate of change_.

There is potentially a third scenario, where we would want to obtain point-in-time estimates, i.e., at any arbitrary point in time $t$, we would like to obtain a total score. Furthermore, this would mean that we do __not__ consider anything that happened before $t$, nor after it. We may also not know the exact value for $t$, i.e., how much of the total time has elapsed. In this case, we would probably require many more scores for trying to make up for the missing history. We will not consider this scenario further and only consider the first two scenarios that should cover most of the use cases.


## Arbitrary-interval scores

Given some labeled training data (observations and assigned scores), we can attempt to learn the possibly non-linear relation for any interval in time, i.e., some $[t_{\text{begin}},t_{\text{end}}]$ (where $t_{\text{end}}\leq t_{\text{now}}$), the set of scores computed over that interval, $\mathtt{S}_{t_{\text{begin}},t_{\text{end}}}$, and a ground truth, expressed as a function over continuous time, $g(t_{\text{begin}},t_{\text{end}})\mapsto R$.

The ground truth may be a constant function, i.e., $g(t_{\text{begin}},t_{\text{end}})\mapsto\mathrm{\text{constant}}$. In that case, the time span of the interval must be part of the input data, as we will then actually learn the relationship between the time span and the scores, i.e., some coefficients for the functions that take the time span delimiters, the scores over that interval, $\mathtt{S}_{t_{\text{begin}},t_{\text{end}}}$, and output the constant total score (resp. a value close to it, as there will be residuals).

If the ground truth is not a constant function, but rather yields an exact or approximate total score for any arbitrarily chosen interval $[t_{\text{begin}},t_{\text{end}}]$, it is likely that we can skip using the time span as input variable. This kind of model is useful for when we do not know at what point in time we are at. If we had a good approximate idea, we could use a model that requires time as input and retrieve some average total score over an interval we are confident we are in. Otherwise, a model with non-constant ground truth is required.

The data we have available only provides us with a constant ground truth, i.e., for each project and at any point in time, it is constant. In section \ref{ssec:arbint-scores} we exemplary attempt to learn the non-linear relationship between arbitrarily chosen time spans and computed scores, and the constant ground truth.


### Process alignment

The procedure for computing scores over arbitrarily chosen intervals as just described requires approximate knowledge about when it was captured, since our ground truth is constant and the begin and end are input parameters to the trained model. The problem in a more generalized way is to align two (potentially multivariate) time series, where one of them is incomplete. Also, the alignment potentially has an open begin and/or end.

For these kind of tasks, Dynamic time warping [@giorgino2009] works usually well. Depending on the nature of the process(es) and process model, we may even solve the problem using some optimization. For example, we modeled the issue-tracking data as cumulative processes. We could pose the essentially same optimization problem as earlier, where we attempted to find where the process model is $0$ (see section \ref{ssec:optim-t1t2}), except that here would look for where the process model minus the observed process at $t_{\text{begin}}$ equals zero (and the same for the end using $t_{\text{end}}$). However, this might not work for non-monotonic processes.

One could also attempt a more general way of mathematical optimization that computes an alignment between the process model and the partially observed process such that some loss is minimized the better start and end match. This approach is essentially a manual way of __one-interval__ boundary time warping. For that, we have __`srBTAW`__, which allows us to use arbitrary intervals, losses, weight, etc. For example, an alignment may be computed using the correlation of the processes. DTW on the other hand uses the Euclidean distance as distance measure, and also requires discrete data, which greatly limits its flexibility.

In section \ref{ssec:proc-align-compute} we take an example and compute all of these.


## Forecasting within Vector Fields

If we were to derive for an _inhomogeneous_ confidence interval, the result would be a vector field that, for each pair of coordinates $x,y$, points into the direction of the largest change, which here means the direction into which the confidence would increase the most, on a cartesian coordinate system. This is exemplary shown in figure \ref{fig:example-vectorfield}. We suggest an operationalization in these ways:

*   Using methods usually applied in time series forecasting, we have the means to determine the trends and probable path of a variable. Also, some of these methods will give as a confidence interval, which, over the vector field, is an area that may (partially) overlap.
*   We can determine the total and average confidence of the vector field that is affected by the overlap. This supports assumptions about whether we are currently in or headed into regions with lower or higher confidence (where the confidence represents the degree to which a process model is present).
*   We can determine the direction and strength of steepest increase of the confidence. The direction can be compared to the direction of the forecast. This may be exploited for making decisions that lead to heading away or into the confidence surface, depending on which choice is desirable. It also gives insights into the current projected trend.


```{r example-vectorfield, echo=FALSE, fig.height=6, fig.cap="The inhomogeneous confidence interval of the dev\\% variable and its vector field, pointing towards the largest increase in confidence for each pair of x/y coordinates. Here we use a non-smooth surface.", fig.align="center", fig.pos="ht!"}

pracma::vectorfield(fun = Vectorize(FUN = function(x,y) {
    v <- CI_dev_p3avg(x, y)
    if (v == 0) -Inf else -log(2 * v)
}), xlim = c(0.4, 0.8), ylim = c(0.2,.8), n = 20, col = "#0000dd99", scale = .015)
curve(dev_p3, 0, 1, col = "blue", lwd = 2, add = TRUE)
curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 3)
```


In figure \ref{fig:example-vectorfield} we show an example of a vector field that is non-smooth. In the following examples however, we use smoothed version of x/y-slices [@green1993nonparametric].


```{r}
CI_dev_smooth_slice_x <- function(y, nsamp = 100, deriv = 0) {
  stopifnot(length(y) == 1)
  
  data_x <- seq(from = 0, to = 1, length.out = nsamp)
  data_y <- sapply(X = data_x, FUN = function(xslice) {
    CI_dev_p3avg(x = xslice, y = y)
  })
  pred_smooth <- suppressWarnings(expr = {
    stats::smooth.spline(x = data_x, y = data_y)
  })
  
  function(x) {
    vals <- stats::predict(object = pred_smooth, deriv = deriv, x = x)$y
    if (deriv == 0) {
      vals[vals < 0] <- 0
      vals[vals > 1] <- 1
    }
    vals
  }
}
```

```{r}
CI_dev_smooth_slice_y <- function(x, nsamp = 100, deriv = 0) {
  stopifnot(length(x) == 1)
  
  data_x <- seq(from = 0, to = 1, length.out = nsamp)
  data_y <- sapply(X = data_x, FUN = function(yslice) {
    CI_dev_p3avg(x = x, y = yslice)
  })
  pred_smooth <- suppressWarnings(expr = {
    stats::smooth.spline(x = data_x, y = data_y)
  })
  
  function(y) {
    vals <- stats::predict(object = pred_smooth, deriv = deriv, x = y)$y
    if (deriv == 0) {
      vals[vals < 0] <- 0
      vals[vals > 1] <- 1
    }
    vals
  }
}
```

Ideally, the x-slice of $y$ at $x$ returns the same value as the y-slice of $x$ at $y$. This is however only approximately true for the smoothed slices, so we return the mean of these two values and the actual value, to obtain a final smoothed $z$ value that is closest to the ground truth, i.e.,

$$
\begin{aligned}
  \operatorname{Slice}^X(y),\;\operatorname{Slice}^Y(x)\dots&\;\text{horizontal/vertical}\;x\text{/}y\text{-slice,}
  \\[1ex]
  \operatorname{Slice}_{\text{smooth}}^X(y),\;\operatorname{Slice}_{\text{smooth}}^Y(x)\dots&\;\text{smoothed versions, such that}
  \\[1ex]
  \operatorname{Slice}^X(y)\approx\operatorname{Slice}_{\text{smooth}}^X(y)\;\land&\;\operatorname{Slice}^Y(x)\approx\operatorname{Slice}_{\text{smooth}}^Y(x)\;\text{,}
  \\[1em]
  \operatorname{CI}_{\text{smooth}}(x,y)=&\;\Big(\operatorname{CI}(x,y)\;+\;\operatorname{Slice}_{\text{smooth}}^X(y)\;+\;\operatorname{Slice}_{\text{smooth}}^Y(x)\Big)\div3\;\text{.}
\end{aligned}
$$

```{r}
CI_dev_smooth_p3avg <- Vectorize(function(x, y, nsamp = 100) {
  stopifnot(length(x) == 1 && length(y) == 1)
  
  xsl <- CI_dev_smooth_slice_x(y = y, nsamp = nsamp)
  ysl <- CI_dev_smooth_slice_y(x = x, nsamp = nsamp)
  
  mean(c(xsl(x = x), ysl(y = y), CI_dev_p3avg(x = x, y = y)))
  
},  vectorize.args = c("x", "y"))
```


The functions `CI_dev_smooth_slice_x()` and `CI_dev_smooth_slice_y()` can also return the derivative (gradient) of the slice, and we will use this when computing the direction and magnitude in each dimension. At every point $x,y$, we can obtain now two vectors pointing into the steepest increase for either dimension, i.e., $\overrightarrow{x},\overrightarrow{y}$.

```{r}
arrow_dir_smooth <- function(x, y) {
  xsl <- CI_dev_smooth_slice_x(y = y, deriv = 1)
  ysl <- CI_dev_smooth_slice_y(x = x, deriv = 1)
  
  c(x=x, y=y, x1 = xsl(x = x), y1 = ysl(y = y))
}
```

```{r echo=FALSE}
temp <- doWithParallelCluster(expr = {
  g <- expand.grid(data.frame(
    x = seq(0.4, 0.9, length.out=20),
    y = seq(0.4, 0.8, length.out=20)
  ))
  
  foreach::foreach(
    idx = rownames(g),
    .inorder = FALSE,
    .combine = rbind
  ) %dopar% {
    p <- g[idx,]
    
    matrix(data = arrow_dir_smooth(x = p$x, y = p$y), nrow = 1)
  }
}, numCores = min(8, parallel::detectCores()))
```


Finally, we compute a vector field based on a smoothed confidence surface, shown in figure \ref{fig:example-vectorfield-smooth}. Note that all arrows have the same length in this figure, such that it does not depend on the magnitude of the steepest increase they are pointing towards.


```{r example-vectorfield-smooth, echo=FALSE, fig.height=5, fig.cap="Example vector field computed using a smoothed surface. We can clearly observe how the arrows point now properly towards the direction with steepest increase.", fig.align="center", fig.pos="ht!"}

image(x = z_dev$x, y = z_dev$y, z = z_dev$z, main = "Variable: DEV",
      xlab = "Relative time", ylab = "Relative cumulative time spent",
      col = colorRampPalette(colors = c("#ffffff", "#0000ff"))(100),
      zlim = c(0, 1), xlim = c(0.4,0.9), ylim = c(0.4,0.6))
grid()


curve(dev_ci_upper_p3avg, 0, 1, add = TRUE, col = "blue", lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, add = TRUE, col = "blue", lty = 2)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$DEV$get0Function()
  curve(tempf, 0, 1, col="#00000033", lty=if (omega[i] == 0) 3 else 2, add=TRUE)
}

for (i in 1:nrow(temp)) {
  r <- temp[i, ]
  to <- r[3:4]
  if (r[2] < dev_ci_lower_p3avg(r[1]) || r[2] > dev_ci_upper_p3avg(r[1])) {
    next
  }
  to <- to / max(abs(to)) * 0.01
  arrows(x0 = r[1] + (if (to[1] < 0) abs(to[1]) else 0), y0 = r[2] + (if (to[2] < 0) abs(to[2]) else 0),
         x1 = r[1] + (if (to[1] < 0) abs(to[1]) else 0) + to[1], y1 = r[2] + (if (to[2] < 0) abs(to[2]) else 0) + to[2], length = 0.05)
}
```

We want to demonstrate the previously suggested methods using a section of the confidence surfaces in figures \ref{fig:example-vectorfield} and \ref{fig:example-vectorfield-smooth} and an example variable. We will be using the `DEV`-variable of project 5, and make a forecast from $0.65$ to $0.75$ in relative time. This also demonstrates another advantage of having modeled discrete variables in continuous time, as we are not required to make forecasts in discrete time either.

### Average confidence in overlapped surface


```{r}
n_periods <- 10
p5_signals <- all_signals$Project5
pr5_dev <- p5_signals$DEV$get0Function()
forecast_y <- sapply(X = seq(from = 0, to = 0.65, length.out = 65), FUN = pr5_dev)

# Fit an EMA to the data and produce 80% and 95% confidence intervals:
fch <- forecast::holt(y = forecast_y, h = 10)
fch_x <- c(65, seq(from = 66, to = 65 + n_periods, by = 1)) / 100

fch_mean <- stats::approxfun(x = fch_x, y = c(pr5_dev(0.65), as.vector(fch$mean)))
fch_80_upper <- stats::approxfun(x = fch_x, y = c(pr5_dev(0.65), as.vector(fch$upper[,1])), yleft = 0, yright = 0)
fch_80_lower <- stats::approxfun(x = fch_x, y = c(pr5_dev(0.65), as.vector(fch$lower[,1])), yleft = 0, yright = 0)
fch_95_upper <- stats::approxfun(x = fch_x, y = c(pr5_dev(0.65), as.vector(fch$upper[,2])), yleft = 0, yright = 0)
fch_95_lower <- stats::approxfun(x = fch_x, y = c(pr5_dev(0.65), as.vector(fch$lower[,2])), yleft = 0, yright = 0)
```


In figure \ref{fig:ed-avg-conf} we use the smoothed confidence surface again. We show a section of the variable `DEV` and its vector field, pointing towards the steepest increase of confidence at every $x,y$. Then, the course of project 5's `DEV` variable is shown until $0.65$, and forecast until $0.75$ (red line). The prediction confidence intervals are shown in lightgray (80%) and darkgray (95%).


```{r ed-avg-conf, echo=FALSE, fig.cap="Overlap of empirical confidence surface and project 5 (variable: dev\\%). The 80\\% and 95\\% prediction confidence intervals are shown in light- and darkgray.", fig.align="center", fig.pos="ht!"}
pracma::vectorfield(fun = Vectorize(FUN = function(x,y) {
  # CI_dev_p3avg(x, y)
  v <- if (y < dev_ci_lower_p3avg(x) || y > dev_ci_upper_p3avg(x)) 0 else CI_dev_smooth_p3avg(x, y)
  if (v == 0) -Inf else -log(2 * v)
}), xlim = c(0.55, 0.8), ylim = c(0.4, 0.9), n = 12, col = "#0000dd99", scale = .01)
curve(dev_p3, 0, 1, col = "blue", lwd = 2, add = TRUE)
curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 3)

curve(pr5_dev, 0, 0.65, col = "blue", lty = 2, lwd = 2, add = TRUE)
curve(pr5_dev, 0.65, 0.75, col = "blue", lty = 1, lwd = 1, add = TRUE)
curve(fch_mean, 0.65, 0.75, col = "red", lty = 1, lwd = 1, add = TRUE)
curve(fch_80_upper, 0.65, 0.75, col="lightgrey", add = TRUE)
curve(fch_80_lower, 0.65, 0.75, col="lightgrey", add = TRUE)
curve(fch_95_upper, 0.65, 0.75, col="darkgrey", add = TRUE)
curve(fch_95_lower, 0.65, 0.75, col="darkgrey", add = TRUE)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_upper(fch_x), fch_95_upper(rev(fch_x))),
  border = NA,
  col = "#00000011"
)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_lower(fch_x), fch_95_lower(rev(fch_x))),
  border = NA,
  col = "#00000011"
)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_upper(fch_x), fch_80_lower(rev(fch_x))),
  border = NA,
  col = "#00000033"
)

legend(0.555, .89, legend = c("dev%", "dev% CI", "Pr. 5 dev%", "Pr. 5 actual", "Pr. 5 prediction"),
       bg="white", col = c(rep("blue", 4), "red"), lty = c(1, 2, 2, 1, 1), lwd = c(3, 1.5, 2, 1.5, 1.5))
```

In order to calculate the average confidence in the area that overlaps with the vector field, we need a double integral. The upper 95% confidence interval is partially outside the vector field. Generally, we must not integrate areas that do not overlap. Therefore, we define a function $\operatorname{CI}_{\text{overlap}}(x,y)$ to help with that:

$$
\begin{aligned}
  f_{\text{lower}}(x),f_{\text{upper}}(x)\dots&\;\text{functions for the lower/upper prediction confidence intervals,}
  \\[1ex]
  \operatorname{CI}_{\text{lower}}(x),\operatorname{CI}_{\text{upper}}(x)\dots&\;\text{lower/upper confidence boundaries for confidence surface,}
  \\[1ex]
  \operatorname{CI}_{\text{overlap}}(x,y)=&\;\begin{cases}
    0,&\text{if}\;f_{\text{lower}}(x)<\operatorname{CI}_{\text{lower}}(x),
    \\
    0,&\text{if}\;f_{\text{upper}}(x)>\operatorname{CI}_{\text{upper}}(x),
    \\
    \operatorname{CI}(x,y),&\text{otherwise}
  \end{cases}\;\text{,}
  \\[1em]
  \text{average confidence}\;=&\;\int_a^b\bigg[\int_{f_{\text{lower}}(x)}^{f_{\text{upper}}(x)}\,\operatorname{CI}_{\text{overlap}}(x,y)\times\big(f_{\text{upper}}(x)-f_{\text{lower}}(x)\big)^{-1}\,dy\bigg]\times(b-a)^{-1}\,dx\;\text{.}
\end{aligned}
$$

The approximate value for 80% / 95% prediction confidence intervals for the concrete example is calculated as:

```{r}
average_confidence <- function(f_low, f_upp, CI_low, CI_upp, CI, lower, upper, maxEval = 15) {
  cubature::cubintegrate(f = Vectorize(function(x) {
    l <- f_low(x)
    u <- f_upp(x)
    cubature::cubintegrate(f = Vectorize(function(y) {
      if (l < CI_low(x) || u > CI_upp(x)) 0 else CI(x = x, y = y)
    }), lower = l, upper = u, maxEval = maxEval)$integral / (u - l)
  }), lower = lower, upper = upper, maxEval = maxEval)$integral / (upper - lower)
}
```


```{r}
loadResultsOrCompute(file = "../results/ed_avgconf_80.rds", computeExpr = {
  average_confidence(
    f_low = fch_80_lower, f_upp = fch_80_upper,
    CI_low = dev_ci_lower_p3avg, CI_upp = dev_ci_upper_p3avg,
    CI = CI_dev_smooth_p3avg, # CI_dev_p3avg,
    lower = 0.65, upper = 0.75)
})
```

```{r}
loadResultsOrCompute(file = "../results/ed_avgconf_95.rds", computeExpr = {
  average_confidence(
    f_low = fch_95_lower, f_upp = fch_95_upper,
    CI_low = dev_ci_lower_p3avg, CI_upp = dev_ci_upper_p3avg,
    CI = CI_dev_smooth_p3avg, # CI_dev_p3avg,
    lower = 0.65, upper = 0.75)
})
```

As expected, the average confidence in the overlap of the 95% prediction interval is a little lower.

### Average direction of steepest confidence increase

Similar to finding the average confidence in the overlapped surface, we may also determine the direction and strength of the steepest increase in confidence. For every point that is part of the overlapped area, we need to sum up the gradient in x- and y-direction. The resulting combined vector then gives the direction of the steepest increase.

This is quite similar to how we computed the average confidence, and requires the following double integral:

$$
\begin{aligned}
  x\text{/}y\;\text{total change}\;=&\;\int_a^b\bigg[\int_{f_{\text{lower}}(x)}^{f_{\text{upper}}(x)}\,\bigg\{\frac{\partial}{\partial\,x}\,\operatorname{CI}_{\text{overlap}}(x,y)\;,\;\frac{\partial}{\partial\,y}\,\operatorname{CI}_{\text{overlap}}(x,y)\bigg\}^\top\,dy\bigg]\,dx\;\text{,}
  \\[1ex]
  &\;\text{or using individual slices and an overlap-helper:}
  \\[1em]
  \operatorname{overlap}(x,y)=&\;\begin{cases}
    0,&\text{if}\;f_{\text{lower}}(x)<\operatorname{CI}_{\text{lower}}(x),
    \\
    0,&\text{if}\;f_{\text{upper}}(x)>\operatorname{CI}_{\text{upper}}(x),
    \\
    1,&\text{otherwise,}
  \end{cases}
  \\[1ex]
  x\;\text{total change}\;=&\;\int_a^b\bigg[\int_{f_{\text{lower}}(x)}^{f_{\text{upper}}(x)}\,\operatorname{overlap}(x,y)\times\frac{\partial}{\partial\,x}\,\operatorname{Slice}_{\text{smooth}}^X(y)\,dy\bigg]\,dx\;\text{, also similarly for}\;y\text{.}
\end{aligned}
$$

```{r}
total_change <- function(axis = c("x", "y"), f_low, f_upp, CI_low, CI_upp, lower, upper, maxEval = 15) {
  use_x <- match.arg(axis) == "x"
  
  cubature::cubintegrate(f = Vectorize(function(x) {
    l <- f_low(x)
    u <- f_upp(x)
    
    cubature::cubintegrate(f = function(y) {
      sapply(X = y, FUN = function(y_) {
        if (l < CI_low(y_) || u > CI_upp(y_)) {
          return(0)
        }
        val <- arrow_dir_smooth(x = x, y = y_)
        if (use_x) val["x1"] else val["y1"]
      })
    }, lower = l, upper = u, maxEval = maxEval)$integral / (u - l)
  }), lower = lower, upper = upper, maxEval = maxEval)$integral / (upper - lower)
}
```

```{r}
ed_tc_x <- loadResultsOrCompute(file = "../results/ed_total_change_x.rds", computeExpr = {
  total_change(
    f_low = fch_95_lower, f_upp = fch_95_upper,
    CI_low = dev_ci_lower_p3avg, CI_upp = dev_ci_upper_p3avg,
    lower = 0.65, upper = 0.75, axis = "x")
})
ed_tc_x
```

```{r}
ed_tc_y <- loadResultsOrCompute(file = "../results/ed_total_change_y.rds", computeExpr = {
  total_change(
    f_low = fch_95_lower, f_upp = fch_95_upper,
    CI_low = dev_ci_lower_p3avg, CI_upp = dev_ci_upper_p3avg,
    lower = 0.65, upper = 0.75, axis = "y")
})
ed_tc_y
```

These results mean that the slope of the average change is $\approx$`r round(ed_tc_y / ed_tc_x, 2)` in the 95% confidence interval of the prediction that overlaps with the vector field of the `DEV`-variable's confidence surface. This is shown in figure \ref{fig:ed-avg-dir}.


```{r ed-avg-dir, echo=FALSE, fig.cap="The average direction towards the steepest increase in confidence for the dev\\% variable in its confidence surface that is overlapped by the 95\\% confidence interval of the prediction. Note that the direction may change if computed over the 80\\% confidence interval.", fig.align="center", fig.pos="ht!"}
pracma::vectorfield(fun = Vectorize(FUN = function(x,y) {
  # CI_dev_p3avg(x, y)
  v <- if (y < dev_ci_lower_p3avg(x) || y > dev_ci_upper_p3avg(x)) 0 else CI_dev_smooth_p3avg(x, y)
  if (v == 0) -Inf else -log(2 * v)
}), xlim = c(0.55, 0.8), ylim = c(0.4, 0.9), n = 12, col = "#0000dd99", scale = .01)
curve(dev_p3, 0, 1, col = "blue", lwd = 2, add = TRUE)
curve(dev_ci_upper_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 2)
curve(dev_ci_lower_p3avg, 0, 1, col = "#0000ff66", add = TRUE, lty = 3)

curve(pr5_dev, 0, 0.65, col = "blue", lty = 2, lwd = 2, add = TRUE)
curve(pr5_dev, 0.65, 0.75, col = "blue", lty = 1, lwd = 1, add = TRUE)
curve(fch_mean, 0.65, 0.75, col = "red", lty = 1, lwd = 1, add = TRUE)
curve(fch_80_upper, 0.65, 0.75, col="lightgrey", add = TRUE)
curve(fch_80_lower, 0.65, 0.75, col="lightgrey", add = TRUE)
curve(fch_95_upper, 0.65, 0.75, col="darkgrey", add = TRUE)
curve(fch_95_lower, 0.65, 0.75, col="darkgrey", add = TRUE)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_upper(fch_x), fch_95_upper(rev(fch_x))),
  border = NA,
  col = "#00000011"
)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_lower(fch_x), fch_95_lower(rev(fch_x))),
  border = NA,
  col = "#00000011"
)

polygon(
  x = c(fch_x, rev(fch_x)),
  y = c(fch_80_upper(fch_x), fch_80_lower(rev(fch_x))),
  border = NA,
  col = "#00000033"
)

legend(0.555, .89, legend = c("dev%", "dev% CI", "Pr. 5 dev%", "Pr. 5 actual", "Pr. 5 prediction", "Avg. direction"),
       bg="white", col = c(rep("blue", 4), "red", "purple"), lty = c(1, 2, 2, 1, 1, 1), lwd = c(3, 1.5, 2, 1.5, 1.5, 2))

arrows(x0 = 0.65, x1 = 0.75, y0 = 0.65, y1 = 0.65 + 0.1 / ed_tc_x * ed_tc_y, lwd = 2, col = "purple")
```

Now we can use this information to calculate, e.g., an angle between the predicted variable and the average direction in the field. Depending on the case, one usually wants to either follow into the average direction of steepest increase or diverge from it. In our case of predicting the presence of the Fire Drill anti-pattern, we would want to move (stay) away from it.

```{r}
matlib::angle(c(.1, fch_mean(0.75) - fch_mean(0.65)),  c(ed_tc_x, ed_tc_y))
```

The angle between the projected (predicted) `DEV`-variable of project 5 and the average direction of steepest increase in confidence is $\approx84.4^{\circ}$.




# Correlation of scores

This is a new section in the fifth iteration of this report. Similar to computing the correlation of scores against all models using source code data, we will do this here for each and every pattern based on issue-tracking data. We will be using the function `score_variable_alignment` (cf. section \ref{sssec:score-mech}) from the technical report using source code data. We do not have any phases in the process models for issue-tracking data. Therefore, no alignment is required. However, we still have to wrap each PM in an alignment, and will do so by defining one closed interval, without warping or amplitude correction.

As a preparation for the pseudo-alignment, we need to present each project's signals as instances of `Signal`, as well as each pattern's signals then later. This has been done at the beginning of the report, and is stored in `all_signals`. It only requires some slight modifications before we can continue.

Also, for the running the projects against type-IV patterns, we'll need to compute their gradients. Recall that the projects' signals were created using a zero-order hold, creating a cumulative quantity that increases step-wise, which results in extreme gradients. In section \ref{ssec:pattern-iv} we have previously examined which kind of smoothing is perhaps most applicable in order to obtain relatively smooth gradients. The recommended solution was to use LOESS smoothing with a span of `0.35`, and not to go lower than `0.2`. A lower span preserves more details, and the results below were computed with a value of `0.35`. We have previously run these computations with the value `0.2`, with no significant different results.

```{r}
use_span <- 0.35

time_warp_wrapper <- function(pattern, derive = FALSE, use_signals = all_signals, use_ground_truth = ground_truth) {
  signals <- list()
  
  for (project in paste0("Project", rownames(use_ground_truth))) {
    temp <- list()
    if (derive) {
      temp[["REQ"]] <- Signal$new(
        name = paste0(project, "_REQ"), support = c(0,1), isWp = FALSE,
        func = smooth_signal_loess(
          x = 1:nrow(use_signals[[project]]$data),
          y = cumsum(use_signals[[project]]$data$req),
          span = use_span, family = "g"))
      temp[["DEV"]] <- Signal$new(
        name = paste0(project, "_DEV"), support = c(0,1), isWp = FALSE,
        func = smooth_signal_loess(
          x = 1:nrow(use_signals[[project]]$data),
          y = cumsum(use_signals[[project]]$data$dev),
          span = use_span, family = "g"))
      temp[["DESC"]] <- Signal$new(
        name = paste0(project, "_DESC"), support = c(0,1), isWp = FALSE,
        func = smooth_signal_loess(
          x = 1:nrow(use_signals[[project]]$data),
          y = cumsum(use_signals[[project]]$data$desc),
          span = use_span, family = "g"))
    } else {
      temp[["REQ"]] <- use_signals[[project]]$REQ$clone()
      temp$REQ$.__enclos_env__$private$name <- paste0(project, "_REQ")
      temp[["DEV"]] <- use_signals[[project]]$DEV$clone()
      temp$DEV$.__enclos_env__$private$name <- paste0(project, "_DEV")
      temp[["DESC"]] <- use_signals[[project]]$DESC$clone()
      temp$DESC$.__enclos_env__$private$name <- paste0(project, "_DESC")
    }
    signals[[project]] <- time_warp_project(
      pattern = pattern, project = temp, thetaB = c(0,1))
  }
  
  signals
}
```



## Pattern I, II(a), and III

We'll first have to assemble a list of signals that define each pattern, before we can wrap it in an empty alignment and calculate the various scores.

```{r}
p1_it_signals <- loadResultsOrCompute(file = "../data/p1_it_signals.rds", computeExpr = {
  list(
    "REQ"  = Signal$new(name = "p1_it_REQ",  func = req,  support = c(0,1), isWp = TRUE),
    "DEV"  = Signal$new(name = "p1_it_DEV",  func = dev,  support = c(0,1), isWp = TRUE),
    "DESC" = Signal$new(name = "p1_it_DESC", func = desc, support = c(0,1), isWp = TRUE))
}) 

p1_it_projects <- time_warp_wrapper(pattern = p1_it_signals, derive = FALSE)
```


```{r echo=FALSE}
#' This method works slightly different from compute_all_scores().
#' It does not aggregate scores across variable-types, and returns
#' a score per type instead.
compute_all_scores_it <- function(alignment, patternName, vartypes = names(weight_vartype), useYRange = NULL) {
  useScores <- c("area", "corr", "jsd", "kl", "arclen", "sd", "var", "mae",
                 "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")
  
  cl <- parallel::makePSOCKcluster(length(alignment))
  parallel::clusterExport(cl, varlist = list("score_variable_alignment", "req_poly", "dev_poly"))
  
  `rownames<-`(doWithParallelClusterExplicit(cl = cl, expr = {
    foreach::foreach(
      projectName = names(alignment),
      .inorder = TRUE,
      .combine = rbind,
      .packages = c("cobs")
    ) %dopar% {
      source("./common-funcs.R")
      source("../models/modelsR6.R")
      source("../models/SRBTW-R6.R")
        
      scores <- c()
      for (score in useScores) {
        for (vt in vartypes) {
          temp <- score_variable_alignment(
            patternName = patternName, projectName = projectName,
            alignment = alignment[[projectName]], use = score,
            vartypes = vt, useYRange = useYRange)
          scores <- c(scores, `names<-`(temp, paste0(vt, "_", score)))
        }
      }
      `colnames<-`(matrix(data = scores, nrow = 1), names(scores))
    }
  }), sort(names(alignment)))
}
```



```{r}
p1_it_scores <- loadResultsOrCompute(file = "../results/p1_it_scores.rds", computeExpr = {
  as.data.frame(
    compute_all_scores_it(
      alignment = p1_it_projects, patternName = "p1_it", vartypes = names(p1_it_signals)))
})
```

Table \ref{tab:p1-it-scores} shows the correlations for each variable. This is different to how we did it for source code data. Each variable is shown separately in order not to conceal more extreme correlations that would otherwise be inaccessible due to aggregation.

```{r echo=FALSE}
if (interactive()) {
  round(t(p1_it_scores), 4)
} else {
  knitr::kable(
    x = round(`colnames<-`(t(p1_it_scores), paste0("pr_", 1:9)), 2),
    booktabs = TRUE,
    caption = "Scores for the aligned projects with pattern I (issue-tracking; p=product, m=mean).",
    label = "p1-it-scores"
  )
}
```

The correlation of just the ground truth with all scores is in table \ref{tab:p1-it-corr}.

```{r echo=FALSE}
corr <- stats::cor(ground_truth$consensus, p1_it_scores)[1, ]

if (interactive()) {
  corr
} else {
  perCol <- ceiling(length(corr) / 3)
  temp <- data.frame(matrix(ncol = 6, nrow = perCol))
  for (idx in 1:3) {
    off <- (idx - 1) * perCol
    temp[, idx * 2 - 1] <- names(corr)[(1 + off):(perCol + off)]
    temp[, idx * 2] <- corr[(1 + off):(perCol + off)]
  }
  colnames(temp) <- rep(c("Score", "Value"), 3)
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Correlation of the ground truth with all other scores for pattern I (issue-tracking).",
    label = "p1-it-corr")
}
```

The correlation matrix looks as in figure \ref{fig:p1-it-corr-mat}.

```{r p1-it-corr-mat, echo=FALSE, fig.cap="Correlation matrix for scores using pattern I (issue-tracking).", fig.align="top", fig.pos="ht!", fig.width=8, fig.height=8}
temp <- cbind(data.frame(gt_consensus = ground_truth$consensus_score), p1_it_scores[,])

corrplot::corrplot(corr = stats::cor(temp), type = "upper",# order = "hclust",
                   tl.col = "black", tl.srt = 90)
```


From now on, we'll only compute the correlation scores for each variable without showing intermediate results. We'll then later build a larger matrix with results from all patterns.

```{r}
p2a_it_signals <- list(
  "REQ"  = Signal$new(name = "p2a_it_REQ",  func = req_p2a,  support = c(0,1), isWp = TRUE),
  "DEV"  = Signal$new(name = "p2a_it_DEV",  func = dev_p2a,  support = c(0,1), isWp = TRUE),
  "DESC" = Signal$new(name = "p2a_it_DESC", func = desc_p2a, support = c(0,1), isWp = TRUE))

p2a_it_projects <- time_warp_wrapper(pattern = p2a_it_signals, derive = FALSE)
```


```{r}
p2a_it_scores <- loadResultsOrCompute(file = "../results/p2a_it_scores.rds", computeExpr = {
  as.data.frame(
    compute_all_scores_it(
    alignment = p2a_it_projects, patternName = "p2a_it", vartypes = names(p2a_it_signals)))
})
```

```{r}
p3_it_signals <- list(
  "REQ"  = Signal$new(name = "p3_it_REQ",  func = req_p3,  support = c(0,1), isWp = TRUE),
  "DEV"  = Signal$new(name = "p3_it_DEV",  func = dev_p3,  support = c(0,1), isWp = TRUE),
  "DESC" = Signal$new(name = "p3_it_DESC", func = desc_p3, support = c(0,1), isWp = TRUE))

p3_it_projects <- time_warp_wrapper(pattern = p3_it_signals, derive = FALSE)
```

```{r}
p3_it_scores <- loadResultsOrCompute(file = "../results/p3_it_scores.rds", computeExpr = {
  as.data.frame(
    compute_all_scores_it(
      alignment = p3_it_projects, patternName = "p3_it", vartypes = names(p3_it_signals)))
})
```


Let's attempt to aggregate the correlation scores from the first three patterns, and show a plot.

```{r echo=FALSE, warning=FALSE}
score_types <- c("area", "corr", "jsd", "kl", "arclen", "sd", "var", "mae",
                 "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")
var_types <- names(p1_it_signals)

pAll_it_corr <- matrix(nrow = 3, ncol = length(var_types) * length(score_types))


i <- 0
c_names <- NULL
for (vt in var_types) {
  for (pIdx in 1:3) {
    p_data <- if (pIdx == 1) {
      p1_it_scores } else if (pIdx == 2) {
        p2a_it_scores } else { p3_it_scores }
  
    pAll_it_corr[pIdx, (i*length(score_types)+1):(i*length(score_types)+length(score_types))] <-
      stats::cor(
        x = ground_truth$consensus_score,
        y = p_data[, grepl(pattern = paste0("^", vt), x = colnames(p_data))])
  }
  
  i <- i + 1
  
  c_names <- c(c_names, paste0(vt, "_", score_types))
}

colnames(pAll_it_corr) <- c_names
rownames(pAll_it_corr) <- paste0("Pr. ", 1:3)
pAll_it_corr[is.na(pAll_it_corr)] <- sqrt(.Machine$double.eps)
```

In \ref{fig:p1-p2a-p3-corr-scores} we show now the correlation scores per variable and score against each of the three patterns I, II(a), and III.

```{r p1-p2a-p3-corr-scores, message=FALSE, echo=FALSE, fig.cap="Table with correlation scores for the first three types of patterns, calculated against each variable separately.", fig.align="center", fig.pos="ht!", fig.width=8}
ggcorrplot::ggcorrplot(corr = t(pAll_it_corr[rev(rownames(pAll_it_corr)),])) + theme(
  axis.text.x.top = element_text(angle = 90, vjust = 1/3, hjust = 0, size = 9),
  legend.position = "bottom") +
  scale_x_discrete(position = "top") +
  scale_fill_gradient2(
    low = "#0037AA", mid = "#eeeeee", high = "#AA0037", guide = "colorbar",
    limits = c(-1,1), breaks = seq(-1, 1, by = .5)) +
  labs(fill = "Pearson sample correlation")
```


## Derivative of Pattern I, II(a), and III

In the following, we'll basically repeat calculating the correlation scores of against each process model, but this time we will use __derivative__ models and processes. That is, we will compute the scores based on the _rate of change_.

For every cumulative quantity we should apply some smoothing in order to get usable gradients. This applies to all projects in all comparisons, as well as to process model III, since its variables are weighted averages of such quantities.

```{r}
get_smoothed_signal_d1 <- function(name, func) {
  xvals <- seq(from = 0, to = 1, length.out = 1e4)
  func <- smooth_signal_loess(
    x = xvals, y = func(xvals),
    support = c(0,1), family = "g", span = use_span
  )
  
  temp <- Signal$new(name = name, isWp = TRUE, support = c(0,1), func = func)
  Signal$new(name = name, isWp = TRUE, support = c(0,1), func = temp$get1stOrderPd())
}
```



```{r echo=FALSE}
p4p1_it_signals <- list(
  "REQ"  = get_smoothed_signal_d1(name = "p4p1_it_REQ",  func = req),
  "DEV"  = get_smoothed_signal_d1(name = "p4p1_it_DEV",  func = dev),
  "DESC" = get_smoothed_signal_d1(name = "p4p1_it_DESC", func = desc))

p4p1_it_projects <- time_warp_wrapper(pattern = p4p1_it_signals, derive = TRUE)


p4p2a_it_signals <- list(
  "REQ"  = get_smoothed_signal_d1(name = "p4p2a_it_REQ",  func = req_p2a),
  "DEV"  = get_smoothed_signal_d1(name = "p4p2a_it_DEV",  func = dev_p2a),
  "DESC" = get_smoothed_signal_d1(name = "p4p2a_it_DESC", func = desc_p2a))

p4p2a_it_projects <- time_warp_wrapper(pattern = p4p2a_it_signals, derive = TRUE)


p4p3_it_signals <- list(
  "REQ"  = get_smoothed_signal_d1(name = "p4p3_it_REQ",  func = req_p3),
  "DEV"  = get_smoothed_signal_d1(name = "p4p3_it_DEV",  func = dev_p3),
  "DESC" = get_smoothed_signal_d1(name = "p4p3_it_DESC", func = desc_p3))

p4p3_it_projects <- time_warp_wrapper(pattern = p4p3_it_signals, derive = TRUE)
```


The derivatives of the projects I, II(a) and III look like in the following figure \ref{fig:p4of-p1-p2a-p3}:

```{r p4of-p1-p2a-p3, message=FALSE, echo=FALSE, fig.cap=paste0("Derivative process models type I, II(a) and III, using a smoothing-span of ", use_span, "."), fig.align="center", fig.pos="ht!", fig.width=8, fig.height=6}
par(mfrow=c(2,2))

curve2(p4p1_it_signals$DEV$get0Function(), 0, 1, col = "red", xlab = "Relative time", ylab = "Change of cumulative time spent", main = "Process model IV(I)", ylim = c(0, 4.1))
curve2(p4p1_it_signals$REQ$get0Function(), 0, 1, col = "blue", add = TRUE)
curve2(p4p1_it_signals$DESC$get0Function(), 0, 1, col = "gold", add = TRUE)
legend(.2, 4, legend = c("d1 cum. req%", "d1 cum. dev%", "d1 cum. desc%"), col = c("blue", "red", "gold"), lty = 1, lwd = 2)



curve2(p4p2a_it_signals$DEV$get0Function(), 0, 1, col = "red", xlab = "Relative time", ylab = "Change of cumulative time spent", main = "Process model IV(II(a))", ylim = c(0, 2))
curve2(p4p2a_it_signals$REQ$get0Function(), 0, 1, col = "blue", add = TRUE)
curve2(p4p2a_it_signals$DESC$get0Function(), 0, 1, col = "gold", add = TRUE)



curve2(p4p3_it_signals$DEV$get0Function(), 0, 1, col = "red", xlab = "Relative time", ylab = "Change of cumulative time spent", main = "Process model IV(III)", ylim = c(0, 2))
curve2(p4p3_it_signals$REQ$get0Function(), 0, 1, col = "blue", add = TRUE)
curve2(p4p3_it_signals$DESC$get0Function(), 0, 1, col = "gold", add = TRUE)
```


Let's compute the scores next.

```{r}
# We'll use this custom range to compute scores for area and sd/var/mae/rmse
use_yrange <- c(-1, 10)

p4p1_it_scores <- loadResultsOrCompute(file = "../results/p4p1_it_scores.rds", computeExpr = {
  as.data.frame(
    compute_all_scores_it(
      useYRange = use_yrange,
      alignment = p4p1_it_projects, patternName = "p4p1_it", vartypes = names(p4p1_it_signals)))
})

p4p2a_it_scores <- loadResultsOrCompute(file = "../results/p4p2a_it_scores.rds", computeExpr = {
  as.data.frame(
    compute_all_scores_it(
      useYRange = use_yrange,
      alignment = p4p2a_it_projects, patternName = "p4p2a_it", vartypes = names(p4p2a_it_signals)))
})

p4p3_it_scores <- loadResultsOrCompute(file = "../results/p4p3_it_scores.rds", computeExpr = {
  as.data.frame(
    compute_all_scores_it(
      useYRange = use_yrange,
      alignment = p4p3_it_projects, patternName = "p4p3_it", vartypes = names(p4p3_it_signals)))
})
```



```{r}
p4All_it_corr <- matrix(nrow = 3, ncol = length(var_types) * length(score_types))

i <- 0
c_names <- NULL
for (vt in var_types) {
  for (pIdx in 1:3) {
    p_data <- if (pIdx == 1) {
      p4p1_it_scores } else if (pIdx == 2) {
        p4p2a_it_scores } else { p4p3_it_scores }
  
    p4All_it_corr[pIdx, (i*length(score_types)+1):(i*length(score_types)+length(score_types))] <-
      stats::cor(
        x = ground_truth$consensus_score,
        y = p_data[, grepl(pattern = paste0("^", vt), x = colnames(p_data))])
  }
  
  i <- i + 1
  
  c_names <- c(c_names, paste0(vt, "_", score_types))
}

colnames(p4All_it_corr) <- c_names
rownames(p4All_it_corr) <- paste0("Pr. ", 1:3)
p4All_it_corr[is.na(p4All_it_corr)] <- sqrt(.Machine$double.eps)
```

In figure \ref{fig:p4of-p1-p2a-p3-corr-scores} we show now the correlation scores per variable and score against each of the three patterns I, II(a), and III.

```{r p4of-p1-p2a-p3-corr-scores, message=FALSE, echo=FALSE, fig.cap="Table with correlation scores for the first three types of patterns, calculated against each variable separately.", fig.align="center", fig.pos="ht!", fig.width=8}
ggcorrplot::ggcorrplot(corr = t(p4All_it_corr[rev(rownames(p4All_it_corr)),])) + theme(
  axis.text.x.top = element_text(angle = 90, vjust = 1/3, hjust = 0, size = 9),
  legend.position = "bottom") +
  scale_x_discrete(position = "top") +
  scale_fill_gradient2(
    low = "#0037AA", mid = "#eeeeee", high = "#AA0037", guide = "colorbar",
    limits = c(-1,1), breaks = seq(-1, 1, by = .5)) +
  labs(fill = "Pearson sample correlation")
```





# Scoring of projects (first batch)

In the technical report for detecting the Fire Drill using source code data, we already explored a wide range of possible patterns and scoring mechanisms. All of them are based on comparing/scoring the variables (process model vs. process). Some of these we will apply here, too, but our focus is first on detection mechanisms that can facilitate the __confidence intervals__.


## Pattern I

This is the expert-guess of how the Fire Drill would manifest in issue-tracking data. The pattern was conceived with precise values for the confidence intervals at certain points ($t_1,t_2$), and the variables were not of importance. It was only used thus far using a binary decision rule.


### Binary detection decision rule

In this section, we will only replicate earlier results by applying the existing rule. It is further formulated using indicators and thresholds as:

$$
\begin{aligned}
  I_1 =&\;\operatorname{req}(t_1) < y_1 \land \operatorname{req}(t_1) > y_2,
  \\[1ex]
  I_2 =&\;\operatorname{dev}(t_1) < y_3 \land \operatorname{dev}(t_2) < y_4,
  \\[1ex]
  I_3 =&\;\operatorname{desc}(1) > y_5,
  \\[1em]
  \operatorname{detect}^{\text{binary}}(I_1,I_2,I_3) =&\;\begin{cases}
    1,&\text{if}\;I_1 \land (I_2 \lor I_3),
    \\
    0,&\text{otherwise}
  \end{cases}\;\text{, using the threshold values}
  \\[1ex]
  \bm{y}=&\;\{0.8,0.4,0.15,0.7,0.15\}^\top\;\text{.}
\end{aligned}
$$

This can be encapsulated in a single function:

```{r}
p1_dr <- function(projName, y = c(0.8, 0.4, 0.15, 0.7, 0.15), signals = all_signals) {
  req <- signals[[projName]]$REQ$get0Function()
  dev <- signals[[projName]]$DEV$get0Function()
  desc <- signals[[projName]]$DESC$get0Function()
  
  I1 <- req(t_1) < y[1] && req(t_1) > y[2]
  I2 <- dev(t_1) < y[3] && dev(t_2) < y[4]
  I3 <- desc(1) > y[5]
  
  I1 && (I2 || I3)
}
```

```{r}
temp <- sapply(X = names(all_signals), FUN = p1_dr)
p1_detect <- data.frame(
  detect = temp,
  ground_truth = ground_truth$consensus,
  correct = (temp & ground_truth$consensus >= 5) | (!temp & ground_truth$consensus < 5)
)
```

```{r echo=FALSE}
if (interactive()) {
  p1_detect
} else {
  knitr::kable(
    x = p1_detect,
    booktabs = TRUE,
    caption = "Binary detection using a decision rule based on homogeneous confidence intervals of pattern I.",
    label = "p1-bin-detect"
  )
}
```

In table \ref{tab:p1-bin-detect} we show the results of the binary detection, which is based on the manually defined homogeneous confidence intervals.


#### Manually adjusting the rule

Through manual inspection, we found out that the decision rule can be re-calibrated in order to achieve 100% accuracy on the projects. If the thresholds were $y_1=0.73$ (instead of $0.8$) and $y_4=0.69$ (instead of $0.7), not only would the detection results align perfectly with the ground truth assessment (eliminating both false positives), but also $I_1$ no longer be detected in all projects, failing in projects 2, 6 and 7.


#### Automatically adjusting the rule

In order to automatically adjust the thresholds, we could use a global search. This should work well because the problem is tiny and can be quickly evaluated. First, we define a function to return the accuracy given some thresholds. This function can then be used in the optimizer:

```{r echo=FALSE, warning=FALSE, message=FALSE}
# This is actually only done later, but I need to load it here
ground_truth_2nd_batch <- read.csv(file = "../data/ground-truth_2nd-batch.csv", sep = ";")
ground_truth_2nd_batch$consensus_score <- ground_truth_2nd_batch$consensus / 10
rownames(ground_truth_2nd_batch) <- paste0((1+nrow(ground_truth)):(nrow(ground_truth)+nrow(ground_truth_2nd_batch)))
library(readxl)
all_signals_2nd_batch <- list()
for (pId in paste0("Project", 10:15)) {
  all_signals_2nd_batch[[pId]] <- load_project_issue_data(pId = pId)
}
```


```{r}
eval_thresholds <- (function() {
  temp <- append(all_signals, all_signals_2nd_batch)
  gt <- c(ground_truth$consensus >= 5, ground_truth_2nd_batch$consensus >= 5)
  function(x) {
    det <- sapply(X = names(temp), FUN = function(pName) {
      p1_dr(projName = pName, signals = temp, y = x)
    })
    sum(det == gt) / length(gt)
  }
})()
```

Using the above function and the manually re-calibrated thresholds, we get an accuracy of `1`:

```{r}
eval_thresholds(c(0.73, 0.4, 0.15, 0.69, 0.15))
```

Now let's try to find thresholds using a global search:


```{r}
set.seed(1337)

x0 <- c(0.8, 0.4, 0.15, 0.7, 0.15)

nloptr(
  x0 = x0,
  eval_f = function(x) {
    1 - eval_thresholds(x=x)
  },
  lb = c(0.7, 0.3, 0.05, 0.6, 0.05),
  ub = c(0.9, 0.5, 0.25, 0.8, 0.25),
  opts = list(
    stopval = 0, # any solution with accuracy=1 is optimal!
    algorithm = "NLOPT_GN_DIRECT_L_RAND"),
)
```

Re-running this global search produces different optimal solutions, such as $\bm{y}=\{0.8,0.4,0.15,0.666,0.15\}^\top$, $\bm{y}=\{0.8,0.4,0.0833,0.633,0.15\}^\top$, $\bm{y}=\{0.8,0.333,0.083,0.633,0.15\}^\top$, or $\bm{y}=\{0.722,0.1,0.5,0.1,0.177\}^\top$.

As an experiment, we could also force to find an optimal solution that is maximally far away from our manually found solution ($\bm{x}_0$), by adding some regularization:

```{r}
res <- loadResultsOrCompute(file = "../results/p1_optim_y.rds", computeExpr = {
  set.seed(1339)

  req_max <- sqrt(sum((c(0, 1, 1, 0, 1) - x0)^2)) # ~1.7141
  
  res <- nloptr::nloptr(
    x0 = x0,
    eval_f = function(x) {
      acc <- eval_thresholds(x=x)
      reg <- sqrt(sum((x - x0)^2)) / req_max # the max is ~1.7131 using the bounds 0,1
      # We want to maximize accuracy + reg (increase distance)!
      -log(reg + 2*acc) # accuracy is twice as important!
    },
    lb = rep(0, 5),
    ub = rep(1, 5),
    opts = list(
      maxeval = 1e4,
      stopval = -log(3 - sqrt(.Machine$double.eps)), # ~-1.0986 but we're not gonna get there
      algorithm = "NLOPT_GN_DIRECT_L_RAND"),
  )
})

res
```

So I have re-run this a couple of times, and it appears that there is a small solution space that is maximally far away, where the first four values for $\bm{y}$ are $\approx\{1,0,1,0\}$, and $y_5$ gets pushed towards $0.18$, but ultimately needs to be lower than that ($\approx 0.179$).


#### Calculating scoring rules

The decision rule is not a probabilistic classifier, and hence does not output class probabilities. By definition, indicators are of binary nature.
Scoring rules are applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive discrete outcomes or classes.
The set of possible outcomes can be either binary or categorical in nature, and the probabilities assigned to this set of outcomes must sum to one (where each individual probability is in the range of 0 to 1).
We can apply scoring rules if we inverse the relation classifier and ground truth. In our case, the classifier outputs binary predictions (probabilities $0,1$), and the ground truth (the consensus score) is of probabilistic nature.

In the following, we will compute two scoring rules:

* Brier score [@brier1950verification]: This is equivalent to the MSE: $\frac{1}{n}\sum_{i=1}^{N}\,\left(f_i-o_i\right)^2$, where $f_i$ is the probability of the ground truth, and $o_i$ is the classification result (either $0$ or $1$). A Brier score of $0$ would hence be a perfect result, and $1$ the worst possible.
* Logarithmic score: $\frac{1}{N}\sum_{i=1}^{N}\,-\log{\left(1-\left|f_i-o_i\right|\right)}$. Like the Brier score, $0$ would be perfect, but there is no limit, so $+\infty$ is possible.

```{r}
temp <- p1_detect[,]
temp$ground_truth <- temp$ground_truth / 10
temp$zero_r <- 0.5
temp$detect_prob <- sapply(temp$detect, function(d) if (d) 1 else 0)

`names<-`(c(
  mean(scoring::brierscore(object = detect_prob ~ ground_truth, data = temp)),
  mean(scoring::logscore(object = detect_prob ~ ground_truth, data = temp)),
  stats::cor(temp$ground_truth, temp$detect_prob)
), c("Brier score", "Log score", "Correlation"))
```

These results indicate that the binary decision rule does not have perfect scores.
If we relax the probabilities it outputs to $0.75$ resp. $0.25$, the Brier- and Log-score deteriorate even more ($\approx0.156$ and $\approx0.445$), so doing that is not in favor of the decision rule.

Also, we need to compute a baseline, using a so-called Zero-Rule, which represents the simplest possible regression model we can suggest _without_ any prior knowledge. Since we try to predict probabilities, the ZeroR regression is just to predict a probability of $0.5$.

```{r}
Metrics::mse(temp$ground_truth, temp$zero_r)
```

For the first batch of projects, ZeroR outperforms our decision rule in terms of MSE.


#### Automatic calibration of the decision rule

We have previously done the automatic calibration of the continuous PMs based on source code, see section \ref{sec:auto-calib}. Note that this section ("calibration") is conceptually different from the previous sections, where we _adjusted_ the decision rule.
The decision rule can be calibrated as well, but since it is discrete, things will work a bit differently:


* `REQ` is expected to be within a threshold at $t_1$. However, the threshold is only a logical delimiter for the decision rule. For the automatic calibration, only the _expectation_ of `REQ(t_1)` is important (which is $0.7$). Since the threshold is supposed to be a confidence interval, it would have an effect on the calibration. However, the threshold is homogeneous, so there is no such effect.
* `DEV` is to be measured twice, once at $t_1$ and once at $t_2$. Again, we have a threshold/confidence interval, but only the expectation at both points ($0.075,0.4$) is of importance.
* `DESC` is measured at $t=1$. The variables as it was drawn in this report is slightly misleading, as we are not drawing the variable, but its upper confidence boundary. De-scoping is _always_ negative (bad), so that our expectation for this activity is __$0$__. In other words, any de-scoping is bad, and $I_3$ would be positive _above_ a value of $0.15$.


The objective for the calibration is always the same, it is the absolute difference of any measurement from the expectation.
The issue-tracking models are normalized. Therefore any measurement must be in the range `[0,1]`. We will simulate uniformly distributed points in that range.
Let's do the calibration for the four points `REQ(t_1)`, `DEV(t_1)`, `DEV(t_2)`, and `DESC(1)` (the empirical distributions are shown in figure \ref{fig:ac-calib-dec-rule}):

```{r}
set.seed(1)

temp <- req(t_1)
ac_itp1_reqt1_ecdf <- stats::ecdf(abs(temp - stats::runif(n = 2e5)))
temp <- dev(t_1)
ac_itp1_devt1_ecdf <- stats::ecdf(abs(temp - stats::runif(n = 2e5)))
temp <- dev(t_2)
ac_itp1_devt2_ecdf <- stats::ecdf(abs(temp - stats::runif(n = 2e5)))
# All have same likelihood since the expectation for DESC(1) = 0 and the distance is the L1-norm:
ac_itp1_desc1_ecdf <- function(x) x
```

```{r ac-calib-dec-rule, echo=FALSE, fig.height=5, fig.cap=""}
par(mfrow = c(2,2))

temp <- seq(from = 0, to = max(req(t_1), abs(1 - req(t_1))), length.out = 1e3)
plot(x = temp, y = ac_itp1_reqt1_ecdf(temp), type = "l", ylab = "Cumulative probability", xlab = paste0("EXP[REQ(t1)]=", round(req(t_1), 1), " deviation"), main = "ECDF for REQ(t1)")

temp <- seq(from = 0, to = max(dev(t_1), abs(1 - dev(t_1))), length.out = 1e3)
plot(x = temp, y = ac_itp1_devt1_ecdf(temp), type = "l", ylab = "Cumulative probability", xlab = paste0("EXP[DEV(t1)]=", round(dev(t_1), 3), " deviation"), main = "ECDF for DEV(t1)")
temp <- seq(from = 0, to = max(dev(t_2), abs(1 - dev(t_2))), length.out = 1e3)
plot(x = temp, y = ac_itp1_devt2_ecdf(temp), type = "l", ylab = "Cumulative probability", xlab = paste0("EXP[DEV(t2)]=", round(dev(t_2), 1), " deviation"), main = "ECDF for DEV(t2)")

temp <- seq(from = 0, to = 1, length.out = 1e3)
plot(x = temp, y = ac_itp1_desc1_ecdf(temp), type = "l", ylab = "Cumulative probability", xlab = paste0("EXP[DESC(1)]=", desc(1), " deviation"), main = "ECDF for DESC(1)")
```

Since we are calibrating around single points, the ECDFs in figure \ref{fig:ac-calib-dec-rule} are all piece-wise linear, with a single break at the expectation. If the expectation were $0.5$, there would be no break. An expectation $\neq 0.5$ however indicates a different uniform probability before and after it.
In this simple case we could have constructed these ECDFs manually instead of random sampling around each expectation.


Next we will define a function that uses the estimated ECDFs and gives us a probabilistic score from the automatically calibrated decision rule.
The binary decision rule is a logical concatenation of the three indicators: $I_1\land(I_2\lor I_3)$. We have now transformed the rule into four independent objectives, and each objective describes the deviation from an expected value using a uniformly scaled and uniformly distributed score in the range $[0,1]$.
As I see it, we have the justification for two distinct setups for the weights. Either, all weights are the same, with each deviation from the corresponding deviation equally important.
In the other case, we see that the first indicator must be true for the binary decision rule to indicate detection (one half of the entire condition). We could therefore reason to give twice the weight to the objective for `REQ(t_1)`. In the following, we will try both.


```{r}
expectations <- c(
  "reqt1" = unname(req(t_1)), "devt1" = unname(dev(t_1)),
  # Note the expectation for DESC(1) = 0!
  "devt2" = unname(dev(t_2)), "desc1" = 0)

ac_p1_dr <- function(
  projName, signals = all_signals,
  w_reqt1 = 1, w_devt1 = 1, w_devt2 = 1, w_desc1 = 1)
{
  p_req <- signals[[projName]]$REQ$get0Function()
  p_dev <- signals[[projName]]$DEV$get0Function()
  p_desc <- signals[[projName]]$DESC$get0Function()
  
  w_l1 <- w_reqt1 + w_devt1 + w_devt2 + w_desc1
  
  (
    # Note: The objective must be given the absolute difference between
    # the expectation and the project's value!
    # Note the objective return the distance, not the similarity.
    # Therefore, we have to subtract the value from 1!
    w_reqt1 * (1 - ac_itp1_reqt1_ecdf(abs(expectations["reqt1"] - p_req(t_1)))) +
    w_devt1 * (1 - ac_itp1_devt1_ecdf(abs(expectations["devt1"] - p_dev(t_1)))) +
    w_devt2 * (1 - ac_itp1_devt2_ecdf(abs(expectations["devt2"] - p_dev(t_2)))) +
    w_desc1 * (1 - ac_itp1_desc1_ecdf(abs(expectations["desc1"] - p_desc(1))))
  ) / w_l1
}
```


Now we can compute the scores from the automatically calibrated decision rule, using either set of weights.
Also, we shore the calibrated score for each of the objectives in table \ref{tab:ac-it-dr-scores}.

```{r}
temp <- append(all_signals, all_signals_2nd_batch)

ac_it_dr_scores <- as.data.frame(`colnames<-`(`rownames<-`(matrix(ncol = 7, nrow = length(temp), byrow = FALSE, data = c(
  sapply(X = names(temp), function(pName) ac_p1_dr(projName = pName, signals = temp)),
  sapply(X = names(temp), function(pName) ac_p1_dr(projName = pName, signals = temp, w_reqt1 = 2)),
  sapply(X = names(temp), function(pName) {
    p_req <- temp[[pName]]$REQ$get0Function()
    1 - ac_itp1_reqt1_ecdf(abs(expectations["reqt1"] - p_req(t_1)))
  }),
  sapply(X = names(temp), function(pName) {
    p_dev <- temp[[pName]]$DEV$get0Function()
    1 - ac_itp1_devt1_ecdf(abs(expectations["devt1"] - p_dev(t_1)))
  }),
  sapply(X = names(temp), function(pName) {
    p_dev <- temp[[pName]]$DEV$get0Function()
    1 - ac_itp1_devt2_ecdf(abs(expectations["devt2"] - p_dev(t_2)))
  }),
  sapply(X = names(temp), function(pName) {
    p_desc <- temp[[pName]]$DESC$get0Function()
    1 - ac_itp1_desc1_ecdf(abs(expectations["desc1"] - p_desc(1)))
  }),
  c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score)
)), names(temp)), c("all_equal", "pref_reqt1", "reqt1_ecdf", "devt1_ecdf", "devt2_ecdf", "desc1_ecdf", "ground_truth")))
```

```{r echo=FALSE}
if (interactive()) {
  ac_it_dr_scores
} else {
  knitr::kable(
    x = round(ac_it_dr_scores, 4),
    booktabs = TRUE,
    caption = "Scores as calculated by the automatically calibrated decision rule, once using equal weights, and once giving preference to the objective for REQ(t1).",
    label = "ac-it-dr-scores"
  )
}
```

The scores from the automatically calibrated decision rule are shown in table \ref{tab:ac-it-dr-scores}.
Next, we will check the correlation with the ground truth.

```{r}
stats::cor.test(x = ac_it_dr_scores[, 1], c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score))
```

```{r}
stats::cor.test(x = ac_it_dr_scores[, 2], c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score))
```

There is almost no correlation between these scores and the ground truth, the large p-values support the null hypothesis (no correlation). Let's check the Brier-scoring rule (we will check the MSE as we do not have discrete outcomes).

```{r}
`names<-`(c(
  Metrics::mse(ac_it_dr_scores$all_equal, ac_it_dr_scores$ground_truth),
  Metrics::mse(ac_it_dr_scores$pref_reqt1, ac_it_dr_scores$ground_truth)
), c("MSE (all_equal)", "MSE (pref_reqt1)"))
```


These results are much worse than those from the scoring rule of the original decision rule. However, some manual tests indicate that the weights we picked might not have been optimal. We will therefore attempt to optimize them. This is now a simple task since we can define the loss to be the Brier score.



```{r}
eval_f <- function(x) {
  actual <- 1 / sum(x) * (
    x[1] * ac_it_dr_scores$reqt1_ecdf +
    x[2] * ac_it_dr_scores$devt1_ecdf +
    x[3] * ac_it_dr_scores$devt2_ecdf +
    x[4] * ac_it_dr_scores$desc1_ecdf)
  
  # Use this instead to maximize the correlation:
  # 1 - stats::cor(x = actual, y = ac_it_dr_scores$ground_truth)
  Metrics::mse(actual = actual, predicted = ac_it_dr_scores$ground_truth)
}

res <- nloptr::nloptr(
  x0 = rep(0.5, 4),
  lb = rep(0, 4),
  ub = rep(1, 4),
  eval_f = eval_f,
  eval_grad_f = function(x) pracma::grad(f = eval_f, x0 = x),
  opts = list(maxeval = 100, algorithm = "NLOPT_LD_TNEWTON")
)
res
```

This Brier/MSE score is the best so far, the value achieved by the binary decision rule across all projects previously was $0.14$ (with a Log-score of $\approx0.431$ and a correlation of $\approx0.696$).
The weights found by the optimization are shown in table \ref{tab:ac-it-dr-opt-scores}.

```{r echo=FALSE}
temp <- as.data.frame(t(`names<-`(res$solution, c("reqt1", "devt1", "devt2", "desc1"))))

if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Relative importance of the scores as found by optimization and used in the automatically calibrated decision rule.",
    label = "ac-it-dr-opt-scores"
  )
}
```


It appears, for example, that we should not put any weight on the indicator for the `DESC`-variable at $t=1$.
Let's use the weights:

```{r}
temp <- append(all_signals, all_signals_2nd_batch)

temp <- `colnames<-`(`rownames<-`(matrix(ncol = 2, nrow = length(temp), byrow = FALSE, data = c(
  sapply(X = names(temp), function(pName) ac_p1_dr(
    projName = pName, signals = temp, w_reqt1 = res$solution[1], w_devt1 = res$solution[2],
    w_devt2 = res$solution[3], w_desc1 = res$solution[4])),
  c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score)
)), names(temp)), c("optimized", "ground_truth"))
temp
```

Brier-score (MSE) and correlation are:

```{r}
`names<-`(c(
  Metrics::mse(c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score), temp[, 1]),
  cor(c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score), temp[, 1])
), c("Brier-score (MSE)", "Correlation"))
```

Note that we can optimize using other objectives, too. For example, if we were to optimize for the correlation between the scores and the ground truth, we can bump the correlation significantly to $\approx0.197$, while the Brier-score would plummet to $\approx0.34$ (however, still far off from the correlation of the original decision rule).
So while we can optimize the Brier-score, it is ultimately up to the decision maker to pick an objective that will maximize the utility of the model.
This short demonstration showed us that we can completely ignore the `desc1` score, and still achieve a lower Brier score than the original decision rule. Given the assumption that the discrete binary decision rule and its continuous counter part measure the same thing, then this would mean that the indicator for `DESC` is redundant.



So far, we have attempted to learn a weight vector where each weight $\geq0$ (however the sum of all weights must be $>0$ for the normalizing linear weighted scalarizer to work).
The reason behind this was that all objectives from the automatic calibration have the properties of being uniformly distributed in the range $[0,1]$. A normalizing linear combination $l(x)$ of such scores has the exact same properties as well, i.e.,

$$
\begin{aligned}
  \bm{s}\dots\;&\text{vector of score functions, where}\;\forall\,s\in\bm{s},\,s\mapsto\mathcal{U}_{[0,1]}\text{,}
  \\[1ex]
  \bm{\omega}\dots\;&\text{weight vector, where}\;\forall\,\omega\in\bm{\omega},\,\omega>0\text{,}
  \\[1ex]
  l(x)=&\;\norm{\bm{\omega}}_1^{-1}\times\left(\sum_{i=1}^{N}\,\omega_i\times s_i(x)\right)\text{, scalarizer with same properties as}\;\forall\,s\text{.}
\end{aligned}
$$


This means, that even after adapting the weights, the scalarizer $l(x)$ still $\mapsto\mathcal{U}_{[0,1]}$, which still allows to compare calibrated models (of the same family) against each other.
Furthermore, if the weights were adapted this way, we gain insights into the _relative importance_ of each objective (here: score). In our case, we learned for example that the objective `devt2` is the most important, while we should completely ignore objective `desc1` and put only marginal importance into the objectives `reqt1` and `devt1` (see table \ref{tab:ac-it-dr-opt-scores}).


In other words, the normalizing linear scalarizer is its own objective, but we may use any other objective to build a regression model that learns coefficients for our features (here: the scores).
For example, in linear least squares [@rpkg_Matrix], we learn an intercept and a coefficient (multiplier) for each score. However, the resulting fitted model will not that clearly allow us to reason about the relative importance of each score.

Linear least squares, that is

$$
\begin{aligned}
  \hat{\bm{\beta}}=&\;\argmin{\bm{\beta}}\,\norm{\bm{y}-\bm{X}}^2
  \\[1ex]
  =&\;\left(\bm{X}^\top\bm{X}\right)^{-1}\,\bm{X}^\top\bm{y}\text{,}
\end{aligned}
$$

is another kind of objective that will lead to a model of which the predictions will "better" resemble the ground truth. "Better" because the choice of objective is subjective. For example, previously we chose to optimize the Brier-score or correlation.
Here is the solution of linear least squares, including an intercept term:

```{r}
m <- cbind(1, as.matrix(ac_it_dr_scores[, c("reqt1_ecdf", "devt1_ecdf", "devt2_ecdf", "desc1_ecdf")]))
solve(t(m) %*% m) %*% t(m) %*% ac_it_dr_scores$ground_truth
```

This should be equivalent to a linear model:

```{r}
temp.lm <- stats::lm(
  data = ac_it_dr_scores,
  formula = ground_truth ~ reqt1_ecdf + devt1_ecdf + devt2_ecdf + desc1_ecdf)

stats::coef(temp.lm)
```

```{r ac-it-lm-fit, echo=FALSE, fig.cap="Residuals and QQ-plot of the automatically calibrated decision rule that was fitted using a linear model."}
par(mfrow = c(1,2))
plot(temp.lm, ask = FALSE, which = 1:2)
```

```{r}
`names<-`(c(
  Metrics::mse(ac_it_dr_scores$ground_truth, stats::predict(temp.lm, ac_it_dr_scores)),
  cor(ac_it_dr_scores$ground_truth, stats::predict(temp.lm, ac_it_dr_scores))
), c("Brier-score (MSE)", "Correlation"))
```

This fitted model has exceptional good values for the Brier-score and correlation (see fig. \ref{fig:ac-it-lm-fit}), but we cannot use it in reality as it is almost certainly overfit due to the scarcity of the data.
Also, because of the intercept, it is much harder to make any valid statements about the variable importance of our scores.





### Average distance to reference (pattern type I)

As a bonus, to demonstrate the versatility and robustness of this method, we will score the projects against the first pattern, and the second pattern type II (a), which is supposed to be a slight improvement over type I. We will use the variables and confidence intervals as they were defined there, i.e., those are neither data-based nor data-enhanced. For the following tests, the variables `REQ` and `DEV` will be considered, as `DESC` does not have non-empirical confidence intervals.

Pattern I's CIs for the `REQ` variable align quite well with the boundaries of the empirical CIs, which means that most projects lie, by a large degree, within the expert-guessed CIs, so this method should work well. For the `DEV` variable's CIs in pattern I however, only a fraction of the projects are within the guessed CI, the averaged-variable appears even to be completely outside of the empirical CI. Therefore, we cannot expect the method to work well in any way in this scenario (cf. figure \ref{fig:req-dev-p3avg-cis}).

```{r echo=FALSE}
p1_avg_area_scores <- loadResultsOrCompute(file = "../results/p1_avg_area_scores.rds", computeExpr =  {
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    # There is no 'dev_ci_lower', because it would just be f(x)=0
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind,
      .packages = c("cobs")
    ) %dopar% {
      req_p <- all_signals[[pId]]$REQ$get0Function()
      dev_p <- all_signals[[pId]]$DEV$get0Function()
      
      `rownames<-`(data.frame(
        REQ = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = req_p, gbar = req,
          CI_upper = req_ci_upper, CI_lower = req_ci_lower, use2ndVariant = TRUE)["dist"],
        
        DEV = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = dev_p, gbar = dev,
          CI_upper = dev_ci_upper, CI_lower = function(x) 0, use2ndVariant = TRUE)["dist"]
      ), pId)
    }
  })
})
```

```{r p1-avg-area-scores, echo=FALSE, fig.height=8, fig.cap="All projects plotted against the two variables req\\% and dev\\% of the first pattern.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(2,1))

set.seed(1)
myCol = sample(c("pink1", "violet", "mediumpurple1", "slateblue1", "purple", "purple3",
          "turquoise2", "skyblue", "steelblue", "blue2", "navyblue",
          "orange", "tomato", "coral2", "palevioletred", "violetred", "red2",
          "springgreen2", "yellowgreen", "palegreen4",
          "wheat2", "tan", "tan2", "tan3", "brown",
          "grey70", "grey50", "grey30"))


plot(x=0, y=0, xlim = c(0,1), ylim = c(0, 1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cum. time spent")
grid()
curve(req, 0, 1, col="red", lwd=2, add = TRUE)
curve(req_ci_upper, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)
curve(req_ci_lower, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)
polygon(
  x = seq(from = 0, to = 1, length.out = 50),
  y = req_ci_upper(seq(from = 0, to = 1, length.out = 50)),
  col = "#ff00000d",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$REQ$get0Function()
  curve(tempf, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.01, .98, legend = c("req%", "req% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))



plot(x=0, y=0, xlim = c(0,1), ylim = c(0, 1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cum. time spent")
grid()
curve(dev, 0, 1, col="blue", lwd=2, add = TRUE)
tempf <- function(x) rep(0, length(x))
curve(tempf, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)
curve(dev_ci_upper, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)
polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff0d",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$DEV$get0Function()
  curve(tempf, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.01, .98, legend = c("dev%", "dev% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))
```

In figure \ref{fig:p1-avg-area-scores} we show the first pattern and its confidence intervals, plotted against all projects' variables. The method used here to compute a score calculates the area between the pattern's variable (here: `REQ` or `DEV`) and a project's variable, where the upper bound would be the confidence intervals. While the first variant of this loss uses as max distance the CI that is farther away, the 2nd variant uses the CI based on whether the signal is above or below the compared-to variable. If we look at figure \ref{fig:p1-avg-area-scores}, it becomes apparent why that is better: Consider, e.g., project 9 and the `DEV` variable. It is completely above the upper CI. The are between the upper CI and `DEV` and the lower CI and `DEV` are differently large, with the latter being definitely larger. In variant 1, due to the confinement, the area-distance, the are between the variable and the signal would be approximately equivalent to the area between `DEV` and the upper CI. That is then put into relation to the maximum area, which is the one between `DEV` and the lower CI. This results in a distance $\ll1$, even though it could not be worse. Variant 2 however considers, for all realizations of $X$, which CI should be used, and correctly determines the distance as $\approx1$ (note: it might be slightly less or more, due to numerical error).


```{r echo=FALSE}
if (interactive()) {
  p1_avg_area_scores
} else {
  knitr::kable(
    x = p1_avg_area_scores,
    booktabs = TRUE,
    caption = "The average distance of the variables REQ and DEV of each project to the reference-variables REQ/DEV as defined by pattern I.",
    label = "p1-avg-area-scores"
  )
}
```

```{r}
cor(x = ground_truth$consensus_score, y = p1_avg_area_scores[, "REQ"])
cor(x = ground_truth$consensus_score, y = p1_avg_area_scores[, "DEV"])
```

I think it is fair to say that the first pattern and its confidence intervals do not work well with this detection approach. While we do get a moderate correlation for `REQ`, it is positive, when it should have been negative.

As expected, `DEV` is quite unusable, as the correlations are low, albeit negative.


### Average distance to reference (pattern type II (a))


Let's check the next pattern: In type II (a), we have adapted the thresholds $t_1,t_2$ according to the ground truth. We can see, that the CIs of this align already much better with the projects, esp. for the `DEV` variable.

```{r echo=FALSE}
p2a_avg_area_scores <- loadResultsOrCompute(file = "../results/p2a_avg_area_scores.rds", computeExpr =  {
  temp_x <- seq(0, 1, length.out = 1e4)
  req_p2a_temp <- stats::approxfun(x = temp_x, y = req_p2a(temp_x), yleft = 0, yright = 1)
  dev_p2a_temp <- stats::approxfun(x = temp_x, y = dev_p2a(temp_x), yleft = 0, yright = 1)
  
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    # There is no 'dev_ci_lower', because it would just be f(x)=0
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind,
      .packages = c("cobs")
    ) %dopar% {
      req_p <- all_signals[[pId]]$REQ$get0Function()
      dev_p <- all_signals[[pId]]$DEV$get0Function()

      `rownames<-`(data.frame(
        REQ = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = req_p, gbar = req_p2a_temp,
          CI_upper = req_ci_upper_p2a, CI_lower = req_ci_lower_p2a, use2ndVariant = TRUE)["dist"],

        DEV = L_areadist_p3_avg(
          x1 = 0, x2 = 1, f = dev_p, gbar = dev_p2a_temp,
          CI_upper = dev_ci_upper_p2a, CI_lower = function(x) 0, use2ndVariant = TRUE)["dist"]
      ), pId)
    }
  })
})
```

```{r p2a-avg-area-scores, echo=FALSE, fig.height=8, fig.cap="All projects plotted against the two variables req\\% and dev\\% of pattern type II (a).", fig.align="center", fig.pos="ht!"}
par(mfrow=c(2,1))


plot(x=0, y=0, xlim = c(0,1), ylim = c(0, 1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cum. time spent")
grid()
curve(req_p2a, 0, 1, col="red", lwd=2, add = TRUE)
curve(req_ci_upper_p2a, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)
curve(req_ci_lower_p2a, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)
polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), t1_wavg),
  y = c(req_ci_upper_p2a(seq(from = 0, to = 1, length.out = 50)), req_ci_lower_p2a(t1_wavg)),
  col = "#ff00000d",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$REQ$get0Function()
  curve(tempf, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.01, .98, legend = c("req%", "req% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))



plot(x=0, y=0, xlim = c(0,1), ylim = c(0, 1), col = "#00000000",
     xlab = "Relative time", ylab = "Relative cum. time spent")
grid()
curve(dev_p2a, 0, 1, col="blue", lwd=2, add = TRUE)
tempf <- function(x) rep(0, length(x))
curve(tempf, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)
curve(dev_ci_upper_p2a, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)
polygon(
  x = c(seq(from = 0, to = 1, length.out = 50), 1),
  y = c(dev_ci_upper_p2a(seq(from = 0, to = 1, length.out = 50)), 0),
  col = "#0000ff0d",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  tempf <- all_signals[[pId]]$DEV$get0Function()
  curve(tempf, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.01, .98, legend = c("dev%", "dev% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))
```

```{r echo=FALSE}
if (interactive()) {
  p2a_avg_area_scores
} else {
  knitr::kable(
    x = p2a_avg_area_scores,
    booktabs = TRUE,
    caption = "The average distance of the variables REQ and DEV of each project to the reference-variables $\\operatorname{REQ},\\operatorname{DEV}$ as taken from pattern I and adjusted by the optimized $t_1,t_2$ thresholds and timewarping.",
    label = "p2a-avg-area-scores"
  )
}
```

```{r}
cor(x = ground_truth$consensus_score, y = p2a_avg_area_scores[, "REQ"])
cor(x = ground_truth$consensus_score, y = p2a_avg_area_scores[, "DEV"])
```

Now we can observe quite an improvement for both variables. The correlation for `REQ` has increased by more than $1$, so it is moderate now and has the right sign. As for `DEV`, the correlation is almost four times as strong.


## Pattern III (average)\label{ssec:score-p3avg}

The third kind of pattern is based on data only, all the variables, confidence intervals and the strength thereof are based on the nine projects and the weight, which is the same as their consensus score.


### Scoring based on the confidence intervals

We have calculated gradated confidence intervals, which means two things. First, we cannot apply a binary detection rule any longer, as the boundaries of the intervals include each project, only the weight is different. Second, when calculating a score, we will obtain a continuous measure, of which we can calculate a correlation to the consensus score of the ground truth, or, e.g., fit a linear model for scaling these scores.

```{r}
p3_avg_ci_scores_compute <- function(pId, x1 = 0, x2 = 1, signals = all_signals) {
  req <- signals[[pId]]$REQ$get0Function()
  dev <- signals[[pId]]$DEV$get0Function()
  desc <- signals[[pId]]$DESC$get0Function()
  
  `rownames<-`(data.frame(
    REQ = L_avgconf_p3_avg(x1 = x1, x2 = x2, f = req, CI = CI_req_p3avg),
    DEV = L_avgconf_p3_avg(x1 = x1, x2 = x2, f = dev, CI = CI_dev_p3avg),
    DESC = L_avgconf_p3_avg(x1 = x1, x2 = x2, f = desc, CI = CI_desc_p3avg),
    Project = pId
  ), pId)
}
```


```{r}
p3_avg_ci_scores <- loadResultsOrCompute(file = "../results/p3_avg_ci_scores.rds", computeExpr =  {
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind
    ) %dopar% {
      p3_avg_ci_scores_compute(pId = pId, x1 = 0, x2 = 1)
    }
  })
})
```

Table \ref{tab:p3-avg-ci-scores} shows the computed scores.

```{r echo=FALSE}
if (interactive()) {
  p3_avg_ci_scores
} else {
  knitr::kable(
    x = p3_avg_ci_scores,
    booktabs = TRUE,
    caption = "The average confidence of the variables REQ, DEV and DESC of each project as integrated over the confidence intervals' hyperplane.",
    label = "p3-avg-ci-scores"
  )
}
```

Let's test the correlation between either kind of score and the ground truth consensus score. The null hypothesis of this test states that both samples have no correlation.

```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_ci_scores[, "REQ"])
```

For the variable `REQ` we get a significant correlation of $\approx0.83$, and there is no significant evidence for the null hypothesis, so must reject it.


```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_ci_scores[, "DEV"])
```

For the variable `DEV` however, the correlation is quite low, $\approx0.17$. Also, we have significant evidence for accepting the null hypothesis (no correlation).

```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_ci_scores[, "DESC"])
```

Looks like we are getting some strong positive correlation for the variable `DESC` of $\approx0.85$. There is almost no evidence at all for accepting the null hypothesis.



### Scoring based on the distance to average

Here we compute the distance of each project's variables to the previously averaged variables. This approach does not rely on inhomogeneous confidence intervals, and only considers the intervals' boundaries to integrate some distance. Ideally, the distance is $0$, and in the worst case it is $1$. If this ought to be used as score, we probably would want to compute it as $\mathit{S}^{\text{areadist}}=1-\mathit{L}^{\text{areadist}}$.

```{r}
p3_avg_area_scores_compute <- function(pId, x1 = 0, x2 = 1, signals = all_signals) {
  req <- signals[[pId]]$REQ$get0Function()
  dev <- signals[[pId]]$DEV$get0Function()
  desc <- signals[[pId]]$DESC$get0Function()
  
  `rownames<-`(data.frame(
    REQ = L_areadist_p3_avg(
      x1 = x1, x2 = x2, f = req, gbar = req_p3, use2ndVariant = TRUE,
      CI_upper = req_ci_upper_p3avg, CI_lower = req_ci_lower_p3avg)["dist"],
    
    DEV = L_areadist_p3_avg(
      x1 = x1, x2 = x2, f = dev, gbar = dev_p3, use2ndVariant = TRUE,
      CI_upper = dev_ci_upper_p3avg, CI_lower = dev_ci_lower_p3avg)["dist"],
    
    DESC = L_areadist_p3_avg(
      x1 = x1, x2 = x2, f = desc, gbar = desc_p3, use2ndVariant = TRUE,
      CI_upper = desc_ci_upper_p3avg, CI_lower = desc_ci_lower_p3avg)["dist"],
    
    Project = pId
  ), pId)
}
```


```{r}
p3_avg_area_scores <- loadResultsOrCompute(file = "../results/p3_avg_area_scores.rds", computeExpr =  {
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind
    ) %dopar% {
      p3_avg_area_scores_compute(pId = pId)
    }
  })
})
```

Table \ref{tab:p3-avg-area-scores} shows the computed scores.

```{r echo=FALSE}
if (interactive()) {
  p3_avg_area_scores
} else {
  knitr::kable(
    x = p3_avg_area_scores,
    booktabs = TRUE,
    caption = "The average distance of the variables REQ, DEV and DESC of each project to the previously averaged reference-variables $\\overline{\\operatorname{REQ}},\\overline{\\operatorname{DEV}},\\overline{\\operatorname{DESC}}$.",
    label = "p3-avg-area-scores"
  )
}
```

As for the correlation tests, ideally, we get negative correlations, as the computed score is __lower__ the less distance we find between the area of the average variable and a project's variable, hence the relation must be antiproportional.


```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_area_scores[, "REQ"])
```

For the variable `REQ` we get a weaker, yet moderate correlation of $\approx-0.42$. However, there is evidence for the null hypothesis, which suggests that there is no statistical significant correlation. This means, we will have to use this with care, if at all.


```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_area_scores[, "DEV"])
```

The correlation for the variable `DEV` is poor again. Also, it is positive, which means that the scores calculated using this method are proportional, when they should not be. The p-value is significant, so there is most likely no significant correlation for this variable and the ground truth.

```{r}
cor.test(x = ground_truth$consensus_score, y = p3_avg_area_scores[, "DESC"])
```

The correlation for `DESC` is substantial, and also it is negative like it should be. The p-value is below the significance level $0.05$, suggesting correlation. We might be able to use this score.



### Linear combination of the two methods

```{r}
temp <- data.frame(
  gt_consensus = ground_truth$consensus_score,
  ci_req = p3_avg_ci_scores$REQ,
  area_req = p3_avg_area_scores$REQ,
  ci_dev = p3_avg_ci_scores$DEV,
  area_dev = p3_avg_area_scores$DEV,
  ci_desc = p3_avg_ci_scores$DESC,
  area_desc = p3_avg_area_scores$DESC)

# ci_req + area_desc gives us ~0.951 already!
# ci_req + ci_dev + area_desc gives ~0.962
p3_avg_lm <- stats::lm(formula = gt_consensus ~ ci_req + ci_dev + area_desc, data = temp)
stats::coef(p3_avg_lm)
par(mfrow=c(1,2))
plot(p3_avg_lm, ask = FALSE, which = 1:2)
```

Using the approximate coefficients of the linear model, we can define the detector as follows:

$$
\begin{aligned}
  x_1,x_2,\operatorname{req},\operatorname{dev},\operatorname{desc}\dots&\;\text{lower/upper integration interval and project signals,}
  \\[1ex]
  \bm{\tau}=&\;\Big\{\mathit{L}^{\text{avgconf}}(x_1,x_2,\operatorname{req}),\;\mathit{L}^{\text{avgconf}}(x_1,x_2,\operatorname{dev}),\;\mathit{L}^{\text{areadist2}}(x_1,x_2,\operatorname{desc})\Big\}\;\text{,}
  \\[1ex]
  \operatorname{detect}^{\text{ci+area}}(\bm{\tau})=&\;-0.029 + 3.011\times\bm{\tau}_1 + 0.839\times\bm{\tau}_2 - 0.478\times\bm{\tau}_3\;\text{.}
\end{aligned}
$$


```{r}
p3_avg_lm_scores <- stats::predict(p3_avg_lm, temp)
# Since we are attempting a regression to positive scores,
# we set any negative predictions to 0. Same goes for >1.
p3_avg_lm_scores[p3_avg_lm_scores < 0] <- 0
p3_avg_lm_scores[p3_avg_lm_scores > 1] <- 1

round(p3_avg_lm_scores * 10, 3)
stats::cor(p3_avg_lm_scores, ground_truth$consensus_score)
```

With this linear combination of only three scores (out of six), we were able to significantly boost the detection to $\approx0.96$, which implies that combining both methods is of worth for a detection using (inhomogeneous) confidence intervals only. If we only combine the scores of the variable `REQ` into a model, we still achieve a correlation of $\approx0.88$. This should probably be preferred to keep the degrees of freedom low, countering overfitting. Using four or more scores goes beyond $0.97$.


### Variable importance: most important scores\label{sssec:var-imp-it}

In the report for source code data (section \ref{sssec:var-imp}), we have previously determined the most important features. We'll do the same computation here. The results will then allow us to compare against the relative importances as determined by source code data.

```{r}
rfe_data_it <- cbind(
  p3_it_scores,
  data.frame(gt = ground_truth$consensus))
```

```{r warning=FALSE}
library(caret, quietly = TRUE)
library(pls)
library(randomForest)

set.seed(1337)

control <- caret::trainControl(method = "repeatedcv", number = 10, repeats = 3)
modelFit_it_all <- caret::train(gt ~., data = rfe_data_it, method = "pls", trControl = control)

imp_it_all <- caret::varImp(object = modelFit_it_all)
```

```{r var-imp-all-features, echo=FALSE, fig.height=6.5, fig.cap="Normalized variable-importance for all features (activities) for predicting the ground truth, relative to each other and expressed in percent.", fig.align="center", fig.pos="ht!"}

plot_var_imp(imp_it_all)
```

The `DESC`-activity with its three most important and dominating features Peak, RMS, and ImpulseFactor dominates the field, which could be due to, e.g., the `DESC`-activity being less characteristic than the others, ergo we get a more coherent predictor. Also, the first three features are quite similar to each other. The resulting model uses __`r modelFit_it_all$bestTune$ncomp`__ components.

For issue-tracking data we do have the scores for each variable (activity) separately, so we will attempt to also compute importances on a per-activity basis. This will also allow us to compare importances across activities. The expectation is that different activities will have a different order and magnitude of importances, since each activity has its own characteristics.

```{r}
temp <- loadResultsOrCompute(file = "../results/modelFit_it_3.rds", computeExpr = {
  set.seed(0xbeef)

  temp <- colnames(rfe_data_it)
  cols_req <- temp[grep(pattern = "^(req_)|(gt)", ignore.case = TRUE, x = temp)]
  cols_dev <- temp[grep(pattern = "^(dev_)|(gt)", ignore.case = TRUE, x = temp)]
  cols_desc <- temp[grep(pattern = "^(desc_)|(gt)", ignore.case = TRUE, x = temp)]
  
  list(
    "req" = caret::train(gt ~., data=rfe_data_it[cols_req], method="pls", trControl=control),
    "dev" = caret::train(gt ~., data=rfe_data_it[cols_dev], method="pls", trControl=control),
    "desc" = caret::train(gt ~., data=rfe_data_it[cols_desc], method="pls", trControl=control))
})

imp_it_req <- caret::varImp(object = temp$req)
imp_it_dev <- caret::varImp(object = temp$dev)
imp_it_desc <- caret::varImp(object = temp$desc)
```


When comparing figures \ref{fig:var-imp-all-features} and \ref{fig:var-imp-all-features-sep}, we observe that the importance of features varies if we were to predict a ground truth on a per-activity basis. We also observe some features being important in all cases, such as RMS, or the Jensen--Shannon divergence.


```{r var-imp-all-features-sep, echo=FALSE, fig.height=5, fig.cap="Variable-importance for all features (activities) separately for predicting the ground truth on a per-activity basis (normalized and relative).", fig.align="center", fig.pos="ht!"}

temp <- rbind(
  cbind(imp_it_req$importance, data.frame(Activity = rep("REQ", nrow(imp_it_req$importance)))),
  cbind(imp_it_dev$importance, data.frame(Activity = rep("DEV", nrow(imp_it_dev$importance)))),
  cbind(imp_it_desc$importance, data.frame(Activity = rep("DESC", nrow(imp_it_desc$importance)))))
temp$Activity <- factor(temp$Activity, levels = unique(temp$Activity), ordered = TRUE)

plot_var_imp(temp) + facet_wrap(Activity~., scales = "free_y", ncol = 2)
```



### Arbitrary-interval scores computing\label{ssec:arbint-scores}

Up to now, we presented two methods to compute losses/scores over the closed interval $[0,1]$. We then have fitted a linear model to weigh, scale and translate the scores. We could evaluate how well this linear model performs if we select arbitrary intervals and compute the scores, but the expected is that smaller and earlier intervals would perform poorly. We will compute some the scores for some arbitrary intervals and then evaluate this. Then, we will attempt to learn a non-linear mapping that should correct for these residuals.

```{r}
p3_avg_area_scores_arb_int <- loadResultsOrCompute(file = "../results/p3_avg_area_scores_arb_int.rds", computeExpr =  {
  doWithParallelCluster(numCores = min(123, parallel::detectCores()), expr = {
    library(foreach)
    
    tempm <- matrix(ncol = 3, byrow = TRUE, data = sapply(X = seq_len(400 * length(all_signals)), FUN = function(x) {
      set.seed(bitwXor(1337, x))
      
      t <- rep(NA_real_, 2)
      if (x <= 400) {
        t <- c(0, runif(n = 1, min = 0.01))
      } else if (x <= 3200) {
        while (TRUE) {
          t <- sort(runif(n = 2))
          if ((t[2] - t[1]) > 0.01) {
            break
          }
        }
      } else {
        t <- c(runif(n = 1, max = 0.99), 1)
      }
      
      pId <- x %% length(all_signals)
      if (pId == 0) {
        pId <- length(all_signals)
      }
      
      c(t, pId)
    }))
    tempm[, 3] <- sample(tempm[, 3]) # randomize projects
    
    temp <- foreach::foreach(
      permIdx = 1:nrow(tempm),
      .inorder = TRUE,
      .combine = rbind
    ) %dopar% {
      p <- tempm[permIdx, ]
      pId <- paste0("Project", p[3])
      
      temp_ci <- p3_avg_ci_scores_compute(pId = pId, x1 = p[1], x2 = p[2])
      colnames(temp_ci) <- paste0(colnames(temp_ci), "_ci")
      temp_ci$begin <- p[1]
      temp_ci$end <- p[2]
      
      temp_area <- p3_avg_area_scores_compute(pId = pId, x1 = p[1], x2 = p[2])
      colnames(temp_area) <- paste0(colnames(temp_area), "_area")
      `rownames<-`(cbind(temp_ci, temp_area), NULL)
    }
    
    temp$gt <- sapply(X = temp$Project_ci, FUN = function(p) {
      ground_truth[ground_truth$project == paste0("project_", substr(p, nchar(p), nchar(p))), ]$consensus
    })
    temp
  })
})
```


Now that we have produced some labeled training data, we can attempt to learn a mapping between the intervals and scores, and the ground truth. We use a Random forest, and as outer resampling method a ten times repeated ten-fold cross validation. We apply z-standardization to the data.


```{r}
p3_avg_arb_int_pred <- loadResultsOrCompute(file = "../results/p3_avg_arb_int_pred.rds", computeExpr =  {
  library(caret)
  set.seed(1337)
  
  temp <- p3_avg_area_scores_arb_int[complete.cases(p3_avg_area_scores_arb_int), ]
  temp <- temp[, c("gt", "begin", "end", "REQ_ci", "DEV_ci", "DESC_ci", "REQ_area", "DEV_area", "DESC_area")]
  
  inTraining <- caret::createDataPartition(temp$gt, p = 0.8, list = FALSE)
  training <- temp[ inTraining, ]
  testing  <- temp[-inTraining, ]
  
  fitControl <- caret::trainControl(
    method = "repeatedcv", number = 10, repeats = 10)
  
  doWithParallelCluster(expr = {
    list(
      fit = caret::train(gt ~ ., data = training, method = "ranger",
                         trControl = fitControl, preProcess = c("center", "scale")),
      train = training,
      test = testing)
  })
})
```

```{r echo=FALSE}
p3_avg_arb_int_pred$fit

c(
  corr = stats::cor(
    p3_avg_arb_int_pred$test$gt,
    stats::predict(object = p3_avg_arb_int_pred$fit, newdata = p3_avg_arb_int_pred$test)),
  MAE = Metrics::mae(
    p3_avg_arb_int_pred$test$gt,
    stats::predict(object = p3_avg_arb_int_pred$fit, newdata = p3_avg_arb_int_pred$test)),
  RMSE = Metrics::rmse(
    p3_avg_arb_int_pred$test$gt,
    stats::predict(object = p3_avg_arb_int_pred$fit, newdata = p3_avg_arb_int_pred$test))
)
```

With a correlation of $\approx0.99$, an MAE of $\approx0.12$ and RMSE of $\approx0.35$ I suppose we already have a predictor that is quite usable. Remember that we trained on the ground truth in the range $[0,10]$, so these mean deviations are probably already acceptable for some use cases. Also, I only produced 400 random ranges/scores per project, so in total we have 3600 records in the data, and the split was made at $0.8$ (during each fold of the  cross-validation, the default split of $0.75$ was used). With about one quarter of the amount of that data, we get similarly good results. Also, this model is likely overfitting, because when I leave out `begin` and `end` of each interval, the results are similarly good. However, the whole purpose of this model fitting here is to merely demonstrate that one could fit this kind of model and then make accurate predictions over arbitrary time intervals.

So, if we take the MAE, then these results mean that for any arbitrarily chosen interval, the deviation from the computed scores $\mathit{L}^{\text{avgconf}}$ and $\mathit{L}^{\text{areadist2}}$ is less than $\pm\approx0.29$, and that on a scale of $[0,10]$. Since this is $<0.5$, it means that we should get an even better result when rounding (at least for the MAE), and indeed:


```{r}
Metrics::mae(p3_avg_arb_int_pred$test$gt,
             round(stats::predict(object = p3_avg_arb_int_pred$fit, newdata = p3_avg_arb_int_pred$test)))
```

While it is not shown here, we can convert the rounded prediction to a factor and then show a confusion matrix. The diagonal in this case is well filled, i.e., only few mismatches exist. All of the mismatches are also in the next neighboring cell, which means that not a single prediction is more off than a single level.

If we run the same routine as a classification task, then we typically achieve $\approx99$% accuracy, and very high Kappa of $>0.98$ ($1$ is perfect). However, we cannot train for levels of ground truth currently not present in our data, which means that this kind of model will not generalize well to new data. However, all this was just a demonstration for the case of arbitrary-interval scores computing.


```{r}
p3_avg_arb_int_pred_cls <- loadResultsOrCompute(file = "../results/p3_avg_arb_int_pred_cls.rds", computeExpr =  {
  library(caret)
  set.seed(1337)
  
  temp <- p3_avg_area_scores_arb_int[complete.cases(p3_avg_area_scores_arb_int), ]
  temp <- temp[, c("gt", "begin", "end", "REQ_ci", "DEV_ci", "DESC_ci", "REQ_area", "DEV_area", "DESC_area")]
  temp$gt <- factor(ordered = TRUE, x = as.character(temp$gt), levels = sort(unique(as.character(temp$gt))))
  
  inTraining <- caret::createDataPartition(temp$gt, p = .8, list = FALSE)
  training <- temp[ inTraining, ]
  testing  <- temp[-inTraining, ]
  
  fitControl <- caret::trainControl(
    method = "repeatedcv", number = 10, repeats = 10)
  
  doWithParallelCluster(numCores = 10, expr = {
    list(
      fit = caret::train(gt ~ ., data = training, method = "ranger",
                         trControl = fitControl, preProcess = c("center", "scale")),
      train = training,
      test = testing)
  })
})
```

```{r echo=FALSE}
caret::confusionMatrix(
  factor(x = as.character(p3_avg_arb_int_pred_cls$test$gt), levels = levels(p3_avg_arb_int_pred_cls$train$gt)),
  stats::predict(object = p3_avg_arb_int_pred_cls$fit, newdata = p3_avg_arb_int_pred_cls$test))
```


### Process alignment: DTW, Optimization, srBTAW\label{ssec:proc-align-compute}

In the previous subsection we learned a predictive model that requires as input the start and end over which some scores were obtained (it is required since the available ground truth is constant). In a real-world scenario we might not know where exactly in the process we are. In such cases, it might be helpful to find the best alignment of some observed process with the process model. As an example, we will be using the processes from project three, and slice out the interval $[0.25,0.45]$.


```{r}
slice_supp <- c(0.25, 0.45)
use_p <- all_signals$Project3

get_slice_func <- function(varname) {
  use_func <- use_p[[varname]]$get0Function()
  function(x) sapply(X = x, FUN = function(x_) {
    use_func((slice_supp[2] - slice_supp[1]) * x_ + slice_supp[1])
  })
}

# Re-define the slices to support [0,1], to make the problem harder and more realistic.
slice_req <- Signal$new(name = "REQ_WP", func = get_slice_func("REQ"), support = c(0, 1), isWp = TRUE)
slice_dev <- Signal$new(name = "DEV_WP", func = get_slice_func("DEV"), support = c(0, 1), isWp = TRUE)
slice_desc <- Signal$new(name = "DESC_WP", func = get_slice_func("DESC"), support = c(0, 1), isWp = TRUE)
```


#### Optimization-based approach

Similar to how we found the optimum in section \ref{ssec:optim-t1t2}, we can pose an optimization problem:

$$
\begin{aligned}
  {[}a,b{]}\dots&\;\text{support of the observed process (here:}\,[0.25,0.45]\text{),}
  \\[1ex]
  \mathcal{L}_{\text{begin}}(t_{\text{begin}})=&\;\norm{\operatorname{\overline{req}}(t_{\text{begin}})-\operatorname{req}(a)}+\norm{\operatorname{\overline{dev}}(t_{\text{begin}})-\operatorname{dev}(a)}+\norm{\operatorname{\overline{desc}}(t_{\text{begin}})-\operatorname{desc}(a)}\,\text{,}
  \\[1ex]
  \mathcal{L}_{\text{end}}(t_{\text{end}})=&\;\norm{\operatorname{\overline{req}}(t_{\text{end}})-\operatorname{req}(b)}+\norm{\operatorname{\overline{dev}}(t_{\text{end}})-\operatorname{dev}(b)}+\norm{\operatorname{\overline{desc}}(t_{\text{end}})-\operatorname{desc}(b)}\,\text{,}
  \\[1ex]
  \mathcal{L}(t_{\text{begin}},t_{\text{end}})=&\;\mathcal{L}_{\text{begin}}(t_{\text{begin}})+\mathcal{L}_{\text{end}}(t_{\text{end}})\,\text{,}
  \\[1ex]
  \min_{t_{\text{begin}},t_{\text{end}}\in R}&\;\mathcal{L}(t_{\text{begin}},t_{\text{end}})\,\text{,}
  \\[1ex]
  \text{subject to}&\;0\leq t_{\text{begin}}<t_{\text{end}}\leq1\;\text{.}
\end{aligned}
$$

```{r}
req_ab <- slice_req$get0Function()(c(0,1))
dev_ab <- slice_dev$get0Function()(c(0,1))
desc_ab <- slice_desc$get0Function()(c(0,1))

set.seed(1)

res <- nloptr(x0 = c(.5, .5), eval_f = function(x) {
  a <- x[1]
  b <- x[2]
  abs(req_p3(a) - req_ab[1]) + abs(dev_p3(a) - dev_ab[1]) + abs(desc_p3(a) - desc_ab[1]) +
    abs(req_p3(b) - req_ab[2]) + abs(dev_p3(b) - dev_ab[2]) + abs(desc_p3(b) - desc_ab[2])
}, lb = c(0, 0), ub = c(1, 1), opts = list(maxeval = 1e3, algorithm = "NLOPT_LN_BOBYQA"))

c(res$solution, res$objective)
```

We found the parameters `r res$solution[1]` and `r res$solution[2]` for the overall begin an end, which is quite close.


#### DTW-based approach

Here, we use open-end/-begin DTW to find the offset per variable, then for all variables combined.

```{r}
dtwRes <- loadResultsOrCompute(file = "../results/proc_align_dtw.rds", computeExpr = {
  library(dtw)
  X <- seq(0, 1, length.out=1e3)
  
  dtwRes_req <- dtw::dtw(x = req_p3(X), y = slice_req$get0Function()(X), keep.internals = TRUE)
  dtwEx_req <- extract_signal_from_window(dtwRes_req, window = X)
  
  dtwRes_dev <- dtw::dtw(x = dev_p3(X), y = slice_dev$get0Function()(X), keep.internals = TRUE)
  dtwEx_dev <- extract_signal_from_window(dtwRes_dev, window = X)
  
  dtwRes_desc <- dtw::dtw(x = desc_p3(X), y = slice_desc$get0Function()(X), keep.internals = TRUE)
  dtwEx_desc <- extract_signal_from_window(dtwRes_desc, window = X)
  
  dtwRes_ALL <- dtw::dtw(
    x = matrix(ncol = 3, data = c(req_p3(X), dev_p3(X), desc_p3(X))),
    y = matrix(ncol = 3, data = c(
      slice_req$get0Function()(X), slice_dev$get0Function()(X),
      slice_desc$get0Function()(X))),
    keep.internals = TRUE)
  dtwEx_ALL <- extract_signal_from_window(dtwRes_ALL, window = X)
  
  c("begin_req" = dtwEx_req$start_rel, "end_req" = dtwEx_req$end_rel,
    "begin_dev" = dtwEx_dev$start_rel, "end_dev" = dtwEx_dev$end_rel,
    "begin_desc" = dtwEx_desc$start_rel, "end_desc" = dtwEx_desc$end_rel,
    "begin_ALL" = dtwEx_ALL$start_rel, "end_ALL" = dtwEx_ALL$end_rel)
})

dtwRes
```

Here we get a couple results, depending on the variable. The overall result is marginally worse compared to the one we got from the optimization. Here it becomes apparent that a custom objective would be needed to detect the true begin and end, which is not possible with DTW.


#### srBTAW-based approach

Here we attempt to align the partially observed process with the process model using two different approaches. First, we attempt to use the RSS loss, which should give us similar results to what DTW achieved for the overall begin and end. Then, we use an objective to compute the correlation between processes.


```{r}
proc_align_srbtaw <- function(use = c("rss", "logratio")) {
  use <- match.arg(use)
  
  srbtaw <- srBTAW$new(
    theta_b = seq(0, 1, by = 0.1), gamma_bed = c(0, 1, .Machine$double.eps),
    lambda = rep(.Machine$double.eps, 10),
    begin = 0, end = 1, openBegin = TRUE, openEnd = TRUE)
  srbtaw$setParams(`names<-`(c(0, 1, rep(1/10, 10)), srbtaw$getParamNames()))
  
  srbtaw$setSignal(signal = Signal$new(name = "REQ", func = req_p3, support = c(0, 1), isWp = FALSE))
  srbtaw$setSignal(slice_req)
  srbtaw$setSignal(signal = Signal$new(name = "DEV", func = dev_p3, support = c(0, 1), isWp = FALSE))
  srbtaw$setSignal(slice_dev)
  srbtaw$setSignal(signal = Signal$new(name = "DESC", func = desc_p3, support = c(0, 1), isWp = FALSE))
  srbtaw$setSignal(slice_desc)
  
  obj <- srBTAW_LossLinearScalarizer$new(
    computeParallel = TRUE, gradientParallel = TRUE, returnRaw = TRUE)
  
  if (use == "rss") {
    loss_req <- srBTAW_Loss_Rss$new(
      wpName = "REQ_WP", wcName = "REQ", intervals = 1:10, returnRaw = FALSE)
    srbtaw$addLoss(loss = loss_req)
    loss_dev <- srBTAW_Loss_Rss$new(
      wpName = "DEV_WP", wcName = "DEV", intervals = 1:10, returnRaw = FALSE)
    srbtaw$addLoss(loss = loss_dev)
    loss_desc <- srBTAW_Loss_Rss$new(
      wpName = "DESC_WP", wcName = "DESC", intervals = 1:10, returnRaw = FALSE)
    srbtaw$addLoss(loss = loss_desc)
    
    obj$setObjective(name = "Loss_REQ", obj = loss_req)
    obj$setObjective(name = "Loss_DEV", obj = loss_dev)
    obj$setObjective(name = "Loss_DESC", obj = loss_desc)
  } else {
    loss_req2 <- srBTAW_Loss2Curves$new(use = "logratio",
      srbtaw = srbtaw, wpName = "REQ_WP", wcName = "REQ", intervals = 1:10)
    srbtaw$addLoss(loss = loss_req2)
    
    loss_dev2 <- srBTAW_Loss2Curves$new(use = "logratio",
      srbtaw = srbtaw, wpName = "DEV_WP", wcName = "DEV", intervals = 1:10)
    srbtaw$addLoss(loss = loss_dev2)
    
    loss_desc2 <- srBTAW_Loss2Curves$new(use = "logratio",
      srbtaw = srbtaw, wpName = "DESC_WP", wcName = "DESC", intervals = 1:10)
    srbtaw$addLoss(loss = loss_desc2)
    
    obj$setObjective(name = "Loss_REQ2", obj = loss_req2)
    obj$setObjective(name = "Loss_DEV2", obj = loss_dev2)
    obj$setObjective(name = "Loss_DESC2", obj = loss_desc2)
  }
  
  reg <- TimeWarpRegularization$new(
    weight = 1/2, use = "exint2", wpName = "REQ_WP", wcName = "REQ", returnRaw = use == "logratio", intervals = 1:10)

  srbtaw$addLoss(loss = reg)
  obj$setObjective(name = "reg_exint2", obj = reg)
  srbtaw$setObjective(obj = obj)
  srbtaw$setIsObjectiveLogarithmic(val = TRUE)
}
```


```{r echo=FALSE}
compute_proc_align <- function(srbtaw) {
  cl <- parallel::makePSOCKcluster(min(123, parallel::detectCores()))
  tempf <- tempfile()
  saveRDS(object = list(a = srbtaw, b = srbtaw$getObjective()), file = tempf)
  parallel::clusterExport(cl, varlist = list("tempf", "slice_supp"))
  
  res <- optimParallel::optimParallel(
    par = srbtaw$getParams(),
    method = "L-BFGS-B",
    lower = rep(0, 12),
    upper = rep(1, 12),
    fn = function(x) {
      temp <- readRDS(file = tempf)
      temp$a$setParams(params = x)
      temp$b$compute0()
    },
    parallel = list(cl = cl, forward = FALSE, loginfo = TRUE)
  )
  parallel::stopCluster(cl)
  
  tempFr <- FitResult$new(paramNames = c("dummy"))
  list(
    srbtaw = srbtaw,
    obj = srbtaw$getObjective(),
    res = res,
    fr = tempFr$fromOptimParallel(optR = res))
}
```


```{r align-srbtaw-rss}
align_srbtaw_rss <- loadResultsOrCompute(file = "../results/proc_align_srbtaw_rss.rds", computeExpr = {
  srbtaw <- proc_align_srbtaw(use = "rss")
  compute_proc_align(srbtaw = srbtaw)
})
align_srbtaw_rss$res$par[c("b", "e")]
```


```{r align-srbtaw-logratio}
align_srbtaw_logratio <- loadResultsOrCompute(file = "../results/proc_align_srbtaw_logratio.rds", computeExpr = {
  srbtaw <- proc_align_srbtaw(use = "logratio")
  compute_proc_align(srbtaw = srbtaw)
})
align_srbtaw_logratio$res$par[c("b", "e")]
```

As expected, the RSS-based result is very similar to the overall result as obtained by DTW (note that the mix-up of begin and end does not matter to srBTAW, is it regularizes this using its internal representation $\beta_l,\beta_u$). The correlation-based loss, surprisingly, manages to discover the true begin of $\approx0.25$. It is likely possible to find a combination of losses, weights, etc., that can find the offsets of some process better, but that is a bit out of scope here. In fig. \ref{fig:proc-align-srbtaw} we show the time warping as computed by the two objectives (RSS and correlation).

```{r proc-align-srbtaw, echo=FALSE, fig.height=3.5, fig.cap="Open alignment of the observed process using srBTAW and 10 intervals. On the left using RSS as loss, and on the right using correlation.", fig.align="center", fig.pos="ht!"}
align_srbtaw_rss$srbtaw$setParams(align_srbtaw_rss$res$par)
align_srbtaw_logratio$srbtaw$setParams(align_srbtaw_logratio$res$par)

ggpubr::ggarrange(
  plotlist = list(
    align_srbtaw_rss$srbtaw$.__enclos_env__$private$instances$`REQ_WP|REQ`$plot_dual_original(),
    align_srbtaw_logratio$srbtaw$.__enclos_env__$private$instances$`REQ_WP|REQ`$plot_dual_original()
  ),
  common.legend = TRUE)
```

For demonstration purposes, we can compute the loss surface of a the model that was previously fit using RSS. Here, we keep the vector $\bm{\vartheta}^{(l)}$ constant, and only update $b,e$.

```{r echo=FALSE}
temp_get_srbtaw <- function() {
  srbtaw <- srBTAW$new(
    theta_b = seq(0, 1, by = 0.1), gamma_bed = c(0, 1, .Machine$double.eps),
    lambda = rep(.Machine$double.eps, 10),
    begin = 0, end = 1, openBegin = TRUE, openEnd = TRUE)
  srbtaw$setParams(`names<-`(c(0, 1, rep(1/10, 10)), srbtaw$getParamNames()))
  
  srbtaw$setSignal(signal = Signal$new(name = "REQ", func = req_p3, support = c(0, 1), isWp = FALSE))
  srbtaw$setSignal(slice_req)
  srbtaw$setSignal(signal = Signal$new(name = "DEV", func = dev_p3, support = c(0, 1), isWp = FALSE))
  srbtaw$setSignal(slice_dev)
  srbtaw$setSignal(signal = Signal$new(name = "DESC", func = desc_p3, support = c(0, 1), isWp = FALSE))
  srbtaw$setSignal(slice_desc)
  
  obj <- srBTAW_LossLinearScalarizer$new(
    computeParallel = TRUE, gradientParallel = TRUE, returnRaw = TRUE)
  
  loss_req <- srBTAW_Loss_Rss$new(
    wpName = "REQ_WP", wcName = "REQ", intervals = 1:10, returnRaw = FALSE)
  srbtaw$addLoss(loss = loss_req)
  loss_dev <- srBTAW_Loss_Rss$new(
    wpName = "DEV_WP", wcName = "DEV", intervals = 1:10, returnRaw = FALSE)
  srbtaw$addLoss(loss = loss_dev)
  loss_desc <- srBTAW_Loss_Rss$new(
    wpName = "DESC_WP", wcName = "DESC", intervals = 1:10, returnRaw = FALSE)
  srbtaw$addLoss(loss = loss_desc)
  
  obj$setObjective(name = "Loss_REQ", obj = loss_req)
  obj$setObjective(name = "Loss_DEV", obj = loss_dev)
  obj$setObjective(name = "Loss_DESC", obj = loss_desc)
  
  reg <- TimeWarpRegularization$new(
    weight = 1/2, use = "exint2", wpName = "REQ_WP", wcName = "REQ", returnRaw = FALSE, intervals = 1:10)

  srbtaw$addLoss(loss = reg)
  obj$setObjective(name = "reg_exint2", obj = reg)
  srbtaw$setObjective(obj = obj)
  srbtaw$setIsObjectiveLogarithmic(val = TRUE)
}
```

In figure \ref{fig:proc-align-srbtaw-loss-surface} we show the loss surface of the parameters begin and end of a one-interval alignment using srBTAW and RSS-losses.

```{r}
use_resol <- 50
minStep <- 1 / use_resol
use_begin <- seq(0, 1 - minStep, by = minStep)
use_par <- align_srbtaw_rss$res$par
  
align_srbtaw_rss_surface <- loadResultsOrCompute(file = "../results/align_srbtaw_rss_surface.rds", computeExpr = {
  library(foreach)
  
  cl <- parallel::makePSOCKcluster(min(123, parallel::detectCores()))
  parallel::clusterExport(cl, varlist = list("slice_supp"))
  
  doWithParallelClusterExplicit(cl = cl, expr = {
    foreach::foreach(
      begin = use_begin,
      .inorder = TRUE,
      .combine = cbind
    ) %dopar% {
      source(file = "../helpers.R")
      source(file = "./common-funcs.R")
      source(file = "../models/modelsR6.R")
      source(file = "../models/SRBTW-R6.R")
      
      end <- begin + minStep
      res <- c()
      while (TRUE) {
        if (end > 1) break
        
        srbtaw <- temp_get_srbtaw()
        params <- c(use_par)
        params["b"] <- begin
        params["e"] <- end
        #srbtaw$setParams(`names<-`(c(begin, end, 1), srbtaw$getParamNames()))
        srbtaw$setParams(params = params)
        res <- c(res, srbtaw$getObjective()$compute0())
        
        end <- end + minStep
      }
      
      # pad left:
      c(rep(NA_real_, use_resol - length(res)), res)
    }
  })
})
```

```{r proc-align-srbtaw-loss-surface, echo=FALSE, fig.cap="The loss surface for ten-interval time warping using srBTAW of the parameters begin and end. The lowest loss is marked with a triangle", fig.align="center", fig.pos="ht!"}
z_temp <- list(
  x = seq(0, 1, by = minStep),
  y = seq(0, 1, by = minStep),
  z = align_srbtaw_rss_surface)

fields::image.plot(z_temp, col = hcl.colors(101, "YlOrRd", rev = TRUE),
                   legend.shrink = .5, legend.lab = "Loss (log(RSS))", legend.line = 2.5,
                   xlab = "", ylab = "")
title(ylab = "begin", xlab = "end", line = 2.5)
grid()

z_temp$x <- z_temp$x[2:51]
z_temp$y <- z_temp$y[2:51]
contour(z_temp, col = "#222222", add = TRUE)

temp <- align_srbtaw_rss_surface
temp[is.na(temp)] <- Inf
temp <- which(temp == min(temp), arr.ind = TRUE)
points(x = temp[, 1] / use_resol, y = temp[, 2] / use_resol, pch = 2, col = "red", cex = 2)

legend(.01, .98, legend = c("Lowest loss"),
       pch = 2, col = "red", bg = "white")
```



## Pattern IV

This pattern emerged only recently, but we have some options of scoring the projects against it. Let me start by emphasizing again how the concrete tests here are performed using an instantiation of type IV using pattern I, and how improper type I actually fits the data. The tests here, in the best case, should be run for all of the previously introduced patterns, and our expectation is that the data-only pattern would perform best, with the data-enhanced one coming in at second place. We mostly chose type I as we have analytical expressions for some of the variables.

Again, we have (derived) confidence intervals for the variables `REQ` and `DEV`. Then we have derivatives for all variables. Scoring based on CIs are hence applicable, but only for the first two variables. Other than that, any method based on comparing two variables is applicable. For our evaluations, we will use LOESS-smoothing with different smoothing-spans for either score, that are applicable.


In figure \ref{fig:p4-deriv-signals} we show all projects against the fourth pattern, as smoothed using LOESS and derived. It is interesting to see that at $\approx0.37$, where the confidence surface is the smallest for `REQ` and the upper confidence interval's rate of change becomes $0$, most projects also have a turning point.


```{r p4-deriv-signals, echo=FALSE, fig.height=8, fig.cap="All projects' derivative variables plotted against the derivatives of the two variables req\\% and dev\\% of pattern type I. The selected smoothing-span was 0.3.", fig.align="center", fig.pos="ht!"}
par(mfrow=c(2,1))
use_span <- 0.3


plot(x=0, y=0, xlim = c(0,1), ylim = c(0, max(d1_vals_req(0))), col = "#00000000",
     xlab = "Relative time", ylab = "Rate of change")
grid()
curve(req_d1_p4, 0, 1, col="red", lwd=2, add = TRUE)
curve(req_ci_lower_d1_p4, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)
curve(req_ci_upper_d1_p4, 0, 1, col="red", lty=2, lwd=1.5, add = TRUE)

polygon(
  x = polygon_x,
  y = c(sapply(X = polygon_x[1:500], FUN = function(x) max(d1_vals_req(x=x))),
        sapply(X = polygon_x[501:1000], FUN = function(x) min(d1_vals_req(x=x)))),
  col = "#ff000033",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  
  X <- 1:nrow(all_signals[[pId]]$data)
  Y_req <- cumsum(all_signals[[pId]]$data$req)
  
  loess_req <- smooth_signal_loess(x = X, y = Y_req, span = use_span)
  req_tempf_d1 <- function(x) func_d1(f = loess_req, x = x)
  
  curve(req_tempf_d1, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.75, max(d1_vals_req(0)) - 0.02, legend = c("req%", "req% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))



plot(x=0, y=0, xlim = c(0,1), ylim = c(0, max(d1_vals_dev(1))), col = "#00000000",
     xlab = "Relative time", ylab = "Rate of change")
grid()
curve(dev_d1_p4, 0, 1, col="blue", lwd=2, add = TRUE)
curve(dev_ci_lower_d1_p4, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)
curve(dev_ci_upper_d1_p4, 0, 1, col="blue", lty=2, lwd=1.5, add = TRUE)

polygon(
  x = polygon_x,
  y = c(sapply(X = polygon_x[1:500], FUN = function(x) max(d1_vals_dev(x=x))),
        sapply(X = polygon_x[501:1000], FUN = function(x) min(d1_vals_dev(x=x)))),
  col = "#0000FF33",
  border = NA)

for (i in 1:length(all_signals)) {
  pId <- names(all_signals)[i]
  
  X <- 1:nrow(all_signals[[pId]]$data)
  Y_dev <- cumsum(all_signals[[pId]]$data$dev)
  
  loess_dev <- smooth_signal_loess(x = X, y = Y_dev, span = use_span)
  dev_tempf_d1 <- function(x) func_d1(f = loess_dev, x = x)
  
  curve(dev_tempf_d1, 0, 1, col=myCol[i], lty=2, add=TRUE)
}

legend(0.01, max(d1_vals_dev(1)) - 0.02, legend = c("dev%", "dev% CI", paste0("Pr. ", 1:9)),
       bg="transparent", cex = 3/4,
       col = c("blue", "blue", myCol[1:9]), lty = c(1, 2, rep(2, 9)), lwd = c(3, 2, rep(1.5, 9)))
```


### Scoring based on the distance to reference

This method is applicable because we only do have the confidence intervals' boundaries, and a homogeneous surface. The derivative of the respective variable is the expectation for the rate of change, and the confidence intervals demarcate a minimum and maximum expectation. This applies only to the variables `REQ` and `DEV`. Since we compute the distance in terms of the area between curves, the gradients should use a lower smoothing-span to be detail-preserving. However, we will try a few to see what works best.


```{r}
p4_compute_areadist <- function(span, intFrom = 0, intTo = 1) {
  d1_vals <- function(f, f_low, f_upp, x, useMax = TRUE) {
    sapply(X = x, FUN = function(x_) {
      temp <- c(f(x_), f_low(x_), f_upp(x_))
      if (useMax) max(temp) else min(temp)
    })
  }
  
  doWithParallelCluster(numCores = length(all_signals), expr = {
    library(foreach)
    
    foreach::foreach(
      pId = names(all_signals),
      .inorder = TRUE,
      .combine = rbind,
      .packages = c("cobs"),
      .export = c("all_signals", "L_areadist_p3_avg",
                  "func_d1", "req", "dev", "req_d1_p4", "dev_d1_p4",
                  "req_ci_upper_d1_p4", "dev_ci_upper_d1_p4",
                  "req_ci_lower_d1_p4", "dev_ci_lower_d1_p4",
                  "smooth_signal_loess", "req_poly", "dev_poly")
    ) %dopar% {
      X <- 1:nrow(all_signals[[pId]]$data)
      Y_req <- cumsum(all_signals[[pId]]$data$req)
      Y_dev <- cumsum(all_signals[[pId]]$data$dev)
      
      req_tempf <- smooth_signal_loess(x = X, y = Y_req, span = span, neval = 1e3)
      dev_tempf <- smooth_signal_loess(x = X, y = Y_dev, span = span, neval = 1e3)
      
      req_d1 <- function(x) func_d1(f = req_tempf, x = x)
      dev_d1 <- function(x) func_d1(f = dev_tempf, x = x)
      
      req_upper_tempf <- function(x) d1_vals(
        f = req_d1_p4, f_low = req_ci_lower_d1_p4, f_upp = req_ci_upper_d1_p4, x = x, useMax = TRUE)
      req_lower_tempf <- function(x) d1_vals(
        f = req_d1_p4, f_low = req_ci_lower_d1_p4, f_upp = req_ci_upper_d1_p4, x = x, useMax = FALSE)
      
      dev_upper_tempf <- function(x) d1_vals(
        f = dev_d1_p4, f_low = dev_ci_lower_d1_p4, f_upp = dev_ci_upper_d1_p4, x = x, useMax = TRUE)
      dev_lower_tempf <- function(x) d1_vals(
        f = dev_d1_p4, f_low = dev_ci_lower_d1_p4, f_upp = dev_ci_upper_d1_p4, x = x, useMax = FALSE)
      
      `rownames<-`(data.frame(
        REQ = L_areadist_p3_avg(
          x1 = intFrom, x2 = intTo, f = req_d1, gbar = req_d1_p4, use2ndVariant = TRUE,
          CI_upper = req_upper_tempf, CI_lower = req_lower_tempf)["dist"],
        
        DEV = L_areadist_p3_avg(
          x1 = intFrom, x2 = intTo, f = dev_d1, gbar = dev_d1_p4, use2ndVariant = TRUE,
          CI_upper = dev_upper_tempf, CI_lower = dev_lower_tempf)["dist"],
        
        span = span,
        begin = intFrom,
        end = intTo
      ), pId)
    }
  })
}
```

```{r echo=FALSE}
p4_area_scores <- loadResultsOrCompute(file = "../results/p4_area_scores.rds", computeExpr =  {
  temp <- NULL
  for (span in seq(from = 0.2, to = 1, by = 0.1)) {
    begin <- as.numeric(Sys.time())
    temp <- rbind(temp, p4_compute_areadist(span = span))
    print(paste0("Finished computing for span=", span, " in ", round(as.numeric(Sys.time()) - begin), "s"))
  }
  temp
})
```

```{r echo=FALSE}
p4_area_scores_corr <- NULL

for (span in sort(unique(p4_area_scores$span))) {
  temp <- p4_area_scores[p4_area_scores$span == span, ]
  temp <- temp[order(rownames(temp)), ]
  
  p4_area_scores_corr <- rbind(p4_area_scores_corr, data.frame(
    span = min(temp$span),
    corr_REQ = cor(x = ground_truth$consensus_score, y = temp[, "REQ"]),
    corr_DEV = cor(x = ground_truth$consensus_score, y = temp[, "DEV"]),
    corr_REQ_pval = cor.test(x = ground_truth$consensus_score, y = temp[, "REQ"])$p.value,
    corr_DEV_pval = cor.test(x = ground_truth$consensus_score, y = temp[, "DEV"])$p.value
  ))
}
```

```{r echo=FALSE}
if (interactive()) {
  p4_area_scores_corr
} else {
  knitr::kable(
    x = p4_area_scores_corr,
    booktabs = TRUE,
    caption = "",
    label = "p4-area-scores-corr"
  )
}
```

In table \ref{tab:p4-area-scores-corr} we can observe a clear impact of the smoothing-span on the correlation of the computed distance vs. the ground truth. Note that ideally, the correlation is negative, as a higher distance corresponds to a lower score.

The correlations for the `REQ` variable get slightly stronger with increasing smoothing-spans. However, the correlations are positive when they should not be. This increase is perhaps explained by the gradually increasing area overlaps. However, since it affects all projects and all of them have a similar overlap with the confidence interval, the distance gets lower, hence resulting in higher correlations.

As for the `DEV` variable, with increasing span the correlations get lower. That is because most of the projects run outside the confidence intervals of the pattern, and with increasing span, those parts that were inside, are getting more and more outside, as the peaks are smoothed out, hence resulting in a lower correlation. The correlation for the smallest span is close to being acceptable, if we consider the p-value. Also, all the correlations are negative, like they should be.


```{r echo=FALSE}
p4_area_scores_partial <- loadResultsOrCompute(file = "../results/p4_area_scores_partial.rds", computeExpr =  {
  temp <- NULL
  for (span in c(.2, .3, .4, .6, .8, 1)) {
    for (end in c(1/3, 1/2, 2/3)) {
      temp <- rbind(temp, p4_compute_areadist(span = span, intTo = end))
    }
    print(paste0("Finished computing for span=", span))
  }
  temp
})
```

```{r echo=FALSE}
p4_area_scores_partial_corr <- NULL

for (span in sort(unique(p4_area_scores_partial$span))) {
  for (end in c(1/3, 1/2, 2/3)) {
    temp <- p4_area_scores_partial[p4_area_scores_partial$span == span & p4_area_scores_partial$end == end, ]
    temp <- temp[order(rownames(temp)), ]
    
    p4_area_scores_partial_corr <- suppressWarnings(expr = {
      rbind(p4_area_scores_partial_corr, data.frame(
        span = min(temp$span),
        end = round(min(temp$end), 2),
        corr_REQ = cor(x = ground_truth$consensus_score, y = temp[, "REQ"]),
        corr_DEV = cor(x = ground_truth$consensus_score, y = temp[, "DEV"]),
        corr_REQ_pval = cor.test(x = ground_truth$consensus_score, y = temp[, "REQ"])$p.value,
        corr_DEV_pval = cor.test(x = ground_truth$consensus_score, y = temp[, "DEV"])$p.value
      ))
    })
  }
}
```

```{r echo=FALSE}
p4_area_scores_partial_corr[is.na(p4_area_scores_partial_corr)] <- 0

if (interactive()) {
  p4_area_scores_partial_corr
} else {
  knitr::kable(
    x = p4_area_scores_partial_corr,
    booktabs = TRUE,
    caption = "",
    label = "p4-area-scores-partial-corr"
  )
}
```

In table \ref{tab:p4-area-scores-partial-corr} we can clearly observe how the correlation of the computed score declines in almost every group of spans (esp. for the `REQ` variable), the more time we consider, i.e., in a consistent manner the scores decline, and it is the same phenomenon for every smoothing-span. This is expected since pattern IV is the derivative of pattern I, which itself is only a poor reconciliation of how the Fire Drill apparently manifests in the data, which means that the more discrepancy we consider, the less good the results get. Again, we should consider computing this correlation-based score when using a data-enhanced or data-only pattern as actually, a typical moderate correlation with the scores of $\approx-0.4$ (or better) as in table \ref{tab:p4-area-scores-partial-corr} for the `DEV`-variable can be expected to substantially increase with a more suitable pattern.


### Correlation between curves

Here we will compare each project's variable with the corresponding variable from the fourth pattern. We will take samples from either, pattern and variable, at the same $x$, and then compute the sample correlation. This is a very simple but efficient test. Also, it is subject to user-defined intervals, i.e., we do not have to sample from project to project end. Note that this would probably not work for the first pattern, as it normalizes the variables at project end. Pattern IV however uses the rate of change, which is not affected by that.

We will be using LOESS-smoothed curved with a somewhat higher smoothing-span of $0.6$. Also, since this test does not consider confidence intervals, we can also compare the `DESC` variable.

```{r}
N <- 200
use_span <- 0.6
X_samp <- seq(from = 0, to = 1, length.out = N)

p4_samples <- list(
  REQ = req_d1_p4(X_samp),
  DEV = dev_d1_p4(X_samp),
  DESC = desc_d1_p4(X_samp))

p4_corr <- NULL
for (pId in names(all_signals)) {
  
  X <- 1:nrow(all_signals[[pId]]$data)
  Y_req <- cumsum(all_signals[[pId]]$data$req)
  Y_dev <- cumsum(all_signals[[pId]]$data$dev)
  Y_desc <- cumsum(all_signals[[pId]]$data$desc)
  
  loess_req <- smooth_signal_loess(x = X, y = Y_req, span = use_span)
  req_tempf_d1 <- function(x) func_d1(f = loess_req, x = x)
  loess_dev <- smooth_signal_loess(x = X, y = Y_dev, span = use_span)
  dev_tempf_d1 <- function(x) func_d1(f = loess_dev, x = x)
  loess_desc <- tryCatch({
    smooth_signal_loess(x = X, y = Y_desc, span = use_span)
  }, error = function(cond) function(x) rep(0, length(x)))
  desc_tempf_d1 <- function(x) func_d1(f = loess_desc, x = x)
        
  p4_corr <- suppressWarnings(expr = {
    rbind(p4_corr, `rownames<-`(data.frame(
      REQ = stats::cor(p4_samples$REQ, req_tempf_d1(X_samp)),
      DEV = stats::cor(p4_samples$DEV, dev_tempf_d1(X_samp)),
      DESC = stats::cor(p4_samples$DESC, desc_tempf_d1(X_samp))
    ), pId))
  })
}
```


```{r echo=FALSE}
p4_corr[is.na(p4_corr)] <- 0
if (interactive()) {
  p4_corr
} else {
  knitr::kable(
    x = p4_corr,
    booktabs = TRUE,
    caption = "Correlation scores of the derivatives with the derived project signals.",
    label = "p4-corr"
  )
}
```

```{r}
c(
  REQ = cor(ground_truth$consensus_score, p4_corr$REQ),
  DEV = cor(ground_truth$consensus_score, p4_corr$DEV),
  DESC = cor(ground_truth$consensus_score, p4_corr$DESC, use = "pa"),
  
  REQ_pval = cor.test(ground_truth$consensus_score, p4_corr$REQ)$p.value,
  DEV_pval = cor.test(ground_truth$consensus_score, p4_corr$DEV)$p.value,
  DESC_pval = cor.test(ground_truth$consensus_score, p4_corr$DESC, use = "pa")$p.value
)
```

The correlations in table \ref{tab:p4-corr} are varying, but their correlation with the ground truth is poor, and in the case of `REQ` and `DESC` even negative. That means that computing the correlation of the derived data with the derived project signals is not a suitable detector using pattern IV, which is the derivative of the expert-designed pattern I. However, this might work with a data-enhanced or data-only pattern. Our previous attempt using the distance in areas was much better.




# Scoring of projects (2nd batch)

In the meantime, we got a new batch of projects (IDs `[10,15]`). We want to use the regression model based on the previous batch of projects to calculate a degree to which the Fire Drill is present. To goal is to find out whether the model built on previous observations can generalize to new, previously unseen data. The model was fit on scores as computed against pattern III (average), and it used two kinds of scores: a) the average confidence as computed by the path of each variable it takes through the confidence surface, and b) the average distance to the reference variable (the weighted average of previous projects).

So, in order to obtain predictions, we need to do n things:

1. Load the new projects -- the file `FD_issue-based_detection.xlsx` was updated to include the new projects. We'll use the function `load_project_issue_data()` to instantiate the signals.
2. Compute the two kinds of scores; we'll run the __new__ projects against the pattern that is based on the __old__ projects.
3. Use the scores in the previously fit regression model and predict a ground truth.


## Ground truth

```{r}
ground_truth_2nd_batch <- read.csv(file = "../data/ground-truth_2nd-batch.csv", sep = ";")
ground_truth_2nd_batch$consensus_score <- ground_truth_2nd_batch$consensus / 10
rownames(ground_truth_2nd_batch) <- paste0((1+nrow(ground_truth)):(nrow(ground_truth)+nrow(ground_truth_2nd_batch)))
```

We have ground truth available for the second batch. Again, the two raters assessed it independently first, and only later found a consensus. The a priori agreement between both raters was better this time, as the quadratic weighted Kappa increased substantially to a value of __`r round(Metrics::ScoreQuadraticWeightedKappa(rater.a = ground_truth_2nd_batch$rater.a, rater.b = ground_truth_2nd_batch$rater.b, min.rating = 0, max.rating = 10), 3)`__ (correlation is __`r round(stats::cor(x = ground_truth_2nd_batch$rater.a, y = ground_truth_2nd_batch$rater.b), 3)`__). Table \ref{tab:groundtruth-2nd-batch} shows the ground truth for the new projects `[10-15]`.

```{r echo=FALSE}
temp <- rbind(ground_truth, ground_truth_2nd_batch)
```

For the entirety of __both batches__, the quadratic weighted Kappa is __`r round(Metrics::ScoreQuadraticWeightedKappa(rater.a = temp$rater.a, rater.b = temp$rater.b, min.rating = 0, max.rating = 10), 3)`__, and the Pearson correlation of both raters' assessments is __`r round(stats::cor(x = temp$rater.a, y = temp$rater.b), 3)`__.


```{r echo=FALSE}
if (interactive()) {
  ground_truth_2nd_batch
} else {
  knitr::kable(
    x = ground_truth_2nd_batch,
    booktabs = TRUE,
    caption = "Entire ground truth as of both raters",
    label = "groundtruth-2nd-batch"
  )
}
```


## Loading the new projects

```{r}
library(readxl)

all_signals_2nd_batch <- list()

for (pId in paste0("Project", 10:15)) {
  all_signals_2nd_batch[[pId]] <- load_project_issue_data(pId = pId)
}
```


Let's see how they look (sanity check):

```{r echo=FALSE}
tempdf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(tempdf) <- c("x", "y", "p", "v")
n <- 500
x <- seq(from = 0, to = 1, length.out = n)
for (pId in names(all_signals_2nd_batch)) {
  for (v in c("REQ", "DEV", "DESC")) {
    tempdf <- rbind(tempdf, data.frame(
      x = x,
      y = sapply(X = x, FUN = all_signals_2nd_batch[[pId]][[v]]$get0Function()),
      p = pId,
      v = v
    ))
  }
}
```

```{r project-it-vars-2nd-batch, echo=FALSE, fig.cap="The second batch of projects [10-15]. All variables over each project's time span.", fig.align="top", fig.pos="ht!"}
library(ggplot2)

plot_all_req_dev <- ggplot(data = tempdf, aes(x = x, y = y, color = v)) +
  geom_line() +
  scale_color_manual(values = c("forestgreen", "blue", "red")) +
  facet_wrap(p ~.) +
  theme_light() +
  labs(color = "Variable") + xlab("Relative Time") + ylab("Cumulative Time spent") +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill="#dfdfdf"),
    strip.text = element_text(color="black"))
plot_all_req_dev
```

The six new projects in figure \ref{fig:project-it-vars-2nd-batch} look like they do in the excel, the import was successful.


## Evaluation of the binary decision rule\label{ssec:eval-dr-2nd-batch}

Here we will evaluate the existing binary decision rule to find out whether it can classify the second batch of projects correctly.

```{r}
temp <- sapply(X = names(all_signals_2nd_batch), FUN = function(pName) {
  p1_dr(projName = pName, signals = all_signals_2nd_batch)
})
p1_detect_2nd_batch <- data.frame(
  detect = temp,
  ground_truth = ground_truth_2nd_batch$consensus,
  correct = (temp & ground_truth_2nd_batch$consensus >= 5) | (!temp & ground_truth_2nd_batch$consensus < 5)
)
```

The results are shown in table \ref{tab:p1-bin-detect-2nd-batch}. The rule classifies __`r sum(p1_detect_2nd_batch$correct) `__ / __`r nrow(p1_detect_2nd_batch)`__ as correct. The precision is __`r Metrics::precision(actual = p1_detect$ground_truth >= 5, predicted = p1_detect$detect)`__, and the recall is __`r Metrics::recall(actual = p1_detect$ground_truth >= 5, predicted = p1_detect$detect)`__.

```{r echo=FALSE}
if (interactive()) {
  p1_detect_2nd_batch
} else {
  knitr::kable(
    x = p1_detect_2nd_batch,
    booktabs = TRUE,
    caption = "Binary detection using the previously defined decision rule based on homogeneous confidence intervals of pattern I, for the second batch of projects.",
    label = "p1-bin-detect-2nd-batch"
  )
}
```

### Calculating scoring rules

Also, let's calculate the Brier- and Log-scoring rules for the second batch, and all projects together.


```{r}
temp <- p1_detect_2nd_batch[,]
temp$ground_truth <- temp$ground_truth / 10
temp$zero_r <- 0.5
temp$detect_prob <- sapply(temp$detect, function(d) if (d) 1 else 0)

`names<-`(c(
  mean(scoring::brierscore(object = detect_prob ~ ground_truth, data = temp)),
  mean(scoring::logscore(object = detect_prob ~ ground_truth, data = temp)),
  stats::cor(temp$ground_truth, temp$detect_prob)
), c("Brier score", "Log score", "Correlation"))
```

ZeroR:

```{r}
Metrics::mse(temp$ground_truth, temp$zero_r)
```

These results are a little worse than for the first batch ($\approx0.133$ and $\approx0.4$).
Also for the second batch, the binary decision rule for the first time is slightly better than ZeroR.
Let's compute the scoring rules for all projects together, the results should be somewhere in between:

```{r}
temp <- rbind(p1_detect, p1_detect_2nd_batch)
temp$ground_truth <- temp$ground_truth / 10
temp$zero_r <- 0.5
temp$detect_prob <- sapply(temp$detect, function(d) if (d) 1 else 0)

`names<-`(c(
  mean(scoring::brierscore(object = detect_prob ~ ground_truth, data = temp)),
  mean(scoring::logscore(object = detect_prob ~ ground_truth, data = temp)),
  stats::cor(temp$ground_truth, temp$detect_prob)
), c("Brier score", "Log score", "Correlation"))
```

We can use these results later for comparing against scores as computed by the continuous process models.

Also, computing ZeroR for all projects:

```{r}
Metrics::mse(temp$ground_truth, temp$zero_r)
```

Again, ZeroR beats the decision rule.


## Computing the scores

We will start by computing the scores that are based on the gradated confidence intervals. We can reuse the function `p3_avg_ci_scores_compute()`.

```{r}
p3_avg_ci_2nd_batch_scores <- loadResultsOrCompute(file = "../results/p3_avg_ci_2nd_batch_scores.rds", computeExpr =  {
  doWithParallelCluster(numCores = length(all_signals_2nd_batch), expr = {
    library(foreach)
    
    foreach::foreach(
      pId = names(all_signals_2nd_batch),
      .inorder = TRUE,
      .combine = rbind
    ) %dopar% {
      p3_avg_ci_scores_compute(pId = pId, x1 = 0, x2 = 1, signals = all_signals_2nd_batch)
    }
  })
})
```


Table \ref{tab:p3-avg-ci-2nd-batch-scores} shows the computed scores.

```{r echo=FALSE}
if (interactive()) {
  p3_avg_ci_2nd_batch_scores
} else {
  knitr::kable(
    x = p3_avg_ci_2nd_batch_scores,
    booktabs = TRUE,
    caption = "The average confidence of the variables REQ, DEV and DESC of each of the 2nd batch's projects as integrated over the confidence intervals' hyperplane (as computed from the first batch of projects).",
    label = "p3-avg-ci-2nd-batch-scores"
  )
}
```

The correlation with the ground truth of these scores is:


```{r}
cor.test(x = ground_truth_2nd_batch$consensus_score, y = p3_avg_ci_2nd_batch_scores[, "REQ"])
```

```{r}
cor.test(x = ground_truth_2nd_batch$consensus_score, y = p3_avg_ci_2nd_batch_scores[, "DEV"])
```

```{r}
cor.test(x = ground_truth_2nd_batch$consensus_score, y = p3_avg_ci_2nd_batch_scores[, "DESC"])
```

While these correlations seem significant, the p-values indicate evidence for the null hypothesis, which means no correlation. Now that we have these scores, we'll compute scores for the average distance to the respective reference variable.


```{r}
p3_avg_area_2nd_batch_scores <- loadResultsOrCompute(file = "../results/p3_avg_area_2nd_batch_scores.rds", computeExpr =  {
  doWithParallelCluster(numCores = length(all_signals_2nd_batch), expr = {
    library(foreach)
    
    foreach::foreach(
      pId = names(all_signals_2nd_batch),
      .inorder = TRUE,
      .combine = rbind
    ) %dopar% {
      p3_avg_area_scores_compute(pId = pId, signals = all_signals_2nd_batch)
    }
  })
})
```

Table \ref{tab:p3-avg-area-2nd-batch-scores} shows the computed scores.

```{r echo=FALSE}
if (interactive()) {
  p3_avg_area_2nd_batch_scores
} else {
  knitr::kable(
    x = p3_avg_area_2nd_batch_scores,
    booktabs = TRUE,
    caption = "The average distance of the variables REQ, DEV and DESC of each of the second batch's projects to the previously averaged reference-variables $\\overline{\\operatorname{REQ}},\\overline{\\operatorname{DEV}},\\overline{\\operatorname{DESC}}$ as computed over tethe first batch.",
    label = "p3-avg-area-2nd-batch-scores"
  )
}
```


## Predicting the ground truth

Now we have all the scores of the new projects computed against the pattern as generated from the previous projects.

```{r}
temp <- data.frame(
  gt_consensus = ground_truth_2nd_batch$consensus_score,
  ci_req = p3_avg_ci_2nd_batch_scores$REQ,
  area_req = p3_avg_area_2nd_batch_scores$REQ,
  ci_dev = p3_avg_ci_2nd_batch_scores$DEV,
  area_dev = p3_avg_area_2nd_batch_scores$DEV,
  ci_desc = p3_avg_ci_2nd_batch_scores$DESC,
  area_desc = p3_avg_area_2nd_batch_scores$DESC)

p3_avg_lm_2nd_batch_scores <- stats::predict(object = p3_avg_lm, temp)
# Since we are attempting a regression to positive scores,
# we set any negative predictions to 0. Same goes for >1.
p3_avg_lm_2nd_batch_scores[p3_avg_lm_2nd_batch_scores < 0] <- 0
p3_avg_lm_2nd_batch_scores[p3_avg_lm_2nd_batch_scores > 1] <- 1

`names<-`(round(p3_avg_lm_2nd_batch_scores * 10, 3), 10:15)
stats::cor(p3_avg_lm_2nd_batch_scores, ground_truth_2nd_batch$consensus_score)
```

These results mean that the prediction is anti-proportional to the ground truth, and weak at the same time. It is actually worse to have a negative correlation than having no correlation. This can only be interpreted as our previously trained regression model not being able to generalize to new, unseen data. This is expected, since a) we did not have sufficient training data, and b) the previous regression model being overfit on the previous data. However, and I wrote that before, the main point of these regression models was translation and scaling of the scores, not prediction. For that, much more data is required (both, instances and features, as the selected features cannot capture all of the important aspects of the deviation process and process model). Furthermore, these regression models (including the one we built for the source code data) were built using hand-picked features. Ideally, we would have a great number of different features, paired with something like a recursive feature elimination, in order to come up with a regression model that has reliable generalizability. Therefore, I am inclined not to repeat this exercise with the source code data, as we will have the same problem there.


## Predicting using the best RFE model

We have previously computed the variable importances in section \ref{sssec:var-imp-it}, using the first batch of projects. We will not attempt this for the second batch. Rather, we will use the best model as found by the recursive feature elimination and use it to make predictions on the second batch. Previously, we evaluated the binary decision rule on the new projects (cf. section \ref{ssec:eval-dr-2nd-batch}).

```{r}
p3_it_2nd_batch_scores <- loadResultsOrCompute(file = "../results/p3_it_2nd_batch_scores.rds", computeExpr = {
  p3_it_projects_2nd_batch <- time_warp_wrapper(
    pattern = p3_it_signals, derive = FALSE,
    use_signals = all_signals_2nd_batch, use_ground_truth = ground_truth_2nd_batch)
  
  as.data.frame(
    compute_all_scores_it(
      alignment = p3_it_projects_2nd_batch, patternName = "p3_it", vartypes = names(p3_it_signals)))
})
```

```{r}
# Note that these predictions are already scaled!
p3_avg_rfe_2nd_batch_scores <- stats::predict(object = modelFit_it_all, p3_it_2nd_batch_scores)
# Since we are attempting a regression to positive scores,
# we set any negative predictions to 0. Same goes for >1.
p3_avg_rfe_2nd_batch_scores[p3_avg_rfe_2nd_batch_scores < 0] <- 0
p3_avg_rfe_2nd_batch_scores[p3_avg_rfe_2nd_batch_scores > 10] <- 10

`names<-`(round(p3_avg_rfe_2nd_batch_scores, 4), rownames(ground_truth_2nd_batch))
stats::cor(p3_avg_rfe_2nd_batch_scores, ground_truth_2nd_batch$consensus)
```

These results are similarly worse to those obtained from the previous section, where we predicted using the linear model. The only conclusion we can now draw confidently is that we have insufficient training data in order to train a model that can generalize to new data (likely both, instances and feature granularity). However, the features we found to be most important have a good chance to be actually the most important ones. If we were to repeat the variable-importance experiment with a lot more data (projects), we might get a similar ranking and relative importance. It is only that currently, the amount of data does not suffice to generalize from the observations, even if we have a good selection and weighting of features.


# Automatic calibration of the continuous process models

```{r echo=FALSE}
# Copy-paste of functions from the SC notebook that are needed here:

segment_unit_transform <- function(func, from, to) {
  ext <- to - from
  
  function(x) {
    # func has a support of [0, 1], and the returned function
    # has a support of [from, to].
    func(from + (x * ext))
  }
}

compute_segment_metric <- function(
  PM, P, segment, numSamples = 1e3,
  use = c("area", "corr", "jsd", "kl", "arclen", "sd", "var", "mae",
          "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")) {
  
  # We'll have to make this transform as all the scores we compute
  # assume a support of [0,1] and compute over that.
  f1 <- segment_unit_transform(PM, from = segment[1], to = segment[2])
  f2 <- segment_unit_transform(P, from = segment[1], to = segment[2])
  
  # Some metrics will require scaling after the segment-unit-transform
  ext <- segment[2] - segment[1]
  
  diff <- NA_real_
  if (use == "area") {
    # This is integration and susceptible to scaling!
    diff <- ext * area_diff_2_functions(f1 = f1, f2 = f2)$value
  } else if (use == "corr") {
    # Returns a correlation in range [-1,1], but we want a difference,
    diff <- stat_diff_2_functions_cor_score(
      allowReturnNA = TRUE, requiredSign = 0, numSamples = numSamples)(f1 = f1, f2 = f2)
    # so we'll subtract it from 1 -> [0,2]
    diff <- 1 - (if (is.na(diff)) 0 else diff) # NA means no correlation +/-
  } else if (use == "jsd") {
    diff <- stat_diff_2_functions_symmetric_JSD_sampled(f1 = f1, f2 = f2, numSamples = numSamples)$value
  } else if (use == "kl") {
    diff <- stat_diff_2_functions_symmetric_KL_sampled(f1 = f1, f2 = f2, numSamples = numSamples)$value
  } else if (use == "arclen") {
    # Returns a ration min/max -> (0,1], where 1 is ideal.
    # We should inverse that because we are returning a difference.
    diff <- 1 - stat_diff_2_functions_arclen_score(requiredSign = 0, numSamples = numSamples)(f1 = f1, f2 = f2)
  } else if (use %in% c("sd", "var", "mae", "rmse")) {
    temp <- switch (use,
      "sd"   = stat_diff_2_functions_sd(f1 = f1, f2 = f2, numSamples = numSamples)$value,
      "var"  = stat_diff_2_functions_var(f1 = f1, f2 = f2, numSamples = numSamples)$value,
      "mae"  = ext * stat_diff_2_functions_mae(f1 = f1, f2 = f2, numSamples = numSamples)$value,
      "rmse" = ext * stat_diff_2_functions_rmse(f1 = f1, f2 = f2, numSamples = numSamples)$value,
      {
        stop(paste0("Don't know ", use, "."))
      })
    diff <- temp
  } else if (use %in% c("RMS", "Kurtosis", "Peak", "ImpulseFactor")) {
    # Returns a ratio min/max -> (0,1], where 1 is ideal.
    diff <- 1 - stat_diff_2_functions_signals_score(use = use, requiredSign = 0, numSamples = numSamples)(f1 = f1, f2 = f2)
  }
  
  diff
}

ac_extract_data <- function(pmName, varName, segIdx, metricName, ac_grid, ac_grid_results) {
  rows <- ac_grid[ac_grid$PM == pmName & ac_grid$Var == varName, ]
  ac_grid_results[ac_grid_results$SegIdx == segIdx & ac_grid_results$GridIdx %in% rownames(rows), metricName]
}

ac_worker <- function(PM, P, seg_idx, total_segments = 10) {
  use <- c("area", "corr", "jsd", "kl", "arclen", "sd", "var", "mae",
           "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")
  df <- `colnames<-`(matrix(nrow = 1, ncol = length(use)), use)
  
  seg_ext <- 1 / total_segments
  
  for (metric in use) {
    df[1, metric] <- compute_segment_metric(PM = PM, P = P, use = metric, segment = c(
      (seg_idx - 1) * seg_ext, seg_idx * seg_ext
    ))
  }
  df
}

ac_pmp_score <- function(pmName, projName, results_uniform, use_metrics) {
  temp <- results_uniform[
    results_uniform$PM == pmName &
    results_uniform$Metric %in% as.character(use_metrics), projName]
  
  mean(temp)
}

ac_pmp_score_weighted <- function(pmName, projName, weightGrid, weightVector, results_uniform) {
  if (length(weightVector) != nrow(weightGrid)) {
    stop("Address register grid does not match weight vector.")
  }
  
  res <- c()
  for (i in 1:length(weightVector)) {
    params <- weightGrid[i,]
    params$Var <- as.character(params$Var)
    params$Metric <- as.character(params$Metric)
    
    # The following selection will result in a single value:
    score <- results_uniform[
      results_uniform$PM == pmName &
      results_uniform$Var == params$Var &
      results_uniform$Metric == params$Metric &
      results_uniform$SegIdx == params$SegIdx, projName]
    
    if (length(score) != 1) {
      stop()
    }
    
    res <- c(res, weightVector[i] * score)
  }
  
  sum(res) / sum(weightVector)
}

get_smoothed_curve <- function(seed = NA, npoints = 15, include_deriv = FALSE) {
  if (!is.na(seed)) {
    set.seed(seed = seed)
  }
  
  x <- sort(c(0, 1, runif(npoints - 2)))
  y <- runif(length(x))
  temp <- loess.smooth(x = x, y = y, span = 0.35, family = "g", evaluation = 1000)
  appr <- stats::approxfun(
    x = ((temp$x - min(temp$x)) / (max(temp$x) - min(temp$x))),
    y = temp$y / (max(temp$x) - min(temp$x)),
    yleft = utils::head(temp$y, 1),
    yright = utils::tail(temp$y, 1))
  
  tempf <- Vectorize(function(x) {
    # Limit the resulting function to the bounding box of [0,1]
    min(1, max(0, appr(x)))
  })
  
  f1 <- NULL
  if (include_deriv) {
    tempf1 <- Vectorize(function(x) {
      ltol <- x < sqrt(.Machine$double.eps)
      rtol <- x > 1 - sqrt(.Machine$double.eps)
      pracma::fderiv(f = tempf, x = x, method = if (ltol) "forward" else if (rtol) "backward" else "central")
    })
    x <- seq(from = 0, to = 1, length.out = 2e3)
    y <- tempf1(x)
    
    f1 = stats::approxfun(
      x = x, y = y, yleft = utils::head(y, 1), yright = utils::tail(y, 1))
  }
  
  list(f0 = tempf, f1 = f1)
}
```

We have previously attempted the automatic calibration of the continuous process models that use source code data (section \ref{sec:auto-calib}), and now we will do the same here for the models using issue-tracking data.
We will reuse the functions defined for source code data as much as possible here.
A notable difference is of course the process models. Here, we have one less variable, but six instead of four models (I, II(a), III, as well as their resp. derivative models).


## Computing the metrics

We will need an extra grid for issue-tracking data, before we can proceed.

```{r}
ac_grid_it <- loadResultsOrCompute(file = "../results/ac_grid_it.rds", computeExpr = {
  expand.grid(list(
    PM = c("I", "II(a)", "III", "IV(I)", "IV(II(a))", "IV(III)"),
    Var = c("REQ", "DEV", "DESC"),
    Seed = seq(from = 1, length.out = 1e4)))
})

nrow(ac_grid_it)
```

Let's compute the full grid:

```{r ac-grid-results-it}
ac_grid_results_it <- loadResultsOrCompute(file = "../results/ac_grid_results_it.rds", computeExpr = {
  library(foreach)
  
  cl <- parallel::makePSOCKcluster(min(123, parallel::detectCores()))
  parallel::clusterExport(cl, varlist = list("req_poly", "dev_poly"))
  
  doWithParallelClusterExplicit(cl = cl, stopCl = TRUE, expr = {
    pb <- utils::txtProgressBar(min = 1, max = nrow(ac_grid_it), style = 3)
    progress <- function(n) {
      if (0 == (n %% 25)) {
        print(n)
      }
      utils::setTxtProgressBar(pb = pb, value = n)
    }
    
    foreach::foreach(
      grid_idx = rownames(ac_grid_it),
      .combine = rbind,
      .inorder = FALSE,
      .verbose = TRUE,
      .packages = c("cobs"),
      .options.snow = list(progress = progress)
    ) %dopar% {
      options(warn = 2)
      params <- ac_grid_it[grid_idx, ]
      params$PM <- as.character(params$PM)
      params$Var <- as.character(params$Var)
      
      pm_var <- switch (params$PM,
        "I" = p1_it_signals,
        "II(a)" = p2a_it_signals,
        "III" = p3_it_signals,
        "IV(I)" = p4p1_it_signals,
        "IV(II(a))" = p4p2a_it_signals,
        "IV(III)" = p4p3_it_signals,
        {
          stop(paste0("Don't know ", params$PM, "."))
        }
      )[[params$Var]]$get0Function()
      
      include_deriv <- params$PM %in% c("IV(I)", "IV(II(a))", "IV(III)")
      smooth_curve <- get_smoothed_curve(seed = params$Seed, include_deriv = include_deriv)
      p_var <- if (include_deriv) smooth_curve$f1 else smooth_curve$f0
      
      df <- NULL
      for (seg_idx in 1:10) {
        # Important so we can correctly concatenate the results with the grid parameters!
        res <- as.data.frame(
          ac_worker(PM = pm_var, P = p_var, seg_idx = seg_idx, total_segments = 10))
        res$GridIdx <- grid_idx
        res$SegIdx <- seg_idx
        res <- cbind(res, params)
        df <- if (is.null(df)) res else rbind(df, res)
      }
      df
    }
  })
})
```


## Approximating marginal cumulative densities

Now with the simulated random processes, we can approximate the marginal (cumulative) densities for each objective. This is the same as we did previously using source code data (cf. section \ref{ssec:ac-approx-marginal-ecdfs}), and we will reuse some of the previously defined functions there.
Let's show some randomly picked examples in figure \ref{fig:ac-grid-example-it}. Similarly to source code data, the ECDFs are all clearly non-linear.

```{r ac-grid-example-it, echo=FALSE, fig.height=5, fig.cap="Four randomly picked ECDFs as simulated using the random processes of the automatic calibration for issue-tracking data."}
par(mfrow = c(2,2), mgp = c(1.5, 1, 0))


set.seed(14)
temp.PM <- levels(ac_grid_it$PM)
temp.Var <- unique(ac_grid_it$Var)
temp.Seg <- 1:10
temp.Metric <- c("area", "corr", "jsd", "kl", "arclen", "sd", "var", "mae",
                 "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")

for (i in 1:4) {
  temp.pm <- sample(temp.PM, 1)
  temp.PM <- temp.PM[temp.PM != temp.pm]
  temp.var <- sample(temp.Var, 1)
  temp.seg <- sample(temp.Seg, 1)
  temp.Seg <- temp.Seg[temp.Seg != temp.seg]
  temp.metric <- sample(temp.Metric, 1)
  temp.Metric <- temp.Metric[temp.Metric != temp.metric]
  
  temp <- ac_extract_data(pmName = temp.pm, varName = temp.var, segIdx = temp.seg, metricName = temp.metric, ac_grid = ac_grid_it, ac_grid_results = ac_grid_results_it)
  tempf <- stats::ecdf(temp)
  curve2(tempf, min(temp), max(temp), xlab = "", main = paste0(
    "PM=", temp.pm,
    ", Var=", temp.var
  ), sub = paste0(
    "SegIdx=", temp.seg,
    ", Metric=", temp.metric
  ))
}
```


## Calculating scores

This is the same approach as for source code data, with only minor differences.
In order to calculate each project's scores, we need to calculate one objective per:

* Project (15),
* Process model (6),
* Variable (3),
* Segment index (10), and
* Metric (13) [this is not part of the grid, the `ac_worker()` does this].

That will leave us with $15\times 6\times 3\times 10\times 13=35,100$ computations. It's probably best to generate a grid for that:

```{r}
ac_grid_projects_it <- expand.grid(list(
  Project = c(names(all_signals), names(all_signals_2nd_batch)),
  PM = c("I", "II(a)", "III", "IV(I)", "IV(II(a))", "IV(III)"),
  Var = c("REQ", "DEV", "DESC"),
  SegIdx = 1:10))

nrow(ac_grid_projects_it)
```

Now we can compute the projects' scores:

```{r ac-grid-projects-results-it}
ac_grid_projects_results_it <- loadResultsOrCompute(file = "../results/ac_grid_projects_results_it.rds", computeExpr = {
  library(foreach)
  
  cl <- parallel::makePSOCKcluster(min(123, parallel::detectCores()))
  parallel::clusterExport(cl, varlist = list("req_poly", "dev_poly"))
  project_signals_all <- append(all_signals, all_signals_2nd_batch)
  
  doWithParallelClusterExplicit(cl = cl, expr = {
    pb <- utils::txtProgressBar(min = 1, max = nrow(ac_grid_projects_it), style = 3)
    progress <- function(n) {
      if (0 == (n %% 25)) {
        print(n)
      }
      utils::setTxtProgressBar(pb = pb, value = n)
    }
    
    foreach::foreach(
      grid_idx = rownames(ac_grid_projects_it),
      .combine = rbind,
      .inorder = FALSE,
      .verbose = TRUE,
      .packages = c("cobs"),
      .options.snow = list(progress = progress)
    ) %dopar% {
      options(warn = 2)
      params <- ac_grid_projects_it[grid_idx, ]
      params$Project <- as.character(params$Project)
      params$PM <- as.character(params$PM)
      params$Var <- as.character(params$Var)
    
      pm_var <- switch (params$PM,
        "I" = p1_it_signals,
        "II(a)" = p2a_it_signals,
        "III" = p3_it_signals,
        "IV(I)" = p4p1_it_signals,
        "IV(II(a))" = p4p2a_it_signals,
        "IV(III)" = p4p3_it_signals,
        {
          stop(paste0("Don't know ", params$PM, "."))
        }
      )[[params$Var]]$get0Function()
      
      tempf <- project_signals_all[[params$Project]][[params$Var]]$get0Function()
      use_deriv <- params$PM %in% c("IV(I)", "IV(II(a))", "IV(III)")
      p_var <- if (use_deriv) {
        # If using the derivative, we approximate a somewhat smooth function of the variable.
        # The recommended span is >= 0.2, and we'll use the lowest since it preserves the
        # most details.
        x <- seq(from = 0, to = 1, length.out = 2e3)
        smooth_signal_loess(x = x, y = tempf(x), span = 0.2, family = "g")
      } else {
        tempf
      }
      
      # Important so we can correctly concatenate the results with the grid parameters!
      res <- as.data.frame(
        ac_worker(PM = pm_var, P = p_var, seg_idx = params$SegIdx, total_segments = 10))
      res$grid_idx <- grid_idx
      cbind(res, params)
    }
  })
})
```


### Rectification of raw scores

Now that we have the raw scores, it is time to transform them using the marginal densities. Each single objective (ECDF) is reused 15 times (once for each project).
We will make a new temporary grid, where the number of rows in this grid corresponds to the number of ECDFs:

```{r}
temp.Metric <- c("area", "corr", "jsd", "kl", "arclen", "sd", "var", "mae",
                 "rmse", "RMS", "Kurtosis", "Peak", "ImpulseFactor")
temp.grid <- expand.grid(list(
  PM = c("I", "II(a)", "III", "IV(I)", "IV(II(a))", "IV(III)"),
  Var = c("REQ", "DEV", "DESC"),
  SegIdx = 1:10,
  Metric = temp.Metric))

nrow(temp.grid)
```

Now it's time to rectify the raw scores:

```{r}
ac_grid_projects_results_uniform_it <- loadResultsOrCompute(file = "../results/ac_grid_projects_results_uniform_it.rds", computeExpr = {
  doWithParallelCluster(numCores = min(123, parallel::detectCores()), expr = {
    library(foreach)
    
    project_signals_all <- append(all_signals, all_signals_2nd_batch)
    
    foreach::foreach(
      grid_idx = rownames(temp.grid),
      .combine = rbind,
      .inorder = FALSE,
      .verbose = TRUE
    ) %dopar% {
      options(warn = 2)
      params <- temp.grid[grid_idx, ]
      params$PM <- as.character(params$PM)
      params$Var <- as.character(params$Var)
      params$Metric <- as.character(params$Metric)
      
      temp.data <- ac_extract_data(
        pmName = params$PM, varName = params$Var, segIdx = params$SegIdx,
        metricName = params$Metric, ac_grid = ac_grid_it, ac_grid_results = ac_grid_results_it)
      tempf <- stats::ecdf(temp.data)
      
      res <- matrix(nrow = 1, ncol = length(project_signals_all))
      for (i in 1:length(project_signals_all)) {
        res[1, i] <- tempf(ac_grid_projects_results_it[
          ac_grid_projects_results_it$Project == names(project_signals_all)[i] &
          ac_grid_projects_results_it$PM == params$PM &
          ac_grid_projects_results_it$Var == params$Var &
          ac_grid_projects_results_it$SegIdx == params$SegIdx, params$Metric])
      }
      cbind(`colnames<-`(res, names(project_signals_all)), params)
    }
  })
})
```


```{r echo=FALSE}
# The results as computed are in a bit of an unpractical format,
# we need to do some partial transpositions:
temp <- NULL
temp.Metric <- sort(unique(ac_grid_projects_results_uniform_it$Metric))

for (projName in names(append(all_signals, all_signals_2nd_batch))) {
  
  project.df <- NULL
  
  for (metric in sort(unique(ac_grid_projects_results_uniform_it$Metric))) {
    temp.df <- ac_grid_projects_results_uniform_it[
      ac_grid_projects_results_uniform_it$Metric == metric, c(projName, "PM", "Var", "SegIdx")]
    temp.df$Project <- projName
    temp.df[[metric]] <- temp.df[[projName]] # rename, then drop
    temp.df[[projName]] <- NULL
    
    project.df <- if (is.null(project.df)) temp.df else cbind(
      project.df, `colnames<-`(as.data.frame(temp.df[, metric]), metric))
  }
  
  temp <- rbind(temp, project.df)
}

ac_grid_projects_results_uniform_it <- temp
```


### Non-weighted


```{r}
temp <- append(all_signals, all_signals_2nd_batch)
temp.gt <- c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score)

temp.df <- NULL

for (pmName in c("I", "II(a)", "III", "IV(I)", "IV(II(a))", "IV(III)")) {
  temp.pred <- data.frame(
    pred = sapply(X = names(temp), function(pName) ac_pmp_score(
      use_metrics = temp.Metric,
      pmName = pmName, projName = pName, results_uniform = ac_grid_projects_results_uniform_it)),
    ground_truth = temp.gt)
  
  temp.df <- rbind(temp.df, data.frame(
    PM = pmName,
    Score_for_best = ac_pmp_score(
      use_metrics = temp.Metric,
      pmName = pmName, projName = paste0("Project", which.max(temp.gt)), results_uniform = ac_grid_projects_results_uniform_it),
    Score_for_worst = ac_pmp_score(
      use_metrics = temp.Metric,
      pmName = pmName, projName = paste0("Project", which.min(temp.gt)), results_uniform = ac_grid_projects_results_uniform_it),
    MSE = Metrics::mse(actual = temp.gt, predicted = temp.pred$pred),
    Log = mean(scoring::logscore(object = ground_truth ~ pred, data = temp.pred))
  ))
}
```

The results of table \ref{tab:ac-compare-pms-it} are odd, in that the scores for best and worst project are interchanged, while the MSE and Log-scores are quite acceptable.
The PMs for source code data did not have this problem (i.e., the score for best was consistently higher than the score for worst).
At this point, this could indicate that the continuous PMs for issue tracking data measure the opposite of the phenomenon, that is, they indicate the absence of the Fire Drill, rather than its presence (negative correlation between these scores and the ground truth).
In terms of MSE and Log-score, the derivative models appear to perform slightly better.

```{r echo=FALSE}
if (interactive()) {
  temp.df
} else {
  knitr::kable(
    x = temp.df,
    booktabs = TRUE,
    caption = "Comparison of continuous PMs using issue-tracking data for best/worst projects, as well as MSE- and Log-scores as average deviation from the ground truth consensus.",
    label = "ac-compare-pms-it"
  )
}
```


### Weighted by optimization

```{r}
ac_weight_grid_it <- expand.grid(list(
  Var = c("REQ", "DEV", "DESC"),
  Metric = temp.Metric,
  SegIdx = 1:10))

nrow(ac_weight_grid_it)
```


Now the objective for the optimization will search for weights that minimize the MSE between the weighted projects and the ground truth.
We will optimize the weights once for each process model, and then follow this up by a closer inspection of the champion model (the model with the lowest MSE).


```{r}
temp.names <- names(append(all_signals, all_signals_2nd_batch))
ac_it_weights_optim <- function(pmName) {
  loadResultsOrCompute(file = paste0("../results/ac_it_weights_pm_", pmName, ".rds"), computeExpr = {
    cl <- parallel::makePSOCKcluster(min(123, parallel::detectCores()))
    parallel::clusterExport(cl, varlist = list(
      "temp.gt", "temp.names", "ac_pmp_score_weighted",
      "ac_weight_grid_it", "ac_grid_projects_results_uniform_it"))

    doWithParallelClusterExplicit(cl = cl, expr = {
      optimParallel::optimParallel(
        par = rep(0.5, nrow(ac_weight_grid_it)),
        method = "L-BFGS-B",
        lower = rep(0, nrow(ac_weight_grid_it)),
        upper = rep(1, nrow(ac_weight_grid_it)),
        fn = function(x) {
          Metrics::mse(actual = temp.gt, predicted = sapply(X = temp.names, FUN = function(projName) {
            ac_pmp_score_weighted(pmName = pmName, projName = projName, weightGrid = ac_weight_grid_it,
                                  results_uniform = ac_grid_projects_results_uniform_it, weightVector = x)
          }))
        },
        parallel = list(cl = cl, forward = FALSE, loginfo = TRUE)
      )
    })
  })
}
```

#### Results PM vs. PM

Now for the actual optimization of each type of process model.
An overview of the optimization process on a per-project basis is shown in table \ref{tab:ac-sc-weights-optim-overview}.

```{r}
ac_it_weights_pm_I <- ac_it_weights_optim(pmName = "I")
ac_it_weights_pm_IIa <- ac_it_weights_optim(pmName = "II(a)")
ac_it_weights_pm_III <- ac_it_weights_optim(pmName = "III")
ac_it_weights_pm_IV_I <- ac_it_weights_optim(pmName = "IV(I)")
ac_it_weights_pm_IV_IIa <- ac_it_weights_optim(pmName = "IV(II(a))")
ac_it_weights_pm_IV_III <- ac_it_weights_optim(pmName = "IV(III)")
```

```{r echo=FALSE}
templ <- list(
  ac_it_weights_pm_I,
  ac_it_weights_pm_IIa,
  ac_it_weights_pm_III,
  ac_it_weights_pm_IV_I,
  ac_it_weights_pm_IV_IIa,
  ac_it_weights_pm_IV_III)
temp <- data.frame(
  PM = c("I", "II(a)", "III", "IV(I)", "IV(II(a))", "IV(III)"),
  MSE = round(unlist(lapply(templ, function(res) res$value)), 5),
  RMSE = round(unlist(lapply(templ, function(res) sqrt(res$value))), 5),
  Num_weights_gt0 = unlist(lapply(templ, function(res) sum(res$par > 0))),
  Weights_pruned = unlist(lapply(templ, function(res) paste0(round(100 * (1 - sum(res$par > 0) / length(res$par)), 2), "%"))),
  Num_iter = unlist(lapply(templ, function(res) res$counts["function"])),
  Num_grad = unlist(lapply(templ, function(res) res$counts["gradient"])))

if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Overview of the results of optimizing the weights using the normalizing linear scalarizer objective, on a per-project basis, for issue-tracking process models.",
    label = "ac-it-weights-optim-overview"
  )
}
```



```{r echo=FALSE}
temp <- append(all_signals, all_signals_2nd_batch)
temp.gt <- c(ground_truth$consensus_score, ground_truth_2nd_batch$consensus_score)

temp.df <- NULL

for (pmName in c("I", "II(a)", "III", "IV(I)", "IV(II(a))", "IV(III)")) {
  temp.weights <- switch (
    pmName,
    "I" = ac_it_weights_pm_I$par,
    "II(a)" = ac_it_weights_pm_IIa$par,
    "III" = ac_it_weights_pm_III$par,
    "IV(I)" = ac_it_weights_pm_IV_I$par,
    "IV(II(a))" = ac_it_weights_pm_IV_IIa$par,
    "IV(III)" = ac_it_weights_pm_IV_III$par,
    {
      stop(paste0("Unknown process model: ", pmName))
    })
  temp.pred <- data.frame(
    pred = sapply(X = names(temp), function(pName) ac_pmp_score_weighted(
      results_uniform = ac_grid_projects_results_uniform_it,
      pmName = pmName, projName = pName, weightGrid = ac_weight_grid_it, weightVector = temp.weights)),
    ground_truth = temp.gt)
  
  temp.df <- rbind(temp.df, data.frame(
    PM = pmName,
    Score_for_best = ac_pmp_score_weighted(
      results_uniform = ac_grid_projects_results_uniform_it,
      pmName = pmName, projName = paste0("Project", which.max(temp.gt)),
      weightGrid = ac_weight_grid_it, weightVector = temp.weights),
    Score_for_worst = ac_pmp_score_weighted(
      results_uniform = ac_grid_projects_results_uniform_it,
      pmName = pmName, projName = paste0("Project", which.min(temp.gt)),
      weightGrid = ac_weight_grid_it, weightVector = temp.weights),
    MSE = Metrics::mse(actual = temp.gt, predicted = temp.pred$pred),
    Log = mean(scoring::logscore(object = ground_truth ~ pred, data = temp.pred))
  ))
}
```

```{r echo=FALSE}
if (interactive()) {
  temp.df
} else {
  knitr::kable(
    x = temp.df,
    booktabs = TRUE,
    caption = "Comparison of continuous PMs using issue-tracking data for best/worst projects, as well as MSE- and Log-scores as average deviation from the ground truth consensus, using some best set of weights per PM as found by optimization.",
    label = "ac-compare-pms-it-optim"
  )
}
```


# References {-}

<div id="refs"></div>


