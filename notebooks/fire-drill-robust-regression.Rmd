---
title: "Technical Report: Building Robust Regression Models for Scoring the Presence of the Fire Drill Anti-pattern"
author: "Sebastian HÃ¶nel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ../inst/REFERENCES.bib
urlcolor: blue
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  md_document:
    toc: yes
    toc_depth: 6
    df_print: kable
    variant: gfm
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: kable
  word_document: default
header-includes:
- \usepackage{bm}
- \usepackage{mathtools}
- \usepackage{xurl}
---

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\abs}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand{\norm}[1]{\left\lVert\,#1\,\right\rVert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}
\newcommand\argmax[1]{\underset{#1}{arg\,max}}
\newcommand\argmin[1]{\underset{#1}{arg\,min}}


```{r setoptions, echo=FALSE, warning=FALSE, message=FALSE}
Sys.setenv(LANG = "en_US.UTF-8")
Sys.setenv(LC_ALL = "en_US.UTF-8")
Sys.setenv(LC_CTYPE = "en_US.UTF-8")
library(knitr)
opts_chunk$set(tidy = TRUE, tidy.opts = list(indent=2))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

devtools::source_url(url = "https://raw.githubusercontent.com/MrShoenel/R-rank-model/master/R/helpers.R")
devtools::source_url(url = "https://raw.githubusercontent.com/MrShoenel/R-rank-model/master/R/rank-model.R")

library(ggplot2)
library(ggridges)
library(ggpubr)
library(fields)
library(dplyr)
```

# Introduction\label{tr:fire-drill-rob-reg-technical-report}

This technical report was added in the eighth iteration of the compilation.
So far, we have developed an unsupervised approach for properly scoring the presence of the anti-pattern.
However, here we will attempt to find a robust regression model in a supervised way. In other words, the regression model to be found will be the surrogate scoring mechanism.
The challenge lies in the scarcity of our data, as it has many more features than data points. It is therefore essential to obtain a regression model that, even when trained on only few instances, delivers a low generalization error.


All complementary data and results can be found at Zenodo [@honel_picha_2021]. This notebook was written in a way that it can be run without any additional efforts to reproduce the outputs (using the pre-computed results). This notebook has a canonical URL^[[Link]](https://github.com/MrShoenel/anti-pattern-models/blob/master/notebooks/fire-drill-issue-tracking-technical-report.Rmd)^ and can be read online as a rendered markdown^[[Link]](https://github.com/MrShoenel/anti-pattern-models/blob/master/notebooks/fire-drill-issue-tracking-technical-report.md)^ version. All code can be found in this repository, too.



# Overview of the Approach

We will be importing the two initial process models/patterns for the Fire Drill anti-pattern in source code and issue-tracking data.
These patterns have four and three activities/metrics, respectively.
Both, patterns and projects, are formulated *continuously*, that is, as curves over time. Also, both have been normalized along both, $x$- and $y$-axes (having a domain/range of $[0,1]$).


As for the features, we will try two approaches.
First we will sample ten equidistantly-spaced points and compute the distance between these points of the process model and a project.
The second approach will subdivide the time axis into ten equally long segments, of which we will calculate the area between curves and the correlation.


Then, we will use a standard machine learning approach with repeated cross-validation in order to find out whether the generalization error on some holdout data will converge with increasing size of the training set.
We will attempt standard pre-processing, such as removing (near-)zero variance features, highly correlated features, or dimensionality reduction of the feature space.


# Slight Adaption of the Source Code Pattern

The pattern used for source code is slightly erroneous with regard to how the source code density is modeled. It does not currently properly account for how the Fire Drill is described.
Also, we want to change the pattern to use smooth CDFs for the maintenance activities, as the plan is to only measure correlation and area between curves. Using CDFs has a few advantages, such as no normalization and better suitability for the chosen metrics.
We will also change the `FREQ` activity and represent it as a mixture of `A` and `CP`, such that all three densities become true densities (integrating to $\approx1$).

## Modeling `A`, `CP`, and `FREQ`

The process for `A`, `CP`, and `FREQ` is the following (both, for the pattern and projects):

* Rejection Sampling:
  * If pattern, sample from under the modeled curve.
  * If project, sample from the KDE of the activity's curve.
* Estimate new KDE from these samples for `A` and `CP`.
* Compose a mixture density for `FREQ` as $f(x)=r_A f_A(x) + r_{CP} f_{CP}(x)$, where $r_a,r_{CP}$ correspond to the ratios of these activities.
* Perform rejection sampling from the three densities, using large sample sizes, so that we can obtain a smooth ECDF.

This process will leave us with three separate smooth ECDFs.


## Modeling `SCD`

The source code density can move quite unpredictably, such that it does not make a lot of sense obtaining curves and measuring their correlation.
Instead, we will only look at the difference between the pattern's modeled density and that of each project.
Therefore, we will simply interpolate linearly between the projects' commits.

As for the pattern, the curve will be smooth and needs to be redesigned.
Note that the Fire Drill description says the following: "only analytical or documentational artefacts for a long time". The current pattern does not account for that.
Instead, the current pattern pretty much reduces all activities and the `SCD` during what we called the "Long Stretch".
While we mostly stick with how the activities were modeled, the `SCD`, however, requires a more accommodating redesign.
It should pick up pace almost immediately, followed by a slight increase until the Fire Drill starts.
Then, we should see a rapid increase that peaks at $1$ and a normalization after the Fire Drill ends (go back to previous level).
We also modify the other activities and add a slight slope to them, but these are rather cosmetic changes.


## Importing the Raw SVG Points

Please note that after sketching the SVG, we rasterize it to points using a tool called "PathToPoints"[^1].
In the CSV, we will find four different data series (attention: of different length), one corresponding to each activity/metric.
Since `FREQ` will be a mixture, we are not modelling it explicitly.


Paths are associated with colors: Coral is `SCD`, Gold is `A`, and Green is `CP`.

```{r}
temp <- read.csv(file = "../data/Fire-Drill_second-guess.csv", header = TRUE, sep = ";")
assocs <- c("coral" = "SCD", "gold" = "A", "green" = "CP")
assoc_inv <- names(assocs)
names(assoc_inv) <- unname(assocs)

# Y's are negative.
for (assoc in names(assocs)) {
  temp[, paste0(assoc, "_y")] <- -1 * temp[, paste0(assoc, "_y")]
}

# Next, we gotta normalize them all together, so we have to determine the y-extent.
y_ext <- range(c(temp$coral_y, temp$gold_y, temp$green_y), na.rm = TRUE)
for (assoc in names(assocs)) {
  temp[, paste0(assoc, "_y")] <- (temp[, paste0(assoc, "_y")] - y_ext[1]) / (y_ext[2] - y_ext[1])
}
```


```{r}
templ <- list()

for (assoc in names(assocs)) {
  data <- temp[, paste0(assoc, c("_x", "_y"))]
  data <- data[stats::complete.cases(data),]
  tempf <- stats::approxfun(x = data[, paste0(assoc, "_x")], y = data[, paste0(assoc, "_y")], yleft = 0, yright = 0)
  # Let's sample from that function linearly
  use_y <- tempf(seq(from = min(data[, paste0(assoc, "_x")]), to = max(data[, paste0(assoc, "_x")]), length.out = 1e3))
  templ[[assocs[assoc]]] <- stats::approxfun(x = seq(from = 0, to = 1, length.out = 1e3), y = use_y, yleft = use_y[1], yright = use_y[2])
}
```


Now we have a list with with `A`, `CP`, and `SCD`.
Next, we will do the rejection sampling from the former two in order to create `FREQ`.
As for the mixture ratio, we will assume that the __adaptive activities make up for $40$%__ of all activities.
So the mixture will be $0.4/0.6$.

```{r}
use_x <- seq(from = 0, to = 1, length.out = 1e7)


use_y <- stats::runif(n = length(use_x), min = 0, max = max(temp[[paste0(assoc_inv["A"], "_y")]], na.rm = TRUE))
tempdens_A <- stats::density(x = use_x[use_y <= templ$A(use_x)], bw = "SJ", cut = TRUE)
tempf_A <- stats::approxfun(x = tempdens_A$x, y = tempdens_A$y, yleft = 0, yright = 0)
tempecdf_A <- make_smooth_ecdf(values = use_x[use_y <= templ$A(use_x)], slope = 0, inverse = FALSE)

use_y <- stats::runif(n = length(use_x), min = 0, max = max(temp[[paste0(assoc_inv["CP"], "_y")]], na.rm = TRUE))
tempdens_CP <- stats::density(x = use_x[use_y <= templ$CP(use_x)], bw = "SJ", cut = TRUE)
tempf_CP <- stats::approxfun(x = tempdens_CP$x, y = tempdens_CP$y, yleft = 0, yright = 0)
tempecdf_CP <- make_smooth_ecdf(values = use_x[use_y <= templ$CP(use_x)], slope = 0, inverse = FALSE)

c(
  cubature::cubintegrate(f = tempf_A, 0, 1)$integral,
  cubature::cubintegrate(f = tempf_CP, 0, 1)$integral) # Those should be very close to 1

pattern_sc <- list(
  SCD = templ$SCD,
  A = tempf_A,
  CP = tempf_CP,
  FREQ = function(x) .4 * tempf_A(x) + .6 * tempf_CP(x))

cubature::cubintegrate(f = pattern_sc$FREQ, 0, 1)$integral
```


```{r}
# We'll need it in the next step.
use_y <- stats::runif(n = length(use_x), min = 0, max = -1.01 * nloptr::nloptr(
  x0 = .5, lb = 0, ub = 1,
  eval_f = function(x) -pattern_sc$FREQ(x),
  opts = list(algorithm = "NLOPT_GN_DIRECT_L_RAND", xtol_rel=1e-3, maxeval=1e5))$objective)
tempdens_FREQ <- stats::density(x = use_x[use_y <= pattern_sc$FREQ(use_x)], bw = "SJ", cut = TRUE)
tempf_FREQ <- stats::approxfun(x = tempdens_FREQ$x, y = tempdens_FREQ$y, yleft = 0, yright = 0)
tempecdf_FREQ <- make_smooth_ecdf(values = use_x[use_y <= pattern_sc$FREQ(use_x)], slope = 0, inverse = FALSE)

cubature::cubintegrate(f = tempf_FREQ, 0, 1)$integral
```

Now it's time to print the new pattern for Source Code!
Note that in figure \ref{fig:fd-sc-2nd-guess} the curves for the variables `A`, `CP`, and `FREQ` are now all proper probability densities (i.e., each of them integrates to $1$).
The variable `SCD` is not a density and peaks at $1$, because that is the maximum value for the source code density (it is ratio, actually).

```{r fd-sc-2nd-guess, echo=FALSE, fig.height=6, fig.cap="The second guess of the Fire Drill anti-pattern in source code.", fig.align="top", fig.pos="ht!"}
curve2(pattern_sc$A, 0, 1, lwd = 2, col = "red")
curve2(pattern_sc$CP, 0, 1, lwd = 2, col = "#00bb00", add = TRUE)
curve2(pattern_sc$FREQ, 0, 1, lwd = 2, col = "magenta", add = TRUE)
curve2(pattern_sc$SCD, 0, 1, lwd = 2, col = "purple", add = TRUE)
grid()
legend(x = 0, y = 3, legend = c("A", "CP", "FREQ", "SCD"), col = c("red", "#00bb00", "magenta", "purple"), lty = 1, lwd = 2)
```


I have to admit this pattern looks quite nice. Let's keep it and produce an _additional_ version using CDFs (we will keep and test both).

Now, the last step is to transform the three maintenance activities into __cumulative__ densities.
We will perform rejection sampling from these KDE-PDFs and estimate the ECDFs.

```{r}
pattern_sc_cdf <- append(pattern_sc, list())

temp_x <- seq(from = 0, to = 1, length.out = 5e3)

for (vname in names(pattern_sc_cdf)) {
  if (vname == "SCD") {
    next # Not this one..
  }
  
  # Let's replace the pattern's activities with approximate ECDFs:
  pattern_sc_cdf[[vname]] <- stats::approxfun(x = temp_x, y = get(paste0("tempecdf_", vname))(temp_x), yleft = 0, yright = 1)
}
```


In figure \ref{fig:fd-sc-2nd-guess-ecdf} we show the final CDF-version of the improved pattern for source code.
The three activities `A`, `CP`, and `FREQ` have now been converted to cumulative densities (no change to `SCD`).


```{r fd-sc-2nd-guess-ecdf, echo=FALSE, fig.height=6, fig.cap="The second guess of the Fire Drill anti-pattern in source code, where the maintenance activities have been converted to cumulative densities.", fig.align="top", fig.pos="ht!"}
curve2(pattern_sc_cdf$A, 0, 1, lwd = 2, col = "red")
curve2(pattern_sc_cdf$CP, 0, 1, lwd = 2, col = "#00bb00", add = TRUE)
curve2(pattern_sc_cdf$FREQ, 0, 1, lwd = 2, col = "magenta", add = TRUE)
curve2(pattern_sc_cdf$SCD, 0, 1, lwd = 2, col = "purple", add = TRUE)
grid()
legend(x = .65, y = .3, legend = c("A (CDF)", "CP (CDF)", "FREQ (CDF)", "SCD"), col = c("red", "#00bb00", "magenta", "purple"), lty = 1, lwd = 2)
```



[^1]: https://github.com/sse-lnu/PathToPoints



# Importing and preparing the Data

We have a total of $15$ projects and a ground truth for each.

## Load the Ground Truth

Here, we will simply join the two batches.

```{r}
ground_truth <- rbind(
  read.csv(file = "../data/ground-truth.csv", sep = ";"),
  read.csv(file = "../data/ground-truth_2nd-batch.csv", sep = ";"))
```


## Importing the Issue-Tracking Pattern

Here we load the issue-tracking pattern. Note that there is only one and we will not be using any variations.

```{r}
pattern_it <- readRDS(file = "../results/pattern_I_it.rds")
```



## Importing the Project Data for Source Code

We cannot just import the projects as they were pre-processed previously for source code data.
Instead, we will load the raw data and transform the projects similar to how we produced the patterns.
Also, for source code, there will be two sets of projects: an ordinary one and a CDF-transformed one.


Let's first import and prepare the source code data:

```{r}
temp <- rbind(
  read.csv(file = "../data/student-projects.csv", sep = ";"),
  read.csv(file = "../data/student-projects_2nd-batch.csv", sep = ";"))
temp$label <- factor(x = toupper(temp$label), levels = sort(toupper(unique(temp$label))))
temp$project <- factor(x = temp$project, levels = sort(unique(temp$project)))
temp$AuthorTimeNormalized <- NA_real_

for (pId in levels(temp$project)) {
  temp[temp$project == pId, ]$AuthorTimeNormalized <-
    (temp[temp$project == pId, ]$AuthorTimeUnixEpochMilliSecs -
      min(temp[temp$project == pId, ]$AuthorTimeUnixEpochMilliSecs))
  temp[temp$project == pId, ]$AuthorTimeNormalized <-
    (temp[temp$project == pId, ]$AuthorTimeNormalized /
      max(temp[temp$project == pId, ]$AuthorTimeNormalized))
}
```


```{r}
projects_sc <- list()
projects_sc_cdf <- list()


get_densities <- function(data, x_samples = 1e4) {
  suppressWarnings({
    use_x <- seq(from = 0, to = 1, length.out = x_samples)
    tempdens <- stats::density(x = data, bw = "SJ", cut = TRUE)
    tempdens_f <- stats::approxfun(x = tempdens$x, y = tempdens$y, yleft = 0, yright = 0)
    use_y <- stats::runif(n = x_samples, min = 0, max = max(tempdens$y))
    
    temp_samples <- use_x[use_y <= tempdens_f(use_x)]
    tempdens <- stats::density(x = temp_samples, bw = "SJ", cut = TRUE)
    
    list(
      PDF = stats::approxfun(x = tempdens$x, y = tempdens$y, yleft = 0, yright = 0),
      CDF = make_smooth_ecdf(values = temp_samples, slope = 0, inverse = FALSE))
  })
}

make_mixture <- function(pdf1, pdf2, pdf1_ratio, x_samples = 1e4) {
  use_x <- seq(from = 0, to = 1, length.out = x_samples)
  tempf <- function(x) pdf1_ratio * pdf1(x) + (1 - pdf1_ratio) * pdf2(x)
  
  use_y <- stats::runif(n = length(use_x), min = 0, max = -1.01 * nloptr::nloptr(
    x0 = .5, lb = 0, ub = 1,
    eval_f = function(x) -tempf(x),
    opts = list(algorithm = "NLOPT_GN_DIRECT_L_RAND", xtol_rel=1e-3, maxeval=1e5))$objective)
  
  list(
    PDF = tempf,
    CDF = make_smooth_ecdf(values = use_x[use_y <= tempf(use_x)], slope = 0, inverse = FALSE))
}


for (lvl in levels(temp$project)) {
  df <- temp[temp$project == lvl,]
  templ <- list()
  templ_cdf <- list()
  
  temp_A <- get_densities(data = df[df$label == "A",]$AuthorTimeNormalized)
  temp_CP <- get_densities(data = df[df$label %in% c("C", "P"),]$AuthorTimeNormalized)
  acp_ratio <- nrow(df[df$label == "A",]) / nrow(df)
  temp_FREQ <- make_mixture(pdf1 = temp_A$PDF, pdf2 = temp_CP$PDF, pdf1_ratio = acp_ratio)
  
  templ[["A"]] <- temp_A$PDF
  templ[["CP"]] <- temp_CP$PDF
  templ[["FREQ"]] <- temp_FREQ$PDF
  
  templ_cdf[["A"]] <- temp_A$CDF
  templ_cdf[["CP"]] <- temp_CP$CDF
  templ_cdf[["FREQ"]] <- temp_FREQ$CDF
  
  templ[["SCD"]] <- suppressWarnings({
    stats::approxfun(x = df$AuthorTimeNormalized, y = df$Density, rule = 2)
  })
  templ_cdf[["SCD"]] <- templ[["SCD"]] # it's the same because there is no CDF of it
  
  projects_sc[[lvl]] <- templ
  projects_sc_cdf[[lvl]] <- templ_cdf
}
```

In figure \ref{fig:project-vars-new} we show the $15$ projects with the new transform applied.

```{r project-vars-new, echo=FALSE, fig.width=8, fig.height=8, fig.cap="The projects in source code with the new transform applied. It is obvious that the densities are now exceeding $1.0$ since they are no longer normalized.", fig.align="top", fig.pos="ht!"}
par(mfrow = c(4, 4), oma = rep(0, 4), mar = c(3, 2, 2, 1))

for (proj in gtools::mixedsort(levels(temp$project))) {
  use_y <- max(c(
    projects_sc[[proj]]$A(seq(from = 0, to = 1, length.out = 100)),
    projects_sc[[proj]]$CP(seq(from = 0, to = 1, length.out = 100)),
    projects_sc[[proj]]$FREQ(seq(from = 0, to = 1, length.out = 100))
  ))
  curve2(projects_sc[[proj]]$A, 0, 1, col = "red", ylim = c(0, use_y), main = proj)
  curve2(projects_sc[[proj]]$CP, 0, 1, col = "#00bb00", ylim = c(0, use_y), add = TRUE)
  curve2(projects_sc[[proj]]$FREQ, 0, 1, col = "magenta", ylim = c(0, use_y), add = TRUE)
  curve2(projects_sc[[proj]]$SCD, 0, 1, col = "purple", ylim = c(0, use_y), add = TRUE)
  grid()
}
```


## Importing the Project Data for Issue-tracking

This case is more straightforward, as we do not change the way this was done before.
Also, there will only be one version, not two like we have for source code.

```{r}
projects_it <- list()
projects_it <- append(projects_it, readRDS(file = "../results/project_signals_it.rds"))
projects_it <- append(projects_it, readRDS(file = "../results/project_signals_2nd_batch_it.rds"))
```


# Creating the Datasets

For each pattern, we will create a dataset that contains the deviation for each project against each pattern.
Since we have three patterns, we will have three datasets.

Since the patterns and projects are modeled as curves, we will compute segment-wise features. For that, each project/pattern is subdivided into ten equally long intervals. Then, a distance metric is compute for each interval and activity.
We will compute **two metrics**: The area between curves and the $2$-dimensional relative continuous Pearson sample correlation coefficient (see section \ref{ssec:m-dim-pearson}).
This means that we will get a total of $2\times 10\times 4=80$ features for source code patterns and $60$ for issue-tracking patterns (because IT has only three activities/variables).
This means that we will get a lot more features than data points.
Also, our data is not balanced. For these reasons, we will have to use oversampling and synthetically inflate our dataset.
Note that this is OK for the intended purpose of finding an upper bound of required training instances for obtaining robust regression models.
In practice, it would of course be better just to label additional instances.


## Function for computing a Distance

The following function will be used to compute the distance metrics for area between curves and correlation.

```{r}
compute_distance <- function(f_pattern, f_project, metric = c("area", "corr"), interval = 1:10, num_samples = 1e3) {
  metric <- match.arg(metric)
  
  interval_ext <- c(interval / 10 - .1, interval / 10)
  use_x <- seq(from = interval_ext[1], to = interval_ext[2], length.out = num_samples)
  
  v1 <- f_pattern(use_x)
  v2 <- f_project(use_x)
  
  if (metric == "area") {
    # It's the same as MAE for large samples:
    return(Metrics::mae(actual = v1, predicted = v2))
  }
  temp <- suppressWarnings({
    stats::cor(x = v1, y = v2, method = "pearson")
  })
  if (is.na(temp)) 0 else temp
}
```


## Create Datasets for Source Code

Here we will create both datasets (ordinary and CDF) in the same loop.

```{r}
grid <- expand.grid(list(
  distance = c("area", "corr"),
  interval = 1:10,
  activity = c("A", "CP", "FREQ", "SCD")
))
grid$distance <- as.character(grid$distance)
grid$activity <- as.character(grid$activity)

dataset_sc <- `colnames<-`(x = matrix(nrow = 0, ncol = nrow(grid)), value = sapply(X = rownames(grid), FUN = function(rn) {
  r <- grid[rn,]
  paste(r$activity, r$interval, r$distance, sep = "_")
}))
dataset_sc_cdf <- dataset_sc[,]


for (pname in names(projects_sc)) {
  newrow_sc <- `colnames<-`(x = matrix(ncol = ncol(dataset_sc)), value = colnames(dataset_sc))
  newrow_sc_cdf <- `colnames<-`(x = matrix(ncol = ncol(dataset_sc)), value = colnames(dataset_sc))
  
  for (rn in rownames(grid)) {
    row <- grid[rn,]
    tempf_pattern <- pattern_sc[[row$activity]]
    tempf_pattern_cdf <- pattern_sc_cdf[[row$activity]]
    tempf_project <- projects_sc[[pname]][[row$activity]]
    tempf_project_cdf <- projects_sc_cdf[[pname]][[row$activity]]
    
    feat_name <- paste(row$activity, row$interval, row$distance, sep = "_")
    newrow_sc[1, feat_name] <- compute_distance(
      f_pattern = tempf_pattern, f_project = tempf_project, metric = row$distance, interval = row$interval)
    newrow_sc_cdf[1, feat_name] <- compute_distance(
      f_pattern = tempf_pattern_cdf, f_project = tempf_project_cdf, metric = row$distance, interval = row$interval)
  }
  
  dataset_sc <- rbind(dataset_sc, newrow_sc)
  dataset_sc_cdf <- rbind(dataset_sc_cdf, newrow_sc_cdf)
}

dataset_sc <- as.data.frame(dataset_sc)
dataset_sc_cdf <- as.data.frame(dataset_sc_cdf)
```


## Create Dataset for Issue-tracking

This is very similar to how we created the datasets for source code, mainly the activities will differ and there is only one version.

```{r}
grid <- expand.grid(list(
  distance = c("area", "corr"),
  interval = 1:10,
  activity = c("REQ", "DEV", "DESC")
))
grid$distance <- as.character(grid$distance)
grid$activity <- as.character(grid$activity)

dataset_it <- `colnames<-`(x = matrix(nrow = 0, ncol = nrow(grid)), value = sapply(X = rownames(grid), FUN = function(rn) {
  r <- grid[rn,]
  paste(r$activity, r$interval, r$distance, sep = "_")
}))

for (pname in names(projects_it)) {
  newrow_it <- `colnames<-`(x = matrix(ncol = ncol(dataset_it)), value = colnames(dataset_it))
  
  for (rn in rownames(grid)) {
    row <- grid[rn,]
    tempf_pattern <- pattern_it[[row$activity]]
    tempf_project <- projects_it[[pname]][[row$activity]]$get0Function()
    
    feat_name <- paste(row$activity, row$interval, row$distance, sep = "_")
    newrow_it[1, feat_name] <- compute_distance(
      f_pattern = tempf_pattern, f_project = tempf_project, metric = row$distance, interval = row$interval)
  }
  
  dataset_it <- rbind(dataset_it, newrow_it)
}

dataset_it <- as.data.frame(dataset_it)
```



# Adaptive Training For Robust Regression Models

In this section, we will train a few state-of-the-art models _adaptively_, which means that we will keep adding labeled training data in order to hopefully observe some convergence towards a generalization error on validation data that will allow us to estimate the minimum amount of training data required to achieve a certain error.
We will repeat fitting the same model many times, using repeated $k$-fold cross validation and bootstrapping, in order to obtain reliable results.
As for the models, it has been shown that the Random forest is the average top performer [@delgado14], so we will pick it.
Furthermore, we will attempt to fit these models: GLM (generalized linear model), GBM (gradient boosting machine), XGB (extreme gradient boosting), SVM-Poly (SVM with polynomial kernel), avNNET (model averaged neural network), rPART2 (recursive partitioning of rules).


## Preparation of the Datasets

Our datasets are problematic in that we have only few instances but many features. Also, we do not have an example for every possible ground truth.
For example, we have no project with a score of `4` or `7` (see figure \ref{fig:gt-label-hist}).
Through a number of measures, however, we can alleviate all these problems to a satisfactory degree.


The first we will do is to synthetically oversample our dataset using SMOTE for regression [@torgo13, @branco16].
SMOTE is a well-established technique that has been proven to significantly increase model performance.
Since our label is numeric (we do regression), we will have to oversample instances such that new data points are generated that can be imputed and fill up the numeric labels.
While ordinary SMOTE cannot oversample unseen classes, the technique works differently for regression and allows us to generate synthetic samples with a previously unseen numeric label.
To reduce the amount of features, we will eliminate those with zero or near-zero variance.
Then, optionally, we further reduce the feature space's dimensionality by applying a principal component analysis (PCA).


```{r gt-label-hist, echo=FALSE, fig.cap="Histogram of the ground truth consensus for all 15 projects.", fig.align="top", fig.pos="ht!"}
hist(ground_truth$consensus, breaks = -1:10)
```

```{r}
#' This function creates new instances for a given numeric label.
oversample_y <- function(dataset, num, lab) {
  new_ds <- NULL
  i <- 0
  while (TRUE) {
    set.seed(i)
    i <- i + 1
    temp <- UBL::SmoteRegress(form = gt~., dat = dataset, thr.rel = 1/15)
    temp$gt <- round(temp$gt)
    temp <- temp[temp$gt == lab,]
    if (nrow(temp) > 0) {
      new_ds <- rbind(new_ds, head(temp, 1))
    }
    if (is.data.frame(new_ds) && nrow(new_ds) == num) {
      break
    }
  }
  new_ds
}
```


We will also create another short function that will balance all numeric labels in a given dataset.

```{r}
balance_num_labels <- function(dataset, num = 10) {
  new_ds <- NULL
  
  for (i in 0:10) { # All possible numeric labels
    has_num_rows <- nrow(dataset[dataset$gt == i,])
    req_rows <- num - has_num_rows
    if (req_rows > 0) {
      new_ds <- suppressWarnings({
        rbind(new_ds, oversample_y(dataset = dataset, num = req_rows, lab = i))
      })
    }
  }
  
  if (!is.null(new_ds)) rbind(dataset, new_ds) else dataset
}
```


And here we create the oversampled datasets.

```{r}
min_rows <- 15 # Number of rows we want to end up with per numeric label

dataset_sc_oversampled <- cbind(dataset_sc, data.frame(gt = ground_truth$consensus))
dataset_sc_oversampled <- balance_num_labels(dataset = dataset_sc_oversampled, num = min_rows)

dataset_sc_cdf_oversampled <- cbind(dataset_sc_cdf, data.frame(gt = ground_truth$consensus))
dataset_sc_cdf_oversampled <- balance_num_labels(dataset = dataset_sc_cdf_oversampled, num = min_rows)

dataset_it_oversampled <- cbind(dataset_it, data.frame(gt = ground_truth$consensus))
dataset_it_oversampled <- balance_num_labels(dataset = dataset_it_oversampled, num = min_rows)
```


So, as we see in the following overview for all three datasets, we have the same amount of instances for each numeric label ($0$ through $10$):

```{r}
rbind(
  table(dataset_sc_oversampled$gt),
  table(dataset_sc_cdf_oversampled$gt),
  table(dataset_it_oversampled$gt))
```


## Adaptive Training

We define a function that can train one of the chosen models adaptively (using an outer grid search).
This function mainly handles models that are trainable with caret, but there are two exceptions:
When the selected model is `gbm`, we use a custom grid to accommodate small and very small sample sizes, as the default grid warrants for much larger training sets.
When the selected model is `rankModel`, we will actually test a new model of our own that uses quantile normalization of the inputs and an inverse rank transform of the outputs, with some non-linearity in between[^2].
This model has been proven efficient with very small sample sizes.

[^2]: See the Rank Model repository at <https://github.com/MrShoenel/R-rank-model>.


```{r}
adaptive_training_caret <- function(
    org_dataset,
    seeds, # number of seeds equals number of repeats
    model_type = c("avNNet", "gbm", "glm", "M5", "rankModel", "rf", "svmPoly", "treebag", "xgbTree"),
    num_train = 5,
    num_valid = 50,
    do_pca = TRUE,
    return_num_comps = NULL
) {
  model_type <- match.arg(model_type)
  
  cn <- colnames(org_dataset)
  cn_x <- cn[cn != "gt"]
  pre_proc_method <- c("nzv")
  if (do_pca) {
    pre_proc_method <- c("pca", pre_proc_method)
  }
  pre_proc_method <- c(pre_proc_method, "center", "scale")
  
  res <- NULL

  
  for (seed in seeds) {
    set.seed(seed)
    idx <- sample(x = rownames(org_dataset), size = nrow(org_dataset))
    
    df_train <- org_dataset[idx[1:num_train],]
    # Fit the pre-processor on the training data, then...
    pre_proc <- caret::preProcess(x = df_train[, cn_x], method = pre_proc_method, thresh = 0.975)
    
    # Transform training AND validation data with it:
    df_train <- stats::predict(pre_proc, newdata = df_train)
    df_valid <- org_dataset[idx[(num_train + 1):(num_train + num_valid)],]
    df_valid <- stats::predict(pre_proc, newdata = df_valid)
    
    # Colnames change to PC1, PC2, ..., if PCA, otherwise they will be a subset
    # of the original column names (because of (N)ZV):
    use_cn_x <- if (do_pca) paste0("PC", 1:(ncol(df_train) - 1)) else colnames(df_train)
    
    if (!is.null(return_num_comps) && return_num_comps) {
      if (!do_pca) {
        stop("Can only return number of components if doing PCA!")
      }
      return(length(use_cn_x))
    }
    
    pred_train <- c()
    pred_valid <- c()
    tryCatch({
      if (model_type == "rankModel") {
        # We're gonna have a small grid, so that each model is fit a few times.
        use_grid <- expand.grid(list(
          cdf_type = c("auto", "gauss", "ecdf"),
          attempt = 1:5 # This is basically a dummy.
        ))
        use_grid$cdf_type <- as.character(use_grid$cdf_type)
        
        best_rm <- 1e30
        for (rn in rownames(use_grid)) {
          row <- use_grid[rn,]
          temp1 <- create_model(df_train = df_train, x_cols = use_cn_x, y_col = "gt", cdf_type = row$cdf_type)
          tempf <- function(x) model_loss(model = temp1, x = x, df = df_train, y_col = "gt")
          
          # Let's fit the model!
          num_params <- 2 + 3 * length(use_cn_x)
          opt_res <- nloptr::nloptr(
            x0 = runif(n = num_params),
            eval_f = tempf,
            eval_grad_f = function(x) pracma::grad(f = tempf, x0 = x),
            lb = rep(-1e3, num_params),
            ub = rep( 1e3, num_params),
            opts = list(algorithm = "NLOPT_LD_TNEWTON", xtol_rel=5e-4, maxeval=250))
          
          temp_pred_valid <- temp1(x = opt_res$solution, df = df_valid)
          temp_best_rm <- Metrics::rmse(actual = df_valid$gt, predicted = temp_pred_valid)
          if (temp_best_rm < best_rm) {
            best_rm <- temp_best_rm
            pred_valid <- temp_pred_valid
            pred_train <- temp1(x = opt_res$solution, df = df_train)
          }
        }
      } else if (model_type == "gbm") {
        use_grid <- expand.grid(list(
          n.trees = c(2:10, seq(from = 12, to = 30, by = 2), seq(from = 35, to = 80, by = 5), 90, 100),
          n.minobsinnode = 1:10,
          bag.fraction = seq(from = 0.1, to = 0.7, by = 0.2)
        ))
        
        best_gbm <- 1e30
        for (rn in rownames(use_grid)) {
          row <- use_grid[rn,]
          tryCatch({
            temp1 <- gbm::gbm(
              formula = gt~., distribution = "gaussian", data = df_train,
              n.trees = row$n.trees, n.minobsinnode = row$n.minobsinnode)
            
            temp_pred_valid <- stats::predict(temp1, df_valid)
            temp_best_gbm <- Metrics::rmse(actual = df_valid$gt, predicted = temp_pred_valid)
            if (temp_best_gbm < best_gbm) {
              best_gbm <- temp_best_gbm
              pred_valid <- temp_pred_valid
              pred_train <- stats::predict(temp1, df_train)
            }
          }, error = function(cond) {
            # Do nothing
          })
          
          if (best_gbm >= 1e30) {
            stop("GBM fit not possible.")
          }
        }
      } else {
        temp1 <- caret::train(
          x = df_train[, use_cn_x], y = df_train[, "gt"], method = model_type,
          trControl = caret::trainControl(method = "LOOCV", number = 10))
        pred_train <- kernlab::predict(temp1, df_train[, use_cn_x])
        pred_valid <- kernlab::predict(temp1, df_valid[, use_cn_x])
      }
    }, error = function(cond) {
      pred_train <- rep(NA_real_, nrow(df_train))
      pred_valid <- rep(NA_real_, nrow(df_valid))
    })
    

    res <- rbind(res, data.frame(
      seed = seed,
      do_pca = do_pca,
      num_train = num_train,
      num_valid = num_valid,
      model_type = model_type,
      num_comps = if (do_pca) length(use_cn_x) else NA_real_,
      
      mae_train = Metrics::mae(actual = df_train$gt, predicted = pred_train),
      mae_valid = Metrics::mae(actual = df_valid$gt, predicted = pred_valid),
      rmse_train = Metrics::rmse(actual = df_train$gt, predicted = pred_train),
      rmse_valid = Metrics::rmse(actual = df_valid$gt, predicted = pred_valid)
    ))
  }
  
  res
}
```


And here we compute the grid (test each model on each dataset).
Attention: The following is expensive.

```{r}
grid <- expand.grid(list(
  seed = 1:25,
  num_train = 2:50,
  do_pca = c(TRUE, FALSE)
))
model_types <- c("avNNet", "gbm", "glm", "M5", "rf", "svmPoly", "treebag", "rankModel")
use_datasets <- c("dataset_it_oversampled", "dataset_sc_oversampled", "dataset_sc_cdf_oversampled")


library(foreach)
res_grid <- loadResultsOrCompute(file = "../results/rob-reg_res-grid.rds", computeExpr = {
  res <- NULL
  
  for (use_dataset in use_datasets) {
    dataset <- get(use_dataset)
    
    for (model_type in model_types) {
      
      the_file <- paste0("../results/rob-reg_res-grid_", model_type, "_", use_dataset, ".rds")
      temp <- as.data.frame(loadResultsOrCompute(file = the_file, computeExpr = {
        temp1 <- doWithParallelCluster(numCores = min(parallel::detectCores(), 123), expr = {
          foreach::foreach(
            rn = rownames(grid),
            .combine = rbind,
            .inorder = FALSE,
            .verbose = TRUE
          ) %dopar% {
            tryCatch({
              row <- grid[rn,]
              adaptive_training_caret(
                org_dataset = dataset, seeds = row$seed, model_type = model_type,
                num_train = row$num_train, do_pca = row$do_pca)
            }, error = function(cond) {
              NULL # Return an empty result that will not disturb .combine
            })
          }
        })
        saveRDS(object = temp1, file = the_file)
        temp1
      }))
      temp$dataset <- rep(use_dataset, nrow(temp))
      
      res <- rbind(res, temp)
    }
  }

  res$dataset <- factor(res$dataset)
  res
})
```


Let's show some results for the Gradient Boosting Machine (figure \ref{fig:rob-reg-gbm1}).
It appears that the GBM needs at least $7$ training examples to work at all.
With $\approx25$ training examples, the expected generalization error (using the RMSE) is $\approx1$, and with about $15$ examples it is $\approx1.5$, both of which might be acceptable in a real-world scenario.


```{r rob-reg-gbm1, echo=FALSE, fig.cap="Robust regression using a Gradient Boosting Machine, PCA, and the oversampled issue-tracking dataset.", fig.align="top", fig.pos="ht!"}
temp <- res_grid[res_grid$model_type == "gbm" & res_grid$do_pca & res_grid$dataset == "dataset_it_oversampled",]
temp <- temp[stats::complete.cases(temp),]
temp <- temp %>% group_by(num_train) %>% summarize(Mean = mean(rmse_valid), Max = max(rmse_valid), Min = min(min(rmse_valid)), .groups = "drop")

use_ylim <- c(0, max(c(temp$Mean, temp$Max, temp$Min)))
plot(x = temp$num_train, y = temp$Mean, type="l", xlab = "Number of training instances", ylab = "RMSE (validation data)", xlim = c(5, 50), ylim = use_ylim)
grid()
lines(x = temp$num_train, y = temp$Max, col="blue")
lines(x = temp$num_train, y = temp$Min, col="red")
legend(x = 32, y = 4.25, legend = c("mean", "max", "min"), col = c("black", "blue", "red"), lwd = 2)
```

In figure \ref{fig:rob-reg-gbm2} we show the distribution of validation errors depending on the number of training instances (same setup as in figure \ref{fig:rob-reg-gbm1}).


```{r rob-reg-gbm2, echo=FALSE, message=FALSE, fig.height=8, fig.width=6, fig.cap="Ridge plot of distributions of RMSEs on validation data, dependent on the number of training samples.", fig.align="top", fig.pos="ht!"}
temp <- res_grid[res_grid$model_type == "gbm" & res_grid$do_pca & res_grid$dataset == "dataset_it_oversampled",]
temp <- temp[stats::complete.cases(temp),]
temp1 <- temp[order(temp$num_train),]
temp1 <- data.frame(
  num_train = factor(x = as.character(temp1$num_train), levels = as.character(rev(sort(unique(temp1$num_train)))), ordered = TRUE),
  rmse_valid = temp1$rmse_valid)

# https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html
ggplot(temp1, aes(x = rmse_valid, y = num_train, fill = ..x..)) +
  #stat_density_ridges(quantile_lines = TRUE) +
  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01) +
  scale_fill_viridis_c(option = "C") +
  theme_ridges() + 
  theme(legend.position = "none") +
  xlab("RMSE (validation data)") + ylab("Number of training instances")
```


We will also extract the number of components in case PCA was applied.

```{r}
# Do a PCA-only grid:
grid <- expand.grid(list(
  seed = 1:25,
  num_train = 2:50,
  use_dataset = use_datasets
))
grid$use_dataset <- as.character(grid$use_dataset)

res_grid_comps <- loadResultsOrCompute(file = "../results/rob-reg_res-grid_comps.rds", computeExpr = {
  cl <- parallel::makePSOCKcluster(min(32, parallel::detectCores()))
  parallel::clusterExport(cl = cl, varlist = use_datasets)
  
  doWithParallelClusterExplicit(cl = cl, stopCl = TRUE, expr = {
    foreach::foreach(
      rn = rownames(grid),
      .combine = rbind,
      .inorder = FALSE
    ) %dopar% {
      row <- grid[rn,]
      row$num_comps <- tryCatch({
        adaptive_training_caret(
          org_dataset = get(row$use_dataset), seeds = row$seed, model_type = model_types[1], # irrelevant
          num_train = row$num_train, do_pca = TRUE, return_num_comps = TRUE)
      }, error = function(cond) NA_real_)
      row
    }
  })
})
table(res_grid_comps$num_comps)
```

Above we see a table with how often a certain number of components was used, and in figure \ref{fig:num-comps-hist} it is illustrated in a histogram.

```{r num-comps-hist, echo=FALSE, fig.cap="Histogram over the number of components used in the grid search (threshold=0.975)."}
hist(res_grid_comps$num_comps, xlab = "Number of PCA components", main = "PCA components with threshold=0.975.")
```



## Analysis

Here we are going to analyze some of the results that we got (e.g., best pattern or model, expectations, lower/upper bounds, etc.).


### PCA vs. non-PCA

Here we want to find out whether applying principal components analysis is useful or not.

```{r echo=FALSE, warning=FALSE}
temp <- res_grid %>% group_by(do_pca) %>% summarise(
  RMSE_valid_mean = mean(rmse_valid, na.rm = TRUE),
  RMSE_valid_median = median(rmse_valid, na.rm = TRUE),
  RMSE_valid_min = min(rmse_valid, na.rm = TRUE),
  RMSE_valid_max = max(rmse_valid, na.rm = TRUE),
  .groups = "drop")


if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Mean, median, min, and max of the validation error, grouped by conditionally applying PCA.",
    label = "pca-comps-table"
  )
}
```

Let's also show the distributions. In figure \ref{fig:pca-comps-ridge} we observe that applying PCA results in more smaller errors which are also more steadily distributed.
While the lowest possible validation error as of above table \ref{tab:pca-comps-table} is $0$ for when not using PCA, this is likely due to the models overfitting, since without PCA we end up with many features and some models introduce extra degrees of freedom for each feature.
We shall therefore conclude that applying PCA is favorable.


```{r pca-comps-ridge, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Distributions of the validation error conditionally applying PCA."}
temp <- res_grid[!is.na(res_grid$rmse_valid),]
temp1 <- rbind(
  data.frame(
    do_pca = TRUE,
    rmse = (temp %>% filter(do_pca & rmse_valid <= quantile((temp %>% filter(do_pca))$rmse_valid, probs = .99)))$rmse_valid),
  data_frame(
    do_pca = FALSE,
    rmse = (temp %>% filter(!do_pca & rmse_valid <= quantile((temp %>% filter(!do_pca))$rmse_valid, probs = .99)))$rmse_valid)
)


ggplot(temp1, aes(x = rmse, y = do_pca, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 1, rel_min_height = 0.01) +
  scale_fill_viridis_c(option = "C") +
  theme_ridges() + 
  theme(legend.position = "none") +
  xlab("RMSE (validation data)") + ylab("Apply PCA") + xlim(-.5, 7)
```




Todo:
- SC vs. IT
- PCA vs. non-PCA
- ...

```{r}
res_grid %>% filter(num_train < 10) %>% group_by(model_type, num_train) %>% summarise(RMSE = mean(rmse_valid, na.rm=TRUE)) %>% arrange(num_train, RMSE)
```





# Variable Importance for Assessing the Pattern

Obtaining variable importance allows us to understand which features are more or less important. Since in our case features are tied to a metric (area, correlation), a data kind (source code, issue tracking), an activity/metric (e.g., `REQ`, `SCD`), and a segment (one of ten equally long consecutive project phases), we can facilitate variable importance to learn about any of these (how important they are to predict a precise severity of the Fire Drill's presence).
We could potentially also (alternatively) learn about the quality of the ground truth. However, at this point we must assume that it is perfect as we have no evidence indicating otherwise.


Let's create a function that can give us a proper dataframe from the variable importance.

```{r}
create_variable_importance <- function(dataset) {
  model <- caret::train(
    x = dataset[, colnames(dataset) != "gt"], y = dataset[,]$gt, method = "pls",
    trControl = caret::trainControl(method = "LOOCV", number = 100))
  
  vi <- caret::varImp(model)
  df <- vi$importance
  res <- NULL
  
  for (rn in rownames(df)) {
    sp <- strsplit(x = rn, split = "_")[[1]]
    res <- rbind(res, data.frame(
      variable = sp[1],
      segment = as.numeric(sp[2]),
      metric = sp[3],
      vi = df[rn,]
    ))
  }
  res$vi_rel <- res$vi / sum(res$vi)
  list(res = res[order(-res$vi),], vi = vi)
}
```

Let's compute the variable importance for each of our datasets:

```{r message=FALSE, warning=FALSE}
vi_sc <- create_variable_importance(
  dataset = cbind(dataset_sc, data.frame(gt = ground_truth$consensus)))
vi_sc_cdf <- create_variable_importance(
  dataset = cbind(dataset_sc_cdf, data.frame(gt = ground_truth$consensus)))
vi_it <- create_variable_importance(
  dataset = cbind(dataset_it, data.frame(gt = ground_truth$consensus)))
```

The variable importance for each dataset is shown in figures \ref{fig:varimp-sc}, \ref{fig:varimp-sc-cdf}, and \ref{fig:varimp-it}.

```{r varimp-sc, echo=FALSE, fig.height=7, fig.align="top", fig.pos="ht!", fig.cap="Variable importance for the source code dataset determined by partial least squares and cross validation."}
plot_var_imp(data = vi_sc$vi, relative = TRUE) + labs(title = "Variable Importance: Source Code")
```

```{r varimp-sc-cdf, echo=FALSE, fig.height=7, fig.align="top", fig.pos="ht!", fig.cap="Variable importance for the source code (CDF) dataset determined by partial least squares and cross validation."}
plot_var_imp(data = vi_sc_cdf$vi, relative = TRUE) + labs(title = "Variable Importance: Source Code (CDFs)")
```

```{r varimp-it, echo=FALSE, fig.height=7, fig.align="top", fig.pos="ht!", fig.cap="Variable importance for the issue-tracking dataset determined by partial least squares and cross validation."}
plot_var_imp(data = vi_it$vi, relative = TRUE) + labs(title = "Variable Importance: Issue-tracking")
```


## Analysis of most important Segments, Metrics, Activities, etc.

```{r echo=FALSE, eval=FALSE}
temp <- vi_sc$res %>% group_by(segment) %>% summarise(Mean_vi = sum(vi_rel))
barplot(height = temp$Mean_vi, names.arg = temp$segment)
```

\clearpage

```{r echo=FALSE, fig.width=8, fig.height=9, fig.align="top", fig.pos="ht!"}
par(mfrow = c(3,1))



use_vars <- unique(vi_sc$res$variable)
mat <- matrix(nrow = 4, ncol = 10, dimnames = list(use_vars))

for (v in rownames(mat)) {
  mat[v,] <- as.numeric((vi_sc$res[vi_sc$res$variable == v,] %>% group_by(segment) %>% summarise(total_imp = sum(vi_rel), .groups = "drop"))$total_imp)
}

mat_sums <- apply(X = mat, MARGIN = 2, FUN = sum)

set.seed(3)
pal_4 <- rev(sample(RColorBrewer::brewer.pal(10, "Set3"), 4))
bp <- barplot(height = mat, names.arg = 1:10, xlab = "Segment", ylab = "Total rel. Importance", main = "Source code pattern", col = pal_4, ylim = c(0, max(mat_sums + .015)), angle = 30, density = 90 * mat_sums / max(mat_sums))
grid()
text(x = bp, y = .01 + mat_sums, labels = paste0(round(mat_sums, 4)))
legend("topright", rev(rownames(mat)), fill = rev(pal_4))


mat <- matrix(nrow = 4, ncol = 10, dimnames = list(use_vars))

for (v in rownames(mat)) {
  mat[v,] <- as.numeric((vi_sc_cdf$res[vi_sc_cdf$res$variable == v,] %>% group_by(segment) %>% summarise(total_imp = sum(vi_rel), .groups = "drop"))$total_imp)
}

mat_sums <- apply(X = mat, MARGIN = 2, FUN = sum)
bp <- barplot(height = mat, names.arg = 1:10, xlab = "Segment", ylab = "Total rel. Importance", main = "Source code pattern (CDF)", col = pal_4, ylim = c(0, max(mat_sums + .015)), angle = 30, density = 90 * mat_sums / max(mat_sums))
grid()
text(x = bp, y = .01 + mat_sums, labels = paste0(round(mat_sums, 4)))
legend("topright", rev(rownames(mat)), fill = rev(pal_4))





use_vars <- unique(vi_it$res$variable)
mat <- matrix(nrow = 3, ncol = 10, dimnames = list(use_vars))

for (v in rownames(mat)) {
  mat[v,] <- as.numeric((vi_it$res[vi_it$res$variable == v,] %>% group_by(segment) %>% summarise(total_imp = sum(vi_rel), .groups = "drop"))$total_imp)
}

mat_sums <- apply(X = mat, MARGIN = 2, FUN = sum)

set.seed(2)
pal_3 <- rev(sample(RColorBrewer::brewer.pal(12, "Set3"), 3))
bp <- barplot(height = mat, names.arg = 1:10, xlab = "Segment", ylab = "Total rel. Importance", main = "Issue-tracking pattern", col = pal_3, ylim = c(0, max(mat_sums + .02)), angle = 30, density = 100 * mat_sums / max(mat_sums))
grid()
text(x = bp, y = .01 + mat_sums, labels = paste0(round(mat_sums, 3)))
legend("topleft", rev(rownames(mat)), fill = rev(pal_3))
```

# References {-}

<div id="refs"></div>

