---
title: "Technical Report: Building Robust Regression Models for Scoring the Presence of the Fire Drill Anti-pattern"
author: "Sebastian HÃ¶nel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ../inst/REFERENCES.bib
urlcolor: blue
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  md_document:
    toc: yes
    toc_depth: 6
    df_print: kable
    variant: gfm
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: kable
  word_document: default
header-includes:
- \usepackage{bm}
- \usepackage{mathtools}
- \usepackage{xurl}
---

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\abs}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand{\norm}[1]{\left\lVert\,#1\,\right\rVert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}
\newcommand\argmax[1]{\underset{#1}{arg\,max}}
\newcommand\argmin[1]{\underset{#1}{arg\,min}}


```{r setoptions, echo=FALSE, warning=FALSE, message=FALSE}
Sys.setenv(LANG = "en_US.UTF-8")
Sys.setenv(LC_ALL = "en_US.UTF-8")
Sys.setenv(LC_CTYPE = "en_US.UTF-8")
library(knitr)
opts_chunk$set(tidy = TRUE, tidy.opts = list(indent=2))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

library(ggplot2)
library(ggridges)
library(ggpubr)
library(fields)
library(dplyr)
```

# Introduction\label{tr:fire-drill-rob-reg-technical-report}

This technical report was added in the eighth iteration of the compilation.
So far, we have developed an unsupervised approach for properly scoring the presence of the anti-pattern.
However, here we will attempt to find a robust regression model in a supervised way. In other words, the regression model to be found will be the surrogate scoring mechanism.
The challenge lies in the scarcity of our data, as it has many more features than data points. It is therefore essential to obtain a regression model that, even when trained on only few instances, delivers a low generalization error.


All complementary data and results can be found at Zenodo [@honel_picha_2021]. This notebook was written in a way that it can be run without any additional efforts to reproduce the outputs (using the pre-computed results). This notebook has a canonical URL^[[Link]](https://github.com/MrShoenel/anti-pattern-models/blob/master/notebooks/fire-drill-issue-tracking-technical-report.Rmd)^ and can be read online as a rendered markdown^[[Link]](https://github.com/MrShoenel/anti-pattern-models/blob/master/notebooks/fire-drill-issue-tracking-technical-report.md)^ version. All code can be found in this repository, too.



# Overview of the Approach

We will be importing the two initial process models/patterns for the Fire Drill anti-pattern in source code and issue-tracking data.
These patterns have four and three activities/metrics, respectively.
Both, patterns and projects, are formulated *continuously*, that is, as curves over time. Also, both have been normalized along both, $x$- and $y$-axes (having a domain/range of $[0,1]$).


As for the features, we will try two approaches.
First we will sample ten equidistantly-spaced points and compute the distance between these points of the process model and a project.
The second approach will subdivide the time axis into ten equally long segments, of which we will calculate the area between curves and the correlation.


Then, we will use a standard machine learning approach with repeated cross-validation in order to find out whether the generalization error on some holdout data will converge with increasing size of the training set.
We will attempt standard pre-processing, such as removing (near-)zero variance features, highly correlated features, or dimensionality reduction of the feature space.


# Slight Adaption of the Source Code Pattern

The pattern used for source code is slightly erroneous with regard to how the source code density is modeled. It does not currently properly account for how the Fire Drill is described.
Also, we want to change the pattern to use smooth CDFs for the maintenance activities, as the plan is to only measure correlation and area between curves. Using CDFs has a few advantages, such as no normalization and better suitability for the chosen metrics.
We will also change the `FREQ` activity and represent it as a mixture of `A` and `CP`, such that all three densities become true densities (integrating to $\approx1$).

## Modeling `A`, `CP`, and `FREQ`

The process for `A`, `CP`, and `FREQ` is the following (both, for the pattern and projects):

* Rejection Sampling:
  * If pattern, sample from under the modeled curve.
  * If project, sample from the KDE of the activity's curve.
* Estimate new KDE from these samples for `A` and `CP`.
* Compose a mixture density for `FREQ` as $f(x)=r_A f_A(x) + r_{CP} f_{CP}(x)$, where $r_a,r_{CP}$ correspond to the ratios of these activities.
* Perform rejection sampling from the three densities, using large sample sizes, so that we can obtain a smooth ECDF.

This process will leave us with three separate smooth ECDFs.


## Modeling `SCD`

The source code density can move quite unpredictably, such that it does not make a lot of sense obtaining curves and measuring their correlation.
Instead, we will only look at the difference between the pattern's modeled density and that of each project.
Therefore, we will simply interpolate linearly between the projects' commits.

As for the pattern, the curve will be smooth and needs to be redesigned.
Note that the Fire Drill description says the following: "only analytical or documentational artefacts for a long time". The current pattern does not account for that.
Instead, the current pattern pretty much reduces all activities and the `SCD` during what we called the "Long Stretch".
While we mostly stick with how the activities were modeled, the `SCD`, however, requires a more accommodating redesign.
It should pick up pace almost immediately, followed by a slight increase until the Fire Drill starts.
Then, we should see a rapid increase that peaks at $1$ and a normalization after the Fire Drill ends (go back to previous level).
We also modify the other activities and add a slight slope to them, but these are rather cosmetic changes.


## Importing the Raw SVG Points

Please note that after sketching the SVG, we rasterize it to points using a tool called "PathToPoints"[^1].
In the CSV, we will find four different data series (attention: of different length), one corresponding to each activity/metric.
Since `FREQ` will be a mixture, we are not modelling it explicitly.


Paths are associated with colors: Coral is `SCD`, Gold is `A`, and Green is `CP`.

```{r}
temp <- read.csv(file = "../data/Fire-Drill_second-guess.csv", header = TRUE, sep = ";")
assocs <- c("coral" = "SCD", "gold" = "A", "green" = "CP")
assoc_inv <- names(assocs)
names(assoc_inv) <- unname(assocs)

# Y's are negative.
for (assoc in names(assocs)) {
  temp[, paste0(assoc, "_y")] <- -1 * temp[, paste0(assoc, "_y")]
}

# Next, we gotta normalize them all together, so we have to determine the y-extent.
y_ext <- range(c(temp$coral_y, temp$gold_y, temp$green_y), na.rm = TRUE)
for (assoc in names(assocs)) {
  temp[, paste0(assoc, "_y")] <- (temp[, paste0(assoc, "_y")] - y_ext[1]) / (y_ext[2] - y_ext[1])
}
```


```{r}
templ <- list()

for (assoc in names(assocs)) {
  data <- temp[, paste0(assoc, c("_x", "_y"))]
  data <- data[stats::complete.cases(data),]
  tempf <- stats::approxfun(x = data[, paste0(assoc, "_x")], y = data[, paste0(assoc, "_y")], yleft = 0, yright = 0)
  # Let's sample from that function linearly
  use_y <- tempf(seq(from = min(data[, paste0(assoc, "_x")]), to = max(data[, paste0(assoc, "_x")]), length.out = 1e3))
  templ[[assocs[assoc]]] <- stats::approxfun(x = seq(from = 0, to = 1, length.out = 1e3), y = use_y, yleft = use_y[1], yright = use_y[2])
}
```


Now we have a list with with `A`, `CP`, and `SCD`.
Next, we will do the rejection sampling from the former two in order to create `FREQ`.
As for the mixture ratio, we will assume that the __adaptive activities make up for $40$%__ of all activities.
So the mixture will be $0.4/0.6$.

```{r}
use_x <- seq(from = 0, to = 1, length.out = 1e7)


use_y <- stats::runif(n = length(use_x), min = 0, max = max(temp[[paste0(assoc_inv["A"], "_y")]], na.rm = TRUE))
tempdens_A <- stats::density(x = use_x[use_y <= templ$A(use_x)], bw = "SJ", cut = TRUE)
tempf_A <- stats::approxfun(x = tempdens_A$x, y = tempdens_A$y, yleft = 0, yright = 0)
tempecdf_A <- make_smooth_ecdf(values = use_x[use_y <= templ$A(use_x)], slope = 0, inverse = FALSE)

use_y <- stats::runif(n = length(use_x), min = 0, max = max(temp[[paste0(assoc_inv["CP"], "_y")]], na.rm = TRUE))
tempdens_CP <- stats::density(x = use_x[use_y <= templ$CP(use_x)], bw = "SJ", cut = TRUE)
tempf_CP <- stats::approxfun(x = tempdens_CP$x, y = tempdens_CP$y, yleft = 0, yright = 0)
tempecdf_CP <- make_smooth_ecdf(values = use_x[use_y <= templ$CP(use_x)], slope = 0, inverse = FALSE)

c(
  cubature::cubintegrate(f = tempf_A, 0, 1)$integral,
  cubature::cubintegrate(f = tempf_CP, 0, 1)$integral) # Those should be very close to 1

pattern_sc <- list(
  SCD = templ$SCD,
  A = tempf_A,
  CP = tempf_CP,
  FREQ = function(x) .4 * tempf_A(x) + .6 * tempf_CP(x))

cubature::cubintegrate(f = pattern_sc$FREQ, 0, 1)$integral
```


```{r}
# We'll need it in the next step.
use_y <- stats::runif(n = length(use_x), min = 0, max = -1.01 * nloptr::nloptr(
  x0 = .5, lb = 0, ub = 1,
  eval_f = function(x) -pattern_sc$FREQ(x),
  opts = list(algorithm = "NLOPT_GN_DIRECT_L_RAND", xtol_rel=1e-3, maxeval=1e5))$objective)
tempdens_FREQ <- stats::density(x = use_x[use_y <= pattern_sc$FREQ(use_x)], bw = "SJ", cut = TRUE)
tempf_FREQ <- stats::approxfun(x = tempdens_FREQ$x, y = tempdens_FREQ$y, yleft = 0, yright = 0)
tempecdf_FREQ <- make_smooth_ecdf(values = use_x[use_y <= pattern_sc$FREQ(use_x)], slope = 0, inverse = FALSE)

cubature::cubintegrate(f = tempf_FREQ, 0, 1)$integral
```

Now it's time to print the new pattern for Source Code!
Note that in figure \ref{fig:fd-sc-2nd-guess} the curves for the variables `A`, `CP`, and `FREQ` are now all proper probability densities (i.e., each of them integrates to $1$).
The variable `SCD` is not a density and peaks at $1$, because that is the maximum value for the source code density (it is ratio, actually).

```{r fd-sc-2nd-guess, echo=FALSE, fig.height=6, fig.cap="The second guess of the Fire Drill anti-pattern in source code.", fig.align="top", fig.pos="ht!"}
curve2(pattern_sc$A, 0, 1, lwd = 2, col = "red")
curve2(pattern_sc$CP, 0, 1, lwd = 2, col = "#00bb00", add = TRUE)
curve2(pattern_sc$FREQ, 0, 1, lwd = 2, col = "magenta", add = TRUE)
curve2(pattern_sc$SCD, 0, 1, lwd = 2, col = "purple", add = TRUE)
grid()
legend(x = 0, y = 3, legend = c("A", "CP", "FREQ", "SCD"), col = c("red", "#00bb00", "magenta", "purple"), lty = 1, lwd = 2)
```


I have to admit this pattern looks quite nice. Let's keep it and produce an _additional_ version using CDFs (we will keep and test both).

Now, the last step is to transform the three maintenance activities into __cumulative__ densities.
We will perform rejection sampling from these KDE-PDFs and estimate the ECDFs.

```{r}
pattern_sc_cdf <- append(pattern_sc, list())

temp_x <- seq(from = 0, to = 1, length.out = 5e3)

for (vname in names(pattern_sc_cdf)) {
  if (vname == "SCD") {
    next # Not this one..
  }
  
  # Let's replace the pattern's activities with approximate ECDFs:
  pattern_sc_cdf[[vname]] <- stats::approxfun(x = temp_x, y = get(paste0("tempecdf_", vname))(temp_x), yleft = 0, yright = 1)
}
```


In figure \ref{fig:fd-sc-2nd-guess-ecdf} we show the final CDF-version of the improved pattern for source code.
The three activities `A`, `CP`, and `FREQ` have now been converted to cumulative densities (no change to `SCD`).


```{r fd-sc-2nd-guess-ecdf, echo=FALSE, fig.height=6, fig.cap="The second guess of the Fire Drill anti-pattern in source code, where the maintenance activities have been converted to cumulative densities.", fig.align="top", fig.pos="ht!"}
curve2(pattern_sc_cdf$A, 0, 1, lwd = 2, col = "red")
curve2(pattern_sc_cdf$CP, 0, 1, lwd = 2, col = "#00bb00", add = TRUE)
curve2(pattern_sc_cdf$FREQ, 0, 1, lwd = 2, col = "magenta", add = TRUE)
curve2(pattern_sc_cdf$SCD, 0, 1, lwd = 2, col = "purple", add = TRUE)
grid()
legend(x = .65, y = .3, legend = c("A (CDF)", "CP (CDF)", "FREQ (CDF)", "SCD"), col = c("red", "#00bb00", "magenta", "purple"), lty = 1, lwd = 2)
```



[^1]: https://github.com/sse-lnu/PathToPoints



# Importing and preparing the Data

We have a total of $15$ projects and a ground truth for each.

## Load the Ground Truth

Here, we will simply join the two batches.

```{r}
ground_truth <- rbind(
  read.csv(file = "../data/ground-truth.csv", sep = ";"),
  read.csv(file = "../data/ground-truth_2nd-batch.csv", sep = ";"))
```


## Importing the Issue-Tracking Pattern

Here we load the issue-tracking pattern. Note that there is only one and we will not be using any variations.

```{r}
pattern_it <- readRDS(file = "../results/pattern_I_it.rds")
```



## Importing the Project Data for Source Code

We cannot just import the projects as they were pre-processed previously for source code data.
Instead, we will load the raw data and transform the projects similar to how we produced the patterns.
Also, for source code, there will be two sets of projects: an ordinary one and a CDF-transformed one.


Let's first import and prepare the source code data:

```{r}
temp <- rbind(
  read.csv(file = "../data/student-projects.csv", sep = ";"),
  read.csv(file = "../data/student-projects_2nd-batch.csv", sep = ";"))
temp$label <- factor(x = toupper(temp$label), levels = sort(toupper(unique(temp$label))))
temp$project <- factor(x = temp$project, levels = sort(unique(temp$project)))
temp$AuthorTimeNormalized <- NA_real_

for (pId in levels(temp$project)) {
  temp[temp$project == pId, ]$AuthorTimeNormalized <-
    (temp[temp$project == pId, ]$AuthorTimeUnixEpochMilliSecs -
      min(temp[temp$project == pId, ]$AuthorTimeUnixEpochMilliSecs))
  temp[temp$project == pId, ]$AuthorTimeNormalized <-
    (temp[temp$project == pId, ]$AuthorTimeNormalized /
      max(temp[temp$project == pId, ]$AuthorTimeNormalized))
}
```


```{r}
projects_sc <- list()
projects_sc_cdf <- list()


get_densities <- function(data, x_samples = 1e4) {
  suppressWarnings({
    use_x <- seq(from = 0, to = 1, length.out = x_samples)
    tempdens <- stats::density(x = data, bw = "SJ", cut = TRUE)
    tempdens_f <- stats::approxfun(x = tempdens$x, y = tempdens$y, yleft = 0, yright = 0)
    use_y <- stats::runif(n = x_samples, min = 0, max = max(tempdens$y))
    
    temp_samples <- use_x[use_y <= tempdens_f(use_x)]
    tempdens <- stats::density(x = temp_samples, bw = "SJ", cut = TRUE)
    
    list(
      PDF = stats::approxfun(x = tempdens$x, y = tempdens$y, yleft = 0, yright = 0),
      CDF = make_smooth_ecdf(values = temp_samples, slope = 0, inverse = FALSE))
  })
}

make_mixture <- function(pdf1, pdf2, pdf1_ratio, x_samples = 1e4) {
  use_x <- seq(from = 0, to = 1, length.out = x_samples)
  tempf <- function(x) pdf1_ratio * pdf1(x) + (1 - pdf1_ratio) * pdf2(x)
  
  use_y <- stats::runif(n = length(use_x), min = 0, max = -1.01 * nloptr::nloptr(
    x0 = .5, lb = 0, ub = 1,
    eval_f = function(x) -tempf(x),
    opts = list(algorithm = "NLOPT_GN_DIRECT_L_RAND", xtol_rel=1e-3, maxeval=1e5))$objective)
  
  list(
    PDF = tempf,
    CDF = make_smooth_ecdf(values = use_x[use_y <= tempf(use_x)], slope = 0, inverse = FALSE))
}


for (lvl in levels(temp$project)) {
  df <- temp[temp$project == lvl,]
  templ <- list()
  templ_cdf <- list()
  
  temp_A <- get_densities(data = df[df$label == "A",]$AuthorTimeNormalized)
  temp_CP <- get_densities(data = df[df$label %in% c("C", "P"),]$AuthorTimeNormalized)
  acp_ratio <- nrow(df[df$label == "A",]) / nrow(df)
  temp_FREQ <- make_mixture(pdf1 = temp_A$PDF, pdf2 = temp_CP$PDF, pdf1_ratio = acp_ratio)
  
  templ[["A"]] <- temp_A$PDF
  templ[["CP"]] <- temp_CP$PDF
  templ[["FREQ"]] <- temp_FREQ$PDF
  
  templ_cdf[["A"]] <- temp_A$CDF
  templ_cdf[["CP"]] <- temp_CP$CDF
  templ_cdf[["FREQ"]] <- temp_FREQ$CDF
  
  templ[["SCD"]] <- suppressWarnings({
    stats::approxfun(x = df$AuthorTimeNormalized, y = df$Density, rule = 2)
  })
  templ_cdf[["SCD"]] <- templ[["SCD"]] # it's the same because there is no CDF of it
  
  projects_sc[[lvl]] <- templ
  projects_sc_cdf[[lvl]] <- templ_cdf
}
```

In figure \ref{fig:project-vars-new} we show the $15$ projects with the new transform applied.

```{r project-vars-new, echo=FALSE, fig.width=8, fig.height=8, fig.cap="The projects in source code with the new transform applied. It is obvious that the densities are now exceeding $1.0$ since they are no longer normalized.", fig.align="top", fig.pos="ht!"}
par(mfrow = c(4, 4), oma = rep(0, 4), mar = c(3, 2, 2, 1))

for (proj in gtools::mixedsort(levels(temp$project))) {
  use_y <- max(c(
    projects_sc[[proj]]$A(seq(from = 0, to = 1, length.out = 100)),
    projects_sc[[proj]]$CP(seq(from = 0, to = 1, length.out = 100)),
    projects_sc[[proj]]$FREQ(seq(from = 0, to = 1, length.out = 100))
  ))
  curve2(projects_sc[[proj]]$A, 0, 1, col = "red", ylim = c(0, use_y), main = proj)
  curve2(projects_sc[[proj]]$CP, 0, 1, col = "#00bb00", ylim = c(0, use_y), add = TRUE)
  curve2(projects_sc[[proj]]$FREQ, 0, 1, col = "magenta", ylim = c(0, use_y), add = TRUE)
  curve2(projects_sc[[proj]]$SCD, 0, 1, col = "purple", ylim = c(0, use_y), add = TRUE)
  grid()
}
```


## Importing the Project Data for Issue-tracking

This case is more straightforward, as we do not change the way this was done before.
Also, there will only be one version, not two like we have for source code.

```{r}
projects_it <- list()
projects_it <- append(projects_it, readRDS(file = "../results/project_signals_it.rds"))
projects_it <- append(projects_it, readRDS(file = "../results/project_signals_2nd_batch_it.rds"))
```


# Creating the Datasets

For each pattern, we will create a dataset that contains the deviation for each project against each pattern.
Since we have three patterns, we will have three datasets.

Since the patterns and projects are modeled as curves, we will compute segment-wise features. For that, each project/pattern is subdivided into ten equally long intervals. Then, a distance metric is compute for each interval and activity.
We will compute **two metrics**: The area between curves and the $2$-dimensional relative continuous Pearson sample correlation coefficient (see section \ref{ssec:m-dim-pearson}).
This means that we will get a total of $2\times 10\times 4=80$ features for source code patterns and $60$ for issue-tracking patterns (because IT has only three activities/variables).
This means that we will get a lot more features than data points.
Also, our data is not balanced. For these reasons, we will have to use oversampling and synthetically inflate our dataset.
Note that this is OK for the intended purpose of finding an upper bound of required training instances for obtaining robust regression models.
In practice, it would of course be better just to label additional instances.


## Function for computing a Distance

The following function will be used to compute the distance metrics for area between curves and correlation.

```{r}
compute_distance <- function(f_pattern, f_project, metric = c("area", "corr"), interval = 1:10, num_samples = 1e3) {
  metric <- match.arg(metric)
  
  interval_ext <- c(interval / 10 - .1, interval / 10)
  use_x <- seq(from = interval_ext[1], to = interval_ext[2], length.out = num_samples)
  
  v1 <- f_pattern(use_x)
  v2 <- f_project(use_x)
  
  if (metric == "area") {
    # It's the same as MAE for large samples:
    return(Metrics::mae(actual = v1, predicted = v2))
  }
  temp <- suppressWarnings({
    stats::cor(x = v1, y = v2, method = "pearson")
  })
  if (is.na(temp)) 0 else temp
}
```


## Create Datasets for Source Code

Here we will create both datasets (ordinary and CDF) in the same loop.

```{r}
grid <- expand.grid(list(
  distance = c("area", "corr"),
  interval = 1:10,
  activity = c("A", "CP", "FREQ", "SCD")
))
grid$distance <- as.character(grid$distance)
grid$activity <- as.character(grid$activity)

dataset_sc <- `colnames<-`(x = matrix(nrow = 0, ncol = nrow(grid)), value = sapply(X = rownames(grid), FUN = function(rn) {
  r <- grid[rn,]
  paste(r$activity, r$interval, r$distance, sep = "_")
}))
dataset_sc_cdf <- dataset_sc[,]


for (pname in names(projects_sc)) {
  newrow_sc <- `colnames<-`(x = matrix(ncol = ncol(dataset_sc)), value = colnames(dataset_sc))
  newrow_sc_cdf <- `colnames<-`(x = matrix(ncol = ncol(dataset_sc)), value = colnames(dataset_sc))
  
  for (rn in rownames(grid)) {
    row <- grid[rn,]
    tempf_pattern <- pattern_sc[[row$activity]]
    tempf_pattern_cdf <- pattern_sc_cdf[[row$activity]]
    tempf_project <- projects_sc[[pname]][[row$activity]]
    tempf_project_cdf <- projects_sc_cdf[[pname]][[row$activity]]
    
    feat_name <- paste(row$activity, row$interval, row$distance, sep = "_")
    newrow_sc[1, feat_name] <- compute_distance(
      f_pattern = tempf_pattern, f_project = tempf_project, metric = row$distance, interval = row$interval)
    newrow_sc_cdf[1, feat_name] <- compute_distance(
      f_pattern = tempf_pattern_cdf, f_project = tempf_project_cdf, metric = row$distance, interval = row$interval)
  }
  
  dataset_sc <- rbind(dataset_sc, newrow_sc)
  dataset_sc_cdf <- rbind(dataset_sc_cdf, newrow_sc_cdf)
}

dataset_sc <- as.data.frame(dataset_sc)
dataset_sc_cdf <- as.data.frame(dataset_sc_cdf)
```


## Create Dataset for Issue-tracking

This is very similar to how we created the datasets for source code, mainly the activities will differ and there is only one version.

```{r}
grid <- expand.grid(list(
  distance = c("area", "corr"),
  interval = 1:10,
  activity = c("REQ", "DEV", "DESC")
))
grid$distance <- as.character(grid$distance)
grid$activity <- as.character(grid$activity)

dataset_it <- `colnames<-`(x = matrix(nrow = 0, ncol = nrow(grid)), value = sapply(X = rownames(grid), FUN = function(rn) {
  r <- grid[rn,]
  paste(r$activity, r$interval, r$distance, sep = "_")
}))

for (pname in names(projects_it)) {
  newrow_it <- `colnames<-`(x = matrix(ncol = ncol(dataset_it)), value = colnames(dataset_it))
  
  for (rn in rownames(grid)) {
    row <- grid[rn,]
    tempf_pattern <- pattern_it[[row$activity]]
    tempf_project <- projects_it[[pname]][[row$activity]]$get0Function()
    
    feat_name <- paste(row$activity, row$interval, row$distance, sep = "_")
    newrow_it[1, feat_name] <- compute_distance(
      f_pattern = tempf_pattern, f_project = tempf_project, metric = row$distance, interval = row$interval)
  }
  
  dataset_it <- rbind(dataset_it, newrow_it)
}

dataset_it <- as.data.frame(dataset_it)
```



# Adaptive Training For Robust Regression Models

In this section, we will train a few state-of-the-art models _adaptively_, which means that we will keep adding labeled training data in order to hopefully observe some convergence towards a generalization error on validation data that will allow us to estimate the minimum amount of training data required to achieve a certain error.
We will repeat fitting the same model many times, using repeated $k$-fold cross validation and bootstrapping, in order to obtain reliable results.
As for the models, it has been shown that the Random forest is the average top performer [@delgado14], so we will pick it.
Furthermore, we will attempt to fit these models: GLM (generalized linear model), GBM (gradient boosting machine), XGB (extreme gradient boosting), SVM-Poly (SVM with polynomial kernel), avNNET (model averaged neural network), rPART2 (recursive partitioning of rules).


## Preparation of the Datasets

Our datasets are problematic in that we have only few instances but many features. Also, we do not have an example for every possible ground truth.
For example, we have no project with a score of `4` or `7` (see figure \ref{fig:gt-label-hist}).
Through a number of measures, however, we can alleviate all these problems to a satisfactory degree.


The first we will do is to synthetically oversample our dataset using SMOTE for regression [@torgo13, @branco16].
SMOTE is a well-established technique that has been proven to significantly increase model performance.
Since our label is numeric (we do regression), we will have to oversample instances such that new data points are generated that can be imputed and fill up the numeric labels.
While ordinary SMOTE cannot oversample unseen classes, the technique works differently for regression and allows us to generate synthetic samples with a previously unseen numeric label.


```{r gt-label-hist, echo=FALSE, fig.cap="Histogram of the ground truth consensus for all 15 projects.", fig.align="top", fig.pos="ht!"}
hist(ground_truth$consensus, breaks = -1:10)
```

```{r}
#' This function creates new instances for a given numeric label.
oversample_y <- function(dataset, num, lab) {
  new_ds <- NULL
  i <- 0
  while (TRUE) {
    set.seed(i)
    i <- i + 1
    temp <- UBL::SmoteRegress(form = gt~., dat = dataset, thr.rel = 1/15)
    temp$gt <- round(temp$gt)
    temp <- temp[temp$gt == lab,]
    if (nrow(temp) > 0) {
      new_ds <- rbind(new_ds, head(temp, 1))
    }
    if (is.data.frame(new_ds) && nrow(new_ds) == num) {
      break
    }
  }
  new_ds
}
```


We will also create another short function that will balance all numeric labels in a given dataset.

```{r}
balance_num_labels <- function(dataset, num = 10) {
  new_ds <- NULL
  
  for (i in 0:10) { # All possible numeric labels
    has_num_rows <- nrow(dataset[dataset$gt == i,])
    req_rows <- num - has_num_rows
    if (req_rows > 0) {
      new_ds <- suppressWarnings({
        rbind(new_ds, oversample_y(dataset = dataset, num = req_rows, lab = i))
      })
    }
  }
  
  if (!is.null(new_ds)) rbind(dataset, new_ds) else dataset
}
```




```{r}
min_rows <- 15 # Number of rows we want to end up with per numeric label

dataset_sc_oversampled <- cbind(dataset_sc, data.frame(gt = ground_truth$consensus))
dataset_sc_oversampled <- balance_num_labels(dataset = dataset_sc_oversampled, num = min_rows)

dataset_sc_cdf_oversampled <- cbind(dataset_sc_cdf, data.frame(gt = ground_truth$consensus))
dataset_sc_cdf_oversampled <- balance_num_labels(dataset = dataset_sc_cdf_oversampled, num = min_rows)

dataset_it_oversampled <- cbind(dataset_it, data.frame(gt = ground_truth$consensus))
dataset_it_oversampled <- balance_num_labels(dataset = dataset_it_oversampled, num = min_rows)
```

So, as we see in the following overview for all three datasets, we have the same amount of instances for each numeric label ($0$ through $10$):

```{r}
rbind(
  table(dataset_sc_oversampled$gt),
  table(dataset_sc_cdf_oversampled$gt),
  table(dataset_it_oversampled$gt))
```


## Adaptive Training

We define a function that can train one of the chosen models adaptively (using an outer grid search).


```{r}
adaptive_training_caret <- function(
    org_dataset,
    seeds, # number of seeds equals number of repeats
    model_type = c("avNNet", "glm", "M5", "rf", "svmPoly", "treebag", "xgbTree", "rankModel"),
    num_train = 5,
    num_valid = 50,
    do_pca = TRUE
) {
  model_type <- match.arg(model_type)
  
  cn <- colnames(org_dataset)
  cn_x <- cn[cn != "gt"]
  pre_proc_method <- c("center", "scale", "nzv")
  if (do_pca) {
    pre_proc_method <- c(pre_proc_method, "pca")
  }
  
  res <- NULL

  
  for (seed in seeds) {
    set.seed(seed)
    idx <- sample(x = rownames(org_dataset), size = nrow(org_dataset))
    
    df_train <- org_dataset[idx[1:num_train],]
    # Fit the pre-processor on the training data, then...
    pre_proc <- caret::preProcess(x = df_train[, cn_x], method = pre_proc_method, thresh = 0.975)
    
    # Transform training AND validation data with it:
    df_train <- stats::predict(pre_proc, newdata = df_train)
    df_valid <- org_dataset[idx[(num_train + 1):(num_train + num_valid)],]
    df_valid <- stats::predict(pre_proc, newdata = df_valid)
    
    # Colnames change to PC1, PC2, ...
    use_cn_x <- if (do_pca) paste0("PC", 1:(ncol(df_train) - 1)) else cn_x
    
    pred_train <- c()
    pred_valid <- c()
    model <- tryCatch({
      if (model_type == "rankModel") {
        devtools::source_url(url = "https://raw.githubusercontent.com/MrShoenel/R-rank-model/master/R/rank-model.R")
        temp1 <- create_model(df_train = df_train, x_cols = use_cn_x, y_col = "gt", cdf_type = "gauss")
        tempf <- function(x) model_loss(model = temp1, x = x, df = df_train, y_col = "gt")
        
        num_params <- 2 + 3 * length(use_cn_x)
        opt_res <- nloptr::nloptr(
          x0 = runif(n = num_params),
          eval_f = tempf,
          eval_grad_f = function(x) pracma::grad(f = tempf, x0 = x),
          lb = rep(-1e3, num_params),
          ub = rep( 1e3, num_params),
          opts = list(algorithm = "NLOPT_LD_TNEWTON", xtol_rel=1e-3, maxeval=250))

        pred_train <- temp1(x = opt_res$solution, df = df_train)
        pred_valid <- temp1(x = opt_res$solution, df = df_valid)
      } else {
        temp1 <- caret::train(
          x = df_train[, use_cn_x], y = df_train[, "gt"], method = model_type,
          trControl = caret::trainControl(method = "LOOCV", number = 10))
        pred_train <- kernlab::predict(temp1, df_train[, use_cn_x])
        pred_valid <- kernlab::predict(temp1, df_valid[, use_cn_x])
      }
    }, error = function(cond) {
      stop(cond)
      pred_train <- rep(NA_real_, nrow(df_train))
      pred_valid <- rep(NA_real_, nrow(df_valid))
    })
    

    res <- rbind(res, data.frame(
      seed = seed,
      do_pca = do_pca,
      num_train = num_train,
      num_valid = num_valid,
      model_type = model_type,
      
      mae_train = Metrics::mae(actual = df_train$gt, predicted = pred_train),
      mae_valid = Metrics::mae(actual = df_valid$gt, predicted = pred_valid),
      rmse_train = Metrics::rmse(actual = df_train$gt, predicted = pred_train),
      rmse_valid = Metrics::rmse(actual = df_valid$gt, predicted = pred_valid)
    ))
  }
  
  res
}
```


```{r}
grid <- expand.grid(list(
  seed = 1:25,
  num_train = 3:50,
  do_pca = c(TRUE, FALSE),
  model_type = c("rf")
))
grid$model_type <- as.character(grid$model_type)


library(foreach)
res_rf <- loadResultsOrCompute(file = "../results/rob-reg_rf.rds", computeExpr = {
  doWithParallelCluster(expr = {
    foreach::foreach(
      rn = rownames(grid),
      .combine = rbind,
      .inorder = FALSE,
      .verbose = TRUE
    ) %dopar% {
      row <- grid[rn,]
      adaptive_training_caret(org_dataset = dataset_sc_oversampled, seeds = row$seed, model_type = row$model_type, num_train = row$num_train, do_pca = row$do_pca)
    }
  })
})
```


```{r}
temp <- res_rf %>% group_by(num_train) %>% summarize(Mean = mean(rmse_valid), Max = max(rmse_valid), Min = min(min(rmse_valid)), .groups = "drop")
plot(temp$Mean, type="l", ylim = c(0, max(c(temp$Mean, temp$Max, temp$Min))))
grid()
lines(temp$Max, col="blue")
lines(temp$Min, col="red")
```

```{r fig.height=10, fig.width=8}
temp1 <- res_rf[order(res_rf$num_train),]
temp1 <- data.frame(
  num_train = factor(x = as.character(temp1$num_train), levels = as.character(rev(sort(unique(temp1$num_train)))), ordered = TRUE),
  rmse_valid = temp1$rmse_valid)

ggplot(temp1, aes(x = rmse_valid, y = num_train, fill = ..x..)) +
  #stat_density_ridges(quantile_lines = TRUE) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  scale_fill_viridis_c(option = "C") +
  theme_ridges() + 
  theme(legend.position = "none")
```




# OLD (REMOVE)



```{r eval=FALSE}
set.seed(1)
idx <- sample(1:nrow(dataset_sc_oversampled), nrow(dataset_sc_oversampled))
data <- dataset_sc_oversampled[idx[1:25],]
pre_proc <- caret::preProcess(x = data[, ("gt" != colnames(data))], method = c("center", "scale", "pca", "nzv"))
data <- stats::predict(pre_proc, newdata = data)

(model <- suppressWarnings({
  caret::train(gt~., data, method="rf", trControl=caret::trainControl(method="repeatedcv", p=.8))
}))
```

```{r eval=FALSE}
pred <- kernlab::predict(model, stats::predict(pre_proc, newdata= dataset_sc_oversampled[idx[91:110],]))
pred[pred < 0] <- 0
pred[pred > 10] <- 10
Metrics::rmse(dataset_sc_oversampled$gt[idx[91:110]], pred)
Metrics::mae(dataset_sc_oversampled$gt[idx[91:110]], pred)
```

## Preparation of point-wise data

We will make a somewhat compromise between both, point- and segment-wise data.
While for the latter, we will calculate two metrics, the correlation and area between curves for ten segments per feature, we will take $21$ points for the point-wise data (from $0$ to $1$ in $0.05$ increments).
That way, we get get almost the same amount of features ($21$ point-wise, $2\times10$ segment-wise).
Since in the point-wise dataset we can only calculate the distance between two points, it does not make much sense to do this in more than one way (e.g., L^1 and L^2 norms). Therefore, we will stick with $L^1$ norm only.

```{r eval=FALSE}
use_x <- seq(from = 0, to = 1, by=.05)
dataset_pointwise_sc <- NULL

for (pname in names(projects_sc)) {
  temp_vec <- c()
  for (vname in names(pattern_sc)) {
    # The feature's name will be "vname_{100*x}"
    temp <- sapply(X = use_x, FUN = function(x) {
      #projects_sc[[pname]][[vname]]$get0Function()(x)
      pattern_sc[[vname]](x) - projects_sc[[pname]][[vname]]$get0Function()(x)
    })
    names(temp) <- paste0(paste0(vname, "_"), 100*use_x)
    temp_vec <- c(temp_vec, temp)
  }
  temp <- as.data.frame(t(temp_vec))
  temp$gt <- ground_truth[ground_truth$project == pname,]$consensus
  dataset_pointwise_sc <- rbind(dataset_pointwise_sc, temp)
}
hist(dataset_pointwise_sc$gt, breaks = -1:10)
```

```{r eval=FALSE}
#df_train <- dataset_pointwise_sc[idx[1:32],]
df_train <- data
model <- create_model(df_train = df_train, x_cols = paste0("PC", 1:8), y_col = "gt", cdf_type = "g")

cl <- parallel::makePSOCKcluster(min(15, parallel::detectCores()))
	
res <- doWithParallelClusterExplicit(cl = cl, stopCl = TRUE, expr = {
	parallel::clusterExport(cl, varlist = list("sigmoid", "swish", "smooth_min", "smooth_max", "hard_sigmoid", "model", "model_loss", "df_train"))
	set.seed(1)
	optimParallel::optimParallel(
	    par = runif(2 + 8*3),
	    lower = rep(-1e3, 2 + 8*3),
	    upper = rep( 1e3, 2 + 8*3),
	    fn = function(x) model_loss(model = model, x = x, df = df_train, y_col = "gt"),
	    parallel = list(cl = cl, forward = FALSE, loginfo = TRUE),
	    control = list(maxit = 250, factr = 1e-3))
})
res
```


```{r eval=FALSE}
oversample_y <- function(dataset, num, lab) {
  new_ds <- NULL
  i <- 0
  while (TRUE) {
    set.seed(i)
    i <- i + 1
    temp <- UBL::SmoteRegress(form = gt~., dat = dataset, thr.rel = 1/15)
    temp$gt <- round(temp$gt)
    temp <- temp[temp$gt == lab,]
    if (nrow(temp) > 0) {
      new_ds <- rbind(new_ds, head(temp, 1))
    }
    if (is.data.frame(new_ds) && nrow(new_ds) == num) {
      break
    }
  }
  new_ds
}
```


```{r eval=FALSE}
min_rows <- 10
new_ds <- NULL

for (i in 0:10) {
  has_num_rows <- nrow(dataset_pointwise_sc[dataset_pointwise_sc$gt == i,])
  req_rows <- min_rows - has_num_rows
  if (req_rows > 0) {
    new_ds <- rbind(new_ds, oversample_y(dataset = dataset_pointwise_sc, num = req_rows, lab = i))
  }
}

dataset_pointwise_sc <- rbind(dataset_pointwise_sc, new_ds)
hist(dataset_pointwise_sc$gt, breaks = -1:10)
```




```{r eval=FALSE}
set.seed(1)
dataset_pointwise_sc$gt <- dataset_pointwise_sc$gt + runif(15, 0, 1/3)
dataset_pointwise_sc$fake <- as.factor(paste0(c(rep(1, 8), rep(2, 7))))
temp <- DMwR::SMOTE(fake~., dataset_pointwise_sc, perc.over = 4000, perc.under = 4000)
dataset_pointwise_sc <- dataset_pointwise_sc[, !(colnames(dataset_pointwise_sc) %in% c("fake"))]
dataset_pointwise_sc$gt <- round(dataset_pointwise_sc$gt)

temp <- temp[temp$gt >= 0 & temp$gt <= 10,]
temp <- temp[, !(colnames(temp) %in% c("fake"))]
temp <- temp[stats::complete.cases(temp),]
temp$gt <- round(temp$gt)
table(temp$gt)
```

Now we impute data into our original dataset such that we have at least some instances of each class.

```{r eval=FALSE}
min_rows <- 4

for (i in 0:10) {
  has_num_rows <- nrow(dataset_pointwise_sc[dataset_pointwise_sc$gt == i,])
  req_rows <- min_rows - has_num_rows
  if (req_rows > 0) {
    dataset_pointwise_sc <- rbind(dataset_pointwise_sc, head(temp[temp$gt == i,], req_rows))
  }
}
hist(dataset_pointwise_sc$gt, breaks = -1:10)
```



```{r eval=FALSE}
set.seed(1)
idx <- sample(1:nrow(dataset_pointwise_sc), nrow(dataset_pointwise_sc))

(model <- suppressWarnings({
  caret::train(gt~., dataset_pointwise_sc[idx[1:32],], method="glm", preProcess=c("center", "scale", "pca"), trControl=caret::trainControl(method = "cv")) # method="LGOCV", p=.75
}))
```

```{r eval=FALSE}
pred <- predict(model, dataset_pointwise_sc[idx[91:110],])
pred[pred < 0] <- 0
pred[pred > 10] <- 10
Metrics::rmse(dataset_pointwise_sc$gt[idx[91:110]], pred)
Metrics::mae(dataset_pointwise_sc$gt[idx[91:110]], pred)
```

```{r eval=FALSE}
boxplot(abs(pred - dataset_pointwise_sc$gt[idx[91:110]]))
grid()
```

```{r eval=FALSE}
hist(round(10 * temp$gt), breaks = -1:10)
```











# References {-}

<div id="refs"></div>






