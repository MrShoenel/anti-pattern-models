---
title: "Technical Report: Building Robust Regression Models for Scoring the Presence of the Fire Drill Anti-pattern"
author: "Sebastian HÃ¶nel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ../inst/REFERENCES.bib
urlcolor: blue
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 6
    df_print: kable
    keep_tex: yes
  md_document:
    toc: yes
    toc_depth: 6
    df_print: kable
    variant: gfm
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: kable
  word_document: default
header-includes:
- \usepackage{bm}
- \usepackage{mathtools}
- \usepackage{xurl}
---

\newcommand*\mean[1]{\overline{#1}}
\newcommand{\abs}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand{\norm}[1]{\left\lVert\,#1\,\right\rVert}
\newcommand{\infdiv}[2]{#1\;\|\;#2}
\newcommand\argmax[1]{\underset{#1}{arg\,max}}
\newcommand\argmin[1]{\underset{#1}{arg\,min}}


```{r setoptions, echo=FALSE, warning=FALSE, message=FALSE}
Sys.setenv(LANG = "en_US.UTF-8")
Sys.setenv(LC_ALL = "en_US.UTF-8")
Sys.setenv(LC_CTYPE = "en_US.UTF-8")
library(knitr)
opts_chunk$set(tidy = TRUE, tidy.opts = list(indent=2))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
source(file = "../models/SRBTW-R6.R")

devtools::source_url(url = "https://raw.githubusercontent.com/MrShoenel/R-rank-model/master/R/helpers.R")
devtools::source_url(url = "https://raw.githubusercontent.com/MrShoenel/R-rank-model/master/R/rank-model.R")

library(ggplot2)
library(ggridges)
library(ggpubr)
library(fields)
library(dplyr)
```

# Introduction\label{tr:fire-drill-rob-reg-technical-report}

This technical report was added in the eighth iteration of the compilation.
So far, we have developed an unsupervised approach for properly scoring the presence of the anti-pattern.
However, here we will attempt to find a robust regression model in a supervised way. In other words, the regression model to be found will be the surrogate scoring mechanism.
The challenge lies in the scarcity of our data, as it has many more features than data points. It is therefore essential to obtain a regression model that, even when trained on only few instances, delivers a low generalization error.


All complementary data and results can be found at Zenodo [@honel_picha_2021]. This notebook was written in a way that it can be run without any additional efforts to reproduce the outputs (using the pre-computed results). This notebook has a canonical URL^[[Link]](https://github.com/MrShoenel/anti-pattern-models/blob/master/notebooks/fire-drill-issue-tracking-technical-report.Rmd)^ and can be read online as a rendered markdown^[[Link]](https://github.com/MrShoenel/anti-pattern-models/blob/master/notebooks/fire-drill-issue-tracking-technical-report.md)^ version. All code can be found in this repository, too.



# Overview of the Approach

We will be importing the two initial process models/patterns for the Fire Drill anti-pattern in source code and issue-tracking data.
These patterns have four and three activities/metrics, respectively.
Both, patterns and projects, are formulated *continuously*, that is, as curves over time. Also, both have been normalized along both, $x$- and $y$-axes (having a domain/range of $[0,1]$).


As for the features, we will try two approaches.
First we will sample ten equidistantly-spaced points and compute the distance between these points of the process model and a project.
The second approach will subdivide the time axis into ten equally long segments, of which we will calculate the area between curves and the correlation.


Then, we will use a standard machine learning approach with repeated cross-validation in order to find out whether the generalization error on some holdout data will converge with increasing size of the training set.
We will attempt standard pre-processing, such as removing (near-)zero variance features, highly correlated features, or dimensionality reduction of the feature space.


# Slight Adaption of the Source Code Pattern

The pattern used for source code is slightly erroneous with regard to how the source code density is modeled. It does not currently properly account for how the Fire Drill is described.
Also, we want to change the pattern to use smooth CDFs for the maintenance activities, as the plan is to only measure correlation and area between curves. Using CDFs has a few advantages, such as no normalization and better suitability for the chosen metrics.
We will also change the `FREQ` activity and represent it as a mixture of `A` and `CP`, such that all three densities become true densities (integrating to $\approx1$).

## Modeling `A`, `CP`, and `FREQ`

The process for `A`, `CP`, and `FREQ` is the following (both, for the pattern and projects):

* Rejection Sampling:
  * If pattern, sample from under the modeled curve.
  * If project, sample from the KDE of the activity's curve.
* Estimate new KDE from these samples for `A` and `CP`.
* Compose a mixture density for `FREQ` as $f(x)=r_A f_A(x) + r_{CP} f_{CP}(x)$, where $r_a,r_{CP}$ correspond to the ratios of these activities.
* Perform rejection sampling from the three densities, using large sample sizes, so that we can obtain a smooth ECDF.

This process will leave us with three separate smooth ECDFs.


## Modeling `SCD`

The source code density can move quite unpredictably, such that it does not make a lot of sense obtaining curves and measuring their correlation.
Instead, we will only look at the difference between the pattern's modeled density and that of each project.
Therefore, we will simply interpolate linearly between the projects' commits.

As for the pattern, the curve will be smooth and needs to be redesigned.
Note that the Fire Drill description says the following: "only analytical or documentational artefacts for a long time". The current pattern does not account for that.
Instead, the current pattern pretty much reduces all activities and the `SCD` during what we called the "Long Stretch".
While we mostly stick with how the activities were modeled, the `SCD`, however, requires a more accommodating redesign.
It should pick up pace almost immediately, followed by a slight increase until the Fire Drill starts.
Then, we should see a rapid increase that peaks at $1$ and a normalization after the Fire Drill ends (go back to previous level).
We also modify the other activities and add a slight slope to them, but these are rather cosmetic changes.


## Importing the Raw SVG Points

Please note that after sketching the SVG, we rasterize it to points using a tool called "PathToPoints"[^1].
In the CSV, we will find four different data series (attention: of different length), one corresponding to each activity/metric.
Since `FREQ` will be a mixture, we are not modelling it explicitly.


Paths are associated with colors: Coral is `SCD`, Gold is `A`, and Green is `CP`.

```{r}
temp <- read.csv(file = "../data/Fire-Drill_second-guess.csv", header = TRUE, sep = ";")
assocs <- c("coral" = "SCD", "gold" = "A", "green" = "CP")
assoc_inv <- names(assocs)
names(assoc_inv) <- unname(assocs)

# Y's are negative.
for (assoc in names(assocs)) {
  temp[, paste0(assoc, "_y")] <- -1 * temp[, paste0(assoc, "_y")]
}

# Next, we gotta normalize them all together, so we have to determine the y-extent.
y_ext <- range(c(temp$coral_y, temp$gold_y, temp$green_y), na.rm = TRUE)
for (assoc in names(assocs)) {
  temp[, paste0(assoc, "_y")] <- (temp[, paste0(assoc, "_y")] - y_ext[1]) / (y_ext[2] - y_ext[1])
}
```


```{r}
templ <- list()

for (assoc in names(assocs)) {
  data <- temp[, paste0(assoc, c("_x", "_y"))]
  data <- data[stats::complete.cases(data),]
  tempf <- stats::approxfun(x = data[, paste0(assoc, "_x")], y = data[, paste0(assoc, "_y")], yleft = 0, yright = 0)
  # Let's sample from that function linearly
  use_y <- tempf(seq(from = min(data[, paste0(assoc, "_x")]), to = max(data[, paste0(assoc, "_x")]), length.out = 1e3))
  templ[[assocs[assoc]]] <- stats::approxfun(x = seq(from = 0, to = 1, length.out = 1e3), y = use_y, yleft = use_y[1], yright = use_y[2])
}
```


Now we have a list with with `A`, `CP`, and `SCD`.
Next, we will do the rejection sampling from the former two in order to create `FREQ`.
As for the mixture ratio, we will assume that the __adaptive activities make up for $40$%__ of all activities.
So the mixture will be $0.4/0.6$.

```{r}
use_x <- seq(from = 0, to = 1, length.out = 1e7)


use_y <- stats::runif(n = length(use_x), min = 0, max = max(temp[[paste0(assoc_inv["A"], "_y")]], na.rm = TRUE))
tempdens_A <- stats::density(x = use_x[use_y <= templ$A(use_x)], bw = "SJ", cut = TRUE)
tempf_A <- stats::approxfun(x = tempdens_A$x, y = tempdens_A$y, yleft = 0, yright = 0)
tempecdf_A <- make_smooth_ecdf(values = use_x[use_y <= templ$A(use_x)], slope = 0, inverse = FALSE)

use_y <- stats::runif(n = length(use_x), min = 0, max = max(temp[[paste0(assoc_inv["CP"], "_y")]], na.rm = TRUE))
tempdens_CP <- stats::density(x = use_x[use_y <= templ$CP(use_x)], bw = "SJ", cut = TRUE)
tempf_CP <- stats::approxfun(x = tempdens_CP$x, y = tempdens_CP$y, yleft = 0, yright = 0)
tempecdf_CP <- make_smooth_ecdf(values = use_x[use_y <= templ$CP(use_x)], slope = 0, inverse = FALSE)

c(
  cubature::cubintegrate(f = tempf_A, 0, 1)$integral,
  cubature::cubintegrate(f = tempf_CP, 0, 1)$integral) # Those should be very close to 1

pattern_sc <- list(
  SCD = templ$SCD,
  A = tempf_A,
  CP = tempf_CP,
  FREQ = function(x) .4 * tempf_A(x) + .6 * tempf_CP(x))

cubature::cubintegrate(f = pattern_sc$FREQ, 0, 1)$integral
```


```{r}
# We'll need it in the next step.
use_y <- stats::runif(n = length(use_x), min = 0, max = -1.01 * nloptr::nloptr(
  x0 = .5, lb = 0, ub = 1,
  eval_f = function(x) -pattern_sc$FREQ(x),
  opts = list(algorithm = "NLOPT_GN_DIRECT_L_RAND", xtol_rel=1e-3, maxeval=1e5))$objective)
tempdens_FREQ <- stats::density(x = use_x[use_y <= pattern_sc$FREQ(use_x)], bw = "SJ", cut = TRUE)
tempf_FREQ <- stats::approxfun(x = tempdens_FREQ$x, y = tempdens_FREQ$y, yleft = 0, yright = 0)
tempecdf_FREQ <- make_smooth_ecdf(values = use_x[use_y <= pattern_sc$FREQ(use_x)], slope = 0, inverse = FALSE)

cubature::cubintegrate(f = tempf_FREQ, 0, 1)$integral
```

Now it's time to print the new pattern for Source Code!
Note that in figure \ref{fig:fd-sc-2nd-guess} the curves for the variables `A`, `CP`, and `FREQ` are now all proper probability densities (i.e., each of them integrates to $1$).
The variable `SCD` is not a density and peaks at $1$, because that is the maximum value for the source code density (it is ratio, actually).

```{r fd-sc-2nd-guess, echo=FALSE, fig.height=6, fig.cap="The second guess of the Fire Drill anti-pattern in source code.", fig.align="top", fig.pos="ht!"}
curve2(pattern_sc$A, 0, 1, lwd = 2, col = "red")
curve2(pattern_sc$CP, 0, 1, lwd = 2, col = "#00bb00", add = TRUE)
curve2(pattern_sc$FREQ, 0, 1, lwd = 2, col = "magenta", add = TRUE)
curve2(pattern_sc$SCD, 0, 1, lwd = 2, col = "purple", add = TRUE)
grid()
legend(x = 0, y = 3, legend = c("A", "CP", "FREQ", "SCD"), col = c("red", "#00bb00", "magenta", "purple"), lty = 1, lwd = 2)
```


I have to admit this pattern looks quite nice. Let's keep it and produce an _additional_ version using CDFs (we will keep and test both).

Now, the last step is to transform the three maintenance activities into __cumulative__ densities.
We will perform rejection sampling from these KDE-PDFs and estimate the ECDFs.

```{r}
pattern_sc_cdf <- append(pattern_sc, list())

temp_x <- seq(from = 0, to = 1, length.out = 5e3)

for (vname in names(pattern_sc_cdf)) {
  if (vname == "SCD") {
    next # Not this one..
  }
  
  # Let's replace the pattern's activities with approximate ECDFs:
  pattern_sc_cdf[[vname]] <- stats::approxfun(x = temp_x, y = get(paste0("tempecdf_", vname))(temp_x), yleft = 0, yright = 1)
}
```


In figure \ref{fig:fd-sc-2nd-guess-ecdf} we show the final CDF-version of the improved pattern for source code.
The three activities `A`, `CP`, and `FREQ` have now been converted to cumulative densities (no change to `SCD`).


```{r fd-sc-2nd-guess-ecdf, echo=FALSE, fig.height=6, fig.cap="The second guess of the Fire Drill anti-pattern in source code, where the maintenance activities have been converted to cumulative densities.", fig.align="top", fig.pos="ht!"}
curve2(pattern_sc_cdf$A, 0, 1, lwd = 2, col = "red")
curve2(pattern_sc_cdf$CP, 0, 1, lwd = 2, col = "#00bb00", add = TRUE)
curve2(pattern_sc_cdf$FREQ, 0, 1, lwd = 2, col = "magenta", add = TRUE)
curve2(pattern_sc_cdf$SCD, 0, 1, lwd = 2, col = "purple", add = TRUE)
grid()
legend(x = .65, y = .3, legend = c("A (CDF)", "CP (CDF)", "FREQ (CDF)", "SCD"), col = c("red", "#00bb00", "magenta", "purple"), lty = 1, lwd = 2)
```



[^1]: https://github.com/sse-lnu/PathToPoints



# Importing and preparing the Data

We have a total of $15$ projects and a ground truth for each.

## Load the Ground Truth

Here, we will simply join the two batches.

```{r}
ground_truth <- rbind(
  read.csv(file = "../data/ground-truth.csv", sep = ";"),
  read.csv(file = "../data/ground-truth_2nd-batch.csv", sep = ";"))
```


## Importing the Issue-Tracking Pattern

Here we load the issue-tracking pattern. Note that there is only one and we will not be using any variations.

```{r}
pattern_it <- readRDS(file = "../results/pattern_I_it.rds")
```



## Importing the Project Data for Source Code

We cannot just import the projects as they were pre-processed previously for source code data.
Instead, we will load the raw data and transform the projects similar to how we produced the patterns.
Also, for source code, there will be two sets of projects: an ordinary one and a CDF-transformed one.


Let's first import and prepare the source code data:

```{r}
temp <- rbind(
  read.csv(file = "../data/student-projects.csv", sep = ";"),
  read.csv(file = "../data/student-projects_2nd-batch.csv", sep = ";"))
temp$label <- factor(x = toupper(temp$label), levels = sort(toupper(unique(temp$label))))
temp$project <- factor(x = temp$project, levels = sort(unique(temp$project)))
temp$AuthorTimeNormalized <- NA_real_

for (pId in levels(temp$project)) {
  temp[temp$project == pId, ]$AuthorTimeNormalized <-
    (temp[temp$project == pId, ]$AuthorTimeUnixEpochMilliSecs -
      min(temp[temp$project == pId, ]$AuthorTimeUnixEpochMilliSecs))
  temp[temp$project == pId, ]$AuthorTimeNormalized <-
    (temp[temp$project == pId, ]$AuthorTimeNormalized /
      max(temp[temp$project == pId, ]$AuthorTimeNormalized))
}
```


```{r}
projects_sc <- list()
projects_sc_cdf <- list()


get_densities <- function(data, x_samples = 1e4) {
  suppressWarnings({
    use_x <- seq(from = 0, to = 1, length.out = x_samples)
    tempdens <- stats::density(x = data, bw = "SJ", cut = TRUE)
    tempdens_f <- stats::approxfun(x = tempdens$x, y = tempdens$y, yleft = 0, yright = 0)
    use_y <- stats::runif(n = x_samples, min = 0, max = max(tempdens$y))
    
    temp_samples <- use_x[use_y <= tempdens_f(use_x)]
    tempdens <- stats::density(x = temp_samples, bw = "SJ", cut = TRUE)
    
    list(
      PDF = stats::approxfun(x = tempdens$x, y = tempdens$y, yleft = 0, yright = 0),
      CDF = make_smooth_ecdf(values = temp_samples, slope = 0, inverse = FALSE))
  })
}

make_mixture <- function(pdf1, pdf2, pdf1_ratio, x_samples = 1e4) {
  use_x <- seq(from = 0, to = 1, length.out = x_samples)
  tempf <- function(x) pdf1_ratio * pdf1(x) + (1 - pdf1_ratio) * pdf2(x)
  
  use_y <- stats::runif(n = length(use_x), min = 0, max = -1.01 * nloptr::nloptr(
    x0 = .5, lb = 0, ub = 1,
    eval_f = function(x) -tempf(x),
    opts = list(algorithm = "NLOPT_GN_DIRECT_L_RAND", xtol_rel=1e-3, maxeval=1e5))$objective)
  
  list(
    PDF = tempf,
    CDF = make_smooth_ecdf(values = use_x[use_y <= tempf(use_x)], slope = 0, inverse = FALSE))
}


for (lvl in levels(temp$project)) {
  df <- temp[temp$project == lvl,]
  templ <- list()
  templ_cdf <- list()
  
  temp_A <- get_densities(data = df[df$label == "A",]$AuthorTimeNormalized)
  temp_CP <- get_densities(data = df[df$label %in% c("C", "P"),]$AuthorTimeNormalized)
  acp_ratio <- nrow(df[df$label == "A",]) / nrow(df)
  temp_FREQ <- make_mixture(pdf1 = temp_A$PDF, pdf2 = temp_CP$PDF, pdf1_ratio = acp_ratio)
  
  templ[["A"]] <- temp_A$PDF
  templ[["CP"]] <- temp_CP$PDF
  templ[["FREQ"]] <- temp_FREQ$PDF
  
  templ_cdf[["A"]] <- temp_A$CDF
  templ_cdf[["CP"]] <- temp_CP$CDF
  templ_cdf[["FREQ"]] <- temp_FREQ$CDF
  
  templ[["SCD"]] <- suppressWarnings({
    stats::approxfun(x = df$AuthorTimeNormalized, y = df$Density, rule = 2)
  })
  templ_cdf[["SCD"]] <- templ[["SCD"]] # it's the same because there is no CDF of it
  
  projects_sc[[lvl]] <- templ
  projects_sc_cdf[[lvl]] <- templ_cdf
}
```

In figure \ref{fig:project-vars-new} we show the $15$ projects with the new transform applied.

```{r project-vars-new, echo=FALSE, fig.width=8, fig.height=8, fig.cap="The projects in source code with the new transform applied. It is obvious that the densities are now exceeding $1.0$ since they are no longer normalized.", fig.align="top", fig.pos="ht!"}
par(mfrow = c(4, 4), oma = rep(0, 4), mar = c(3, 2, 2, 1))

for (proj in gtools::mixedsort(levels(temp$project))) {
  use_y <- max(c(
    projects_sc[[proj]]$A(seq(from = 0, to = 1, length.out = 100)),
    projects_sc[[proj]]$CP(seq(from = 0, to = 1, length.out = 100)),
    projects_sc[[proj]]$FREQ(seq(from = 0, to = 1, length.out = 100))
  ))
  curve2(projects_sc[[proj]]$A, 0, 1, col = "red", ylim = c(0, use_y), main = proj)
  curve2(projects_sc[[proj]]$CP, 0, 1, col = "#00bb00", ylim = c(0, use_y), add = TRUE)
  curve2(projects_sc[[proj]]$FREQ, 0, 1, col = "magenta", ylim = c(0, use_y), add = TRUE)
  curve2(projects_sc[[proj]]$SCD, 0, 1, col = "purple", ylim = c(0, use_y), add = TRUE)
  grid()
}
```


## Importing the Project Data for Issue-tracking

This case is more straightforward, as we do not change the way this was done before.
Also, there will only be one version, not two like we have for source code.

```{r}
projects_it <- list()
projects_it <- append(projects_it, readRDS(file = "../results/project_signals_it.rds"))
projects_it <- append(projects_it, readRDS(file = "../results/project_signals_2nd_batch_it.rds"))
```


# Creating the Datasets

For each pattern, we will create a dataset that contains the deviation for each project against each pattern.
Since we have three patterns, we will have three datasets.

Since the patterns and projects are modeled as curves, we will compute segment-wise features. For that, each project/pattern is subdivided into ten equally long intervals. Then, a distance metric is compute for each interval and activity.
We will compute **two metrics**: The area between curves and the $2$-dimensional relative continuous Pearson sample correlation coefficient (see section \ref{ssec:m-dim-pearson}).
This means that we will get a total of $2\times 10\times 4=80$ features for source code patterns and $60$ for issue-tracking patterns (because IT has only three activities/variables).
This means that we will get a lot more features than data points.
Also, our data is not balanced. For these reasons, we will have to use oversampling and synthetically inflate our dataset.
Note that this is OK for the intended purpose of finding an upper bound of required training instances for obtaining robust regression models.
In practice, it would of course be better just to label additional instances.


## Function for computing a Distance

The following function will be used to compute the distance metrics for area between curves and correlation.

```{r}
compute_distance <- function(f_pattern, f_project, metric = c("area", "corr"), interval = 1:10, num_samples = 1e3) {
  metric <- match.arg(metric)
  
  interval_ext <- c(interval / 10 - .1, interval / 10)
  use_x <- seq(from = interval_ext[1], to = interval_ext[2], length.out = num_samples)
  
  v1 <- f_pattern(use_x)
  v2 <- f_project(use_x)
  
  if (metric == "area") {
    # It's the same as MAE for large samples:
    return(Metrics::mae(actual = v1, predicted = v2))
  }
  temp <- suppressWarnings({
    stats::cor(x = v1, y = v2, method = "pearson")
  })
  if (is.na(temp)) 0 else temp
}
```


## Create Datasets for Source Code

Here we will create both datasets (ordinary and CDF) in the same loop.

```{r}
grid <- expand.grid(list(
  distance = c("area", "corr"),
  interval = 1:10,
  activity = c("A", "CP", "FREQ", "SCD")
))
grid$distance <- as.character(grid$distance)
grid$activity <- as.character(grid$activity)

dataset_sc <- `colnames<-`(x = matrix(nrow = 0, ncol = nrow(grid)), value = sapply(X = rownames(grid), FUN = function(rn) {
  r <- grid[rn,]
  paste(r$activity, r$interval, r$distance, sep = "_")
}))
dataset_sc_cdf <- dataset_sc[,]


for (pname in names(projects_sc)) {
  newrow_sc <- `colnames<-`(x = matrix(ncol = ncol(dataset_sc)), value = colnames(dataset_sc))
  newrow_sc_cdf <- `colnames<-`(x = matrix(ncol = ncol(dataset_sc)), value = colnames(dataset_sc))
  
  for (rn in rownames(grid)) {
    row <- grid[rn,]
    tempf_pattern <- pattern_sc[[row$activity]]
    tempf_pattern_cdf <- pattern_sc_cdf[[row$activity]]
    tempf_project <- projects_sc[[pname]][[row$activity]]
    tempf_project_cdf <- projects_sc_cdf[[pname]][[row$activity]]
    
    feat_name <- paste(row$activity, row$interval, row$distance, sep = "_")
    newrow_sc[1, feat_name] <- compute_distance(
      f_pattern = tempf_pattern, f_project = tempf_project, metric = row$distance, interval = row$interval)
    newrow_sc_cdf[1, feat_name] <- compute_distance(
      f_pattern = tempf_pattern_cdf, f_project = tempf_project_cdf, metric = row$distance, interval = row$interval)
  }
  
  dataset_sc <- rbind(dataset_sc, newrow_sc)
  dataset_sc_cdf <- rbind(dataset_sc_cdf, newrow_sc_cdf)
}

dataset_sc <- as.data.frame(dataset_sc)
dataset_sc_cdf <- as.data.frame(dataset_sc_cdf)
```


## Create Dataset for Issue-tracking

This is very similar to how we created the datasets for source code, mainly the activities will differ and there is only one version.

```{r}
grid <- expand.grid(list(
  distance = c("area", "corr"),
  interval = 1:10,
  activity = c("REQ", "DEV", "DESC")
))
grid$distance <- as.character(grid$distance)
grid$activity <- as.character(grid$activity)

dataset_it <- `colnames<-`(x = matrix(nrow = 0, ncol = nrow(grid)), value = sapply(X = rownames(grid), FUN = function(rn) {
  r <- grid[rn,]
  paste(r$activity, r$interval, r$distance, sep = "_")
}))

for (pname in names(projects_it)) {
  newrow_it <- `colnames<-`(x = matrix(ncol = ncol(dataset_it)), value = colnames(dataset_it))
  
  for (rn in rownames(grid)) {
    row <- grid[rn,]
    tempf_pattern <- pattern_it[[row$activity]]
    tempf_project <- projects_it[[pname]][[row$activity]]$get0Function()
    
    feat_name <- paste(row$activity, row$interval, row$distance, sep = "_")
    newrow_it[1, feat_name] <- compute_distance(
      f_pattern = tempf_pattern, f_project = tempf_project, metric = row$distance, interval = row$interval)
  }
  
  dataset_it <- rbind(dataset_it, newrow_it)
}

dataset_it <- as.data.frame(dataset_it)
```



# Adaptive Training For Robust Regression Models

In this section, we will train a few state-of-the-art models _adaptively_, which means that we will keep adding labeled training data in order to hopefully observe some convergence towards a generalization error on validation data that will allow us to estimate the minimum amount of training data required to achieve a certain error.
We will repeat fitting the same model many times, using repeated $k$-fold cross validation and bootstrapping, in order to obtain reliable results.
As for the models, it has been shown that the Random forest is the average top performer [@delgado14], so we will pick it.
Furthermore, we will attempt to fit these models: GLM (generalized linear model), GBM (gradient boosting machine), XGB (extreme gradient boosting), SVM-Poly (SVM with polynomial kernel), avNNET (model averaged neural network), rPART2 (recursive partitioning of rules).


## Preparation of the Datasets

Our datasets are problematic in that we have only few instances but many features. Also, we do not have an example for every possible ground truth.
For example, we have no project with a score of `4` or `7` (see figure \ref{fig:gt-label-hist}).
Through a number of measures, however, we can alleviate all these problems to a satisfactory degree.


The first we will do is to synthetically oversample our dataset using SMOTE for regression [@torgo13, @branco16].
SMOTE is a well-established technique that has been proven to significantly increase model performance.
Since our label is numeric (we do regression), we will have to oversample instances such that new data points are generated that can be imputed and fill up the numeric labels.
While ordinary SMOTE cannot oversample unseen classes, the technique works differently for regression and allows us to generate synthetic samples with a previously unseen numeric label.
To reduce the amount of features, we will eliminate those with zero or near-zero variance.
Then, optionally, we further reduce the feature space's dimensionality by applying a principal component analysis (PCA).


```{r gt-label-hist, echo=FALSE, fig.cap="Histogram of the ground truth consensus for all 15 projects.", fig.align="top", fig.pos="ht!"}
hist(ground_truth$consensus, breaks = -1:10)
```

```{r}
#' This function creates new instances for a given numeric label.
oversample_y <- function(dataset, num, lab) {
  new_ds <- NULL
  i <- 0
  while (TRUE) {
    set.seed(i)
    i <- i + 1
    temp <- UBL::SmoteRegress(form = gt~., dat = dataset, thr.rel = 1/15)
    temp$gt <- round(temp$gt)
    temp <- temp[temp$gt == lab,]
    if (nrow(temp) > 0) {
      new_ds <- rbind(new_ds, head(temp, 1))
    }
    if (is.data.frame(new_ds) && nrow(new_ds) == num) {
      break
    }
  }
  new_ds
}
```


We will also create another short function that will balance all numeric labels in a given dataset.

```{r}
balance_num_labels <- function(dataset, num = 10) {
  new_ds <- NULL
  
  for (i in 0:10) { # All possible numeric labels
    has_num_rows <- nrow(dataset[dataset$gt == i,])
    req_rows <- num - has_num_rows
    if (req_rows > 0) {
      new_ds <- suppressWarnings({
        rbind(new_ds, oversample_y(dataset = dataset, num = req_rows, lab = i))
      })
    }
  }
  
  if (!is.null(new_ds)) rbind(dataset, new_ds) else dataset
}
```


And here we create the oversampled datasets.

```{r}
min_rows <- 15 # Number of rows we want to end up with per numeric label

dataset_sc_oversampled <- cbind(dataset_sc, data.frame(gt = ground_truth$consensus))
dataset_sc_oversampled <- balance_num_labels(dataset = dataset_sc_oversampled, num = min_rows)

dataset_sc_cdf_oversampled <- cbind(dataset_sc_cdf, data.frame(gt = ground_truth$consensus))
dataset_sc_cdf_oversampled <- balance_num_labels(dataset = dataset_sc_cdf_oversampled, num = min_rows)

dataset_it_oversampled <- cbind(dataset_it, data.frame(gt = ground_truth$consensus))
dataset_it_oversampled <- balance_num_labels(dataset = dataset_it_oversampled, num = min_rows)
```


So, as we see in the following overview for all three datasets, we have the same amount of instances for each numeric label ($0$ through $10$):

```{r}
rbind(
  table(dataset_sc_oversampled$gt),
  table(dataset_sc_cdf_oversampled$gt),
  table(dataset_it_oversampled$gt))
```


## Adaptive Training

We define a function that can train one of the chosen models adaptively (using an outer grid search).
This function mainly handles models that are trainable with caret, but there are two exceptions:
When the selected model is `gbm`, we use a custom grid to accommodate small and very small sample sizes, as the default grid warrants for much larger training sets.
When the selected model is `rankModel`, we will actually test a new model of our own that uses quantile normalization of the inputs and an inverse rank transform of the outputs, with some non-linearity in between[^2].
This model has been proven efficient with very small sample sizes.

[^2]: See the Rank Model repository at <https://github.com/MrShoenel/R-rank-model>.


```{r}
adaptive_training_caret <- function(
    org_dataset,
    seeds, # number of seeds equals number of repeats
    model_type = c("avNNet", "gbm", "glm", "M5", "nnet", "rankModel", "rf", "svmPoly", "treebag", "xgbTree"),
    num_train = 5,
    num_valid = 50,
    do_pca = TRUE,
    return_num_comps = NULL,
    num_caret_repeats = 10
) {
  model_type <- match.arg(model_type)
  
  cn <- colnames(org_dataset)
  cn_x <- cn[cn != "gt"]
  pre_proc_method <- c("nzv")
  if (do_pca) {
    pre_proc_method <- c("pca", pre_proc_method)
  }
  pre_proc_method <- c(pre_proc_method, "center", "scale")
  
  res <- NULL

  
  for (seed in seeds) {
    set.seed(seed)
    idx <- sample(x = rownames(org_dataset), size = nrow(org_dataset))
    
    df_train <- org_dataset[idx[1:num_train],]
    # Fit the pre-processor on the training data, then...
    pre_proc <- caret::preProcess(x = df_train[, cn_x], method = pre_proc_method, thresh = 0.975)
    
    # Transform training AND validation data with it:
    df_train <- stats::predict(pre_proc, newdata = df_train)
    df_valid <- org_dataset[idx[(num_train + 1):(num_train + num_valid)],]
    df_valid <- stats::predict(pre_proc, newdata = df_valid)
    
    # Colnames change to PC1, PC2, ..., if PCA, otherwise they will be a subset
    # of the original column names (because of (N)ZV):
    use_cn_x <- if (do_pca) paste0("PC", 1:(ncol(df_train) - 1)) else colnames(df_train)
    use_cn_x <- use_cn_x[use_cn_x != "gt"]
    
    if (!is.null(return_num_comps) && return_num_comps) {
      if (!do_pca) {
        stop("Can only return number of components if doing PCA!")
      }
      return(length(use_cn_x))
    }
    
    pred_train <- c()
    pred_valid <- c()
    tryCatch({
      if (model_type == "rankModel") {
        # We're gonna have a small grid, so that each model is fit a few times.
        use_grid <- expand.grid(list(
          cdf_type = c("auto", "gauss", "ecdf"),
          attempt = 1:5 # This is basically a dummy.
        ))
        use_grid$cdf_type <- as.character(use_grid$cdf_type)
        
        best_rm <- 1e30
        for (rn in rownames(use_grid)) {
          row <- use_grid[rn,]
          temp1 <- create_model(df_train = df_train, x_cols = use_cn_x, y_col = "gt", cdf_type = row$cdf_type)
          tempf <- function(x) model_loss(model = temp1, x = x, df = df_train, y_col = "gt")
          
          # Let's fit the model!
          num_params <- 2 + 3 * length(use_cn_x)
          opt_res <- nloptr::nloptr(
            x0 = runif(n = num_params),
            eval_f = tempf,
            eval_grad_f = function(x) pracma::grad(f = tempf, x0 = x),
            lb = rep(-1e3, num_params),
            ub = rep( 1e3, num_params),
            opts = list(algorithm = "NLOPT_LD_TNEWTON", xtol_rel=5e-4, maxeval=250))
          
          temp_pred_valid <- temp1(x = opt_res$solution, df = df_valid)
          temp_best_rm <- Metrics::rmse(actual = df_valid$gt, predicted = temp_pred_valid)
          if (temp_best_rm < best_rm) {
            best_rm <- temp_best_rm
            pred_valid <- temp_pred_valid
            pred_train <- temp1(x = opt_res$solution, df = df_train)
          }
        }
      } else if (model_type == "nnet") {
        use_grid <- expand.grid(list(
          hidden1 = c(seq(from = 1, to = 25, by = 3), 35, 50),
          hidden2 = c(NA_real_, seq(from = 1, to = 16, by = 3)),
          act.fct = c("sigmoid", "tanh")
        ))
        
        best_nnet <- 1e30
        for (rn in rownames(use_grid)) {
          tryCatch({
            row <- use_grid[rn,]
            act.fct <- if (row$act.fct == "tanh") "tanh" else function(x) 1 / (1 + exp(-x)) # sigmoid
            hidden <- row$hidden1
            if (!is.na(row$hidden2)) {
              hidden <- c(hidden, row$hidden2)
            }
            
            temp1 <- neuralnet::neuralnet(
              formula = gt~., data = df_train, act.fct = act.fct,
              hidden = hidden, threshold = 5e-3, linear.output = TRUE, rep = 3)
            
            temp_pred_valid <- stats::predict(temp1, df_valid)
            temp_best_nnet <- Metrics::rmse(actual = df_valid$gt, predicted = temp_pred_valid)
            if (temp_best_nnet < best_nnet) {
              best_nnet <- temp_best_nnet
              pred_valid <- temp_pred_valid
              pred_train <- stats::predict(temp1, df_train)
            }
          }, error = function(cond) {
            print(paste("The model did not converge:", cond))
            # Do nothing
          })
        }
        
        if (best_nnet >= 1e30) {
          stop("NNET fit not possible.")
        }
      } else if (model_type == "gbm") {
        use_grid <- expand.grid(list(
          n.trees = c(2:10, seq(from = 12, to = 30, by = 2), seq(from = 35, to = 80, by = 5), 90, 100),
          n.minobsinnode = 1:10,
          bag.fraction = seq(from = 0.1, to = 0.7, by = 0.2)
        ))
        
        best_gbm <- 1e30
        for (rn in rownames(use_grid)) {
          row <- use_grid[rn,]
          tryCatch({
            temp1 <- gbm::gbm(
              formula = gt~., distribution = "gaussian", data = df_train, n.cores = 1,
              n.trees = row$n.trees, n.minobsinnode = row$n.minobsinnode,
              cv.folds = num_caret_repeats, train.fraction = 1 - (1 / nrow(df_train))) # Let's do repeated, nested LOOCV
            
            temp_pred_valid <- stats::predict(temp1, df_valid)
            temp_best_gbm <- Metrics::rmse(actual = df_valid$gt, predicted = temp_pred_valid)
            if (temp_best_gbm < best_gbm) {
              best_gbm <- temp_best_gbm
              pred_valid <- temp_pred_valid
              pred_train <- stats::predict(temp1, df_train)
            }
          }, error = function(cond) {
            # Do nothing, silently swallow error (it's handled by having NA-predictions)
          })
        }
        
        if (best_gbm >= 1e30) {
          stop("GBM fit not possible.")
        }
      } else {
        temp1 <- caret::train(
          x = df_train[, use_cn_x], y = df_train[, "gt"], method = model_type,
          trControl = caret::trainControl(method = "LOOCV", number = num_caret_repeats))
        pred_train <- kernlab::predict(temp1, df_train[, use_cn_x])
        pred_valid <- kernlab::predict(temp1, df_valid[, use_cn_x])
      }
    }, error = function(cond) {
      print(paste("Training failed:", cond))
      pred_train <- rep(NA_real_, nrow(df_train))
      pred_valid <- rep(NA_real_, nrow(df_valid))
    })
    
    
    res <- rbind(res, data.frame(
      seed = seed,
      do_pca = do_pca,
      num_train = num_train,
      num_valid = num_valid,
      model_type = model_type,
      num_comps = if (do_pca) length(use_cn_x) else NA_real_,
      
      mae_train = Metrics::mae(actual = df_train$gt, predicted = pred_train),
      mae_valid = Metrics::mae(actual = df_valid$gt, predicted = pred_valid),
      rmse_train = Metrics::rmse(actual = df_train$gt, predicted = pred_train),
      rmse_valid = Metrics::rmse(actual = df_valid$gt, predicted = pred_valid)
    ))
  }
  
  res
}
```


And here we compute the grid (test each model on each dataset).
Attention: The following is expensive.

```{r}
grid <- expand.grid(list(
  seed = 1:25,
  num_train = 2:50,
  do_pca = c(TRUE, FALSE)
))
model_types <- c("avNNet", "gbm", "glm", "M5", "rf", "svmPoly", "treebag", "rankModel")
use_datasets <- c("dataset_it_oversampled", "dataset_sc_oversampled", "dataset_sc_cdf_oversampled")


library(foreach)
res_grid <- loadResultsOrCompute(file = "../results/rob-reg_res-grid.rds", computeExpr = {
  res <- NULL
  
  for (use_dataset in use_datasets) {
    dataset <- get(use_dataset)
    
    for (model_type in model_types) {
      
      the_file <- paste0("../results/rob-reg_res-grid_", model_type, "_", use_dataset, ".rds")
      temp <- as.data.frame(loadResultsOrCompute(file = the_file, computeExpr = {
        temp1 <- doWithParallelCluster(numCores = min(parallel::detectCores(), 123), expr = {
          foreach::foreach(
            rn = rownames(grid),
            .combine = rbind,
            .inorder = FALSE,
            .verbose = TRUE
          ) %dopar% {
            tryCatch({
              row <- grid[rn,]
              adaptive_training_caret(
                org_dataset = dataset, seeds = row$seed, model_type = model_type,
                num_train = row$num_train, do_pca = row$do_pca)
            }, error = function(cond) {
              NULL # Return an empty result that will not disturb .combine
            })
          }
        })
        saveRDS(object = temp1, file = the_file)
        temp1
      }))
      temp$dataset <- rep(use_dataset, nrow(temp))
      
      res <- rbind(res, temp)
    }
  }

  res$dataset <- factor(res$dataset)
  res
})
```


Let's show some results for the Gradient Boosting Machine (figure \ref{fig:rob-reg-gbm1}).
It appears that the GBM needs at least $7$ training examples to work at all.
With $\approx25$ training examples, the expected generalization error (using the RMSE) is $\approx1$, and with about $15$ examples it is $\approx1.5$, both of which might be acceptable in a real-world scenario.


```{r rob-reg-gbm1, echo=FALSE, fig.cap="Robust regression using a Gradient Boosting Machine, PCA, and the oversampled issue-tracking dataset.", fig.align="top", fig.pos="ht!"}
temp <- res_grid[res_grid$model_type == "gbm" & res_grid$do_pca & res_grid$dataset == "dataset_it_oversampled",]
temp <- temp[stats::complete.cases(temp),]
temp <- temp %>% group_by(num_train) %>% summarize(Mean = mean(rmse_valid), Max = max(rmse_valid), Min = min(min(rmse_valid)), .groups = "drop")

use_ylim <- c(0, max(c(temp$Mean, temp$Max, temp$Min)))
plot(x = temp$num_train, y = temp$Mean, type="l", xlab = "Number of training instances", ylab = "RMSE (validation data)", xlim = c(5, 50), ylim = use_ylim)
grid()
lines(x = temp$num_train, y = temp$Max, col="blue")
lines(x = temp$num_train, y = temp$Min, col="red")
legend(x = 32, y = 4.25, legend = c("mean", "max", "min"), col = c("black", "blue", "red"), lwd = 2)
```

In figure \ref{fig:rob-reg-gbm2} we show the distribution of validation errors depending on the number of training instances (same setup as in figure \ref{fig:rob-reg-gbm1}).


```{r rob-reg-gbm2, echo=FALSE, message=FALSE, fig.height=8, fig.width=6, fig.cap="Ridge plot of distributions of RMSEs on validation data, dependent on the number of training samples.", fig.align="top", fig.pos="ht!"}
temp <- res_grid[res_grid$model_type == "gbm" & res_grid$do_pca & res_grid$dataset == "dataset_it_oversampled",]
temp <- temp[stats::complete.cases(temp),]
temp1 <- temp[order(temp$num_train),]
temp1 <- data.frame(
  num_train = factor(x = as.character(temp1$num_train), levels = as.character(rev(sort(unique(temp1$num_train)))), ordered = TRUE),
  rmse_valid = temp1$rmse_valid)

# https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html
ggplot(temp1, aes(x = rmse_valid, y = num_train, fill = ..x..)) +
  #stat_density_ridges(quantile_lines = TRUE) +
  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01) +
  scale_fill_viridis_c(option = "C") +
  theme_ridges() + 
  theme(legend.position = "none") +
  xlab("RMSE (validation data)") + ylab("Number of training instances")
```


We will also extract the number of components in case PCA was applied.

```{r}
# Do a PCA-only grid:
grid <- expand.grid(list(
  seed = 1:25,
  num_train = 2:50,
  use_dataset = use_datasets
))
grid$use_dataset <- as.character(grid$use_dataset)

res_grid_comps <- loadResultsOrCompute(file = "../results/rob-reg_res-grid_comps.rds", computeExpr = {
  cl <- parallel::makePSOCKcluster(min(32, parallel::detectCores()))
  parallel::clusterExport(cl = cl, varlist = use_datasets)
  
  doWithParallelClusterExplicit(cl = cl, stopCl = TRUE, expr = {
    foreach::foreach(
      rn = rownames(grid),
      .combine = rbind,
      .inorder = FALSE
    ) %dopar% {
      row <- grid[rn,]
      row$num_comps <- tryCatch({
        adaptive_training_caret(
          org_dataset = get(row$use_dataset), seeds = row$seed, model_type = model_types[1], # irrelevant
          num_train = row$num_train, do_pca = TRUE, return_num_comps = TRUE)
      }, error = function(cond) NA_real_)
      row
    }
  })
})
table(res_grid_comps$num_comps)
```

Above we see a table with how often a certain number of components was used, and in figure \ref{fig:num-comps-hist} it is illustrated in a histogram.

```{r num-comps-hist, echo=FALSE, fig.cap="Histogram over the number of components used in the grid search (threshold=0.975)."}
hist(res_grid_comps$num_comps, xlab = "Number of PCA components", main = "PCA components with threshold=0.975.")
```



## Analysis

Here we are going to analyze some of the results that we got (e.g., best pattern or model, expectations, lower/upper bounds, etc.).


### PCA vs. non-PCA

Here we want to find out whether applying principal components analysis is useful or not.

```{r echo=FALSE, warning=FALSE}
temp <- res_grid %>% group_by(do_pca) %>% summarise(
  RMSE_val_mean = mean(rmse_valid, na.rm = TRUE),
  RMSE_val_median = median(rmse_valid, na.rm = TRUE),
  RMSE_val_sd = sd(rmse_valid, na.rm = TRUE),
  RMSE_val_min = min(rmse_valid, na.rm = TRUE),
  RMSE_val_max = max(rmse_valid, na.rm = TRUE),
  .groups = "drop")


if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Mean, median, standard deviation, min, and max of the validation error, grouped by conditionally applying PCA.",
    label = "pca-comps-table"
  )
}
```

Let's also show the distributions. In figure \ref{fig:pca-comps-ridge} we observe that applying PCA results in more smaller errors which are also more steadily distributed.
While the lowest possible validation error as of above table \ref{tab:pca-comps-table} is $0$ for when not using PCA, this is likely due to the models overfitting, since without PCA we end up with many features and some models introduce extra degrees of freedom for each feature.
We shall therefore conclude that applying PCA is favorable.


```{r pca-comps-ridge, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Distributions of the validation error conditionally applying PCA."}
temp <- res_grid[!is.na(res_grid$rmse_valid),]
temp1 <- rbind(
  data.frame(
    do_pca = TRUE,
    rmse = (temp %>% filter(do_pca & rmse_valid <= quantile((temp %>% filter(do_pca))$rmse_valid, probs = .99)))$rmse_valid),
  data_frame(
    do_pca = FALSE,
    rmse = (temp %>% filter(!do_pca & rmse_valid <= quantile((temp %>% filter(!do_pca))$rmse_valid, probs = .99)))$rmse_valid)
)


ggplot(temp1, aes(x = rmse, y = do_pca, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 1, rel_min_height = 0.01) +
  scale_fill_viridis_c(option = "C") +
  theme_ridges() + 
  theme(legend.position = "none") +
  xlab("RMSE (validation data)") + ylab("Apply PCA") + xlim(-.5, 7)
```



```{r echo=FALSE, eval=FALSE}
#Todo:
#- SC vs. IT
#- PCA vs. non-PCA
#- ...
res_grid %>% filter(num_train < 10) %>% group_by(model_type, num_train) %>% summarise(RMSE = mean(rmse_valid, na.rm=TRUE)) %>% arrange(num_train, RMSE)
```





# Variable Importance for Assessing the Pattern

Obtaining variable importance allows us to understand which features are more or less important. Since in our case features are tied to a metric (area, correlation), a data kind (source code, issue tracking), an activity/metric (e.g., `REQ`, `SCD`), and a segment (one of ten equally long consecutive project phases), we can facilitate variable importance to learn about any of these (how important they are to predict a precise severity of the Fire Drill's presence).
We could potentially also (alternatively) learn about the quality of the ground truth. However, at this point we must assume that it is perfect as we have no evidence indicating otherwise.


Let's create a function that can give us a proper dataframe from the variable importance.

```{r}
create_variable_importance <- function(dataset) {
  model <- caret::train(
    x = dataset[, colnames(dataset) != "gt"], y = dataset[,]$gt, method = "pls",
    trControl = caret::trainControl(method = "LOOCV", number = 100))
  
  vi <- caret::varImp(model)
  df <- vi$importance
  res <- NULL
  
  for (rn in rownames(df)) {
    sp <- strsplit(x = rn, split = "_")[[1]]
    res <- rbind(res, data.frame(
      variable = sp[1],
      segment = as.numeric(sp[2]),
      metric = sp[3],
      vi = df[rn,]
    ))
  }
  res$vi_rel <- res$vi / sum(res$vi)
  list(res = res[order(-res$vi),], vi = vi)
}
```

Let's compute the variable importance for each of our datasets:

```{r message=FALSE, warning=FALSE}
vi_sc <- create_variable_importance(
  dataset = cbind(dataset_sc, data.frame(gt = ground_truth$consensus)))
vi_sc_cdf <- create_variable_importance(
  dataset = cbind(dataset_sc_cdf, data.frame(gt = ground_truth$consensus)))
vi_it <- create_variable_importance(
  dataset = cbind(dataset_it, data.frame(gt = ground_truth$consensus)))
```


The variable importance for each dataset is shown in figures \ref{fig:varimp-sc}, \ref{fig:varimp-sc-cdf}, and \ref{fig:varimp-it}.
Figure \ref{fig:varimp-all} shows a more detailed drill-down.


```{r varimp-sc, echo=FALSE, fig.height=7, fig.align="top", fig.pos="ht!", fig.cap="Variable importance for the source code dataset determined by partial least squares and cross validation."}
plot_var_imp(data = vi_sc$vi, relative = TRUE) + labs(title = "Variable Importance: Source Code")
```

```{r varimp-sc-cdf, echo=FALSE, fig.height=7, fig.align="top", fig.pos="ht!", fig.cap="Variable importance for the source code (CDF) dataset determined by partial least squares and cross validation."}
plot_var_imp(data = vi_sc_cdf$vi, relative = TRUE) + labs(title = "Variable Importance: Source Code (CDFs)")
```

```{r varimp-it, echo=FALSE, fig.height=7, fig.align="top", fig.pos="ht!", fig.cap="Variable importance for the issue-tracking dataset determined by partial least squares and cross validation."}
plot_var_imp(data = vi_it$vi, relative = TRUE) + labs(title = "Variable Importance: Issue-tracking")
```


## Analysis of most important Segments, Metrics, Activities, etc.

```{r echo=FALSE, eval=FALSE}
temp <- vi_sc$res %>% group_by(segment) %>% summarise(Mean_vi = sum(vi_rel))
barplot(height = temp$Mean_vi, names.arg = temp$segment)
```

\clearpage

```{r varimp-all, echo=FALSE, fig.width=8, fig.height=9, fig.align="top", fig.pos="ht!", fig.cap="Variable importance per pattern, activity, and segment."}
par(mfrow = c(3,1))



use_vars <- unique(vi_sc$res$variable)
mat <- matrix(nrow = 4, ncol = 10, dimnames = list(use_vars))

for (v in rownames(mat)) {
  mat[v,] <- as.numeric((vi_sc$res[vi_sc$res$variable == v,] %>% group_by(segment) %>% summarise(total_imp = sum(vi_rel), .groups = "drop"))$total_imp)
}

mat_sums <- apply(X = mat, MARGIN = 2, FUN = sum)

set.seed(3)
pal_4 <- rev(sample(RColorBrewer::brewer.pal(10, "Set3"), 4))
bp <- barplot(height = mat, names.arg = 1:10, xlab = "Segment", ylab = "Total rel. Importance", main = "Source code pattern", col = pal_4, ylim = c(0, max(mat_sums + .015)), angle = 30, density = 90 * mat_sums / max(mat_sums))
grid()
text(x = bp, y = .01 + mat_sums, labels = paste0(round(mat_sums, 4)))
legend("topright", rev(rownames(mat)), fill = rev(pal_4))


mat <- matrix(nrow = 4, ncol = 10, dimnames = list(use_vars))

for (v in rownames(mat)) {
  mat[v,] <- as.numeric((vi_sc_cdf$res[vi_sc_cdf$res$variable == v,] %>% group_by(segment) %>% summarise(total_imp = sum(vi_rel), .groups = "drop"))$total_imp)
}

mat_sums <- apply(X = mat, MARGIN = 2, FUN = sum)
bp <- barplot(height = mat, names.arg = 1:10, xlab = "Segment", ylab = "Total rel. Importance", main = "Source code pattern (CDF)", col = pal_4, ylim = c(0, max(mat_sums + .015)), angle = 30, density = 90 * mat_sums / max(mat_sums))
grid()
text(x = bp, y = .01 + mat_sums, labels = paste0(round(mat_sums, 4)))
legend("topright", rev(rownames(mat)), fill = rev(pal_4))





use_vars <- unique(vi_it$res$variable)
mat <- matrix(nrow = 3, ncol = 10, dimnames = list(use_vars))

for (v in rownames(mat)) {
  mat[v,] <- as.numeric((vi_it$res[vi_it$res$variable == v,] %>% group_by(segment) %>% summarise(total_imp = sum(vi_rel), .groups = "drop"))$total_imp)
}

mat_sums <- apply(X = mat, MARGIN = 2, FUN = sum)

set.seed(2)
pal_3 <- rev(sample(RColorBrewer::brewer.pal(12, "Set3"), 3))
bp <- barplot(height = mat, names.arg = 1:10, xlab = "Segment", ylab = "Total rel. Importance", main = "Issue-tracking pattern", col = pal_3, ylim = c(0, max(mat_sums + .02)), angle = 30, density = 100 * mat_sums / max(mat_sums))
grid()
text(x = bp, y = .01 + mat_sums, labels = paste0(round(mat_sums, 3)))
legend("topleft", rev(rownames(mat)), fill = rev(pal_3))
```


# Pattern-less detection

Let's try something radically different and simplified. From the beginning, we had used patterns so that we could measure the differences between a project and each pattern.
That was useful for the research case of developing a scoring mechanism.
If we do simple regression, this is not needed. Using a (constant) pattern can be thought of applying a (constant) non-linear transform to each activity.
For example, consider we have a pattern activity $f(t)$ (where $t$ is time) and a project activity $g(t)$. What we have done so far was to produce $g'(t)$ as, for example, $g'=g-f$. When we used automatic calibration, the goal was to uniformly sample (create random instances) of project activities in order to estimate the probability density of $g'(t)$.
Since $f(t)$ was a constant function (i.e., the same $f$ was used for each project), this transform is redundant if everything we want is simply a regression model.


We will therefore attempt the following:
We will model each activity, both for source code and issue-tracking, as probability distributions (PDFs).
Since there is nothing to compare these distributions to, we will simply create features by integrating the PDF on equally-long segments (e.g., $[0.0,0.1]; \dots; [0.9,1.0]$), which is equivalent to $\operatorname{CDF}(b)-\operatorname{CDF}(a)$ for some segment $[a,b]$.
Each feature will therefore express the amount of activity happening in the corresponding segment.
As for the source code density, we will simply take the average of it in each segment.
We will have to re-design the issue-tracking activities. The plan is to do this very similar to how we re-designed the source code activities in this report, using rejection sampling to obtain somewhat smooth densities.
We can reuse our method for adaptively computing grid results, we'll only have to prepare new datasets.


Let's define a shortcut function for rejection sampling (using $x/y$ data to approximate a function from first):

```{r}
rejection_sampling <- function(x_data, y_data, xlim = c(0, 1), num_x = 1e5) {
  tempf <- stats::approxfun(x = x_data, y = y_data, rule = 2)
  use_x <- seq(from = xlim[1], to = xlim[2], length.out = num_x)
  use_y <- stats::runif(n = length(use_x), min = 0, max = max(y_data))
  tempdens <- stats::density(x = use_x[use_y <= tempf(use_x)], bw = "SJ", cut = TRUE)
  
  list(
    func = tempf,
    PDF = stats::approxfun(x = tempdens$x, y = tempdens$y, yleft = 0, yright = 0),
    CDF = make_smooth_ecdf(values = use_x[use_y <= tempf(use_x)], slope = 0, inverse = FALSE))
}
```


Let's create a function to compute the Jensen--Shannon divergence between two PDFs on a segment:

```{r}
kl_div_segment <- function(p, q, ext = c(0, 0.1), xtol = 1e-20) {
  cubature::cubintegrate(f = function(x) {
    p_ <- p(x)
    q_ <- q(x)
    if (abs(p_) < xtol || abs(q_) < xtol) 0 else p_ * log(p_ / q_)
  }, lower = ext[1], upper = ext[2])$integral
}

jsd_segment <- function(p, q, ext = c(0, 0.1), xtol = 1e-20) {
  cubature::cubintegrate(f = function(x) {
    p_ <- p(x)
    q_ <- q(x)
    m_ <- .5 * (p_ + q_)
    if (abs(p_) < xtol || abs(q_) < xtol || abs(m_) < xtol) 0 else .5 * p_ * log(p_ / m_) + .5 * q_ * log(q_ / m_)
  }, lower = ext[1], upper = ext[2])$integral
}
```


## Create Dataset for Source Code

Source code projects are already present as PDFs, so we can just integrate for the three maintenance activities (actually, we'll use the CDFs directly).
For the source code density, we'll draw some samples per segment and average the result, that's it.


```{r}
grid <- expand.grid(list(
  interval = 1:10,
  activity = c("A", "CP", "FREQ", "A_vs_CP", "A_vs_FREQ", "CP_vs_FREQ") # Let's skip SCD and only use the activities for now
))
grid$activity <- as.character(grid$activity)

dataset_np_sc <- `colnames<-`(x = matrix(nrow = 0, ncol = nrow(grid)), value = sapply(X = rownames(grid), FUN = function(rn) {
  r <- grid[rn,]
  paste(r$activity, r$interval, sep = "_")
}))


for (pname in names(projects_sc)) {
  newrow_sc <- `colnames<-`(x = matrix(ncol = ncol(dataset_np_sc)), value = colnames(dataset_np_sc))
  
  for (rn in rownames(grid)) {
    row <- grid[rn,]
    interval_ext <- c(row$interval / 10 - .1, row$interval / 10)
    feat_name <- paste(row$activity, row$interval, sep = "_")
    is_versus <- grepl(pattern = "_vs_", x = row$activity)
    
    if (is_versus) {
      # Calculate symmetric divergence of two activities
      sp <- strsplit(x = row$activity, split = "_vs_")[[1]]
      pdf_p <- projects_sc[[pname]][[sp[1]]]
      pdf_q <- projects_sc[[pname]][[sp[2]]]
      newrow_sc[1, feat_name] <- jsd_segment(p = pdf_p, q = pdf_q, ext = interval_ext)
    } else {
      # Integrate the PDF of the activity
      newrow_sc[1, feat_name] <- cubature::cubintegrate(
        f = projects_sc[[pname]][[row$activity]], lower = interval_ext[1], upper = interval_ext[2])$integral
    }
  }
  
  dataset_np_sc <- rbind(dataset_np_sc, newrow_sc)
}

dataset_np_sc <- as.data.frame(dataset_np_sc)
```


## Create Dataset for Issue-Tracking

Here we load the original data and transform it into a density. Issue-tracking data has one major difference compared to source code data.
In the latter, we know the timely accumulation of certain activities, but we do not know how much time was spent on an activity.
In issue-tracking data, we do know that. Therefore, we will use the following procedure for creating the issue-tracking data:

1. Use normalized time as $x$ and the activity's spent time as $y$.
2. Filter $x$ by using the condition $y>0$.
3. Estimate a density over __$x$__, using the duration of each activity ($y$) as weights. We will have to normalize the non-zero activities to sum up to $1$ first.
4. Perform rejection sampling on that density to obtain a smooth CDF which will be used to estimate the amount of each activity in each segment.


```{r}
library(readxl)

grid <- expand.grid(list(
  interval = 1:10,
  activity = c("REQ", "DEV", "DESC", "REQ_vs_DEV", "REQ_vs_DESC", "DEV_vs_DESC")
))
grid$activity <- as.character(grid$activity)

dataset_np_it <- `colnames<-`(x = matrix(nrow = 0, ncol = nrow(grid)), value = sapply(X = rownames(grid), FUN = function(rn) {
  r <- grid[rn,]
  paste(r$activity, r$interval, sep = "_")
}))



rejection_sampling_issue_tracking <- function(use_x, use_y) {
  # Apply filter:
  temp_x <- use_x[use_y > 0]
  temp_y <- use_y[use_y > 0]
  
  tryCatch({
    temp_dens <- suppressWarnings({
      stats::density(x = temp_x, weights = temp_y / sum(temp_y), bw = "SJ", cut = TRUE)
    })
    use_x <- temp_dens$x
    use_y <- temp_dens$y
  }, error = function(cond) {
    if (interactive()) {
      print(paste0("Cannot estimate density for project ", pname, " and activity ", activity))
    }
  })
  
  rejection_sampling(x_data = use_x, y_data = use_y, xlim = range(use_x))
}
```


```{r}
for (pname in names(projects_it)) {
  temp <- read_excel("../data/FD_issue-based_detection.xlsx", sheet = pname)
  newrow_it <- `colnames<-`(x = matrix(ncol = ncol(dataset_np_it)), value = colnames(dataset_np_it))
  
  for (activity in c("REQ", "DEV", "DESC")) {
    use_y <- as.numeric(temp[[tolower(activity)]])
    use_y[is.na(use_y)] <- 0
    cdf <- rejection_sampling_issue_tracking(use_x = temp$`time%`, use_y = use_y)$CDF
    
    for (idx in 1:10) {
      interval_ext <- c(idx / 10 - .1, idx / 10)
      feat_name <- paste(activity, idx, sep = "_")
      newrow_it[1, feat_name] <- cdf(interval_ext[2]) - cdf(interval_ext[1])
    }
  }
  
  for (divergence in c("REQ_vs_DEV", "REQ_vs_DESC", "DEV_vs_DESC")) {
    sp <- strsplit(x = divergence, split = "_vs_")[[1]]
    
    use_y_p <- as.numeric(temp[[tolower(sp[1])]])
    use_y_p[is.na(use_y_p)] <- 0
    pdf_p <- rejection_sampling_issue_tracking(use_x = temp$`time%`, use_y = use_y_p)$PDF
    
    use_y_q <- as.numeric(temp[[tolower(sp[2])]])
    use_y_q[is.na(use_y_q)] <- 0
    pdf_q <- rejection_sampling_issue_tracking(use_x = temp$`time%`, use_y = use_y_q)$PDF
    
    for (idx in 1:10) {
      interval_ext <- c(idx / 10 - .1, idx / 10)
      feat_name <- paste(divergence, idx, sep = "_")
      newrow_it[1, feat_name] <- jsd_segment(p = pdf_p, q = pdf_q, ext = interval_ext)
    }
  }
  
  dataset_np_it <- rbind(dataset_np_it, newrow_it)
}
dataset_np_it <- as.data.frame(dataset_np_it)
```


## Oversampling

The non-pattern datasets have the same problem as the ones using a pattern: the data is scarce and we have many features.
Therefore, we will go through the same procedure and oversample them.

```{r}
dataset_np_sc_oversampled <- cbind(dataset_np_sc, data.frame(gt = ground_truth$consensus))
dataset_np_sc_oversampled <- balance_num_labels(dataset = dataset_np_sc_oversampled, num = min_rows)

dataset_np_it_oversampled <- cbind(dataset_np_it, data.frame(gt = ground_truth$consensus))
dataset_np_it_oversampled <- balance_num_labels(dataset = dataset_np_it_oversampled, num = min_rows)
```



## Adaptive Training

And here we compute the grid (test each model on each dataset).

__Note__: We have computed the following actually three times. The first time using both type of features (amount of activity per segment, pair-wise symmetric divergence between activities), and then once using only one type of feature.
There are three folders in the results-folder: `np_both`, `np_amount`, and `np_divergence` holding the results for each complete run.
We tested all three combinations to find out whether fewer features are sufficient for a low generalization error.

Attention: The following is expensive.

```{r}
compute_np_grid_adaptive <- function(type = c("both", "amount", "divergence")) {
  type <- match.arg(type)
  
  grid <- expand.grid(list(
    seed = 1:50,
    num_train = 2:50,
    do_pca = c(TRUE, FALSE)
  ))
  model_types <- c("gbm", "glm", "nnet", "rf", "svmPoly", "treebag")#, "avNNet", "M5", "rankModel")
  use_datasets <- c("dataset_np_it_oversampled", "dataset_np_sc_oversampled")
  
  
  library(foreach)
  loadResultsOrCompute(file = paste0("../results/np_", type, "/rob-reg-np_res-grid.rds"), computeExpr = {
    res <- NULL
    
    for (use_dataset in use_datasets) {
      dataset <- get(use_dataset)
      # Let's select the right columns. If type = "both", then use all.
      if (type == "amount") {
        # Take out the "_vs_"-columns:
        cn <- colnames(dataset)
        # This also keeps the "gt"-column
        dataset <- dataset[, cn[!grepl(pattern = "_vs_", x = cn)]]
      } else if (type == "divergence") {
        cn <- colnames(dataset)
        # Only take "_vs_"-columns and re-append the "gt"-column
        dataset <- dataset[, c(cn[grepl(pattern = "_vs_", x = cn)], "gt")]
      }
      
      for (model_type in model_types) {
        
        the_file <- paste0("../results/np_", type, "/rob-reg-np_res-grid_", model_type, "_", use_dataset, ".rds")
        temp <- as.data.frame(loadResultsOrCompute(file = the_file, computeExpr = {
          temp1 <- doWithParallelCluster(numCores = min(parallel::detectCores(), 123), expr = {
            foreach::foreach(
              rn = rownames(grid),
              .combine = rbind,
              .inorder = FALSE,
              .verbose = TRUE
            ) %dopar% {
              tryCatch({
                row <- grid[rn,]
                adaptive_training_caret(
                  org_dataset = dataset, seeds = row$seed, model_type = model_type,
                  num_train = row$num_train, do_pca = row$do_pca,
                  num_caret_repeats = 25)
              }, error = function(cond) {
                NULL # Return an empty result that will not disturb .combine
              })
            }
          })
          saveRDS(object = temp1, file = the_file)
          temp1
        }))
        temp$dataset <- rep(use_dataset, nrow(temp))
        
        res <- rbind(res, temp)
      }
    }
  
    res$dataset <- factor(res$dataset)
    res
  })
}
```


```{r}
res_grid_np_both <- compute_np_grid_adaptive(type = "b")
res_grid_np_amount <- compute_np_grid_adaptive(type = "a")
res_grid_np_divergence <- compute_np_grid_adaptive(type = "d")
```


```{r np-overview, echo=FALSE, fig.width=12, fig.height=15, fig.cap="Comparing the expected validation error of the trained models of the No-pattern approach. Amount-only features are red, divergence-only are green, and both are blue."}
par(mfrow = c(6,4), mar = c(4.5,4.5,2,0.25))

#set.seed(1)
#pal_3 <- rev(sample(RColorBrewer::brewer.pal(10, "Set3"), 3))
pal_3 <- c("red", "darkgreen", "blue")
names(pal_3) <- c("amount", "divergence", "both")

for (model_type in c("gbm", "glm", "nnet", "rf", "svmPoly", "treebag")) {
  for (ds_pca in c("sc_yes", "sc_no", "it_yes", "it_no")) {
    sp <- strsplit(x = ds_pca, split = "_")[[1]]
    use_data <- sp[1]
    do_pca <- sp[2] == "yes"
    
    use_ylim <- c(0, 3)
    plot(x = -100, y = -100, xlim = c(0, 50), ylim = use_ylim,
         xlab = "Number of training instances", ylab = "RMSE (validation data)",
         main = paste(model_type, use_data, if (do_pca) "PCA" else "no PCA", sep = ", "))
    grid()
    
    
    for (type in c("amount", "divergence", "both")) {
      temp <- get(paste0("res_grid_np_", type))
      temp <- temp[temp$model == model_type & temp$do_pca == do_pca & temp$dataset == paste0("dataset_np_", use_data, "_oversampled"),]
      temp <- temp[stats::complete.cases(temp[, colnames(temp)[colnames(temp) != "num_comps"]]),]
      temp <- temp %>% group_by(num_train) %>% summarize(Mean = mean(rmse_valid), .groups = "drop")
      
      lines(x = temp$num_train, y = temp$Mean, col = pal_3[type])
    }
  }
}
```

\clearpage

Figure \ref{fig:np-overview} compares all of the six trained models across both data types, source code and issue-tracking, with and without applied PCA, and using different feature sets (amount only, divergence only, both).
The visual inspection tells us that the best model is the neural network for source code, and the one not using PCA being a little better ("nnet, sc, no PCA").
That model has, for the number of training instances of $15$, an expectation $<1$, regardless of the feature set used.
The best feature set by a small margin is "divergence", followed by "both".


## PCA vs. non-PCA

We will inspect the best model (neural network) and check whether it makes a difference to apply PCA or not when using both types of features (amount and divergence).
Table \ref{tab:nnet-pca-comps-table} shows the effects of conditionally applying PCA.
It appears that the expected average validation error is lower and has also lower variance when __not__ using PCA.
Simplified, more features is better, but the difference is not large (but still significant).


```{r echo=FALSE, warning=FALSE}
temp <- res_grid_np_both %>% filter(model_type == "nnet") %>% group_by(do_pca) %>% summarise(
  RMSE_val_mean = mean(rmse_valid, na.rm = TRUE),
  RMSE_val_median = median(rmse_valid, na.rm = TRUE),
  RMSE_val_sd = sd(rmse_valid, na.rm = TRUE),
  RMSE_val_min = min(rmse_valid, na.rm = TRUE),
  RMSE_val_max = max(rmse_valid, na.rm = TRUE),
  .groups = "drop")


if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Mean, median, standard deviation, min, and max of the validation error, grouped by conditionally applying PCA.",
    label = "nnet-pca-comps-table"
  )
}
```


## Confidence Intervals

We show some results for this model in table \ref{tab:nnet-best-model}.
Using all features, only $14$ instances are required to achieve a generalization error of $\leq1$ (with a standard deviation of $\approx0.31$).
The error falls to $\approx0.9$ for $15$ instances, and for $17$ instances, the error plus one standard deviation is even below $1$.


Before we evaluate the confidence intervals, let's take a look at the expected validation error, and the observed maximum and minimum (figure \ref{fig:nnet-expected-gen-err}).
We observe that the error steadily decreases with more and more training data being added.
The best results were obtained when ___no___ PCA was applied and the dataset was using source code data (without pattern) and both type of features (amount and divergence).
The expected error on the withheld validation data falls below $1$ for about $15$ or more training instances.
Examining the confidence intervals will allow us to understand the likelihood for the RMSE to deviate from the expectation.


```{r nnet-expected-gen-err, echo=FALSE, fig.cap="The expected validation error for the best-performing model and dataset."}
temp <- res_grid_np_both[,]
temp <- temp[temp$model_type == "nnet" & !temp$do_pca & temp$dataset == "dataset_np_sc_oversampled",]
temp <- temp[stats::complete.cases(temp[, colnames(temp)[colnames(temp) != "num_comps"]]),]
temp <- temp %>% group_by(num_train) %>% summarize(Mean = mean(rmse_valid), Max = max(rmse_valid), Min = min(min(rmse_valid)), .groups = "drop")

use_ylim <- c(0, max(c(temp$Mean, temp$Max, temp$Min)))
use_ylim <- c(0, 5)
plot(x = temp$num_train, y = temp$Mean, type="l", xlab = "Number of training instances", ylab = "RMSE (validation data)", xlim = c(0, 50), ylim = use_ylim, main = "Neural network performance in relation to available training data.")
grid()
lines(x = temp$num_train, y = temp$Max, col="blue")
lines(x = temp$num_train, y = temp$Min, col="red")
legend(x = 32, y = 4.25, legend = c("mean", "max", "min"), col = c("black", "blue", "red"), lwd = 2)
```


```{r echo=FALSE}
temp <- res_grid_np_both
temp <- temp[temp$model == "nnet" & !temp$do_pca & temp$dataset == "dataset_np_sc_oversampled" & temp$num_train <= 41,]
temp <- temp[stats::complete.cases(temp[, colnames(temp)[colnames(temp) != "num_comps"]]),]
temp <- temp %>% group_by(num_train) %>% summarize(
  "Unimodal?" = LaplacesDemon::is.unimodal(x = rmse_valid),
  "Normal?" = stats::shapiro.test(x = rmse_valid)$p.value >= 0.05,
  p.value = stats::shapiro.test(x = rmse_valid)$p.value,
  Mean = mean(rmse_valid), Median = median(rmse_valid), Sd = sd(rmse_valid), Min = min(rmse_valid), Max = max(rmse_valid), .groups = "drop")

if (interactive()) {
  temp
} else {
  knitr::kable(
    x = cbind(temp[, 1:4], round(temp[, 5:9], 4)),
    booktabs = TRUE,
    caption = "Normality test and p-value, mean, median, standard deviation, min, and max of the validation error, for the neural network model, using the source code data, divergence-only features, and no PCA. The error reduces steadily with the number of training instances.",
    label = "nnet-best-model"
  )
}
```


Table \ref{tab:nnet-best-model} also shows whether the generalization error is normally distributed [@shapiro1965normal; @royston1982], which it is in many cases (and in some almost).
The advantage of a normally distributed generalization error allows us to make statements with regard to the standard deviation, additionally to the expected error.
Taking the case of $17$ training instances again, the expected generalization error is $\approx0.75$ and the standard deviation is $\approx0.25$.
Since the generalization error is normally distributed, we expect the error to deviate less than or equal to $0.25$ in about __$\approx68$%__ of all cases, since for a normal distribution, cases that are up to one standard deviation away from the mean account for $\approx68$% of all cases (three sigma rule; @pukelsheim1994threesigma).
In other words, a neural network trained on $17$ instances will deliver predictions with a probability of $\approx68$% that are off by $\leq1$.


Another interesting example from table \ref{tab:nnet-best-model} is the one using $26$ instances.
The validation error there is unimodal, normally distributed, and has an expectation of $0.497$ and standard deviation of $0.153$.
This means, according to the Three Sigma rule which we can apply in this case, that we can expect $99.7$% of all predicted values to be below $\approx0.956$ (mean plus three standard deviations).
In other words, we can practically guarantee to predict an error that is always less than $1$.
With about $68$% certainty, the error will be less than $\approx0.65$. With about $95$% certainty, it will be below $\approx0.803$.
I'd say these are pretty good expectations for a model trained on only $26$ instances.


Table \ref{tab:nnet-best-model-confidence-intervals} shows some common confidence intervals (where "Low" and "High" demarcate the lowest and highest expected deviation, according to the mean and standard deviation of the validation error and the fact that it is normally distributed).
For example, with a confidence of $99$%, the lowest expected error is just $0.1115$, and the highest is only $1.3960$.
Note that the error cannot be negative and that this happens because it is not a perfect normal distribution.
So, for example, the confidence is the probability that $Pr(\mu-a\leq X\leq\mu+a)$. If we use two times the standard deviation, then this will cover $95.45$% of all values.


The table also shows lows and high according to Chebyshev's inequality [@chebyshev1867].
Chebyshev's inequality is useful in cases when the actual distribution is not known. While the lows are lower and the highs are higher, we still get upper and lower bounds, even for cases where the generalization error is _not_ normally distributed.


```{r echo=FALSE}
temp <- res_grid_np_both
temp <- temp[temp$model == "nnet" & !temp$do_pca & temp$dataset == "dataset_np_sc_oversampled" & temp$num_train == 17,]$rmse_valid

temp <- data.frame(
  Confidence = sort(c(seq(from = 50, to = 95, by = 5), 68.27, 95.45, 97.5, 98, 99, 99.5, 99.73))
) %>% group_by(Confidence) %>% summarize(
  Low = round(qnorm(c(.5 - Confidence / 200), mean = mean(temp), sd = sd(temp)), 4),
  Mean1 = round(mean(temp), 2),
  High = round(qnorm(c(.5 + Confidence / 200), mean = mean(temp), sd = sd(temp)), 4),
  Chebyshev_Low = round(mean(temp) - sd(temp) * sqrt(1 / (1 - Confidence / 100)), 4),
  Mean2 = round(mean(temp), 2),
  Chebyshev_High = round(mean(temp) + sd(temp) * sqrt(1 / (1 - Confidence / 100)), 4),
  .groups = "drop")

if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Confidence intervals of the best neural network model trained on $17$ instances. The table includes values according to the 68-95-99.7\\%-rule and Chebyshev's inequality.",
    label = "nnet-best-model-confidence-intervals"
  )
}
```


Table \ref{tab:confidence-intervals-inequalities} demonstrates four different inequalities.
For $17$ training instances and a hypothetical RMSE (the `diff` column), it shows the confidence for the three sigma rule, the Vysochanskij--Petunin inequality, Gauss' inequality, and Chebyshev's inequality.
The order of the inequalities reflects their tightness. The more assumptions we can make about the distribution of the sample, the tighter we can select the bounds.
There are various other ___concentration inequalities___, such as Markov's, Paley--Zygmund's, Cantelli's, or Chernoff's bounds.
A disadvantage with Chebyshev's inequality is that it only works for more than one standard deviation ($\approx1.63$ standard deviations for Vysochanskij--Petunin's inequality).
While Gauss' inequality works in that case, it requires to determine the mode of the sample, and that the sample is unimodal.


Therefore, in practice, I suggest to try one inequality after the other and take the one that has its requirements satisfied first (testing the tight ones first).


```{r echo=FALSE}
tempf <- function(vals, diff_) {
  mean_ <- mean(vals)
  sd_ <- sd(vals)
  min_ <- sqrt(8/3)
  diff_ <- abs(diff_)
  lambda <- sd_ / diff_
  modes <- LaplacesDemon::Modes(x = vals)
  mode_ <- modes$modes[which.max(modes$size)]
  r_ <- sqrt((mean_ - mode_)^2 + sd_^2)
  k2 <- (diff_ / sd_)^2
  
  data.frame(
    diff = diff_,
    ThreeSigmaR = 200 * abs(.5 - pnorm(q = mean_ + diff_, mean = mean_, sd = sd_)),
    VysoPetunin = if (diff_ >= min_ * sd_) 100 * (1 - 4 / 9 * lambda^2) else 0,
    GaussIneq = 100 * (1 - (if (diff_ >= 2 * r_ / sqrt(3)) ((2 * r_) / (3 * diff_))^2 else (1 - diff_ / (r_ * sqrt(3))))),
    Chebyshev = if (k2 < 1) 0 else 100 * (1 - 1 / k2)
  )
}
```


```{r echo=FALSE}
temp <- res_grid_np_both
temp <- temp[temp$model == "nnet" & !temp$do_pca & temp$dataset == "dataset_np_sc_oversampled" & temp$num_train == 17,]
temp <- temp[stats::complete.cases(temp[, colnames(temp)[colnames(temp) != "num_comps"]]),]

# Let's create a data.frame that computes the bounds based on multiple methods
res <- NULL

for (diff_ in seq(0, 2, by=0.1)) {
  res <- rbind(res, tempf(vals = temp$rmse_valid, diff_ = diff_))
}


if (interactive()) {
  res
} else {
  knitr::kable(
    x = res,
    booktabs = TRUE,
    caption = "Four different inequalities giving and their confidence for a given deviation (for the best neural network model trained on $17$ instances). The Three Sigma rule has the tightest bounds, while Gauss' inequality has the loosest.",
    label = "confidence-intervals-inequalities"
  )
}
```







```{r echo=FALSE}
temp <- res_grid_np_both
temp <- temp[temp$model == "nnet" & !temp$do_pca & temp$dataset == "dataset_np_sc_oversampled",]# & temp$num_train <= 20,]
temp <- temp[stats::complete.cases(temp[, colnames(temp)[colnames(temp) != "num_comps"]]),]

tempf <- function(num_train, rmse) {
  vals <- temp[temp$num_train == num_train,]$rmse_valid
  mean_ <- mean(vals)
  sd_ <- sd(vals)
  temp <- 200 * abs(.5 - pnorm(q = rmse, mean = mean_, sd = sd_))
  return(temp)
  #if (temp < 99.9) temp else NA_real_
}

tempgrid <- outer(X = seq(min(temp$num_train), max(temp$num_train), by = 1), Y = seq(0, 2, length.out = 250), FUN = Vectorize(tempf))
```


Figure \ref{fig:nnet-rmse-cis} shows the confidence interval of the generalization error per number of training instances.
It is obvious that with a lower generalization error, the size of the confidence interval reduces as well (i.e., less variance and more stable predictions).


```{r nnet-rmse-cis, echo=FALSE, fig.width=8, fig.height=5, fig.cap="Continuous confidence of the neural network predictor, with regard to number of training instances. Shown are the values according to the 68-95-99.7\\%-rule (assuming a normal distribution for every generalization error)."}
fields::image.plot(list(
  x = seq(min(temp$num_train), max(temp$num_train), by = 1), y = seq(0, 2, length.out = 250), z = tempgrid),
  col = viridis::inferno(50), legend.shrink = .75, legend.lab = "Confidence", legend.line = 2.75,
  xlab = "Number of training instances", ylab = "RMSE (validation data)",
  main = "Confidence intervals according to the 68-95-99.7%-rule.")
grid(col = "darkgrey", lwd = 2)
abline(h = c(.5, 1), lwd = 3, lty = c(2,5), col = "#0377fc")
```


Figure \ref{fig:nnet-rmse-cis-chebyshev} shows the same confidence intervals for the generalization error, according to Chebyshev's inequality (ignoring that some errors are, in fact, normally distributed).
Note that this figure contains a wide area where the confidence is pictured as unknown.
This is because Chebyshev's inequality is only useful for $k>1$, that is, more than one standard deviations. For $k\leq1$, we cannot guarantee that any of the values will be within the mean plus/minus up to one standard deviation using Chebyshev's rule, so its utility lies more in obtaining upper bounds for when the distribution is not known.
For the sake of the figure, I have implemented it as $\frac{1}{\max{(1,k^2)}}$. You see that we have a wide dark area in which we cannot guarantee anything (that it will contain values).
This would then be essentially the same as Selberg's generalization [@selberg1940].

There is also the Vysochanskij--Petunin inequality, but it only gives bounds for more than $\sqrt{(\frac{8}{3})}$ standard deviations [@vysochanskij1980justification].
It is defined as $\frac{4}{9\lambda^2}$, so for the minimum lambda it includes $\approx\frac{5}{6}$ of all values (and is not applicable for probabilities below that).

```{r echo=FALSE}
compute_inequalities <- function(num_train, rmse, use = c("threesigma", "vyso", "chebyshev", "gauss")) {
  vals <- temp[temp$num_train == num_train,]$rmse_valid
  mean_ <- mean(vals)
  sd_ <- sd(vals)
  use <- match.arg(use)
  
  if (use == "threesigma") {
    # Three Sigma rule
    temp <- 200 * abs(.5 - pnorm(q = rmse, mean = mean_, sd = sd_))
    temp
    #if (temp < 99.9) temp else NA_real_
  } else if (use == "vyso") {
    # VysochanskijâPetunin inequality
    min_ <- sqrt(8/3)
    diff_ <- abs(mean_ - rmse)
    lambda <- sd_ / diff_
    #if (diff_ >= min_ * sd_) 100 * (1 - 4 / 9 * lambda^2) else NA_real_
    max(0, 100 * (1 - 4 / 9 * lambda^2))
  } else if (use == "chebyshev") {
    # Chebyshev:
    diff <- abs(mean_ - rmse)
    k2 <- (diff / sd_)^2
    100 * (1 - 1 / max(1, k2))
    #if (k2 < 1) NA_real_ else 100 * (1 - 1 / k2)
  } else if (use == "gauss") {
    # Gauss's inequality:
    k <- abs(mean_ - rmse)
    modes <- LaplacesDemon::Modes(x = vals)$modes
    if (length(modes) != 1) {
      #return(NA_real_) # Only works for unimodal data; fall back to Chebyshev:
      k2 <- (k / sd_)^2
      100 * (1 - 1 / max(1, k2))
    }
    mode_ <- modes[1]
    r <- sqrt((mean_ - mode_)^2 + sd_^2)
    100 * (1 - (if (k >= 2 * r / sqrt(3)) ((2*r) / (3*k))^2 else (1 - k / (r * sqrt(3)))))
  }
}
```


```{r echo=FALSE}
x <- seq(min(temp$num_train), max(temp$num_train), by = 1)
y <- seq(0, 2, length.out = 250)

tempgrids <- loadResultsOrCompute(file = "../results/rob-reg_inequalities.rds", computeExpr = {
  list(
    threesigma = outer(X = x, Y = y, FUN = Vectorize(FUN = function(a, b) compute_inequalities(num_train = a, rmse = b, use = "t"))),
    vyso = outer(X = x, Y = y, FUN = Vectorize(FUN = function(a, b) compute_inequalities(num_train = a, rmse = b, use = "v"))),
    chebyshev = outer(X = x, Y = y, FUN = Vectorize(FUN = function(a, b) compute_inequalities(num_train = a, rmse = b, use = "c"))),
    gauss = outer(X = x, Y = y, FUN = Vectorize(FUN = function(a, b) compute_inequalities(num_train = a, rmse = b, use = "g"))))
})
```

```{r nnet-rmse-cis-chebyshev, echo=FALSE, fig.width=8, fig.height=6.5, fig.cap="Continuous confidence of the neural network predictor, with regard to number of training instances. Shown are the values according to four different inequalities."}

par(mfrow = c(2,2), mar = c(2,4,2,1))

use <- c(threesigma = "the Three Sigma Rule", vyso = "the Vysochanskij--Petunin inequality", chebyshev = "Chebyshev's inequality", gauss = "Gauss' inequality")

for (idx in 1:4) {
  if (idx > 2) {
    par(mar = c(4,4,2,1))
  }
  key <- names(use)[idx]
  fields::image.plot(list(
    x = x, y = y, z = tempgrids[[key]]),
    col = viridis::inferno(50), legend.shrink = .75, legend.lab = "Confidence", legend.line = 2.5, zlim = c(0, 100),
    xlab = if (idx < 3) "" else "Number of training instances", ylab = if (idx == 2 || idx == 4) "" else "RMSE (validation data)",
    main = paste0("Approx. confidence intervals according to ", use[key], "."), cex.main = .7)
  grid(col = "darkgrey", lwd = 2)
  abline(h = c(.5, 1), lwd = 3, lty = c(2,5), col = "#0377fc")
}
```


## Variable Importance



```{r echo=FALSE}
# We will have to override the function for variable importance:
create_variable_importance <- function(dataset) {
  model <- caret::train(
    x = dataset[, colnames(dataset) != "gt"], y = dataset[,]$gt, method = "pls",
    trControl = caret::trainControl(method = "LOOCV", number = 100))
  
  vi <- caret::varImp(model)
  df <- vi$importance
  res <- NULL
  
  for (rn in rownames(df)) {
    # "A_10"
    # "A_vs_FREQ_8"
    is_versus <- grepl(pattern = "_vs_", x = rn)
    sp <- NULL
    if (is_versus) {
      temp <- strsplit(x = rn, split = "_")[[1]]
      sp <- c(paste(temp[1:3], collapse = "_"), temp[4])
    } else {
      sp <- strsplit(x = rn, split = "_")[[1]]
    }
    
    res <- rbind(res, data.frame(
      feature = sp[1],
      is_versus = is_versus,
      segment = as.numeric(sp[2]),
      vi = df[rn,]
    ))
  }
  res$vi_rel <- res$vi / sum(res$vi)
  list(res = res[order(-res$vi),], vi = vi)
}
```


```{r warning=FALSE}
vi_np_sc <- create_variable_importance(dataset = dataset_np_sc_oversampled)
```


```{r varimp-np-sc, echo=FALSE, fig.width=8, fig.height=5, fig.cap="Variable importance for the no-pattern source code dataset estimated using 100 repeats and partial least squares."}
par(mar = c(4.5,4.5,3,8))
use_vars <- rev(c("A", "CP", "FREQ"))#, "A_vs_CP", "A_vs_FREQ", "CP_vs_FREQ")) # rev(unique(vi_np_sc$res$feature))
mat <- matrix(nrow = 3, ncol = 10, dimnames = list(use_vars))

for (v in rownames(mat)) {
  mat[v,] <- as.numeric((vi_np_sc$res[vi_np_sc$res$feature == v,] %>% group_by(segment) %>% summarise(total_imp = sum(vi_rel), .groups = "drop"))$total_imp)
}

mat_sums <- apply(X = mat, MARGIN = 2, FUN = sum)

pal_4 <- RColorBrewer::brewer.pal(6, "Set3")[4:6]
bp <- barplot(height = mat, names.arg = 1:10, xlab = "Segment", ylab = "Total rel. Importance", main = "No-pattern source code variable importance", col = pal_4, ylim = c(0, max(mat_sums + .02)), angle = 30, density = 40 + 60 * mat_sums / max(mat_sums))
grid()
text(x = bp, y = .01 + mat_sums, labels = paste0(round(mat_sums, 4)))
legend(x = 12.5, y= .14, rev(rownames(mat)), fill = rev(pal_4), bg = "white", xpd = TRUE)
```


In figure \ref{fig:varimp-np-sc} it becomes clear that the divergence features are much less important than the amount features.
That could indicate, that these features contribute towards are more robust estimator.
So if both types of features are used simultaneously, it is much more important to know _how much_ of each activity happens in each segment, than it is to know _how different_ the measured amounts across activities were.
When we plot the same figure using amount features only, the picture is almost the same, minus the importance of the divergence features.


The total importance for the divergence features is `r vi_np_sc$res %>% filter(is_versus) %>% select(vi_rel) %>% sum() %>% round(digits = 3)` and the total importance for the amount features is `r vi_np_sc$res %>% filter(!is_versus) %>% select(vi_rel) %>% sum() %>% round(digits = 3)`.
Table \ref{tab:varimp-np-sc-per-feature} shows the total importance per feature. Here it becomes blear that the activities `CP` and `A` are the most important ($\approx62$% total importance).
In almost all segments we see this large share reflected.

```{r echo=FALSE}
temp <- vi_np_sc$res %>% group_by(feature) %>% summarize(Sum = sum(vi_rel), .groups = "drop") %>% arrange(-Sum)

if (interactive()) {
  temp
} else {
  knitr::kable(
    x = temp,
    booktabs = TRUE,
    caption = "Total variable importance per feature for the no-pattern source code dataset.",
    label = "varimp-np-sc-per-feature"
  )
}
```

In figure \ref{fig:varimp-np-sc} we can already see the total importance per segment.
We can see that the two most important segments $2,3$ are surrounded by the two least important segments $1,4$.
It might be the case that a potential Fire Drill cannot be matched well at all using data captured at the very beginning of a project.
Segments two and three then, however, could give strong early indications as to how the project may play out, hence their importance.





# References {-}

<div id="refs"></div>

