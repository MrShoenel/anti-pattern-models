---
title: "Boundary Time Warping"
bibliography: ../inst/REFERENCES.bib
output:
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 6
    df_print: kable
  html_document:
    number_sections: true
    toc: true
    toc_depth: 6
    toc_float: true
    df_print: kable
  md_document:
    toc: true
    toc_depth: 6
    df_print: kable
  word_document: default
---


# Boundary Time Warping

We have previously developed the Multilevel-Model, sub-models and stages that allow us to warp a query to a reference, by moving boundaries around. Our only option was to try some boundaries, then use DTW to rectify the query within, and to calculate an error subsequently. DTW itself uses dynamic programming, and it is not differentiable. That means that our entire model is not differentiable, even if the metrics or scores were.

We introduce a new algorithm, __Boundary Time Warping__. Given a reference signal that we want to align a query signal to, the idea is to subdivide the query signal into arbitrary many sub-intervals (no equal spacing required), and then to alter the extent of each interval during optimization, such that the warped query is becoming closest to the reference signal. For each interval, we obtain a set of parameters that is used to locally translate and scale the query. After convergence, these adjusted parameters are then used to build a warping path that is minimal w.r.t. the chosen cost function. The section captured in each interval of the query signal is constant, the boundaries are only used for translation and scaling. For a given $x$, the model determines which interval (given the currently used warp-boundaries) it falls into, then selects the corresponding scaling- and translation parameters and returns a warped $y$.

```{r echo=FALSE, fig.height=7, fig.cap="Reference- and Query-signals. BTW subdivides the Query signal and then scales and translates each captured interval such that a cost is minimized (or maximized).\\label{fig:btw}"}
library(ggplot2)
library(ggpubr)

btwRef <- data.frame(
  x = seq(0, 1, length.out = 1e3),
  y = sin(seq(0, 2 * pi, length.out = 1e3))
)

plotBtw <- function(df, bounds = c()) {
  g <- ggplot(data = df, aes(x = x, y = y)) + theme_light() + geom_line()
  for (i in 1:length(bounds)) {
    g <- g + geom_vline(xintercept = bounds[i])
  }
  g
}

btwQueryBounds <- c(.1, .2, .5, .6, .8)
btwQuery <- data.frame(
  x = c(
    seq(0, btwQueryBounds[1], length.out =  75),
    seq(btwQueryBounds[1], btwQueryBounds[2], length.out =  25),
    seq(btwQueryBounds[2], btwQueryBounds[3], length.out = 150),
    seq(btwQueryBounds[3], btwQueryBounds[4], length.out = 300),
    seq(btwQueryBounds[4], btwQueryBounds[5], length.out =  50),
    seq(btwQueryBounds[5], 1, length.out = 400)
  ),
  y = btwRef$y
)

ggarrange(
  ncol = 1,
  
  plotBtw(btwRef) + labs(title = "Reference Signal"),
  plotBtw(btwQuery, seq(0, 1, by = .05)) +
    labs(title = "Query Signal", subtitle = "(using equidistantly spaced boundaries)"),
  plotBtw(btwQuery, btwQueryBounds) +
    labs(title = "Query Signal'", subtitle = "(using the bounds that distorted the reference)")
)
```

In Figure \ref{fig:btw} an example of a reference- and distorted query-signal is shown. BTW sets some boundaries and scales and translates the captured intervals, such that a cost is minimized. These boundaries can be set automatically, e.g., spaced equidistantly or heuristically, or they can be set arbitrarily by the user. In the bottom plot, we show the boundaries that were used to distort the reference signal, so these would be the best boundaries to optimally undo the distort. However, any sufficiently large set of boundaries should give a satisfactory un-warping (rectifying) result.


## Features of BTW

__BTW__ has the following features:

BTW is differentiable, as it relies on the given signal being differentiable. It works both with continuous and discrete signals. In most cases, the given signal is not a function, but rather a discrete vector of values. In these cases, a numerical gradient and Hessian can be computed. If the given signal is analytically differentiable however, then this will increase the precision of the gradient and the Hessian.

BTW can be fit using gradient-based and (quasi-)Newtonian methods. Below, we will do some tests of what works best using a synthetic and a random signal.

BTW allows to use virtually any error function, while DTW does not and always only optimizes for the shortest distance between two signals. This is often needed, but not always. Also, BTW allows to combine any number of (weighted) error functions for optimization, such that more than one optimization goal may be pursued. An important property is also that we can perform _maximization_ with BTW -- this may be useful in scenarios when we need to find an upper bound (scoring for example).

BTW can use arbitrary many boundaries/intervals, resulting in arbitrary warping precision. Also, boundaries do not need to be equally spaced, so that it is possible to capture sections with higher interest with more degrees of freedom. BTW is a classical optimization algorithm, so that information criteria may be computed. These can be used to find the optimal amount of boundaries or a trade-off between model complexity and generalizability.

## Formal description of the model

Each signal is modeled as a function over time, $\mathcal{X}$, such that $t(x),\,x\in\mathcal{X}$ returns the magnitude of the time series $t$ at $x$. The user may have an actual analytical function for their signal, or they choose to approximate it using, e.g., linear or spline interpolation.

In the following, an arbitrary choice for the query boundaries, $\mathbf{\theta_q}$, is made. For example, one may choose a set of equidistantly-spaced boundaries. With an increasing amount of boundaries, the choice becomes less important. Then, the function representing the query signal, below $f(x)$, is subdivided into intervals according to the chosen boundaries. For each interval, we usually want to internally create a function that scales and translates $f(x)$, such that its support is $[0,1]$. That way later, we can comfortably compute $m(x,s,t)$. Computing the gradient will tell us how to move each boundary, effectively translating and scaling each captured interval. The model determines the correct interval (and hence the corresponding parameters $s,t$) by $x$.


$$
\begin{aligned}
  r(x) &= \dots\;\text{, a function over the reference signal,}
  \\
  f(x) &= \dots\;\text{, a function over the query signal,}
  \\[1em]
  m(x,s,t) &= f(s\times (x-t))\;\text{, model that locally translates and scales the query,}
  \\[1em]
  \nabla\,m &= \Big[ f'(s\times (x-t))\times(x-t)\;\frac{\partial\;m}{\partial\;s}\;,\;f'(s\times (x-t)) -s\;\frac{\partial\;m}{\partial\;t}\Big]\;\text{,}
  \\[1em]
  \text{RSSC} &= \int_{\mathcal{X}} (f(x) - m(x,s,t))^2\;\text{, RSSC for continuous case,}
  \\[1em]
  \text{RSS} &= \sum_{i=1}^{N} (\mathbf{y}_i - m(x_i,s,t))^2\;\text{, RSS for discrete case,}
  \\[1em]
  \nabla\,\text{RSS}_m &= \Bigg[ \frac{\partial\;\text{RSS}_m}{\partial\;s}\;,\;\frac{\partial\;\text{RSS}_m}{\partial\;t} \Bigg]\;\text{,}
  \\[1em]
  \frac{\partial\;\text{RSS}_m}{\partial\;s} &= \sum_{i=1}^{N} 2 (t-x_i)\times (\mathbf{y}_i-f(s\times (x_i-t)))\times f'(s\times (x_i-t))\;\text{,}
  \\[1em]
  \frac{\partial\;\text{RSS}_m}{\partial\;t} &= \sum_{i=1}^{N} 2s\times (\mathbf{y}_i - f(s\times(x_i-t)))\times f'(s\times(x_i-t))\text{.}
\end{aligned}
$$

It becomes apparent, that we can use any error-function, if we were to pre-compute $\mathbf{\hat{y}}, \mathbf{\hat{y}}'$ and $\mathbf{\hat{y}}''$. The loss of the model requires $\mathbf{\hat{y}}$, the gradient additionally $\mathbf{\hat{y}}'$, and for the Hessian, we would additionally need $\mathbf{\hat{y}}''$. Pre-computing these vectors let's us plug in any error function effortlessly, i.e.,


$$
\begin{aligned}
  \mathbf{\hat{y}}_{s,t} &= m(\mathbf{x},s,t)\;\text{,}
  \\
  &= f(s\times (\mathbf{x}-t))\;\text{, all y of the current model,}
  \\[1em]
  \mathbf{\hat{y}}'_{s,t} &= f'(s\times (\mathbf{x}-t))\;\text{, and}
  \\
  \mathbf{\hat{y}}''_{s,t} &= f''(s\times (\mathbf{x}-t))\;\text{,}
  \\[1em]
  \frac{\partial\;\text{RSS}_m}{\partial\;s} &= \sum_{i=1}^{N} 2 (t-x_i)\times (\mathbf{y}_i-\mathbf{\hat{y}}^{s,t}_i)\times \mathbf{\hat{y}}'^{s,t}_i\text{.}
\end{aligned}
$$


Let's make another example, using the Kullback-Leibler divergence. We will assume that $\mathbf{y},\mathbf{\hat{y}}$ represent vectors of discrete probabilities, each summing to $1$.


$$
\begin{aligned}
  D_{\text{KL}}(P\,\|\,Q) &= \sum_{x\in \mathcal{X}} P(x) \log{\frac{P(x)}{Q(x)}}\;\text{, or, in our case,}
  \\[1em]
  &= \sum_{i=1}^{N} \mathbf{y}_i\log{\frac{\mathbf{y}_i}{f(s\times (x-t))}}\;\text{, using vectors:}
  \\[1em]
  &= \sum_{i=1}^{N} \mathbf{y}_i\log{\frac{\mathbf{y}_i}{\mathbf{\hat{y}}^{s,t}_i}}\;\text{; with the derivative for s being}
  \\[1em]
  \frac{\partial\;D_{\text{KL}_m}}{\partial\;s} &= \sum_{i=1}^{N} \frac{\mathbf{y}_i\times (t-x_i)\times f'(s\times (x_i-t))}{f(s\times (x_i-t))}\;\text{, or, using vectors,}
  \\[1em]
  &= \sum_{i=1}^{N} \frac{\mathbf{y}_i\,\mathbf{\hat{y}}'^{s,t}_i\times (t-x_i)}{\mathbf{\hat{y}}^{s,t}_i}\;\text{(notice the ' in the numerator).}
\end{aligned}
$$


The 2nd derivative (differentiated here twice for $s$) would be:

$$
\begin{aligned}
  \frac{\partial^2\;D_{\text{KL}_m}}{\partial^2\;s} &= \sum_{i=1}^{N} \frac{\mathbf{y}_i\times (t-x_i)^2\times \Big( f'(s\times (x-t))^2 - f(s\times (x-t))\times f''(s\times (x-t))\Big)}{f(s\times (x-t))^2}\;\text{, or, using vectors,}
  \\[1em]
  &= \sum_{i=1}^{N} \mathbf{y}_i\times (t-x_i)^2\times\Big( \big(\mathbf{\hat{y}}\prime^{s,t}_i\big)^2\,-\;\mathbf{\hat{y}}^{s,t}_i\, \mathbf{\hat{y}}''^{s,t}_i \Big)\times\big(\mathbf{\hat{y}}^{s,t}_i\big)^{-2}\;\text{.}
\end{aligned}
$$


So any once- or twice- (for Hessian-based optimization) -differentiable error function can be used.


# Optimization of BTW

BTW supports gradient-based optimization, and can optionally use a Hessian for Newton-based optimization. Theoretically, BTW does not need to constrain the boundaries, which could result in overlapping intervals. Also, the reference boundaries may chosen in a way such that the reference intervals would overlap. This is not something that DTW allows, and it is probably an unlikely use-case for BTW, too. However, we design BTW in a way that we leave this up to the user, as well as whether it makes sense for their scenario or not.

While overlapping reference intervals are less critical, problems arise for when query intervals overlap, as then the model is not bijective any longer (i.e., $x$ falls into more than one interval, and hence we obtain a $y$ for each interval that applies). Again, we will let the user chose how to deal with such cases that break bijection, using some decision function that picks or derives a single $\hat{y}\in\mathbf{\hat{y}}$. In general, _constrained optimization_ should be chosen, with constraints that ensure that any boundary $b_n<b_{n+1}$.


## Minimization and maximization


## Finding the optimal amount of boundaries

Since we have a classical statistical model that produces a likelihood $\mathcal{L}$ for a given set of parameters $\mathbf{\theta}$ (of which we know the cardinality), we can calculate some information criterion (such as `AIC`) and compare fitted models. Then, according to Occam's razor, we choose the one with the best trade-off between amount of parameters and log-likelihood.

I could think of two ways, heuristically and using some kind of optimization. In the following, both are briefly described (I will expand on this later).


### Heuristic search with Log2 runtime

Very short description:

* Start fitting two models with two differently sized sets of boundaries, where one set has double the cardinality of the other (i.e., $|S_a| = 2|S_b|$).
* Choose set-sizes that are in some way appropriate to the length of the time-series, e.g., $10$ and $20$. I do believe that the initial choice is rather unimportant, and maybe we find a way to estimate a initial set strength $s$, such that $|S_a|=\frac{s}{2},\;|S_b|=s+\frac{s}{2}$.
* Use an information criterion to compare which of the two models is a better fit.
* Estimate two new models, one in between the two current sets, and one smaller or larger than the smaller/larger set (depending on which one is better).
* Repeat until the information criterion does not signal further improvement or choices are exhausted (the solution space is discrete and final).

Since we are basically halving the solution space in each step, this algorithm should converge in approximately Log2-time.


### Hessian Boundary Pruning/Collapsing

If we have twice-differentiable model- and error-functions, we can use the Hessian to jump to a set of boundaries that minimizes (or maximizes) the current model error. However, this method cannot facilitate any constraints, and may jump into an infeasible region, if the user does not allow overlapping intervals. We can use these overlaps to either prune or collapse boundaries that appear to be superfluous. After doing that, we fit the model again using the remaining boundaries. This process is repeated until the amount of boundaries remaining is minimized (i.e., the chosen information criterion does not signal an improvement of the model).

Contrary to the previously proposed heuristic search, this method is only one-way, as it does not add new boundaries. It may be useful for reducing overfit that is introduced by using too many boundaries. It may be feasible to combine both methods, e.g., use this approach first to remove superfluous boundaries, then further optimize using the other approach.


# Testing

Let's make some tests! We want to attempt rectifying the distorted query from the introduction. The reference signal it stems from is sine, but we sampled from it -- so let's approximate a function over reference and query signal, as this should be the most common use-case.

```{r}
ggarrange(
  ncol = 1,

  plotBtw(btwRef),
  plotBtw(btwQuery, seq(0, 1, by = 0.05))
)
```

Let's approximate functions for both signals and initialize the boundaries: We will just use equidistantly-spaced bounds with a distance of $0.05$. The amount of boundaries is equal to the degrees of freedom of BTW, except for when the first and/or last boundary is fixed (so-called closed begin/end time warping). While we know that it would make sense in this case, we will not impose these constraints here.

```{r}
signal_ref <- stats::approxfun(x = btwRef$x, y = btwRef$y, ties = mean)
signal_query <- stats::approxfun(x = btwQuery$x, y = btwQuery$y, ties = mean)

query_bounds <- seq(0, 1, by = 0.05)
```

Note that two neighboring boundaries delimit an interval, which leaves us with $|\mathtt{B}|-1$ intervals and thus with $20$ tuples of $\langle s,t\rangle$ parameters. However, only the boundaries are altered, and we derive the index for each interval as well as the current parameters in the objective function.

We will define the objective function next:

```{r}
#' @param x vector with boundaries
btw_objective <- function(x, returnYHat = FALSE) {
  totalSamples <- 1e3
  # For each pair of boundaries, we'll scale and translate
  # the query function, and then sample from it. The number
  # of samples we take from each interval is proportional to
  # its length.
  
  y_hat <- c()
  
  for (idx in 1:(length(x) - 1)) {
    t <- x[idx] # translate
    iEnd <- x[idx + 1]
    s <- iEnd - t # scale (the extent of the interval)
    numSamples <- round(s * totalSamples)
    # NOTE: THE FOLLOWING IS A SLIGHT HACK, so that we
    # can attempt unconstrained optimization! This is
    # NOT a valid situation, but we want to show the
    # result anyhow.
    if (numSamples <= 0) {
      next
    }
    
    # In the following, take the corresponding original-
    # interval from the query, and translate+scale it to [0,1]:
    offR <- query_bounds[idx]
    signal_query_01 <- function(p) {
      signal_query(p * (1 - offR) + offR)
    }
    
    # .. and now we scale and transform that function into
    # the requested interval:
    s_mult <- 1 / s
    signal_query_prime <- function(p) {
      signal_query_01((p - t) * s)
    }
    
    # Let's finally sample from the function over the transformed
    # interval!
    y_hat <- c(y_hat, sapply(
      seq(t, iEnd, length.out = numSamples),
      signal_query_prime))
  }
  
  y <- sapply(seq(0, 1, length.out = length(y_hat)), signal_ref)
  
  if (returnYHat) {
    return(list(
      y = y,
      y_hat = y_hat
    ))
  }
  
  # Return the RSS:
  sum((y - y_hat)^2)
}
```

Let's see what's the error of it using the unchanged query-boundaries:

```{r}
btw_objective(query_bounds)
```

## Unconstrained, no gradient

Before we go any further, let's try _unconstrained_ optimization __without__ a gradient (`optim` is going to use Nelder-Mead simplex optimization in this case). Note that this kind of optimization will most likely result in boundaries that lead to _overlapping_ intervals. Still, it's a somewhat test as to whether our objective function works.

```{r}
btw_optim_no_grad_no_const <- loadResultsOrCompute(file = "../results/btw_optim_no_grad_no_const.rds", computeExpr = {
  optim(query_bounds, btw_objective, gr = function(x) numDeriv::grad(func = btw_objective, x = x))
})
btw_optim_no_grad_no_const
```

Well, the error is an order of magnitude smaller, but `optim` did not actually finally converge, which is no surprise for $21$-dimensional problem and a Nelder-Mead approach limited to $500$ iterations. Also, as predicted, some boundaries/intervals currently overlap. So here's where the optimization stopped:

```{r echo=FALSE}
temp <- btw_objective(x = btw_optim_no_grad_no_const$par, returnYHat = TRUE)

ggarrange(
  ncol = 1,

  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y)), y = temp$y)),
  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y_hat)), y = temp$y_hat))
)
```

## No gradient, with constraints

Let's define linear inequality constraints: There are only two things we will impose:

* $b_{\text{first}} \geq 0,\;b_{\text{last}}\leq1$
* $b_n < b_{n+1}$

The starting query-boundaries already satisfy these constraints, but we need to encode them properly as `theta`, `ui` and `ci` for usage with `constrOptim`.

```{r}
# Let's use the fancy class we created for MLMs!
li <- LinearInequalityConstraints$new(theta = query_bounds)
li$flushLinIneqConstraints()

# Let's add the first two constraints (constrOptim requires >=):
li$setLinIneqConstraint(
  name = "b_first_geq_v", ineqs = c(1, rep(0, length(query_bounds) - 1), 0))
li$setLinIneqConstraint(
  name = "b_last_geq_v", ineqs = c(rep(0, length(query_bounds) - 1), -1, -1))

# And we'll also add the second type of constraint for each
# pair of consecutive boundaries:
for (idx in 1:(length(query_bounds) - 1)) {
  i1 <- idx
  i2 <- idx + 1
  name <- paste0("-b", i1, "_geqv_+b", i2)
  # bounds before i1, -b1, +b2, bounds after b2, >= 0
  ineqs <- c(rep(0, i1 - 1), -1, 1, rep(0, length(query_bounds) - i2), 0)
  li$setLinIneqConstraint(name = name, ineqs = ineqs)
}

li$validateLinIneqConstraints()
```

Ok, let's try `constrOptim` instead:

```{r}
btw_optim_no_grad_with_const <- loadResultsOrCompute(file = "../results/btw_optim_no_grad_with_const.rds", computeExpr = {
  theta <- li$getTheta()
  theta[theta == 0] <- .Machine$double.eps
  theta[theta == 1] <- 1 - .Machine$double.eps
  
  constrOptim(
    theta = theta,
    ui = li$getUi(),
    ci = li$getCi(),
    f = btw_objective,
    grad = NULL,
    control = list(maxit = 500))
})
btw_optim_no_grad_with_const
```

This did not work so well; the amount of calls to the objective function is extraordinarily high, so this took quite long before it stopped, not converging. The final loss is higher than without using constraints. However, the results is valid ;)

```{r echo=FALSE}
temp <- btw_objective(x = btw_optim_no_grad_with_const$par, returnYHat = TRUE)

ggarrange(
  ncol = 1,

  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y)), y = temp$y)),
  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y_hat)), y = temp$y_hat))
)
```

## Approximated gradient, with constraints

The next we do is to try an approximated gradient, __derived over the objective function__. Now we went through the troubles of defining an __analytical__ gradient for these tests, which should work very well in theory. However, we can also approximate a gradient over the objective function and see how that works:

```{r}
#btw_optim_approx_grad_with_const <- loadResultsOrCompute(file = "../results/btw_optim_approx_grad_with_const.rds", computeExpr = {
  theta <- li$getTheta()
  theta[theta == 0] <- .Machine$double.eps
  theta[theta == 1] <- 1 - .Machine$double.eps
  
  constrOptim2(
    theta = theta,
    ui = li$getUi(),
    ci = li$getCi(),
    #outer.eps = 1e-16,
    f = btw_objective,
    grad = function(x) {
      numDeriv::grad(func = btw_objective, x = x)
    })
#})
#btw_optim_approx_grad_with_const
```

```{r echo=FALSE}
temp <- btw_objective(x = btw_optim_approx_grad_with_const$par, returnYHat = TRUE)

ggarrange(
  ncol = 1,

  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y)), y = temp$y)),
  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y_hat)), y = temp$y_hat))
)
```




























```{r}
btwRef <- list(
  x = seq(0, 1, length.out = 1e3),
  y = sin(seq(0, pi, length.out = 1e3))
)
plot(btwRef, xlim = c(0,1), ylim = c(0,1), type = "l")
```

```{r}
btwDist <- list(
  x = c(
    seq(0, .2, length.out = 1e2),
    seq(.2, .5, length.out = 2e2),
    seq(.5, .6, length.out = 3e2),
    seq(.6, .8, length.out = 1e2),
    seq(.8, 1, length.out = 3e2)
  ),
  y = sin(seq(0, pi, length.out = 1e3))
)
plot(btwDist, xlim = c(0,1), ylim = c(0,1), type = "l")
```


Let's define our boundaries: equally spaced in `[0,1]` (`0,1` are not included as we are trying closed begin/end time warping):

```{r}
btwBoundsRef <- seq(0.1, 0.9, by = 0.1)
btwBoundsRef
```

```{r}
btwRef_f <- stats::approxfun(x = btwRef$x, y = btwRef$y)

btwXPrime <- function(br, boundsQuery, bqIdx) {
  x <- seq(br - 0.1, br, length.out = 1e2) / 0.1 # slice out x, translate and scale
  
  start <- if (bqIdx == 1) 0 else boundsQuery[bqIdx - 1]
  end <- if (bqIdx == length(boundsQuery)) 1 else boundsQuery[bqIdx]
  ext <- end - start
  
  # x' is the scaled x + the offset
  x * ext + start
}
```



```{r}
temp <- sin(seq(0, pi/1.5, length.out = 1e2))
tempf <- approxfun(x = seq(0, 1, length.out = 1e2), y = temp)
plot(temp)
curve(tempf, 0, 1)
curve(tempf, .75, 1)
```

```{r}
offR <- .75
endR <- 1
offQ <- .50
extQ <- .4
trans <- offR - offQ
scale <- 1 / (extQ / (endR - offR))

tempf2 <- (function() {
  function(x) {
    tempf(((x - offQ) * scale) + offR)
  }
})()

tempf3 <- (function() {
  function(x) {
    tempf(x * scale + offR)
  }
})()

tempf4 <- (function() {
  function(x) {
    tempf((x - offQ) / scale)
  }
})()

curve(tempf2, offQ, offQ + extQ)
curve(tempf3, 0, extQ)
curve(tempf4, offQ, 1 / offR)
```

Now we want to pre-translate and pre-scale the reference function to the interval `[0,1]`, so that we make a closure over all model-constant parameters, and the final function is a function over only the parameters derived from the current query-interval (the query boundaries).

```{r}
tempfPrime <- function(x) {
  tempf(x * (1 - offR) + offR)
}

curve(tempfPrime, 0, 1)
```

So, here's the final function that only needs the absolute start and end of the query-interval, to properly translate and scale.

```{r}
# We use this transform so we can multiply!
extQm <- 1 / extQ

tempf2Prime <- function(x) {
  tempfPrime((x - offQ) * extQm)
}

curve(tempf2Prime, offQ, offQ + extQ)
```

```{r}
plot(loess.smooth(x=seq(.51, .89, len=500), y=sapply(seq(.51, .89, len=500), function(x) numDeriv::grad(tempf2Prime, x))))
```













