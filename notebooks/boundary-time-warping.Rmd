---
title: "Boundary Time Warping"
bibliography: ../inst/REFERENCES.bib
output:
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 6
    df_print: kable
  html_document:
    number_sections: true
    toc: true
    toc_depth: 6
    toc_float: true
    df_print: kable
  md_document:
    toc: true
    toc_depth: 6
    df_print: kable
  word_document: default
---

```{r echo=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
```


# Boundary Time Warping

We have previously developed the Multilevel-Model, sub-models and stages that allow us to warp a query to a reference, by moving boundaries around. Our only option was to try some boundaries, then use DTW to rectify the query within, and to calculate an error subsequently. DTW itself uses dynamic programming, and it is not differentiable. That means that our entire model is not differentiable, even if the metrics or scores were.

We introduce a new algorithm, __Boundary Time Warping__. Given a reference signal that we want to align a query signal to, the idea is to subdivide the query signal into arbitrary many sub-intervals (no equal spacing required), and then to alter the extent of each interval during optimization, such that the warped query is becoming closest to the reference signal. For each interval, we obtain a set of parameters that is used to locally translate and scale the query. After convergence, these adjusted parameters are then used to build a warping path that is minimal w.r.t. the chosen cost function. The section captured in each interval of the query signal is constant, the boundaries are only used for translation and scaling. For a given $x$, the model determines which interval (given the currently used warp-boundaries) it falls into, then selects the corresponding scaling- and translation parameters and returns a warped $y$.

```{r echo=FALSE, fig.height=7, fig.cap="Reference- and Query-signals. BTW subdivides the Query signal and then scales and translates each captured interval such that a cost is minimized (or maximized).\\label{fig:btw}"}
library(ggplot2)
library(ggpubr)

btwRef <- data.frame(
  x = seq(0, 1, length.out = 1e3),
  y = sin(seq(0, 2 * pi, length.out = 1e3))
)

plotBtw <- function(df, bounds = c()) {
  g <- ggplot(data = df, aes(x = x, y = y)) + theme_light() + geom_line()
  for (i in 1:length(bounds)) {
    g <- g + geom_vline(xintercept = bounds[i])
  }
  g
}

btwQueryBounds <- c(.1, .2, .5, .6, .8)
btwQuery <- data.frame(
  x = c(
    seq(0, btwQueryBounds[1], length.out =  75),
    seq(btwQueryBounds[1], btwQueryBounds[2], length.out =  25),
    seq(btwQueryBounds[2], btwQueryBounds[3], length.out = 150),
    seq(btwQueryBounds[3], btwQueryBounds[4], length.out = 300),
    seq(btwQueryBounds[4], btwQueryBounds[5], length.out =  50),
    seq(btwQueryBounds[5], 1, length.out = 400)
  ),
  y = btwRef$y
)

ggarrange(
  ncol = 1,
  
  plotBtw(btwRef) + labs(title = "Reference Signal"),
  plotBtw(btwQuery, seq(0, 1, by = .05)) +
    labs(title = "Query Signal", subtitle = "(using equidistantly spaced boundaries)"),
  plotBtw(btwQuery, btwQueryBounds) +
    labs(title = "Query Signal'", subtitle = "(using the bounds that distorted the reference)")
)
```

In Figure \ref{fig:btw} an example of a reference- and distorted query-signal is shown. BTW sets some boundaries and scales and translates the captured intervals, such that a cost is minimized. These boundaries can be set automatically, e.g., spaced equidistantly or heuristically, or they can be set arbitrarily by the user. In the bottom plot, we show the boundaries that were used to distort the reference signal, so these would be the best boundaries to optimally undo the distort. However, any sufficiently large set of boundaries should give a satisfactory un-warping (rectifying) result.


## Features of BTW

__BTW__ has the following features:

BTW is differentiable, as it relies on the given signal being differentiable. It works both with continuous and discrete signals. In most cases, the given signal is not a function, but rather a discrete vector of values. In these cases, a numerical gradient and Hessian can be computed. If the given signal is analytically differentiable however, then this will increase the precision of the gradient and the Hessian.

BTW can be fit using gradient-based and (quasi-)Newtonian methods. Below, we will do some tests of what works best using a synthetic and a random signal.

BTW allows to use virtually any error function, while DTW does not and always only optimizes for the shortest distance between two signals. This is often needed, but not always. Also, BTW allows to combine any number of (weighted) error functions for optimization, such that more than one optimization goal may be pursued. An important property is also that we can perform _maximization_ with BTW -- this may be useful in scenarios when we need to find an upper bound (scoring for example).

BTW can use arbitrary many boundaries/intervals, resulting in arbitrary warping precision. Also, boundaries do not need to be equally spaced, so that it is possible to capture sections with higher interest with more degrees of freedom. BTW is a classical optimization algorithm, so that information criteria may be computed. These can be used to find the optimal amount of boundaries or a trade-off between model complexity and generalizability.

## Formal description of the model

Each signal is modeled as a function over time and its support, $\mathcal{X}$, such that $t(x),\,x\in\mathcal{X}$ returns the magnitude of the time series $t$ at $x$. The user may have an actual analytical function for their signal, or they choose to approximate it using, e.g., linear or spline interpolation.

In the following, an arbitrary choice for the query boundaries, $\mathbf{\theta_q}$, is made. For example, one may choose a set of equidistantly-spaced boundaries. With an increasing amount of boundaries, the choice becomes less important. Then, the function representing the query signal, below $f(x)$, is subdivided into intervals according to the chosen boundaries. For each interval, we usually want to internally create a function that scales and translates $f(x)$, such that its support becomes $[0,1]$. That way later, we can comfortably compute $m(x,s,t)$. Computing the gradient will tell us how to move each boundary, effectively translating and scaling each captured interval. The model determines the correct interval (and hence the corresponding parameters $s,t$) by $x$.


$$
\begin{aligned}
  r(x) &= \dots\;\text{, a function over the reference signal,}
  \\
  f(x),f\prime(x),f\prime\prime(x) &= \dots\;\text{, functions over the query signal (and its 1st and 2nd derivatives),}
  \\[1em]
  m(x,s,t) &= f(s\times (x-t))\;\text{, model that locally translates and scales the query,}
  \\[1em]
  \nabla\,m &= \Big[ f'(s\times (x-t))\times(x-t)\;\frac{\partial\;m}{\partial\;s}\;,\;f'(s\times (x-t)) -s\;\frac{\partial\;m}{\partial\;t}\Big]\;\text{.}
\end{aligned}
$$

The above descriptions are for scaling the query-signal's function using a set of parameters $\langle\,s,t\,\rangle$. However, we will have as many sets as we have intervals, and the overall model's task is to select the correct set of parameters, based on into which of the intervals $x$ falls.

$$
\begin{aligned}
  \mathsf{M}(\mathbf{\theta}_{b_{\text{org}}}, \mathbf{\theta}_b, n, r, f) &= \langle\, \mathbf{y},\mathbf{\hat{y}} \,\rangle
\end{aligned}
$$

$\mathsf{M}$ has the following arguments:

* $\mathbf{\theta}_{b_{\text{org}}}$: The boundaries that were originally chosen to subdivide the query signal by. Those are needed to determine how to translate and scale the intervals. These boundaries are constants once the model is created.
* $\mathbf{\theta}_b$: The query-boundaries that the model should apply: These are used in conjunction with the original boundaries to determine translation and scaling for each interval. These boundaries are altered in each training epoch. In below pseudo-code, we have created $f'$ such that it also involves the constant original boundaries, to avoid currying too many functions.
* $n$: number of samples to take over the total support of $\mathcal{X}_b$ (the extent covered by the boundaries).
* $r,f$: Functions for the reference- and query-signal, respectively. Note that in open-begin and/or open-end time warping, these do not necessarily have the same support. BTW will sample the same support of $f$ in $r$. The error function may penalize any discrepancies between both supports.

$\mathsf{M}$ has the following outputs:

* $\mathcal{X}_b$: The support used for sampling over the query. We return this for convenience.
* $\mathbf{y}$: $r(x)$ sampled over the support established by the query-boundaries, $\mathbf{\theta}_b$. It would be possible to use a user-defined relative sampling-frequency in each query-interval, and then to sample proportionally from it (not shown here).
* $\mathbf{\hat{y}}$: Piece-wise (for each interval) sampling of $f(x)$, according to local translation and scaling, also using the query-boundaries and original-boundaries.

In pseudo-code, this is what (the discrete version of) $\mathsf{M}$ does (for the continuous case, we would do piece-wise integration):

```{r eval = FALSE}
M <- function(theta_b_org, theta_b, num_samples, r, f) {
  X <- create_range(start = min(theta_b), end = max(theta_b), amount = num_samples)
  y <- r(X)
  y_hat <- zeros(num_samples)
  
  for (x, x_idx) in X do:
    int_idx <- determine_interval_x_falls_into(x)
    
    start_org, end_org <- theta_b_org[int_idx]
    start, end <- theta_b[int_idx]
    extent <- end - start
    
    # This translates and scales the original interval into
    # the requested query interval. For example, the original
    # interval may be at [0.4, .55], and now it is requested
    # to reside at [0.37, 0.62]. 'x' in this loop falls into
    # the requested interval, so we can use it in f_prime!
    f_prime <- function(x1) {
      f( ((x1 - start) / extent) * (end_org - start_org) + start_org )
    }
    
    y_hat[x_idx] <- f_prime(x)
  endfor
  
  return(X, y, y_hat)
}
```


We are also defining some of the error functions:

$$
\begin{aligned}
  \text{RSSC} &= \int_{\mathcal{X}} (f(x) - m(x,s,t))^2\;\text{, RSSC for continuous case,}
  \\[1em]
  \text{RSS} &= \sum_{i=1}^{N} (\mathbf{y}_i - m(x_i,s,t))^2\;\text{, RSS for discrete case,}
  \\[1em]
  \nabla\,\text{RSS}_m &= \Bigg[ \frac{\partial\;\text{RSS}_m}{\partial\;s}\;,\;\frac{\partial\;\text{RSS}_m}{\partial\;t} \Bigg]\;\text{,}
  \\[1em]
  \frac{\partial\;\text{RSS}_m}{\partial\;s} &= \sum_{i=1}^{N} 2 (t-x_i)\times (\mathbf{y}_i-f(s\times (x_i-t)))\times f'(s\times (x_i-t))\;\text{,}
  \\[1em]
  \frac{\partial\;\text{RSS}_m}{\partial\;t} &= \sum_{i=1}^{N} 2s\times (\mathbf{y}_i - f(s\times(x_i-t)))\times f'(s\times(x_i-t))\text{.}
\end{aligned}
$$

All the 2nd-order partial derivatives for $\text{RSS}_m$ are:

$$
\begin{aligned}
  \frac{\partial^2\;\text{RSS}_m}{\partial\,s\,\partial\,s} = &\;\;\sum_{i=1}^{N} 2 (t - x_i)\times (x_i - t)
  \\
  &\;\;\times\Big((y_i - f(s\times (x - t)))\times f\prime\prime(s\times (x_i - t)) - f\prime(s\times (x_i - t))^2\Big)\;\text{,}
  \\[1em]
  \frac{\partial^2\;\text{RSS}_m}{\partial\,s\,\partial\,t} = &\;\;\sum_{i=1}^{N} -2 s (t - x_i)\times \big(y_i - f(s\times (x_i - t))\big)
  \\
  &\;\;\times f\prime\prime(s\times (x_i - t)) + 2 \big(y_i - f(s\times (x_i - t))\big)
  \\
  &\;\;\times f\prime(s\times (x_i - t)) + 2 s (t - x_i)\times f\prime(s\times (x_i - t))^2\;\text{,}
  \\[1em]
  \frac{\partial^2\;\text{RSS}_m}{\partial\,t\,\partial\,s} = &\;\;\sum_{i=1}^{N} 2 (s\times (x_i - t)\times \big(y_i - f(s (x_i - t))\big)
  \\
  &\;\;\times f''(s\times (x_i - t)) + \big(y_i - f(s\times (x_i - t))\big)
  \\
  &\;\;\times f'(s\times (x_i - t)) + s\times (t - x_i)\times f'(s\times (x_i - t))^2)\;\text{,}
  \\[1em]
  \frac{\partial^2\;\text{RSS}_m}{\partial\,t\,\partial\,t} &= \sum_{i=1}^{N} 2 s^2\times \Big((f(s\times (x_i - t)) - y_i)\times f''(s\times (x_i - t)) + f'(s\times (x_i - t))^2\Big)\;\text{.}
  \\[1em]
\end{aligned}
$$

The three terms that repeat in the 1st- and 2nd-order partial derivatives are $f(s\times(x_i-t))$, $f'(s\times(x_i-t))$ and $f''(s\times(x_i-t))$, which are the functions over the query-signal and its 1st- and 2nd derivative. Recall that all these need local translation and scaling, and we can plug in $f(x)$ into our model-function, $\mathsf{M}$, to obtain $\mathbf{\hat{y}}$. Likewise, if we plug in the 1st- or 2nd derivatives, $f'(x),f''(x)$, we can obtain $\mathbf{\hat{y}}\prime$ and $\mathbf{\hat{y}}\prime\prime$, respectively. These vectors can then be substituted in the error functions for the actual function calls. The advantage of all this is that we can facilitate symbolic derivation.

It becomes apparent, that we can use any error-function, if we were to pre-compute $\mathbf{\hat{y}}, \mathbf{\hat{y}}'$ and $\mathbf{\hat{y}}''$. The loss of the model requires $\mathbf{\hat{y}}$, the gradient additionally $\mathbf{\hat{y}}'$, and for the Hessian, we would additionally need $\mathbf{\hat{y}}''$. Pre-computing these vectors let's us plug in any error function effortlessly, i.e.,


$$
\begin{aligned}
  \mathbf{\hat{y}}_{s,t} &= m(\mathbf{x},s,t)\;\text{,}
  \\
  &= f(s\times (\mathbf{x}-t))\;\text{, all y of the current model,}
  \\[1em]
  \mathbf{\hat{y}}\prime_{s,t} &= f\prime(s\times (\mathbf{x}-t))\;\text{, and}
  \\
  \mathbf{\hat{y}}\prime\prime_{s,t} &= f\prime\prime(s\times (\mathbf{x}-t))\;\text{,}
  \\[1em]
  \frac{\partial\;\text{RSS}_m}{\partial\;s} &= \sum_{i=1}^{N} 2 (t-x_i)\times (\mathbf{y}_i-\mathbf{\hat{y}}^{s,t}_i)\times {\mathbf{\hat{y}}\prime}^{s,t}_i\text{.}
\end{aligned}
$$

As one can see, the 1st-order partial derivative with respect to $s$ of the RSS error function, with pre-computed vectors instead of function calls is straight-forward.

Let's make another example, using the Kullback-Leibler divergence. We will assume that $\mathbf{y},\mathbf{\hat{y}}$ represent vectors of discrete probabilities, each summing to $1$.


$$
\begin{aligned}
  D_{\text{KL}}(P\,\|\,Q) &= \sum_{x\in \mathcal{X}} P(x) \log{\frac{P(x)}{Q(x)}}\;\text{, or, in our case,}
  \\[1em]
  &= \sum_{i=1}^{N} \mathbf{y}_i\log{\frac{\mathbf{y}_i}{f(s\times (x-t))}}\;\text{, using vectors:}
  \\[1em]
  &= \sum_{i=1}^{N} \mathbf{y}_i\log{\frac{\mathbf{y}_i}{\mathbf{\hat{y}}^{s,t}_i}}\;\text{; with the derivative for s being}
  \\[1em]
  \frac{\partial\;D_{\text{KL}_m}}{\partial\;s} &= \sum_{i=1}^{N} \frac{\mathbf{y}_i\times (t-x_i)\times f'(s\times (x_i-t))}{f(s\times (x_i-t))}\;\text{, or, using vectors,}
  \\[1em]
  &= \sum_{i=1}^{N} \frac{\mathbf{y}_i\,\mathbf{\hat{y}}'^{s,t}_i\times (t-x_i)}{\mathbf{\hat{y}}^{s,t}_i}\;\text{(notice the ' in the numerator).}
\end{aligned}
$$


The 2nd derivative (differentiated here twice for $s$) would be:

$$
\begin{aligned}
  \frac{\partial^2\;D_{\text{KL}_m}}{\partial^2\;s} &= \sum_{i=1}^{N} \frac{\mathbf{y}_i\times (t-x_i)^2\times \Big( f'(s\times (x-t))^2 - f(s\times (x-t))\times f''(s\times (x-t))\Big)}{f(s\times (x-t))^2}\;\text{, or, using vectors,}
  \\[1em]
  &= \sum_{i=1}^{N} \mathbf{y}_i\times (t-x_i)^2\times\Big( \big(\mathbf{\hat{y}}\prime^{s,t}_i\big)^2\,-\;\mathbf{\hat{y}}^{s,t}_i\, \mathbf{\hat{y}}''^{s,t}_i \Big)\times\big(\mathbf{\hat{y}}^{s,t}_i\big)^{-2}\;\text{.}
\end{aligned}
$$


So any once- or twice- (for Hessian-based optimization) -differentiable error function can be used.


# Optimization of BTW

BTW supports gradient-based optimization, and can optionally use a Hessian for Newton-based optimization. Theoretically, BTW does not need to constrain the boundaries, which could result in overlapping intervals. Also, the reference boundaries may be chosen in a way such that the reference intervals would overlap. This is not something that DTW allows, and it is probably an unlikely use-case for BTW, too. However, we design BTW in a way that we leave this up to the user, as well as whether it makes sense for their scenario or not.

While overlapping reference intervals are less critical, problems arise for when query intervals overlap, as then the model is not bijective any longer (i.e., $x$ falls into more than one interval, and hence we obtain a $y$ for each interval that applies). Again, we will let the user chose how to deal with such non-surjective cases that break bijection, using some decision function that picks or derives a single $\hat{y}\in\mathbf{\hat{y}}$. In general, _constrained optimization_ should be chosen, with constraints that ensure that any boundary $b_n<b_{n+1}$.

Let's create our overall model-function, $\mathsf{M}$:

```{r}
# Please look into this file for all BTW-related models and functions:
source(file = "../models/BTW.R")
```

Let's approximate functions for both signals and initialize the boundaries: We will just use equidistantly-spaced bounds with a distance of $0.1$. The amount of boundaries is equal to the degrees of freedom of BTW, except for when the first and/or last boundary is fixed (so-called closed begin/end time warping). While we know that it would make sense in this case, we will not impose these constraints here.

```{r}
signal_ref <- stats::approxfun(x = btwRef$x, y = btwRef$y, ties = mean)
signal_query <- stats::approxfun(x = btwQuery$x, y = btwQuery$y, ties = mean)

query_bounds <- seq(0, 1, by = 0.1)
```

Here is a short test. We use the same boundaries as the original- and query-boundaries, so the following output should produce the exact same plots we have seen before. The bottom plot is our manual attempt at repairing (warping) the signal.

```{r}
temp <- M(
  theta_b_org = query_bounds,
  theta_b = query_bounds,
  r = signal_ref,
  f = signal_query)

# We know how the reference signal was distorted previously. Let's make
# a test where we manually undo this by moving the boundaries.
temp2_bounds <- c(
  0,
  .075, .1, .15, .2, .25,
  .55, .575, .6,  .8, 1)
temp2 <- M(
  theta_b_org = query_bounds,
  theta_b = temp2_bounds,
  r = signal_ref,
  f = signal_query)

ggarrange(
  ncol = 1,
  plotBtw(data.frame(x = temp$X, y = temp$y)),
  plotBtw(data.frame(x = temp$X, y = temp$y_hat), bounds = query_bounds),
  plotBtw(data.frame(x = temp2$X, y = temp2$y_hat), bounds = temp2_bounds)
)
```

The errors (residual sum of squares) for the un-optimized and the manually rectified queries are, respectively:

```{r}
round(c(
  sum((temp$y - temp$y_hat)^2),
  sum((temp2$y - temp2$y_hat)^2)
), 5)
```



## Minimization and maximization


## Finding the optimal amount of boundaries

Since we have a classical statistical model that produces a likelihood $\mathcal{L}$ for a given set of parameters $\mathbf{\theta}$ (of which we know the cardinality), we can calculate some information criterion (such as `AIC`) and compare fitted models. Then, according to Occam's razor, we choose the one with the best trade-off between amount of parameters and log-likelihood.

I could think of two ways, heuristically and using some kind of optimization. In the following, both are briefly described (I will expand on this later).


### Heuristic search with Log2 runtime

Very short description:

* Start fitting two models with two differently sized sets of boundaries, where one set has double the cardinality of the other (i.e., $|S_a| = 2|S_b|$).
* Choose set-sizes that are in some way appropriate to the length of the time-series, e.g., $10$ and $20$. I do believe that the initial choice is rather unimportant, and maybe we find a way to estimate a initial set strength $s$, such that $|S_a|=\frac{s}{2},\;|S_b|=s+\frac{s}{2}$.
* Use an information criterion to compare which of the two models is a better fit.
* Estimate two new models, one in between the two current sets, and one smaller or larger than the smaller/larger set (depending on which one is better).
* Repeat until the information criterion does not signal further improvement or choices are exhausted (the solution space is discrete and final).

Since we are basically halving the solution space in each step, this algorithm should converge in approximately Log2-time.


### Hessian Boundary Pruning/Collapsing

If we have twice-differentiable model- and error-functions, we can use the Hessian to jump to a set of boundaries that minimizes (or maximizes) the current model error. However, this method cannot facilitate any constraints, and may jump into an infeasible region, if the user does not allow overlapping intervals. We can use these overlaps to either prune or collapse boundaries that appear to be superfluous. After doing that, we fit the model again using the remaining boundaries. This process is repeated until the amount of boundaries remaining is minimized (i.e., the chosen information criterion does not signal an improvement of the model).

Contrary to the previously proposed heuristic search, this method is only one-way, as it does not add new boundaries. It may be useful for reducing overfit that is introduced by using too many boundaries. It may be feasible to combine both methods, e.g., use this approach first to remove superfluous boundaries, then further optimize using the other approach.


# Testing (new)

When we defined the code for our model, $\mathsf{M}$, we showed a short test of it using manually repaired query-boundaries, which result in an error $\approx 0$. In the following, we want to make some tests for automatic optimization of our model, with and without a gradient, with and without constraints etc.

```{r}
errf <- function(y, y_hat) {
  sum((y - y_hat)^2)
}

Stabilize <- function(f, lb, ub) {
  function(x) {
    if (x < lb) f(lb) else if (x > ub) f(ub) else f(x)
  }
}

fr <- FitResult$new(paramNames = "loss")
o <- function(theta, isGrad = FALSE) {
  if (!isGrad) {
    fr$startStep()
  }
  
  res <- M(theta_b_org = query_bounds, theta_b = theta,
           r = Stabilize(signal_ref, 0, 1),
           f = Stabilize(signal_query, 0, 1))
  
  loss <- errf(y = res$y, y_hat = res$y_hat)
  if (!isGrad) {
    fr$stopStep(resultParams = loss)#, verbose = TRUE)
  }
  loss
}

optR <- loadResultsOrCompute(file = "../results/btw-constr1.rds", computeExpr = {
  # Note: the constraints are defined later in this notebook..
  constrOptim(
    control = list(maxit = 1e6, abstol = 1e-2),
    #par = query_bounds,
    #fn = o,
    f = o,
    method = "Nelder-Mead",
    theta = (function() {
      t <- query_bounds
      t[t == 0] <- .Machine$double.eps
      t[t == 1] <- 1 - .Machine$double.eps
      t
    })(),
    ui = li$getUi(),
    ci = li$getCi()
    #gr = function(x) numDeriv::grad(func = o, x = x, isGrad = TRUE),
    #method = "L-BFGS-B",
    #lower = rep(0, length(query_bounds)),
    #upper = rep(1, length(query_bounds))
  )
})
```

```{r}
temp3 <- M(
  theta_b_org = query_bounds,
  theta_b = optR$par,
  r = signal_ref,
  f = signal_query)

ggarrange(
  ncol = 1,
  plotBtw(data.frame(x = temp$X, y = temp$y)),
  plotBtw(data.frame(x = temp3$X, y = temp3$y_hat), bounds = optR$par)
)
```

```{r}
library(optimParallel)

cl <- parallel::makePSOCKcluster(8)
parallel::clusterExport(cl, varlist = c("fr", "M", "query_bounds", "Stabilize", "signal_ref", "signal_query", "errf"))

set.seed(1337)
optRp <- optimParallel::optimParallel(
  par = query_bounds,
  fn = o,
  lower = rep(0, length(query_bounds)),
  upper = rep(1, length(query_bounds)),
  parallel = list(
    cl = cl,
    forward = FALSE
  )
)

stopCluster(cl)
optRp
```

```{r}
temp4 <- M(
  theta_b_org = query_bounds,
  theta_b = optRp$par,
  r = signal_ref,
  f = signal_query)

ggarrange(
  ncol = 1,
  plotBtw(data.frame(x = temp$X, y = temp$y)),
  plotBtw(data.frame(x = temp$X, y = temp$y_hat), bounds = query_bounds),
  plotBtw(data.frame(x = temp4$X, y = temp4$y_hat), bounds = optRp$par)
)
```

The following is equivalent to the previous, parallel optimization. Note that we should not try to numerically derive the gradient at this point ourselves -- whenever I tried that, it never converged. `optim` uses a finite-difference approximation if the gradient is missing, and that works really well at this point. However, we cannot do constrained optimization this way, because there, we explicitly need to specify a gradient. Maybe we can borrow the one from `optim` that works so well for these tests? Anyhow, since we __do__ have an analytical gradient, we will see how that works compared to estimating a gradient later.

```{r}
fr$clearFitHist()

optR2 <- optim(
  par = query_bounds,
  fn = o,
  method = "L-BFGS-B",
  lower = rep(0, length(query_bounds)),
  upper = rep(1, length(query_bounds))
)

optR2
```

```{r}
plot(log(fr$getFitHist()[, "loss"]), type = "l")
```

## With analytical gradient

We have previously established $\mathsf{M}(\mathbf{\theta}_{b_{\text{org}}}, \mathbf{\theta}_b, n, r, f)$ as our overall model that uses local models, $m(x,s,t)$, to locally translate and scale $f$, which is a function over the query-signal. The overall model, given the original- and query-boundaries, outputs the two vectors $\mathbf{y},\mathbf{\hat{y}}$.

We therefore need to define a __gradient__ over $\mathsf{M}$, that returns $\mathbf{\hat{\theta}}_b$. Recall that $\mathbf{\theta}_b$ is a vector of boundaries that subdivide a signal into intervals, and the number of intervals hence is $|B| - 1$. Each interval is then translated and scaled using the two parameters $\langle\,s,t\,\rangle$, which are derived from two neighboring boundaries. We have previously laid out how the gradient is computed using these parameters. It becomes apparent, that calculating the gradient yields $\langle\,\hat{s},\hat{t}\,\rangle$, and that these already determine how to move the __two__ boundaries that encompass a interval. This may pose a problem: If we were to calculate the next interval's gradient, then that would include the last boundary of the previous interval. Ideally, the result would be to move the beginning boundary of the next interval exactly where previous interval's ending boundary is. At this point, I am not sure whether calculating the gradient will lead exactly to that. If it turns out that this is the case, then we use this to our advantage and only calculate adjacent pairs of boundaries, effectively halving the amount of required computations. If it turns out that this is not the case, then we have to think about why this happens and what a solution might look like.

$$
\begin{aligned}
  \nabla\,\mathsf{M}(\mathbf{\theta}_b, n, r, f) &= \mathbf{\hat{\theta}}
\end{aligned}
$$

The parameters are the same as for the model description above, but we have removed $\mathbf{\theta}_{b_{\text{org}}}$, as for calculating the new boundaries, $\mathbf{\hat{\theta}}$, only the current boundaries are required. For example, for an interval $i$ with parameters $s,t$, the gradient will suggest $\hat{s},\hat{t}$. While $\hat{t}$ directly corresponds to the offset of the interval, and hence to its first boundary, the second boundary corresponds to the previous size of the interval, as well as the new scale, $\hat{s}$ and offset $\hat{t}$.


```{r}
errf_grad <- function(s, t, X, Y, y_hat, y_hat_prime) {
  temp <- (Y - y_hat) * y_hat_prime
  ds <- sum(2 * (t - X) * temp)
  dt <- sum(2 * s * temp)
  c(ds, dt)
}
```


## With analytical Hessian (2nd-order partial derivatives)

```{r}
errf_grad2 <- function(s, t, X, Y, y_hat, y_hat_prime, y_hat_prime_prime) {
  dss <- sum(2 * (t - X) * (X - t) * ((Y - y_hat) * y_hat_prime_prime - y_hat_prime)^2)
  
  dst <- sum(-2 * s * (t - X) * (Y - y_hat) *
               y_hat_prime_prime + 2 * (Y - y_hat) *
               y_hat_prime + 2 * s * (t - X) * y_hat_prime^2)
  
  dts <- sum(2 * (s * (X - t) * (Y - y_hat)) *
               y_hat_prime_prime + (Y - y_hat) *
               y_hat_prime + s * (t - X) * y_hat_prime^2)
  
  dtt <- sum(2 * s^2 *
               ((y_hat - Y) * y_hat_prime_prime + y_hat_prime^2))

  c(dss, dst, dts, dtt)
}
```




# Testing (old)

Let's make some tests! We want to attempt rectifying the distorted query from the introduction. The reference signal it stems from is sine, but we sampled from it -- so let's approximate a function over reference and query signal, as this should be the most common use-case.

```{r echo=FALSE}
ggarrange(
  ncol = 1,

  plotBtw(btwRef),
  plotBtw(btwQuery, seq(0, 1, by = 0.1))
)
```

Let's approximate functions for both signals and initialize the boundaries: We will just use equidistantly-spaced bounds with a distance of $0.1$. The amount of boundaries is equal to the degrees of freedom of BTW, except for when the first and/or last boundary is fixed (so-called closed begin/end time warping). While we know that it would make sense in this case, we will not impose these constraints here.

```{r}
signal_ref <- stats::approxfun(x = btwRef$x, y = btwRef$y, ties = mean)
signal_query <- stats::approxfun(x = btwQuery$x, y = btwQuery$y, ties = mean)

query_bounds <- seq(0, 1, by = 0.1)
```

Note that two neighboring boundaries delimit an interval, which leaves us with $|\mathtt{B}|-1$ intervals and thus with $10$ tuples of $\langle s,t\rangle$ parameters. However, only the boundaries are altered, and we derive the index for each interval as well as the current parameters in the objective function.

We will define the objective function next:

```{r}
#' @param x vector with boundaries
btw_objective <- function(x, returnYHat = FALSE) {
  totalSamples <- 1e3
  # For each pair of boundaries, we'll scale and translate
  # the query function, and then sample from it. The number
  # of samples we take from each interval is proportional to
  # its length.
  
  y_hat <- c()
  
  for (idx in 1:(length(x) - 1)) {
    t <- x[idx] # translate
    iEnd <- x[idx + 1]
    s <- iEnd - t # scale (the extent of the interval)
    numSamples <- round(s * totalSamples)
    # NOTE: THE FOLLOWING IS A SLIGHT HACK, so that we
    # can attempt unconstrained optimization! This is
    # NOT a valid situation, but we want to show the
    # result anyhow.
    if (numSamples <= 0) {
      next
    }
    
    # In the following, take the corresponding original-
    # interval from the query, and translate+scale it to [0,1]:
    offR <- query_bounds[idx]
    signal_query_01 <- function(p) {
      signal_query(p * (1 - offR) + offR)
    }
    
    # .. and now we scale and transform that function into
    # the requested interval:
    s_mult <- 1 / s
    signal_query_prime <- function(p) {
      signal_query_01((p - t) * s)
    }
    
    # Let's finally sample from the function over the transformed
    # interval!
    y_hat <- c(y_hat, sapply(
      seq(t, iEnd, length.out = numSamples),
      signal_query_prime))
  }
  
  y <- sapply(seq(0, 1, length.out = length(y_hat)), signal_ref)
  
  if (returnYHat) {
    return(list(
      y = y,
      y_hat = y_hat
    ))
  }
  
  # Return the RSS:
  sum((y - y_hat)^2)
}
```

Let's see what's the error of it using the unchanged query-boundaries:

```{r}
btw_objective(query_bounds)
```

## Unconstrained, no gradient

Before we go any further, let's try _unconstrained_ optimization __without__ a gradient (`optim` is going to use Nelder-Mead simplex optimization in this case). Note that this kind of optimization will most likely result in boundaries that lead to _overlapping_ intervals. Still, it's a somewhat test as to whether our objective function works.

```{r}
btw_optim_no_grad_no_const <- loadResultsOrCompute(file = "../results/btw_optim_no_grad_no_const.rds", computeExpr = {
  optim(query_bounds, btw_objective, gr = function(x) numDeriv::grad(func = btw_objective, x = x))
})
btw_optim_no_grad_no_const
```

Well, the error is an order of magnitude smaller, but `optim` did not actually finally converge, which is no surprise for $11$-dimensional problem and a Nelder-Mead approach limited to $500$ iterations. Also, as predicted, some boundaries/intervals currently overlap. So here's where the optimization stopped:

```{r echo=FALSE}
temp <- btw_objective(x = btw_optim_no_grad_no_const$par, returnYHat = TRUE)

ggarrange(
  ncol = 1,

  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y)), y = temp$y)),
  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y_hat)), y = temp$y_hat))
)
```

## No gradient, with constraints

Let's define linear inequality constraints: There are only two things we will impose:

* $b_{\text{first}} \geq 0,\;b_{\text{last}}\leq1$
* $b_n < b_{n+1}$

The starting query-boundaries already satisfy these constraints, but we need to encode them properly as `theta`, `ui` and `ci` for usage with `constrOptim`.

```{r}
# Let's use the fancy class we created for MLMs!
li <- LinearInequalityConstraints$new(theta = query_bounds)
li$flushLinIneqConstraints()

# Let's add the first two constraints (constrOptim requires >=):
li$setLinIneqConstraint(
  name = "b_first_geq_v", ineqs = c(1, rep(0, length(query_bounds) - 1), 0))
li$setLinIneqConstraint(
  name = "b_last_geq_v", ineqs = c(rep(0, length(query_bounds) - 1), -1, -1))

# And we'll also add the second type of constraint for each
# pair of consecutive boundaries:
for (idx in 1:(length(query_bounds) - 1)) {
  i1 <- idx
  i2 <- idx + 1
  name <- paste0("-b", i1, "_geqv_+b", i2)
  # bounds before i1, -b1, +b2, bounds after b2, >= 0.01
  ineqs <- c(rep(0, i1 - 1), -1, 1, rep(0, length(query_bounds) - i2), 0.01)
  li$setLinIneqConstraint(name = name, ineqs = ineqs)
}

li$validateLinIneqConstraints()
```

Ok, let's try `constrOptim` instead:

```{r}
btw_optim_no_grad_with_const <- loadResultsOrCompute(file = "../results/btw_optim_no_grad_with_const.rds", computeExpr = {
  theta <- li$getTheta()
  theta[theta == 0] <- .Machine$double.eps
  theta[theta == 1] <- 1 - .Machine$double.eps
  
  constrOptim(
    theta = theta,
    ui = li$getUi(),
    ci = li$getCi(),
    f = btw_objective,
    grad = NULL,
    control = list(maxit = 500))
})
btw_optim_no_grad_with_const
```

This did not work so well; the amount of calls to the objective function is extraordinarily high, so this took quite long before it stopped, not converging. The final loss is higher than without using constraints. However, the results is valid ;)

```{r echo=FALSE}
temp <- btw_objective(x = btw_optim_no_grad_with_const$par, returnYHat = TRUE)

ggarrange(
  ncol = 1,

  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y)), y = temp$y)),
  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y_hat)), y = temp$y_hat))
)
```

## Approximated gradient, with constraints

The next we do is to try an approximated gradient, __derived over the objective function__. Now we went through the troubles of defining an __analytical__ gradient for these tests, which should work very well in theory. However, we can also approximate a gradient over the objective function and see how that works:

```{r}
btw_optim_approx_grad_with_const <- loadResultsOrCompute(file = "../results/btw_optim_approx_grad_with_const.rds", computeExpr = {
  theta <- li$getTheta()
  theta[theta == 0] <- .Machine$double.eps
  theta[theta == 1] <- 1 - .Machine$double.eps
  
  constrOptim(
    theta = theta,
    ui = li$getUi(),
    ci = li$getCi(),
    #outer.eps = 1e-16,
    f = btw_objective,
    grad = function(x) {
      numDeriv::grad(func = btw_objective, x = x)
    })
})
btw_optim_approx_grad_with_const
```

```{r echo=FALSE}
temp <- btw_objective(x = btw_optim_approx_grad_with_const$par, returnYHat = TRUE)

ggarrange(
  ncol = 1,

  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y)), y = temp$y)),
  plotBtw(data.frame(x = seq(0, 1, length.out = length(temp$y_hat)), y = temp$y_hat))
)
```




























```{r}
btwRef <- list(
  x = seq(0, 1, length.out = 1e3),
  y = sin(seq(0, pi, length.out = 1e3))
)
plot(btwRef, xlim = c(0,1), ylim = c(0,1), type = "l")
```

```{r}
btwDist <- list(
  x = c(
    seq(0, .2, length.out = 1e2),
    seq(.2, .5, length.out = 2e2),
    seq(.5, .6, length.out = 3e2),
    seq(.6, .8, length.out = 1e2),
    seq(.8, 1, length.out = 3e2)
  ),
  y = sin(seq(0, pi, length.out = 1e3))
)
plot(btwDist, xlim = c(0,1), ylim = c(0,1), type = "l")
```


Let's define our boundaries: equally spaced in `[0,1]` (`0,1` are not included as we are trying closed begin/end time warping):

```{r}
btwBoundsRef <- seq(0.1, 0.9, by = 0.1)
btwBoundsRef
```

```{r}
btwRef_f <- stats::approxfun(x = btwRef$x, y = btwRef$y)

btwXPrime <- function(br, boundsQuery, bqIdx) {
  x <- seq(br - 0.1, br, length.out = 1e2) / 0.1 # slice out x, translate and scale
  
  start <- if (bqIdx == 1) 0 else boundsQuery[bqIdx - 1]
  end <- if (bqIdx == length(boundsQuery)) 1 else boundsQuery[bqIdx]
  ext <- end - start
  
  # x' is the scaled x + the offset
  x * ext + start
}
```



```{r}
temp <- sin(seq(0, pi/1.5, length.out = 1e2))
tempf <- approxfun(x = seq(0, 1, length.out = 1e2), y = temp)
plot(temp)
curve(tempf, 0, 1)
curve(tempf, .75, 1)
```

```{r}
offR <- .75
endR <- 1
offQ <- .50
extQ <- .4
scale <- 1 / (extQ / (endR - offR))

tempf2 <- (function() {
  function(x) {
    tempf(((x - offQ) * scale) + offR)
  }
})()

tempf3 <- (function() {
  function(x) {
    tempf(x * scale + offR)
  }
})()

tempf4 <- (function() {
  function(x) {
    tempf((x - offQ) / scale)
  }
})()

curve(tempf2, offQ, offQ + extQ)
curve(tempf3, 0, extQ)
curve(tempf4, offQ, 1 / offR)
```

Now we want to pre-translate and pre-scale the reference function to the interval `[0,1]`, so that we make a closure over all model-constant parameters, and the final function is a function over only the parameters derived from the current query-interval (the query boundaries).

```{r}
tempfPrime <- function(x) {
  tempf(x * (endR - offR) + offR)
}

curve(tempfPrime, 0, 1)
```

```{r}
tempfPrimePrime <- function(x) {
  tempf(((x - offQ) * (1 / extQ)) * (endR - offR) + offR)
}

curve(tempfPrimePrime, offQ, offQ + extQ)
```


So, here's the final function that only needs the absolute start and end of the query-interval, to properly translate and scale.

```{r}
# We use this transform so we can multiply!
extQm <- 1 / extQ

tempf2Prime <- function(x) {
  tempfPrime((x - offQ) * extQm)
}

curve(tempf2Prime, offQ, offQ + extQ)
```

```{r}
plot(loess.smooth(x=seq(.51, .89, len=500), y=sapply(seq(.51, .89, len=500), function(x) numDeriv::grad(tempf2Prime, x))))
```













