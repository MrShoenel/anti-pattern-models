---
title: "Boundary Time Warping (new)"
bibliography: ../inst/REFERENCES.bib
header-includes:
  - \usepackage{bm}
output:
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 6
    df_print: kable
  html_document:
    number_sections: true
    toc: true
    toc_depth: 6
    toc_float: true
    df_print: kable
  md_document:
    toc: true
    toc_depth: 6
    df_print: kable
  word_document: default
---

```{r echo=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")
source(file = "../models/modelsR6.R")
source(file = "../models/BTW.R")

library(ggplot2)
library(ggpubr)
```


# Boundary Time Warping (new)

Note that we have abandoned the previous BTW notebook (`boundary-time-warping.Rmd`), as we want to reformulate BTW using a slightly easier formula. Previously, we had formulated BTW using __two__ parameters, translate, ($t$), and scale, ($s$). It is however enough to derive a single parameter, which we will call $\tau$, that translates $x$ to where we need it. The new model hence becomes:

$$
\begin{aligned}
  m(x,\tau) &= f(\tau + x)\;\text{.}
\end{aligned}
$$

Again, $\tau$ is a local parameter, that depends on which query-interval $x$ falls into, and on $x$ itself -- $\mathbf{\tau}$ is actually __vector-valued__ (in the discrete case). Given the original- and the query-interval, $\tau$ is calculated using the otherwise constant parameters as follows:

$$
\begin{aligned}
  \text{}&\;q \in \mathcal{I}\;\text{, the index of the query-interval}\;x_q\;\text{falls into,}
  \\[1ex]
  &\;\mathbf{x}_q\subset\mathcal{X}\;\text{, the vector of all}\;x\;\text{in the current query-interval,}
  \\[1ex]
  &\;x_i\in\mathbf{x}_q\;\text{, an}\;x\text{-coordinate we need the}\;y\;\text{for,}
  \\[1ex]
  &\;\langle s_o,e_o\rangle\;\text{, start and end of the original interval,}
  \\[1ex]
  &\;\langle s_q,e_q\rangle\;\text{, start and end of the query-interval,}
  \\[1ex]
  x^{\text{rel}}_i = &\;(x_i - s_q) \times (e_q - s_q)^{-1}\;\text{, the relative postion of}\;x_i\;\text{in the query-interval,}
  \\[1ex]
  x_{i}^o = &\; s_o + \big(\,x^{\text{rel}}_i\,(e_o - s_o)\,\big)\;\text{, the corresponding}\;x\;\text{in the original-interval,}
  \\[1ex]
  x_i^o = &\;\tau_i + x_i\;\text{, or, alternatively,}
  \\[1ex]
  \tau_i = &\;s_o + \bigg(\frac{x_i - s_q}{e_q - s_q}\times (e_o - s_o)\bigg) - x_i\;\text{, vector-valued as}
  \\[1ex]
  \bm{\tau} = &\ x_i^o - \mathbf{x}_q\;\text{, resulting in}
  \\[1ex]
  \hat{y}_i = &\; f(\tau_i+x_i)\;\text{, or,}
  \\[1ex]
  \mathbf{\hat{y}} = &\; f(\bm{\tau}+\mathbf{x}_q)\;\text{ (vector-valued version).}
\end{aligned}
$$

In other words, $x_o$ is the $x$ we want to return $y$ for, and it needs to be derived from $x_i$ and the original- and query-intervals. In more other words, when boundaries are moved, and intervals get translated and scaled, $x_i$ always falls into an interval, and it assumes a relative position in it. So the correct $y$ to return is the one in the original interval, at that same relative position. All this can be reduced to a single offset, $\tau_i$, added to $x_i$ at index $i$.

In pseudo-code, this is what (the discrete version of) $\mathsf{M}$ does:

```{r eval = FALSE}
M <- function(theta_b_org, theta_b, num_samples, r, f) {
  X <- create_range(start = min(theta_b), end = max(theta_b), amount = num_samples)
  y <- r(X)
  y_hat <- zeros(num_samples)
  
  for (x, x_idx) in X do:
    q <- determine_interval_x_falls_into(x)
    
    start_org, end_org <- theta_b_org[q]
    start_q, end_q <- theta_b[q]
    
    x_rel <- (x - start_q) / (end_q - start_q)
    x_o <- start_org + (x_rel * (end_org - start_org))
    # Instead of calculating tau, let's call f directly with x_o.
    # We need tau only for the analytical formulation.
    y_hat[x_idx] <- f(x_o)
  endfor
  
  return(X, y, y_hat)
}
```

Let's how a short demonstration:

```{r}
btwRef <- data.frame(
  x = seq(0, 1, length.out = 1e3),
  y = sin(seq(0, 2 * pi, length.out = 1e3))
)

btwQueryBounds <- c(.1, .2, .5, .6, .8)
btwQuery <- data.frame(
  x = c(
    seq(0, btwQueryBounds[1], length.out =  75),
    seq(btwQueryBounds[1], btwQueryBounds[2], length.out =  25),
    seq(btwQueryBounds[2], btwQueryBounds[3], length.out = 150),
    seq(btwQueryBounds[3], btwQueryBounds[4], length.out = 300),
    seq(btwQueryBounds[4], btwQueryBounds[5], length.out =  50),
    seq(btwQueryBounds[5], 1, length.out = 400)
  ),
  y = btwRef$y
)

plotBtw <- function(df, bounds = c()) {
  g <- ggplot(data = df, aes(x = x, y = y)) + theme_light() + geom_line()
  for (i in 1:length(bounds)) {
    g <- g + geom_vline(xintercept = bounds[i])
  }
  g
}

signal_ref <- stats::approxfun(x = btwRef$x, y = btwRef$y, ties = mean)
signal_query <- stats::approxfun(x = btwQuery$x, y = btwQuery$y, ties = mean)

query_bounds <- seq(0, 1, by = 0.1)
```


```{r}
temp <- M_new(
  theta_b_org = query_bounds,
  theta_b = query_bounds,
  r = signal_ref,
  f = signal_query)

# We know how the reference signal was distorted previously. Let's make
# a test where we manually undo this by moving the boundaries.
temp2_bounds <- c(
  0,
  .075, .1, .15, .2, .25,
  .55, .575, .6,  .8, 1)
temp2 <- M_new(
  theta_b_org = query_bounds,
  theta_b = temp2_bounds,
  r = signal_ref,
  f = signal_query)

ggarrange(
  ncol = 1,
  plotBtw(data.frame(x = temp$X, y = temp$y)),
  plotBtw(data.frame(x = temp$X, y = temp$y_hat), bounds = query_bounds),
  plotBtw(data.frame(x = temp2$X, y = temp2$y_hat), bounds = temp2_bounds)
)
```

Good, our new model, `M_new(..)`, works! Now we need to reformulate everything, all the derivatives etc., as this changes. We will, for demonstrative purposes mostly, again make use of the residual sum of squares (RSS) as error function.


## Formal description of the model

The model is now much simpler, and follow the structure of the original notebook here. Note that almost all of the remarks of the previous notebook apply, only the way we calculate BTW has changed.

$$
\begin{aligned}
  r(x) &= \dots\;\text{, a function over the reference signal,}
  \\
  f(x),f\prime(x),f\prime\prime(x) &= \dots\;\text{, functions over the query signal (and its 1st and 2nd derivatives),}
  \\[1em]
  m(x,\tau) &= f(\tau+x)\;\text{, model that picks a transformed x,}
  \\[1em]
  \frac{\partial\;\text{m}}{\partial\;\tau}\;,\;\frac{\partial^2\;\text{m}}{\partial^2\;\tau} &= f\prime(\tau+x)\;,\;f\prime\prime(\tau+x)\;\text{.}
\end{aligned}
$$

Since we're using only one variable, there is only one partial derivative, and we are getting very pleasing derivatives -- they're exactly the same, except for that we plug in $\tau+x$ in the derivatives of $f$. The description of our overall model has not changed, and its parameters are the same (check previous notebook for descriptions):

$$
\begin{aligned}
  \mathsf{M}(\mathbf{\theta}_{b_{\text{org}}}, \mathbf{\theta}_b, n, r, f) &= \langle\, \mathbf{y},\mathbf{\hat{y}} \,\rangle
\end{aligned}
$$

## Using the RSS error function

We use the RSS as an example for an error function, but any objective/cost/loss etc. function can be used, if its derivatives exist.

$$
\begin{aligned}
  \text{RSS}_m &= \sum_{i=1}^{N}\,(\mathbf{y}_i - m(x_i,\tau_i))^2\;\text{, the RSS for the discrete case,}
  \\[1ex]
  &= \sum_{i=1}^{N}\,(\mathbf{y}_i - f(\tau_i+x_i))^2\;\text{,}
  \\[1ex]
  \frac{\partial\;\text{RSS}_m}{\partial\;\tau_i} &= \sum_{i=1}^{N}\,-2\,\big(y_i - f(\tau_i + x_i)\big)\times f'(\tau_i + x_i)\;\text{,}
  \\[1ex]
  \frac{\partial^2\;\text{RSS}_m}{\partial^2\;\tau_i} &= \sum_{i=1}^{N}\,2\,\Big((f(\tau_i + x_i) - y_i)\times f''(\tau_i + x_i) + f'(\tau_i + x_i)^2\Big)\;\text{.}
\end{aligned}
$$

Again, it becomes apparent that we can pre-compute $\mathbf{\hat{y}}$, $\mathbf{\hat{y}}\prime$ and $\mathbf{\hat{y}}\prime\prime$ using the derivatives of $f$.

$$
\begin{aligned}
  \mathbf{\hat{y}} &= f(\mathbf{\tau} + \mathbf{x})\;\text{, add vectors}\;\tau\;\text{and}\;x\;\text{already togeter,}
  \\[1ex]
  \mathbf{\hat{y}}\prime &= f'(\mathbf{\tau} + \mathbf{x})\;\text{,}
  \\[1ex]
  \text{RSS}_m &= \sum_{i=1}^{N}\,(\mathbf{y}_i - \mathbf{\hat{y}}_i)^2\;\text{,}
  \\[1ex]
  \frac{\partial\;\text{RSS}_m}{\partial\;\tau} &= \sum_{i=1}^{N}\,-2\,\big(\mathbf{y}_i - \mathbf{\hat{y}}_i\big)\times \mathbf{\hat{y}}\prime_i\;\text{,}
  \\[1ex]
  \frac{\partial^2\;\text{RSS}_m}{\partial^2\;\tau} &= \sum_{i=1}^{N}\,2\,\Big((\mathbf{\hat{y}}_i - \mathbf{y}_i)\times \mathbf{\hat{y}}\prime\prime_i + \big(\mathbf{\hat{y}}\prime_i\big)^2\Big)\;\text{.}
\end{aligned}
$$

And in `R`-code, this looks like:

```{r}
RSS_m <- function(y, y_hat) {
  sum((y - y_hat)^2)
}

RSS_m_deriv1 <- function(y, y_hat, y_hat_prime) {
  sum(-2 * (y - y_hat) * y_hat_prime)
}

RSS_m_deriv2 <- function(y, y_hat, y_hat_prime, y_hat_prime_prime) {
  sum(2 * ((y_hat - y) * y_hat_prime_prime + y_hat_prime^2))
}
```


# Optimization

Let's do some quick testing with `optim`:

```{r}
library(optimParallel)

Stabilize <- function(f, lb, ub) {
  Vectorize(function(x) {
    if (x < lb) f(lb) else if (x > ub) f(ub) else f(x)
  })
}

r <- Stabilize(signal_ref, 0, 1)
f <- Stabilize(signal_query, 0, 1)

o <- function(theta, isGrad = FALSE) {
  n <- 1e3
  res <- M_new(
    theta_b_org = query_bounds, theta_b = theta, r = r, f = f, num_samples = n)
  
  loss <- RSS_m(y = res$y, y_hat = res$y_hat)
  print(paste0(isGrad, " / ", loss))
  
  reg <- (1 - max(theta) + min(theta)) * n
  
  loss + reg
}


cl <- parallel::makePSOCKcluster(12)
parallel::clusterExport(cl, varlist = c("M_new", "query_bounds", "r", "f", "RSS_m", "Stabilize", "signal_ref", "signal_query", "f_deriv1", "RSS_m_deriv1"))

set.seed(1337)
optRp <- optimParallel::optimParallel(
  par = query_bounds,
  fn = o,
  #gr = o_deriv1_test,
  lower = rep(0, length(query_bounds)),
  upper = rep(1, length(query_bounds)),
  parallel = list(
    cl = cl,
    forward = FALSE
  )
)

stopCluster(cl)
optRp
```

## Using an explicit analytical gradient

Computing the gradient requires us to "re-assemble" the boundaries from $\tau$. Recall that we have separate $\mathbf{\tau}$-vectors in each interval, which is demarcated by two consecutive boundaries. The boundaries represent the actual parameter vector, the parameters we want to optimize, $\mathbf{\theta}$. So we have to reverse the steps we undertook for establishing $\tau$, in order to get back to the boundaries. In the following, $\langle s_q,e_q\rangle$ represent those absolute offsets for an interval, in which the current $\bm{\tau}$ is valid. The gradient of our model will tell us how to change $\bm{\tau}$, and our task is to translate that back to the changes required for $s_q,e_q$, namely $\delta_{s_q},\delta_{e_q}$.


$$
\begin{aligned}
  \bm{\tau} &= x_i^o - \mathbf{x}_q\;\text{, fully expanded as}
  \\[1ex]
  \tau_i &= s_o + \bigg(\frac{x_i - s_q}{e_q - s_q}\times (e_o - s_o)\bigg) - x_i\;\text{, vector-valued as}
  \\[1ex]
  \bm{\tau} &= s_o + \bigg(\frac{\mathbf{x}_q - s_q}{e_q - s_q}\times (e_o - s_o)\bigg) - \mathbf{x}_q,
  \\[1ex]
  \delta &= \frac{\partial\;\text{RSS}_m}{\partial\;\bm{\tau}}\big(\mathbf{x}_q,\bm{\tau}\big)\;\text{, the gradient-suggested change for that}\;\bm{\tau}\;\text{,}
  \\[1ex]
  \delta_{s_q} &= \frac{x_{\text{first}}\,(s_o - e_o) + e_q\,\Big(\delta + x_{\text{first}} - s_o\Big)}{\delta + x_{\text{first}} - e_o}\;\text{, and}
  \\[1em]
  \delta_{e_q} &= \frac{e_o\,(s_q-x_{\text{last}}) + e_o\,x_{\text{last}} - s_q\,(\delta + x_{\text{last}})}{s_o - x_{\text{last}} - \delta}\;\text{.}
\end{aligned}
$$

$\bm{\tau}$ is vector-valued, as we require a different offset for each $x_i$. However, we can associate a cost and a gradient $\delta$ with $\bm{\tau}$, which could be used to adjust each and every single $\tau_i$ separately, but what we really want to adjust are the current interval's boundaries. We can re-arrange above equation for $\tau_i$ for either $s_q$ or $e_q$, and plug in $\delta$, so that this delta gives us information as to how to change $s_q,e_q$, instead of $\tau$.

Note that the first boundary is at $x_{\text{first}}$, so that $\delta_{s_q}$ needs to be calculated using the first $x$ in the query-interval. Also note, and this is even more important, that all but the last interval are defined to __not__ include the last boundary, i.e., $i_n=[b_{s_q}, b_{e_q})$, which means that the end of the current interval, $e_q$, is not equal to the second boundary (except in the last interval). The distance between $e_q$ and the next boundary is $0$, but technically, in the discrete case, $x_{e_q+1}=b_{s_{q+1}}$ (the $x$ after $e_q$ is equal to the starting boundary of the next interval). $b_{s_{q+1}}$ lies hence _outside_ the current interval $q$. However, in most cases where the signal does not abruptly change in the next interval, we expect $x_{e_q}\approx b_{s_{q+1}}$. We point out this interesting case as it may preserve half the computations. When adjusting the second boundary of an interval using $e_q$, we expect that:

$$
\begin{aligned}
  &\;\text{sgn},\delta\;\text{, functions to obtain the sign and}\;\delta\text{,}
  \\[1ex]
  \text{sgn}\big(\delta(x_{e_q})\big) =&\;\text{sgn}\big(\delta(b_{s_{q+1}})\big)\;\text{, (the gradient-suggested change has the same sign),}
  \\[1ex]
  \log{\Big\lvert\,\delta\big(x_{e_q}\big)\,\Big\rvert} \approx&\;\log{\Big\lvert\,\delta\big(b_{s_{q+1}}\big)\Big\rvert\,}\;\text{, the magnitudes of both changes are similar,}
  \\[1ex]
  \Big\lvert\,\delta\big(x_{e_q}\big)\,\Big\rvert \neq&\;\Big\lvert\,\delta\big(b_{s_{q+1}}\big)\Big\rvert\,\;\text{, but not the same.}
\end{aligned}
$$

TODO: Describe below code.

```{r}
#' This is the (overly expressive) gradient of our objective function.
#' It evaluates the gradient for each pair of neighboring boundaries,
#' instead of going 2-by-2. We implement it this way for testing purposes.
o_deriv1_test <- function(theta, explicit = FALSE) {
  res <- M_new(
    theta_b_org = query_bounds, theta_b = theta, r = r, f = f)
  res1 <- M_new(
    theta_b_org = query_bounds, theta_b = theta, r = r, f = f_deriv1)
  
  delta_sq <- Vectorize(function(so, eo, eq, x1, delta_tau) {
    (x1 * (so - eo) + eq * (delta_tau + x1 - so)) / (delta_tau + x1 - eo)
  }, vectorize.args = c("x1", "delta_tau"))
  
  delta_eq <- Vectorize(function(so, eo, sq, xL, delta_tau) {
    (eo * (sq - xL) + eo * xL - sq * (delta_tau + xL)) / (so - xL - delta_tau)
  }, vectorize.args = c("xL", "delta_tau"))
  
  # Now, for each consecutive pair of neighboring boundaries, piece-wise
  # evaluate the derivative of the cost-function, obtain tau, and translate
  # it back to the two boundaries of the enclosing interval.
  
  theta_full <- c()
  
  for (iIdx in 1:(length(theta) - 1)) {
    y <- res$y[res$int_idx == iIdx]
    y_hat <- res$y_hat[res$int_idx == iIdx]
    y_hat_prime <- res1$y_hat[res1$int_idx == iIdx]
    
    if (length(y) == 0) {
      theta_full[paste0("S", iIdx)] <- 0
      theta_full[paste0("E", iIdx)] <- 0
      next
    }
    
    delta_tau <- sapply(1:length(y), function(i) {
      RSS_m_deriv1(y = y[i], y_hat = y_hat[i], y_hat_prime = y_hat_prime[i])
    })
    
    so <- query_bounds[iIdx]
    eo <- query_bounds[iIdx + 1]
    sq <- theta[iIdx]
    eq <- theta[iIdx + 1]
    
    sq_delta <- c(0, na.omit(delta_sq(
      so = so, eo = eo, eq = eq, x1 = res$X[res$int_idx == iIdx], delta_tau = delta_tau)))
    sq_min <- min(sq_delta)
    sq_max <- max(sq_delta)
    
    eq_delta <- c(0, na.omit(delta_eq(
      so = so, eo = eo, sq = sq, xL = res$X[res$int_idx == iIdx], delta_tau = delta_tau)))
    eq_min <- min(eq_delta)
    eq_max <- max(eq_delta)
    
    theta_full[paste0("S", iIdx)] <- if (abs(sq_max) > abs(sq_min)) sq_max else sq_min
    theta_full[paste0("E", iIdx)] <- if (abs(eq_max) > abs(eq_min)) eq_max else eq_min
    
    # delta_tau <- RSS_m_deriv1(y = y, y_hat = y_hat, y_hat_prime = y_hat_prime)
    # 
    # theta_full[paste0(iIdx)] <- delta_tau
    # 
    # # # Now init the parameters we need:
    # # so <- query_bounds[iIdx]
    # # eo <- query_bounds[iIdx + 1]6
    # # sq <- theta[iIdx]
    # # eq <- theta[iIdx + 1]
    # # 
    # # theta_full[paste0("S", iIdx)] <- delta_sq(
    # #   so = so, eo = eo, eq = eq, x1 = res$X[1], delta_tau = delta_tau)
    # # theta_full[paste0("E", iIdx)] <- delta_eq(
    # #   so = so, eo = eo, sq = sq, xL = utils::tail(res$X, 1), delta_tau = delta_tau)
  }
  # theta_full[paste0(length(theta))] <- 0
  
  if (!explicit) {
    s_all <- paste0("S", 1:(length(theta) - 1))
    eq_last <- paste0("E", length(theta) - 1)
    theta_full <- theta_full[c(s_all, eq_last)]
  }
  
  print(unname(theta_full))
  theta_full
}
```


```{r}
#' Note that here we use the stabilized version of the query-signal.
#' TODO: We should supply helper functions to the user that use the
#' best possible method of derivation, like we do here.
f_deriv1 <- function(x) {
  m <- if (x == 0) "forward" else if (x == 1) "backward" else "central"
  pracma::fderiv(f = f, x = x, method = m)
}
```


```{r}
optR <- optim(
  par = query_bounds,
  fn = o,
  gr = o_deriv1_test,
  method = "L-BFGS-B",
  lower = rep(0, length(query_bounds)),
  upper = rep(1, length(query_bounds))
)
optR
```

```{r}
temp <- query_bounds
temp[temp == 0] <- .Machine$double.eps
temp[temp == 1] <- 1 - .Machine$double.eps

optRC <- constrOptim(
  theta = temp,
  f = o,
  grad = o_deriv1_test,
  # grad = function(x) {
  #   pracma::grad(f = o, x0 = x)
  # },
  ui = li$getUi(),
  ci = li$getCi(),
  method = "BFGS"
)
optRC
```
















