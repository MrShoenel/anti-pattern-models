---
title: "Closed-window optimization"
bibliography: ../inst/REFERENCES.bib
header-includes:
  - \usepackage{bm}
output:
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 6
    df_print: kable
    keep_tex: true
  html_document:
    number_sections: true
    toc: true
    toc_depth: 6
    toc_float: true
    df_print: kable
  md_document:
    toc: true
    toc_depth: 6
    df_print: kable
  word_document: default
---

\newcommand{\norm}[1]{\left\lvert#1\right\rvert}

```{r echo=FALSE, warning=FALSE}
source(file = "../helpers.R")
source(file = "./common-funcs.R")

library(ggplot2)
library(ggpubr)
```

In this notebook, we will go back to our initial Fire Drill (FD) problem, and design models to accomodate it. The goal is to bring together everything we learned from all the notebooks in between, and to come up with solutions specifically designed to address the matter.

# Problem description

Initially I thought the answer would be using dynamic time Warping, but it is not suitable for an analytical solution (actually, closed-form expression). What makes our problem a bit special are the following:

* __Reference__ and __Query__ signal have the same support: The FD is an AP that spans the entire project, start to finish. Matching must be attempted at the very beginning of the data, and is concluded at the very end. The query signal is scaled such that the extent of its support is the same as the reference's.
  * As we will see later, this constraint is optionally relaxed by allowing to the query-signal to cover a proper sub-support of the reference using open begin-/ and/or -end time warping.
* All data has to be used: We can apply local time warping to the query signal, but nothing can be left out. The warping may even result in a length of zero of some intervals, but they must not be negative.


## Optimization goals

We have __3__ optimization goals for the joint paper/article (each of them has a dedicated section in this notebook):

* Fitting of project data to a defined AP (here: the Fire Drill)
* Validate selection of sub-models andscores; also: calibration of scores
* Averaging the reference pattern over allknown ground truths


# Optimization goal I: Fitting of project data to a defined AP

This is the primary problem, and we should start with a visualization:

```{r echo=FALSE}
# Save everything so it can be reused elsewhere:
dens_acp_data <- readRDS(file = "../data/dens_acp_data.rds")
# Also save the pattern:
fd_data_concat <- readRDS(file = "../data/fd_data_concat.rds")

bounds <- c(0.085, 0.625, 0.875)

ggarrange(
  plot_project_data(fd_data_concat, boundaries = bounds),
  plot_project_data(dens_acp_data),
  nrow = 2
)
```

The first plot is the reference pattern, the second is a random project. All data has already been scaled and normalized, for example, both supports are the same. We are going to use an entirely new approach to warp the project data from the 2nd plot the reference data from the first plot.

## Approach

In order to understand this problem, we need to make some statements:

* The reference pattern is subdivided into intervals by some boundaries (blue in the above plot). The data in each such interval is considered __constant__. That means, if we were to sample from that interval, these samples would always be identical. If we were to look at each interval as a function, then it will always show the same behavior. Each interval has its own __support__.
* The first interval starts at $0$ and the last interval ends at $1$ (we always expect that all data has been scaled and translated to $[0,1]$ prior to this, to keep things simple).
* Any and all variables are strictly __positive__, with a co-domain of $[0,1]$ as well. We require this for reasons of simplification, although negative values would be no problem (but require extra effort at this point).
* An interval's __length__ is never negative. In practice, one may even want to define a minimum length for all or for each individual interval.
* An interval's __offset__ is the sum of the lengths of its preceding intervals. All intervals' lengths sum up to $1$.

To find the sub-support of the query that best matches a variable (or many) in an interval, the query signal is translated and scaled. Another way to think about this is that the query signal is translated and only valid in the support that starts with the translate, and ends there plus the length of the current interval. I mention this because this is how we will begin.

In the above plot, there are __$4$__ intervals, and let's say we index them with the parameter $q$. If we were to describe the reference- and query-signals through the two functions $r,f$, and we were given some boundary $b_q$ that delimits the reference-interval, for $q=1$ we would get:

$$
\begin{aligned}
  r=&\;\mathbb{R}\mapsto\mathbb{R}^m\;\land\;m>0\text{,}
  \\[1ex]
  r_q=&\;r(\cdot)\;\text{, reference-signal for interval}\;q,
  \\[1ex]
  \text{supp}(r_q)=&\;[0,b_q)\;\text{, the support of}\;r_q.
\end{aligned}
$$

During optimization, the goal is to find the best length for each interval. Assuming a given vector that contains these four lengths, $\bm{\vartheta}^{(l)}$, we get:

$$
\begin{aligned}
  f=&\;\mathbb{R}\mapsto\mathbb{R}^n\;\land\;n>0\text{,}
  \\[1ex]
  f_q=&\;f(\cdot)\;\text{, query-signal for interval}\;q,
  \\[1ex]
  \text{supp}(f_q)=&\;\big\{\;x\in\mathbb{R}\;\rvert\;x\geq0\;\land\;x<\bm{\vartheta}^{(l)}_q\;\big\}\;\text{, the support of}\;f_q.
\end{aligned}
$$

The supports will almost always differ (especially for $q>1$). For the discrete case this may not be important, because the model may choose to always take the same amount of equidistantly-spaced samples from the reference- and query-intervals, regardless of the support. This is essentially the same as scaling and translating the query-support to be the same as the reference support. This is what would be required for the continuous case.

The result of the previous definition is an __unchanged__ query-signal; however, we specify its support that _corresponds_ to the support of the current reference interval. In general, to translate (and scale) an interval from one offset and length to another offset and length, the expression is:

$$
\begin{aligned}
  f(x)\dots&\;\text{function to be translated,}
  \\[1ex]
  f'(x)=&\;f\Bigg(\frac{(x - t_b) \times (s_e - s_b)}{t_e - t_b} + s_b\Bigg)\;\text{, where}
  \\[1ex]
  s_b,s_e,t_b,t_e\dots&\;\text{begin (b) and end (e) of the source- (s) and target-intervals/supports (t).}
\end{aligned}
$$

We will do some testing with the pattern and data we have, focusing on the __adaptive__ (__A__) variable.

However, notice how we have defined $r,f$ to map to dimensionalities of $1$ or greater, effectively making them vector-valued:

* A loss between both functions is concerning a specific $q$-th interval, and also usually concerning a specific index in each function, e.g., $\mathcal{L}_q=(r_q^{(13)}(x)-f_q^{(4)}(x))^2$. Each such loss may have its __individual weight__ assigned.
* This concept of being multivariate can be extended to having __multiple signals__ per _Warping Pattern/Candidate_, too, by simple concatenation of the output of two or more such vector-valued functions, e.g., $f(x)=f_a^{(1,2,3)}\frown f_b^{(7,11,13)}\frown\dots$, where $f_a,f_b$ are individual vector-valued/multivariate Warping Candidates.
* The dimensionalities of $r,f$ do not necessarily have to be the same: Imagine the case where we compute some loss of one variable of the Warping Pattern over the same variable of one or more Warping Candidates. Also, as in the previous example, not all of a signal's variables have to be used.


```{r echo=FALSE}
a_ref <- fd_data_concat[fd_data_concat$t == "A", ]
a_query <- dens_acp_data[dens_acp_data$t == "A", ]

r <- stats::approxfun(x = a_ref$x, y = a_ref$y)
f <- stats::approxfun(x = a_query$x, y = a_query$y)

ggarrange(
  plot_project_data(a_ref, boundaries = bounds),
  plot_project_data(a_query) + ylim(0,0.3),
  nrow = 2
)
```

We will do a test with $f_q$ and $f'_q$, and set $q$ to $1$ and then to $3$. In the first case, no re-definition of $f$ is required. We only need to define where to query-intervals should be (thus their length).

```{r warning=FALSE}
# Use these lengths for q=3:
vt_q3 <- c(0.15, 0.45, 0.1, 0.3)

a_ref_q3 <- a_ref[a_ref >= bounds[2] & a_ref < bounds[3], ]
a_query_q3 <- a_query[a_query >= (vt_q3[1] + vt_q3[2]) & a_query < (1 - vt_q3[4]), ]

r_q3 <- stats::approxfun(x = a_ref_q3$x, y = a_ref_q3$y)
f_q3 <- stats::approxfun(x = a_query_q3$x, y = a_query_q3$y)

ggplot() +
  stat_function(fun = r_q3, aes(color = "r_q3")) +
  stat_function(fun = f_q3, aes(color = "f_q3")) +
  xlim(.6, .9)
```

And now we re-define $f$ such that it has the same support as `r_q3`:

```{r warning=FALSE}
f_prime_proto <- function(x, source_b, source_e, target_b, target_e) {
  f((((x - target_b) * (source_e - source_b)) / (target_e - target_b)) + source_b)
}

f_q3_prime <- function(x) {
  # These two are where the signal currently is (translation source range)
  phi_q <- vt_q3[1] + vt_q3[2]
  vartheta_q <- vt_q3[3]
  # These two is where we want the signal to be (translation target range)
  # theta_q <- bounds[3] - bounds[2]
  
  source_b <- phi_q
  source_e <- phi_q + vartheta_q
  target_b <- bounds[2]
  target_e <- bounds[3]
  
  f_prime_proto(x, target_b = target_b, target_e = target_e,
                source_b = source_b, source_e = source_e)
}

curve(f_q3_prime, 0, 1)

ggplot() +
  stat_function(fun = r_q3, aes(color = "r_q3")) +
  stat_function(fun = f_q3_prime, aes(color = "f_q3_prime")) +
  geom_vline(xintercept = bounds[2]) +
  geom_vline(xintercept = bounds[3]) +
  xlim(bounds[2:3])
```

Another test, squeezing $f$ into the interval $[0.5,1]$:

```{r}
f_q3_test <- function(x) {
  f_prime_proto(x, source_b = 0, source_e = 1, target_b = .5, target_e = 1)
}

f_q3_test2 <- function(x) {
  f_prime_proto(x, source_b = .55, source_e = .7, target_b = 0, target_e = 1)
}

curve(f, 0, 1)
curve(f_q3_test, 0, 1)
curve(f_q3_test2, 0, 1)
```


## Sub-Model formulation

In practice, we want to re-define $f$, such that we scale and translate it in a way that its support and the reference support perfectly overlap. This will be done using above expression to derive $f$ into $f'$. Also, we want our model to satisfy additional constraints, without having to use regularization or (in-)equality constraints. These constraints are:

* All intervals must have a length greater than $0$ (or any arbitrary positive number).
* The lengths of all intervals must sum up to $1$.

During unconstrained optimization, there is no way to enforce these constraints, but we can build these into our model! To satisfy the first constraint, we will replace using any $l_q$ directly by $\max{(\bm{\lambda}_q,l_q)}$ (where $\bm{\lambda}_q$ is the lower bound for interval $q$ and $>0$). This could however lead to the sum of all such lengths being greater than the extent of the support. The solution to the second constraint will satisfy this, by normalizing all lengths using the sum of all lengths. This way, lengths are converted to ratios. Later, using the _target-extent_, these ratios are scaled back to actual lengths.

We define the new model with the built-in constraints ($m_q^c$):

$$
\begin{aligned}
  \bm{\theta}^{(b)}\;\dots&\;\text{ordered vector of boundaries, where the first and last boundary together}
  \\[0ex]
  &\;\text{represent the support of the signal they subdivide,}
  \\[1ex]
  b\dots&\;\text{the begin (absolute offset) of the first source-interval (usually}\;0\text{),}
  \\[1ex]
  e\dots&\;\text{the end (absolute offset) of the last source-interval (usually}\;1\text{), also}\;e>b\text{,}
  \\[1ex]
  \gamma_b,\gamma_e,\gamma_d\dots&\;\text{absolute min/max for}\;b,e\;\text{and min-distance between them,}
  \\[1ex]
  &\;\text{where}\;\gamma_d\geq 0\;\land\;\gamma_b+\gamma_d\leq\gamma_e\;\text{,}
  \\[1ex]
  \beta_l=&\;\min{\Big(\gamma_e-\gamma_d,\max{\big(\gamma_b, \min{(b,e)}\big)}\Big)}\;\text{, lower boundary as of}\;b,e\text{,}
  \\[1ex]
  \beta_u=&\;\max{\Big(\gamma_b+\gamma_d,\min{\big(\gamma_e, \max{(b,e)}\big)}\Big)}\;\text{, upper boundary as of}\;b,e\text{,}
  \\[1ex]
  \bm{\lambda}\dots&\;\text{vector with minimum lengths for each interval,}\;\forall\,\bm{\lambda}_q\geq 0\text{,}
  \\[1em]
  l_q\dots&\;\text{the length of the }q\text{-th source-interval,}
  \\[1ex]
  l'_q=&\;\max{\big(\bm{\lambda}_q,\norm{l_q}\big)}\;\text{, where}\;\norm{l_q}\equiv\max{\big(-l_q,l_q\big)}\equiv\mathcal{R}(2\times l_q)-l_q\text{,}
  \\[0ex]
  &\;\text{(the corrected (positive) interval-length that is strictly}\;\geq\bm{\lambda}_q\text{),}
  \\[1ex]
  \psi=&\;\sum_{i=1}^{\max{(Q)}}\,l'_i\equiv \bm{l'}^\top\hat{\bm{u}}\;\text{, the sum used to normalize each and every}\;q\text{-th}\;l\text{,}
  \\[1ex]
  l_q^{(c)}=&\;\frac{l'_q}{\psi}\times(\beta_u-\beta_l)\;\text{, corrected, normalized and re-scaled version of}\;l_q\text{,}
  \\[1ex]
  \phi_q\equiv&\;\begin{cases}
    0,&\text{if}\;q=1,
    \\
    \sum_{i=1}^{q-1}\,l_i^{(c)},&\text{otherwise}
  \end{cases}\;,
  \\[1ex]
  &\;\text{(the sum of the lengths of all corrected preceding intervals).}
\end{aligned}
$$

With all these, we can now formulate the sub-model using the previously used parameters $s_b,s_e,t_b,t_e$ for translation and scaling of one interval (the source) to another interval (the target):

$$
\begin{aligned}
  s_b^{(q)}=&\;\beta_l+\phi_q\;\text{, and}
  \\[1ex]
  s_e^{(q)}=&\;s_b^{(q)}+l_q^{(c)}\;\text{, begin- and end-offset of the }q\text{-th source-interval,}
  \\[1ex]
  t_b^{(q)}=&\;\bm{\theta}^{(b)}_q\;\text{, and}
  \\[1ex]
  t_e^{(q)}=&\;\bm{\theta}^{(b)}_{q+1}\;\text{, begin- and end-offset of the }q\text{-th target-interval,}
  \\[1ex]
  \delta_q^{(t)}=&\;t_e^{(q)}-t_b^{(q)}\;\text{, the extent of the }q\text{-th target-interval,}
  \\[1ex]
  m_q^c\Big(f,x,t_b^{q},l_q^{(c)},\delta_q^{(t)},s_b^{(q)}\Big)=&\;f\Bigg(\frac{\Big(x-t_b^{(q)}\Big)\times l_q^{(c)}}{\delta_q^{(t)}}+s_b^{(q)}\Bigg)\;\text{, with all non-constants expanded:}
  \\[1ex]
  =&\;f\Bigg(\overbrace{\Bigg[\overbrace{\min{\Big(\gamma_e-\gamma_d,\max{\big(\gamma_b, \min{(b,e)}\big)}\Big)}}^{\beta_l}+\overbrace{\begin{cases}
    0,&\text{if}\;q=1,
    \\
    \sum_{i=1}^{q-1}\,l_i^{(c)},&\text{otherwise}
  \end{cases}}^{\phi_q}\Bigg]}^{s_b^{(q)}}
  \\[0ex]
  &\;\;\;\;\;\;+\Big(x-t_b^{(q)}\Big)\times{\delta_q^{(t)}}^{-1}\times \overbrace{\max{\Big(\bm{\lambda}_q,\mathcal{R}(2\times l_q)-l_q\Big)}\times\Bigg[\sum_{i=1}^{\max{(Q)}}\,l'_i\Bigg]^{-1}}^{\text{(recall}\;l_q^{(c)}=\frac{l'_q}{\psi}\times(\beta_u-\beta_l)\text{, here we got}\;l'_q\times\psi^{-1}\text{)}}
  \\[0ex]
  &\;\;\;\;\;\;\times\bigg(\overbrace{\max{\Big(\gamma_b+\gamma_d,\min{\big(\gamma_e, \max{(b,e)}\big)}\Big)}}^{\beta_u}-\overbrace{\min{\Big(\gamma_e-\gamma_d,\max{\big(\gamma_b, \min{(b,e)}\big)}\Big)}}^{\beta_l}\bigg)\Bigg)\;\text{.}
\end{aligned}
$$


The model $m_q^c$ now satisfies these properties:

* Each interval has a length greater than or equal to $\bm{\lambda}_q$ (which must be strictly positive; a length of zero however is allowed).
* The first interval begins at $\beta_l$, the last interval ends at $\beta_u$ (these parameters are the absolute begin/end of the target-intervals, and hence apply to where onto the reference the query will be mapped to). These parameters can either be constant or learned during optimization, effectively allowing open/closed begin- and/or -end time warping.
* Each interval begins exactly after its predecessor, such that there are no overlaps. Intervals are seamlessly strung together.
* The sum of the lengths of all intervals is normalized and then re-scaled using $\phi$, considering the constraints of $\gamma_d$ and the minimum length of each $q$-th interval (using its corresponding $\bm{\lambda}_q$).

### Reference vs. Query

Thus far, we have made the difference between these two notions. The implicit assumption thus far was, that the reference is constant, i.e., once given as a pattern, it is never altered, and the query is warped to it. I came across the important difference while thinking about closed- vs. open -begin and/or -end time warping: In the closed case, the __whole__ query is warped onto the __whole__ reference, i.e., both signals have to be used to their full extent. If, however, the begin or end (or both) are open, only a portion of the query is used to match still the entire reference. So, not only is the reference constant, it also has to be matched wholly, i.e., all of its reference-intervals need to be used. This raises two important points.

First, regarding the differences between both signals during warping, we should propose more appropriate names. I suggest __"Warping Pattern"__ for the constant signal that has to be used in its entirety, and __"Warping Candidate"__ for the signal that is translated and scaled to be closest to the Warping Pattern.

Second, in some cases it may be useful __not__ to use the entire Warping Pattern. In this case, both signals can simply be swapped with each other, and a non-closed model is fit. After fitting, the warping path is inverted. Also, this is how DTW works, one signal has to be mapped in full. This flipping case is useful to find a sub-support of the Warping Pattern within the Warping Candidate.

### Min/Max as ramp function

Note that we define $\min,\max$ in terms of the ramp-function and its derivative, the Heaviside step function.

$$
\begin{aligned}
  \max{(x,y)}\equiv&\;\mathcal{R}(y-x)+x\;\text{, with gradient}
  \\[1ex]
  \nabla\,\mathcal{R}(y-x)+x=\Bigg[\frac{\partial\,\mathcal{R}(\cdot)}{\partial\,x},\frac{\partial\,\mathcal{R}(\cdot)}{\partial\,y}\Bigg]=&\;\Bigg[\mathcal{H}(x-y)\;,\;\mathcal{H}(y-x)\Bigg]\;\text{, also, we define}\;\min\;\text{as:}
  \\[1ex]
  \min{(x,y)}=&\;\,y-\mathcal{R}(y-x)\;\text{, with gradient}
  \\[1ex]
  \nabla\,y-\mathcal{R}(y-x)=\Bigg[\frac{\partial\,\mathcal{R}(\cdot)}{\partial\,x},\frac{\partial\,\mathcal{R}(\cdot)}{\partial\,y}\Bigg]=&\;\Bigg[\mathcal{H}(y-x)\;,\;\mathcal{H}(x-y)\Bigg]\;\text{.}
\end{aligned}
$$


## Overall model formulation

Now that we have sub-models that are able to transform a section of the query-signal for a given a section of the reference-signal, the logical next step is to define the entire (or overall) model in terms of all intervals. The following is in large parts identical to how we defined it in the notebook _"Boundary Time Warping (final, update)"_.

It is important to once more recall the mechanics of this suggested model:

* The reference signal is segmented into two or more intervals, each with a length $>0$. This segmentation is done using a given vector of _reference-boundaries_.
  * These intervals are constant, and never changed afterwards. For each variable in each reference-interval, we have a constant segment of the corresponding reference-signal (continuous case), or a constant vector of data.
* Each $q$-th reference-interval is associated with a $q$-th sub-model. Each such sub-model has access to the whole query-signal and its gradient, and the goal is to translate and scale the query-signal to __best-fit__ the current reference interval (this requires a loss).
  * The sub-models are designed in a way that the intervals they cover cannot overlap and connect seamlessly, among other criteria (see above).

The input to the overall model is, among the signals, a set of reference- and query-boundaries, which are transformed into vectors of interval lengths. Albeit defined above with the sub-models, the parameters $b,e,\gamma_b,\gamma_e,\gamma_d$ are parameters of the overall model. In practice, it may be more or less convenient to work with boundaries, and internally the overall model uses lengths/ratios, and an absolute offset for the first interval. Both of these concepts are convertible to each other. In the following, we start with a definition that uses boundaries, and then transform them to lengths and an offset.

$$
\begin{aligned}
  \mathsf{T}^{(l)}\Big(\bm{\tau}^{(b)}\Big)=&\;\Big\{\;l_1=\bm{\tau}^{(b)}_2-\bm{\tau}^{(b)}_1,\;\dots,\;l_{\norm{\bm{\tau}}-1}=\bm{\tau}^{(b)}_{\norm{\bm{\tau}}}-\bm{\tau}^{(b)}_{\norm{\bm{\tau}}-1}\;\Big\}\;\text{,}
  \\[0ex]
  &\;\text{(boundaries-to-lengths transform operator),}
  \\[1ex]
  \mathcal{X}^{(\text{WP})}=&\;\Big[\min{\bm{\theta}^{(b)}}\,,\,\max{\bm{\theta}^{(b)}}\Big]\;\text{, the support of the Warping Pattern's signal,}
  \\[1ex]
  \bm{\vartheta}^{(l)}=\mathsf{T}^{(l)}\Big(\bm{\vartheta}^{(b)}\Big)\;\dots&\;\text{the query-intervals as lengths and boundaries,}
  \\[1ex]
  \mathcal{X}^{(\text{WC})}=&\;\Big[\beta_l\,,\,\beta_u\Big]\;\text{, the support of the Warping Candidate's signal,}
  \\[1ex]
  q\in Q;\;Q=&\;\Big\{1,\;\dots\;,\norm{\bm{\theta}^{(b)}}-1\Big\}\;\text{, where}\;Q\;\text{is ordered low to high, i.e.,}\;q_i\prec q_{i+1}\text{,}
  \\[0ex]
  &\;\text{(boundaries converted to intervals),}
  \\[1ex]
  \mathcal{I}^{(\text{WP})},\mathcal{I}^{(\text{WC})}\;\dots&\;\text{the set of Warping Pattern- and Candidate-intervals with length}\;[\max{Q}]\text{,}
  \\[0ex]
  &\;\text{with each Warping Candidate-interval delimited by}
  \\[0ex]
  \mathbf{x}_q^{(\text{WC})}\subset\mathcal{X}^{(\text{WC})}=&\;\Big[\bm{\vartheta}^{(b)}_q\,,\,\bm{\vartheta}^{(b)}_{q+1}\Big)\;\text{, for the first}\;Q-1\;\text{intervals,}
  \\[0ex]
  =&\;\Big[\bm{\vartheta}^{(b)}_q\,,\,\bm{\vartheta}^{(b)}_{q+1}\Big]\;\text{, for the last interval; }
  \\[0ex]
  &\;\text{proper sub-supports for model}\;m_q\;\text{and its interval}\;\mathbf{x}_q^{(\text{WC})}\text{, such that}
  \\[0ex]
  &\;\mathbf{x}_q^{(\text{WC})}\prec\mathbf{x}_{q+1}^{(\text{WC})}\;\text{, i.e., sub-supports are ordered,}
  \\[1ex]
  \mathbf{y}_q^{(\text{WP})}=&\;r\Big(I_q^{(\text{WP})}\Big)\;\text{, the reference data for the}\;q\text{-th interval,}
  \\[1ex]
  \mathbf{y}=&\;\Big\{\;\mathbf{y}_1^{(\text{WP})}\,\frown\,\dots\,\frown\,\mathbf{y}_q^{(\text{WP})}\;\Big\},\;\forall\,q\in Q\;\text{.}
\end{aligned}
$$


Likewise, we can obtain $\hat{\mathbf{y}}_q^{(\text{WC})}$ and concatenate those by calling each sub-model. During optimization, The vector of query-interval-lengths, $\bm{\vartheta}^{(l)}$, is altered. If the model allows either or both, open begin or end, then these parameters are altered, too.


$$
\begin{aligned}
  \mathbf{\hat{y}}=\mathsf{M}\Big(r,f,\bm{\theta}^{(b)},\bm{\vartheta}^{(l)},&b,e,\gamma_b,\gamma_e,\gamma_d,\bm{\lambda}\Big)=
  \\[1ex]
  \Big\{\;&m^c_1(\cdot)\,\frown\,\dots\,\frown\,m^c_q\Big(f,\mathbf{x}^{(\text{WP})}_q,t_b^{q},l_q^{(c)},\delta_q^{(t)},s_b^{(q)}\Big)\;\Big\},\;\forall\,q\in Q
\end{aligned}
$$

Instead of returning a discrete vector for each sub-model, $\mathbf{\hat{y}}_q$, in practice we probably want to return a tuple with the sub-model function, the support of it, as well as the relative length of its interval, i.e., $\langle m_q^c,\text{supp}(q),l_q^{\text{rel}} \rangle$. The advantage is, that any loss function can decide how to sample from this function, or even do numerical integration.

## Gradient of the model

__ATTENTION__: I had previously fixed an error in the model formulation, so that this entire section needs to be reworked. While some parts are still correct, some are not!

We have previously shown the sub-model with built-in constraints, $m_q^c$. We have quite many $\min,\max$-expressions and dynamically-long sums in our model, and for a symbolic solution, we suggest to expand all terms to create a sum, where each term can be derived for separately. Before we get there, however, it makes sense to derive some of the recurring expressions, so that we can later just plug them in.

It is important to create a fully-expanded version of the model $m_q^c$, to better understand its gradient. Fully expanded, the model $m_q^c$ is given as:

$$
\begin{aligned}
  m_q^c(\dots)=f\Bigg(s_b^{(q)}\;+\;&\bigg(\delta_q
  \\[1ex]
  &\times\bigg(x-\overbrace{\min{\Big(\gamma_e-\gamma_d,\max{\big(\gamma_b, \min{(b,e)}\big)}\Big)}}^{\beta_l}-\overbrace{\Big(\max{(\bm{\lambda}_1,l_1)}+\;\dots\;+\max{(\bm{\lambda}_{q-1},l_{q-1})}\Big)}^{\phi_q\;\text{(sum of all preceding intervals' lengths;}\;0\;\text{if}\;q=1\text{)}}\bigg)
  \\[1ex]
  &\times\overbrace{\Bigg(\overbrace{\max{\Big(\gamma_b+\gamma_d,\min{\big(\gamma_e, \max{(b,e)}\big)}\Big)}}^{\beta_u}-\overbrace{\min{\Big(\gamma_e-\gamma_d,\max{\big(\gamma_b, \min{(b,e)}\big)}\Big)}}^{\beta_l}\Bigg)}^{\phi^{(e)}\;\text{(extent of}\;\phi\text{)}}
  \\[1ex]
  &\times\overbrace{\max{\Big(\gamma_d,\;\max{(\bm{\lambda}_1,l_1)}+\;\dots\;+\max{(\bm{\lambda}_{\max{(Q)}},l_{\max{(Q)}})}\Big)}}^{\phi^{(s)}\;\text{(scale of}\;\phi\text{; sum of all intervals' minimum lengths)}}
  \\[1ex]
  &\times\max{\big(\bm{\lambda}_q,l_q\big)}^{-1}\bigg)\Bigg)
\end{aligned}
$$

In the remainder of this section, we will step-wise produce a gradient of the open begin- and -end model $m_{q=4}^c$ (meaning that there is a gradient for $b,e$, too). The reason to use $q=4$ is simple: This guarantees that $\phi_q$ is not zero (and that we can effortlessly see what would happen if $q=1$), and the $\phi$ is not zero, either. Recall the overall model, and how it is composed of these sub-models. If we have four intervals, we get for sub-models. The difference in these sub-models lies in $\phi_q$, which is different for every $q$-th sub-model ($\phi$ is the same).

The gradient of the overall model has "categories", meaning that partial derivation for some of the parameters is very similar:

* Derivation for $b$ or $e$: This is category 1, and affects all terms that use $\beta_l,\beta_u$.
* Derivation for any $l_r,r<q$: Category 2, this mainly affects $\phi$ and $\phi_q$.
* Derivation for the current $l_q$: Category 3, affects the denominator and $\phi$.


### Expanding the denominator

The denominator for the product/fraction given to $f$ is $\max{(\bm{\lambda_q},l_q)^{-1}}$ (the last factor in the above expansion). It can only be differentiated for $l_q$, as all $\bm{\lambda}$ are constant:

$$
\begin{aligned}
  \max{(\bm{\lambda}_q,l_q)}^{-1}=&\;\Big(\mathcal{R}(l_q-\bm{\lambda}_q)+\bm{\lambda}_q\Big)^{-1}\;\text{, with derivative for}\;l_q
  \\[1ex]
  \frac{\partial\,\max{(\bm{\lambda}_q,l_q)}^{-1}}{\partial\,l_q}=&\;\frac{\mathcal{H}(l_q-\bm{\lambda}_q)}{\Big(\mathcal{R}(l_q-\bm{\lambda}_q)+\bm{\lambda}_q\Big)^2}
\end{aligned}
$$



### Expanding $\beta_l,\beta_u$

We also should expand $\beta_l,\beta_u$ and do the substitutions for our $\min,\max$ definitions, so that we can derive our model without having to check conditions later:

$$
\begin{aligned}
  \beta_l=&\;\overbrace{\min{\Big(\gamma_e-\gamma_d,\overbrace{\max{\big(\gamma_b, \overbrace{\min{(b,e)}}^{s_2=e-\mathcal{R}(e-b)}\big)}\Big)}^{s_1=\mathcal{R}(s_2-\gamma_b)+\gamma_b}}}^{\beta_l=s_1-\mathcal{R}(s_1-(\gamma_e-\gamma_d))}\;\text{, rearranged as}
  \\[1ex]
  =&\;\overbrace{\mathcal{R}(\overbrace{e-\mathcal{R}(e-b)}^{s_2}-\gamma_b)+\gamma_b}^{s_1}-\mathcal{R}(\overbrace{\mathcal{R}(\overbrace{e-\mathcal{R}(e-b)}^{s_2}-\gamma_b)+\gamma_b}^{s_1}-(\gamma_e-\gamma_d))\;\text{, with gradient}
  \\[1ex]
  \bigg[\frac{\partial\,\beta_l}{\partial\,b},\frac{\partial\,\beta_l}{\partial\,e}\bigg]=&\bigg[\;-\mathcal{H}(e-b)\times\mathcal{H}\big(-\mathcal{R}(e-b)-\gamma_b+e\big)\times\Big(\mathcal{H}\big(\mathcal{R}(-\mathcal{R}(e-b)-\gamma_b+e)+\gamma_b-\gamma_e-\gamma_d\big)-1\Big),
  \\[0ex]
  &\;\;\;\;\Big(\mathcal{H}\big(-\mathcal{R}(e-b)-\gamma_b+e\big)-1\Big)\times\Big(e\times\mathcal{H}\Big(\mathcal{R}\big(-\mathcal{R}(e-b)-\gamma_b+e\big)+\gamma_b-\gamma_e+\gamma_d\Big)-1\Big)\;\bigg]\;\text{.}
\end{aligned}
$$

Since we get a lot of Heaviside step functions in these products, evaluation can be stopped early if the result of any is $0$. Here we do the same for $\beta_u$:

$$
\begin{aligned}
  \beta_u=&\;\overbrace{\max{\Big(\gamma_b+\gamma_d,\overbrace{\min{\big(\gamma_e,\overbrace{\max{(b,e)}\big)}^{s_2=\mathcal{R}(e-b)+b}}\Big)}^{s_1=s_2-\mathcal{R}(s_2-\gamma_e)}}}^{\beta_u=\mathcal{R}(s_1-(\gamma_b+\gamma_d))+(\gamma_b+\gamma_d)}\;\text{, rearranged as}
  \\[1ex]
  =&\;\mathcal{R}(\overbrace{\overbrace{\mathcal{R}(e-b)+b}^{s_2}-\mathcal{R}(\overbrace{\mathcal{R}(e-b)+b}^{s_2}-\gamma_e)}^{s_1}-(\gamma_b+\gamma_d))+(\gamma_b+\gamma_d)\;\text{, with gradient}
  \\[1ex]
  \bigg[\frac{\partial\,\beta_u}{\partial\,b},\frac{\partial\,\beta_u}{\partial\,e}\bigg]=&\bigg[\;\big(\mathcal{H}(e-b)-1\big)\times\Big(\mathcal{H}\big(\mathcal{R}(e-b)+b-\gamma_e\big)-1\Big)
  \\[0ex]
  &\;\;\;\;\;\;\;\;\times\mathcal{H}\Big(-\mathcal{R}\big(\mathcal{R}(e-b)+b-\gamma_e\big)+\mathcal{R}(e-b)+b-\gamma_b-\gamma_d\Big),
  \\[0ex]
  &\;\;\;\;e-\mathcal{H}(-\mathcal{R}(\mathcal{R}(e-b)+b-\gamma_e)+\mathcal{R}(e-b)+b-\gamma_b-\gamma_d)\;\bigg]\;\text{.}
\end{aligned}
$$


### Expanding $\phi$

Recall the definition of $\phi=(\beta_u-\beta_l)\times\max{\Bigg(\gamma_d,\Bigg[\sum_{j=1}^{\max{(Q)}}\,\max{(\bm{\lambda}_j,l_j)}\Bigg]\Bigg)}$.

As for the final expansion, we want to expand the denominator and $\phi$ together, i.e., $\phi\times\max{(\bm{\lambda_q},l_q)^{-1}}$. All these expansions will lead to very large expressions, and we are starting here to add substitution letters. The expansion with parts $j,k$ is:

$$
\begin{aligned}
  f\Big(s_b^{(q)}+\Big[\;\dots\;\times&\overbrace{\phi\times\max{(\bm{\lambda_q},l_q)}^{-1}}^{\text{(will become}\;[j-k]\text{ after expansion)}}\;\Big]\Big)
  \\[1ex]
  f\Big(s_b^{(q)}+\Big[\;\dots\;\times&\Big(\beta_u\times\max{(\bm{\lambda_q},l_q)}^{-1}-\beta_l\times\max{(\bm{\lambda_q},l_q)}^{-1}\Big)\times\max{(\gamma_d,\max{(\bm{\lambda}_1,l_1)}+\dots+\max{(\bm{\lambda}_4,l_4)})}
  \\[1ex]
  f\Big(s_b^{(q)}+\Big[\;\dots\;\times&\Bigg(\overbrace{\bigg(\beta_u\times\max{(\bm{\lambda_q},l_q)}^{-1}\times\overbrace{\max{(\gamma_d,\max{(\bm{\lambda}_1,l_1)}+\dots+\max{(\bm{\lambda}_4,l_4)})}\bigg)}^{\phi^{(s)}}}^{j}
  \\[0ex]
  &\;\;\;\;-\overbrace{\bigg(\beta_l\times\max{(\bm{\lambda_q},l_q)}^{-1}\times\overbrace{\max{(\gamma_d,\max{(\bm{\lambda}_1,l_1)}+\dots+\max{(\bm{\lambda}_4,l_4)})}\bigg)}^{\phi^{(s)}}}^{k}\Bigg)
\end{aligned}
$$

Previously, we split $\phi$ into $\phi^{(e)}$ and $\phi^{(s)}$, which represent the portions _extent_ and _scale_ of it. The extent only consists of $(\beta_u-\beta_l)$, and we have already derived them. The scale expression is a nested $\max$ with a summation inside, that affects $\forall\,l$, regardless of what $q$ equals.

$$
\begin{aligned}
  \phi^{(s)}=&\;\max{\Bigg(\gamma_d,\Bigg[\sum_{i=1}^{\max{(Q)}}\,\max{(\bm{\lambda}_i,l_i)}\Bigg]\Bigg)}
  \\[1ex]
  =&\;\max{\Big(\gamma_d,\max{(\bm{\lambda}_1,l_1)}+\dots+\max{(\bm{\lambda}_4,l_4)}\Big)}\;\text{, (for}\;q=4\text{),}
  \\[1ex]
  =&\;\overbrace{\mathcal{R}\Big(\Big[\;\overbrace{\mathcal{R}(l_1-\bm{\lambda}_1)+\bm{\lambda}_1}^{\text{(first }\max\text{ of summation)}}+\;\dots\;+\overbrace{\mathcal{R}(l_4-\bm{\lambda}_4)+\bm{\lambda}_4}^{\text{(}n\text{-th }\max\text{ of summation)}}\;\Big]-\gamma_d\Big)+\gamma_d}^{\text{(outer }\max\text{; recall that }\max{(x,y)=\mathcal{R}(y-x)+x}\text{)}}\;\text{.}
\end{aligned}
$$

In the above expansion, $\gamma_d$ is a constant defined at initialization time, and the same is true for $\bm{\lambda}$. However, $\phi^{(s)}$ will appear in each part of the model as we see later, and its gradient is sensitive $\forall\,l$, i.e., not just for, e.g., $l_q$ or $\forall\,l_r,r<q$. However, since it is the summation that will be affected, and since it does the same $\forall\,l$, the gradient, conveniently, will be the same for each.

$$
\begin{aligned}
  \nabla\,\phi^{(s)}=&\;\frac{\partial\,\phi^{(s)}}{\partial\,l_q,\forall\,q\in Q}\;\text{,}
  \\[1ex]
  =&\;\mathcal{H}\Big(l_q-\bm{\lambda}_q\Big)\times \mathcal{H}\Big(\Big[\;\overbrace{\mathcal{R}(l_1-\bm{\lambda}_1)+\bm{\lambda}_1+\;\dots\;+R(l_4-\bm{\lambda}_4)+\bm{\lambda}_4}^{\sum_{i=1}^{\max{(Q)}}\,\max{(\bm{\lambda}_i,l_i)}}\;\Big]-\gamma_d\Big)\;\text{.}
\end{aligned}
$$


### Expanding $m_{q=4}^c$

The goal is to fully expand the current sub-model into a big sum, where we can effortlessly derive each term separately. We also have now all of the sub-expressions and their partial gradient. Here we continue to assign substitution letters so that we can derive the model more easily.

$$
\begin{aligned}
  m_{q=4}^c=&\;f\bigg(s_b^{(q)}+\frac{(x-\beta_l-\phi_q)\times\delta_q\times\phi}{\max{(\bm{\lambda_q},l_q)}}\bigg)
  \\[1ex]
  =&\;f\bigg(s_b^{(q)}+\frac{(\delta_qx-\delta_q\beta_l-\delta_q\phi_q)\times\phi}{\max{(\bm{\lambda_q},l_q)}}\bigg)
  \\[1ex]
  =&\;f\Big(s_b^{(q)}+\Big[\;\overbrace{\delta_qx-\delta_q\beta_l-\overbrace{\delta_q\max{(\bm{\lambda_1},l_1)}-\delta_q\max{(\bm{\lambda_2},l_2)}-\delta_q\max{(\bm{\lambda_3},l_3)}}^{\phi_{q=4}}}^{(a-b-c-d-e)}\;\times\overbrace{\phi\times\max{(\bm{\lambda_q},l_q)}^{-1}}^{(j-k)}\;\Big]\Big)
  \\
  \vdots
  \\
  =&\;f\Big(s_b^{(q)}+aj-ak-bj+bk-cj+ck-dj+dk-ej+ek\Big)\;\text{.}
\end{aligned}
$$

The final expression for our model is now much more convenient to work with. It is worth noting that summands with $c$ exist if $q>1$, those with $d$ exist if $q>2$ and so on. Those are essentially what distinguishes each $q$-th sub-model from each other sub-model. For example, sub-model $m_{q=1}^c$ has none of these summands, whereas sub-model $m_{q=2}^c$ has those with $c$, and sub-model $m_{q=3}^c$ additionally has those with $d$ and so on. That is why we chose an example with $q$ up to $4$, to better illustrate the differences.

$$
\begin{aligned}
  f\Bigg(&s_b^{(q)}
  \\[0ex]
  &+aj=\Big[\;\delta_q\times x\times \overbrace{\beta_u\times\max{(\bm{\lambda}_q,l_q)}^{-1}\times\max{\Big(\gamma_d,\max{(\bm{\lambda}_1,l_1)}+\dots+\max{(\bm{\lambda}_4,l_4)}\Big)}}^{j}\;\Big]
  \\[1ex]
  &-ak=\Big[\;\delta_q\times x\times\overbrace{\beta_l\times\max{(\bm{\lambda}_q,l_q)}^{-1}\times\max{\Big(\gamma_d,\;\dots\;\Big)}}^{k}\;\Big]
  \\[1ex]
  &-bj=\Big[\;\delta_q\times\beta_l\times \beta_u\times\max{(\dots)}^{-1}\times\max{(\dots)}\;\Big]
  \\[1ex]
  &+bk=\Big[\;\delta_q\times{\beta_l}^2\times{\dots}\times{\dots}\;\Big]
  \\[1ex]
  &-cj=\Big[\;\delta_q\times\max{(\bm{\lambda}_1,l_1)}\times\overbrace{\beta_u\times{\dots}\times{\dots}}^{j}\;\Big]
  \\[1ex]
  &+ck=\Big[\;\delta_q\times\max{(\bm{\lambda}_1,l_1)}\times\overbrace{\beta_l\times{\dots}\times{\dots}}^{k}\;\Big]
  \\[1em]
  &\overbrace{-dj=[\dots]+dk=[\dots]}^{\text{(if }q>1\text{)}}
  \\[1ex]
  &\overbrace{-ej=[\dots]+ek=[\dots]}^{\text{(if }q>2\text{)}}
  \\
  &\;\;\;\;\vdots
  \\
  &\;\text{(}\pm\;\text{additional terms for larger}\;q\text{)}\Bigg)\text{.}
  \\[1ex]
\end{aligned}
$$

The derivation of this model is now straightforward, as we can tackle each summand separately. Also, we have already created the partial derivatives of all sub-expressions earlier. Before we assemble the actual analytical gradient, we will test our model.


## Testing the model

The main features of the proposed model are, that it is __self-regularizing__ and optionally allows for half- or full-open window time warping. In code, we will call the model just `SRBTW`, and have the optional openness as setters and getters.

Before we go any further, we should test the model. When this notebook was started, it was called _Closed-window Optimization_. However, we made some additions to the proposed model that allow partially- or fully-open window optimization. In this section, we will do some testing with the pattern and data we have, focusing on the __adaptive__ (__A__) variable. We will be using the four intervals as defined earlier


```{r echo=FALSE, message=FALSE}
source(file = "../models/SRBTW-R6.R")

# vartheta_l <- c(.25,.275,.25,.225)
# vartheta_l <- 4:1
# These are from a JSD-only based fit and quite good!
vartheta_l <- c(0.4119631, 0.1651927, 0.0737804, 0.3831419)
# .. slightly worse:
# vartheta_l <- c(0.35320284, 0.21300932, 0.07073679, 0.37062611)


srbtw <- SRBTW$new(
  wp = r,
  wc = f,
  theta_b = c(0, bounds, 1),
  gamma_bed = c(0, 1, 0),
  lambda = rep(0, length(vartheta_l)),
  begin = 0,
  end = 1,
  openBegin = FALSE,
  openEnd = FALSE
)

srbtw$setParams(vartheta_l = vartheta_l)

smData <- matrix(nrow = 0, ncol = 14)
for (q in seq_len(length.out = length(vartheta_l))) {
  sm <- srbtw$getSubModel(q)
  t <- sm$asTuple()
  smData <- rbind(smData, c(
    t$q, t$beta_l, t$beta_u, t$lambda_q, t$l_q, t$l_prime_q,
    t$l_q_c, t$psi, t$phi_q, t$delta_t_q, t$sb_q, t$se_q, t$tb_q, t$te_q
  ))
}
colnames(smData) <- names(t)[1:14]
smData

tempf <- Vectorize(function(x) {
  stopifnot(length(x) == 1 && !is.na(x))
  
  q <- srbtw$getQForX(x)
  sm <- srbtw$getSubModel(q = q)
  t <- sm$asTuple()
  t$mqc(x)
})

# curve(tempf, 0, 1)

ggplot() +
  stat_function(fun = tempf, xlim = c(0,1), mapping = aes(color="red")) +
  stat_function(fun = r, xlim = c(0,1))
```

```{r echo=FALSE}
objF <- function(x, isGrad = FALSE) {
  # 'x' is our vartheta_l [, +b[, +e]]
  b <- if (srbtw$isOpenBegin()) tail(x, 2)[1] else NULL
  e <- if (srbtw$isOpenEnd()) tail(x, 1) else NULL
  
  srbtw$setParams(vartheta_l = x[1:max(srbtw$getQ())])
  if (!is.null(b)) srbtw$setParams(begin = b)
  if (!is.null(e)) srbtw$setParams(end = e)
  
  tempf <- Vectorize(function(x_) {
    stopifnot(length(x_) == 1 && !is.na(x_))
    
    q <- srbtw$getQForX(x_)
    sm <- srbtw$getSubModel(q = q)
    t <- sm$asTuple()
    t$mqc(x_)
  })
  
  loss <- 0
  # X <- seq(srbtw$getBeta_l(), srbtw$getBeta_u(), length.out = 1e3)
  # y <- sapply(X, r)
  # y_hat <- sapply(X, tempf)
  
  # RSS:
  # loss <- log(1 + sum((y - y_hat)^2))
  
  # KL:
  # y <- y - min(y) + .1
  # y <- y / max(y)
  # y_hat <- y_hat - min(y_hat) + .1
  # y_hat <- y_hat / max(y_hat)
  # loss <- sum(na.omit(y * log(y / y_hat)))
  
  # Correlation:
  # eps <- .Machine$double.eps
  # loss <- loss - log(eps + max(0, stat_diff_2_functions_cor_score()(r, tempf) - eps))
  
  # # Area:
  # loss <- loss - log(area_diff_2_functions_score()(r, tempf))
  
  # JSD:
  loss <- loss - log(stat_diff_2_functions_symmetric_JSD_score()(r, tempf))
  
  
  # R: Open begin/end:
  if (srbtw$isOpenBegin() || srbtw$isOpenEnd()) {
    loss <- loss - .5 * log(max(.Machine$double.eps, srbtw$getBeta_u() - srbtw$getBeta_l()))
  }
  
  if (!isGrad) {
    print(loss)
  }
  if (is.na(loss) || is.infinite(loss)) {
    stop(paste(x, collapse = ", "))
  }
  
  loss
}
```

Let's try some optimization using box-bounds and closed open and begin:

```{r}
library(optimParallel)

cl <- parallel::makePSOCKcluster(12)
parallel::clusterExport(cl, varlist = c(
  "srbtw", "r", "f", "objF", "stat_diff_2_functions", "stat_diff_2_functions_cor",
  "stat_diff_2_functions_cor_score", "area_diff_2_functions", "area_diff_2_functions_score",
  "stat_diff_2_functions_symmetric_JSD_score", "stat_diff_2_functions_symmetric_JSD_sampled",
  "stat_diff_2_functions_philentropy_sampled"))

cow_og1_test1p <- loadResultsOrCompute(file = "../results/cow_og1_test1p.rds", computeExpr = {
  doWithParallelClusterExplicit(cl = parallel::makePSOCKcluster(12), expr = {
    set.seed(1337)
    optRp <- optimParallel::optimParallel(
      par = rep(1/length(vartheta_l), length(vartheta_l)),
      fn = objF,
      lower = rep(0, length(vartheta_l)),
      upper = rep(1, length(vartheta_l)),
      parallel = list(
        cl = cl,
        forward = FALSE
      )
    )
  })
})

cow_og1_test1p
```



# Optimization goal II


# Optimization goal III

As for now, I want to note down two approaches that come to my mind:

* Use the AP as reference with the 4 intervals that we have defined and warp all data to it. If we know how to warp the data to the AP, we can inverse the process. As for warping Y-values, we would need to wrap our proposed model from goal I to allow Y-translation (offset+scale approach, where each sub-model learns a scale and the first sub-model additionally an offset). The advantage is that we reuse the model, as manipulation happens outside of it. In each interval and for each variable, we need to learn a translate for all data series capturing that variable, so that this translation can then also be inverted and applied to the AP (if we have, e.g., 10 projects, then we have to learn one common translation for all that best fits the single reference). This approach also allows us to give low or no weight to, e.g., the Long Stretch phase.
* Use the AP as query instead. This means that the project data is constant, and that we subdivide it into some intervals by some strategy or just equidistantly-spaced. Using, e.g., AIC/BIC, we find the right amount of boundaries. We would make the same change to our model and wrap it such that sub-models can learn a Y-translation. This approach cannot consider the original intervals of the AP as we defined them, as the optimization chooses new intervals (also, it would most likely end up using a different number of intervals). This approach could result in a vastly different pattern that does not preserve too many shapes from the original pattern, but it may still worth testing.

A third approach: Start with an empty Warping Pattern: A straight horizontal line located at $0.5$ for each variable and variably many equidistantly-spaced intervals (use information criterion to find best number). This is the approach where we throw away our definition of the Fire Drill AP and create one that is solely based on data.






















